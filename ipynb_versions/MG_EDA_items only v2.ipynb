{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MG_EDA_items only v2.ipynb","provenance":[{"file_id":"14t_SkT4SYL-JAcrbJIJ60cbNgCrJlvIN","timestamp":1589069755220},{"file_id":"1mFtJLElc2hyopq6yrPAoi0zQVUegsAP5","timestamp":1589018631041},{"file_id":"1y04qp_hoyBnsJQwkX67pk4iZsKGQIqNy","timestamp":1588805435380},{"file_id":"1b_K0QD9U6dofQ7VtTAtzUrqbKJMdj64l","timestamp":1588785261238},{"file_id":"1gcbeu-d1GUUzznZwTzfqYYbaD6cJ7EQ4","timestamp":1588238522691},{"file_id":"1pSGNRDJGzdeI69bw1zWefzPifBq-rv9H","timestamp":1588151557805},{"file_id":"1hq-ivO1BBtc5IC5xd-JdH8HNAQ81bkRf","timestamp":1587386702728},{"file_id":"1I7DWo2B7q7g9Ne2khD11YGTq_gD2FoaT","timestamp":1587321559573},{"file_id":"1fyZv-jgb8twsCBQwPxjgk_XSYA6dOa2t","timestamp":1587303588700},{"file_id":"1iKsplqpLQQZqdr3Trflk7TapksgkQXjX","timestamp":1587145642564},{"file_id":"https://github.com/migai/Kag/blob/master/Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587076517706}],"collapsed_sections":["s0J5l5H98Xsh","lZfgOx-0_KXg","V7QY11R1QmlN"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn","colab_type":"text"},"source":["#**Investigation of *items* database and correlations between items**\n","\n","**EDA, NLP, Feature Generation**\n","\n","Andreas Theodoulou and Michael Gaidis (May, 2020)"]},{"cell_type":"markdown","metadata":{"id":"L6ErXnphuc2t","colab_type":"text"},"source":["##**summary so far (May 9, 2020):**\n","\n","I refined the \"delimiter\" characters, and did a bit more cleaning on some stuff I noticed with \"blu-ray\" vs. \"bluray\" vs. \"bd\"...\n","--> generated a new set of unique n-grams for n=1 to highest n, and filtered to include only where there are at least 2 item names containing that n-gram\n","\n","--> created \"word vector\" representations of the item names in items dataset, including roughly 4000 elements (all of the delimited n-grams mentioned above)\n","\n","--> for each of the 21700 items, they were encoded as word vectors, and then I used dot-product to identify which items were similar to others.  In the encoding of the word vectors, I did some \"weighting\" such that I didn't just have a word vector that was all 0's except for 1's in locations representing the particular n-grams that are found in the item name string (English translation).  I used 2 types of weighting: \n","1. like TF-IDF, I counted the number of occurrences of each of the roughly 4000 n-grams in the set of all item names, and I binned the n-grams to more heavily weight n-grams that have less representation in the item names. (For example, \"klompferstietnitz\" is more valuable than \"dvd\", because so many item names contain \"dvd\".  So, the former 1-gram gets a larger integer inserted into its location in the word vector.)  I used binning rather than strict TF-IDF \"continuous\" weighting because I believe there is a cutoff at which a term no longer holds much weight at all.\n","2. longer n-grams get heavier weight as well... if two item names have the same 10-word string (10-gram), it is much more relevant than if they have the same 1-word string (1-gram).\n","\n","--> after running the dot products between items in the 21700 x 4000 size matrix containing word vectors for each item, I end up with a 21700 x 21700 matrix containing integers = dot products of the word vectors i,j for cell at i,j coming from item i and item j.\n","\n","--> then, I found a nifty jit-accelerated function (reference below) to pick out the top K largest dot-product values for a given item.  The function gives me the top K items and their dot-product values with the item of interest.  I run this function on all 21700 items, and pick (at first the top 3, but now...) the top 10 highest dot-product values for a given item.\n","\n","**To date** I have only then taken the 5100 items that we know are in the test set (I was afraid of overwhelming the computing power or memory allocation in Colab).  \n","--> so, now I have a dataset with 5100 rows (each test item), and columns for the top-10 matching item ids as per the dot product, and for the actual top-10 dot products also.\n","\n","--> I \"explode\" (unravel) the list of 10 matching items and dot product values so now I have 51000 rows and columns indicating item id of interest (one of the 5100 in the test set), the top-10 matching item ids, and a column for the dot product values between the two.\n","\n","**Now we need to create features from this**\n","\n","**First, look for clusters of tightly-matched item-item pairs**\n","\n","I decided to use the networkX package to map these item pairs into an undirected graph with nodes = item ids, and edges weighted by the value of the dot product.  Then, I can utilize some of the pre-made algorithms that can automatically identify strongly-clustered groups.\n","\n","Before feeding the 51000 row matrix into the graph, I applied a threshold so only dot products above a certain value would be allowed as nodes/edges in the graph.  (This is to filter out \"matches\" where both items have some common term like \"dvd\" but nothing else.  But, as some of the item names are short an nondescriptive, even this \"dvd\" match can place the item-item pair in the top 10.  Of course, you could have 1000 items like this that match the subject item with the same dot-product value, but the aforementioned algorithm just picks the first 10.)\n","\n","Ok, so it goes into the graph, and I apply a \"community\" algorithm that takes into account the weighted edge values (preferentially grouping together items that have higher dot-product values).\n","I didn't find a way to get much control over how many clusters are identified by the algorithm.  It seems to go up roughly linearly in the number of nodes(items) in the graph.  Anyhow, I get about 1400 clusters for my 5100 input items.  These clusters contain item_ids both in and not in the test set, as the graph was made with 51000 edges (minus about 10000 from thresholding) that used top-10 matches with the 5100 test items.  These top-10 matches may or may not be in the test set.\n","\n","These 1400 clusters have anywhere from 2 items (one edge) to perhaps 100 items.  I computed an average dot product value between all elements in a cluster, and used that to estimate the overall \"strength\" of clustering.  This provides a natural way to do category encoding.  I simply use the integer average of cluster dot-products as the \"cluster category code\".  (One minor complication is that some clusters have identical averages... I gave a small boost to the clusters with greater number of elements, so n=2, avg=300 might get a category value of 300, whereas n=5 avg=300 might get category value = 320)\n","\n","I assign all items in a given cluster the same \"cluster category code\".\n","\n","Any items that do not belong to a cluster (either because they didn't make the top-10 list for any of the test item matches, or because the thresholding eliminated them from inclusion in the graph) were assigned a \"cluster category code\" equal to their original item_category_code.  The cluster category code is a minimum of 2x or 3x larger than the largest original item category code (83), and the cluster category code can be quite a bit larger ... 100x or more, for the strongest-matching clusters.\n","\n","</br>\n","\n","**This was all done with the v1.1 items EDA ipynb on GitHub, and the dataset containing the \"cluster category codes\" is saved as csv.gz in the data_output directory.  You can just load in that dataset and use the cluster category code column (alongside the item_id column) as a feature in the model.  It shouldn't need further category encoding.**\n","\n","I'm now working on v2.0 of this items EDA (this file), to remove unnecessary code stragglers, and I hope to try a graph/clustering with all 21700 items rather than just the 5100 test items.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ruw_WyRxhqpx"},"source":["#0. Configure Environment\n","**NOT OPTIONAL**"]},{"cell_type":"code","metadata":{"id":"sTVAxnMnenrB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"d06417d4-887b-483b-b955-08c54a136993","executionInfo":{"status":"ok","timestamp":1589075331019,"user_tz":240,"elapsed":4243,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# General python libraries/modules used throughout the notebook\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import MultipleLocator, FormatStrFormatter, AutoMinorLocator\n","import numpy as np\n","from scipy import sparse\n","import seaborn as sns\n","from numba import jit, prange\n","import networkx as nx\n","from networkx.algorithms import community, cluster\n","\n","import os\n","from itertools import product\n","import re\n","import json\n","import time\n","from time import sleep, localtime, strftime\n","import pickle\n","\n","\n","# Magics\n","%matplotlib inline\n","\n","\n","# NLP packages\n","import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer \n","lemmatizer = WordNetLemmatizer() \n","\n","# # ML packages\n","# from sklearn.linear_model import LinearRegression\n","\n","# !pip install catboost\n","# from catboost import CatBoostRegressor \n","\n","# %tensorflow_version 2.x\n","# import tensorflow as tf\n","# import keras as K\n","\n","# # List of the modules we need to version-track for reference\n","modules = ['pandas','matplotlib','numpy','scipy','numba','seaborn','sklearn','tensorflow','keras','catboost','pip','nltk','networkx']"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NoOf7oi8Oe-Q","colab_type":"code","colab":{}},"source":["# Notebook formatting\n","# Adjust as per your preferences.  I'm using a FHD monitor with a full-screen browser window containing my IPynb notebook\n","\n","# format pandas output so we can see all the columns we care about (instead of \"col1  col2  ........ col8 col9\", we will see \"col1 col2 col3 col4 col5 col6 col7 col8 col9\" if it fits inside display.width parameter)\n","pd.set_option(\"display.max_columns\",30)  \n","pd.set_option(\"display.max_rows\",100)     # Override pandas choice of how many rows to show, so, for example, we can see the full 84-row item_category dataframe instead of the first few rows, then ...., then the last few rows\n","pd.set_option(\"display.width\", 300)       # Similar to the above for showing more rows than pandas defaults to, we can show more columns than default, if we tune this to our monitor window size\n","pd.set_option(\"max_colwidth\", None)\n","\n","#pd.set_option(\"display.precision\", 3)  # Nah, this is helpful, but below is even better\n","#Try to convince pandas to print without decimal places if a number is actually an integer (helps keep column width down, and highlights data types)\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"miCS3XqUhDXz"},"source":["#1. Load Data Files\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kjWzoXizEa5O","colab_type":"text"},"source":["##1.1) Enter Data File Names and Paths\n","\n","**NOT Optional**"]},{"cell_type":"code","metadata":{"id":"p9vsd3EynZLO","colab_type":"code","colab":{}},"source":["#  FYI, data is coming from a public repo on GitHub at github.com/migai/Kag\n","# List of the data files (path relative to GitHub master), to be loaded into pandas DataFrames\n","data_files = [  \"readonly/final_project_data/items.csv\",\n","                \"readonly/final_project_data/item_categories.csv\",\n","                #\"readonly/final_project_data/shops.csv\",\n","                #\"readonly/final_project_data/sample_submission.csv.gz\",\n","                \"readonly/final_project_data/sales_train.csv.gz\",\n","                \"readonly/final_project_data/test.csv.gz\",\n","                #\"data_output/shops_transl.csv\",\n","                #\"data_output/shops_augmented.csv\",\n","                \"data_output/item_categories_transl.csv\",\n","                \"data_output/item_categories_augmented.csv\",\n","                \"data_output/items_transl.csv\",\n","                #\"data_output/items_clustered.csv.gz\",\n","                #\"readonly/en_50k.csv\"\n","              ]\n","\n","\n","# Dict of helper code files, to be loaded and imported {filepath : import_as}\n","code_files = {}  # not used at this time; example dict = {\"helper_code/kaggle_utils_at_mg.py\" : \"kag_utils\"}\n","\n","\n","# GitHub file location info\n","git_hub_url = \"https://raw.githubusercontent.com/migai\"\n","repo_name = 'Kag'\n","branch_name = 'master'\n","base_url = os.path.join(git_hub_url, repo_name, branch_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPrPvh7sorJd","colab_type":"text"},"source":["##1.2) Load Data Files"]},{"cell_type":"code","metadata":{"id":"CUIE1PVjSAmg","colab_type":"code","outputId":"d92b4d8d-c779-483a-c419-befad2a92ce3","executionInfo":{"status":"ok","timestamp":1589075356180,"user_tz":240,"elapsed":29388,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, then paste it into the input box and hit 'enter' to complete mounting of the drive\n","from google.colab import drive  \n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dy5i7jl00oX-","colab_type":"code","outputId":"82c61f47-f721-40cd-a575-2ac1bf62e200","executionInfo":{"status":"ok","timestamp":1589075364446,"user_tz":240,"elapsed":37650,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":782}},"source":["'''\n","############################################################\n","############################################################\n","'''\n","# Replace this path with the path on *your* Google Drive where the repo master branch is stored\n","#   (on GitHub, the remote repo is located at github.com/migai/Kag --> below is my cloned repo location)\n","GDRIVE_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\"\n","'''\n","############################################################\n","############################################################\n","'''\n","\n","%cd \"{GDRIVE_REPO_PATH}\"\n","\n","print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","\n","# Loop to load the data files into appropriately-named pandas DataFrames\n","for path_name in data_files:\n","    filename = path_name.rsplit(\"/\")[-1]\n","    data_frame_name = filename.split(\".\")[0]\n","    exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","    if data_frame_name == 'sales_train':\n","        sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","    print(\"Data Frame: \" + data_frame_name)\n","    print(eval(data_frame_name).head(2))\n","    print(\"\\n\")\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","Loading Files from Google Drive repo into Colab...\n","\n","Data Frame: items\n","                                                              item_name  item_id  item_category_id\n","0                             ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0                40\n","1  !ABBYY FineReader 12 Professional Edition Full [PC, Цифровая версия]        1                76\n","\n","\n","Data Frame: item_categories\n","        item_category_name  item_category_id\n","0  PC - Гарнитуры/Наушники                 0\n","1         Аксессуары - PS2                 1\n","\n","\n","Data Frame: sales_train\n","        date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0 2013-01-02               0       59    22154         999             1\n","1 2013-01-03               0       25     2552         899             1\n","\n","\n","Data Frame: test\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","\n","\n","Data Frame: item_categories_transl\n","        item_category_name  item_category_id                 en_cat_name\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones\n","1         Аксессуары - PS2                 1           Accessories - PS2\n","\n","\n","Data Frame: item_categories_augmented\n","        item_category_name  item_category_id                 en_cat_name item_category1 item_category2 item_category3 item_category4  item_cat_tested\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones          Audio             PC    Accessories             PC             True\n","1         Аксессуары - PS2                 1           Accessories - PS2    Accessories    PlayStation    Accessories    PlayStation            False\n","\n","\n","Data Frame: items_transl\n","                                                              item_name  item_id  item_category_id                                                           en_item_name\n","0                             ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0                40                                           ! POWER IN glamor (PLAST.) D\n","1  !ABBYY FineReader 12 Professional Edition Full [PC, Цифровая версия]        1                76  ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P99yjzAasJva"},"source":["#2. Explore Data (EDA), Clean Data, and Generate Features"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xr0FDeno_EUQ"},"source":["#2.5) ***items*** Dataset: EDA, Cleaning, Correlations, and Feature Generation\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s0J5l5H98Xsh"},"source":["###Thoughts regarding items dataframe\n","Let's first look at how many training examples we have to work with..."]},{"cell_type":"markdown","metadata":{"id":"1lg7NbEchkuM","colab_type":"text"},"source":["Many of the items have similar names, but slightly different punctuation, or only very slightly different version numbers or types.  (e.g., 'Call of Duty III' vs. 'Call of Duty III DVD')\n","\n","One can expect that these two items would have similar sales in general, and by grouping them into a single feature category, we can eliminate some of the overfitting that might come as a result of the relatively small ratio of (training set shop-item-date combinations = 2935849)/(total number of unique items = 22170).  (This is an average of about 132 rows in the sales_train data for each shop-item-date combination that we are using to train our model.  Our task is to produce a monthly estimate of sales (for November 2015), so it is relevant to consider training our model based on how many sales in a month vs. how many sales in the entire training set.  Given that the sales_train dataset covers the time period from January 2013 to October 2015 (34 months), we have on average fewer than 4 shop-item combinations in our training set for a given item in any given month.  Furthermore, as we are trying to predict for a particular month (*November* 2015), it is relevant to consider how many rows in our training set occur in the month of November.  The sales_train dataset contains data for two 'November' months out of the total 34 months of data.  Another simple calculation gives us an estimate that our training set contains on average 0.23 shop-item combinations per item for November months.\n","\n","To summarize:\n","\n","*  *sales_train* contains 34 months of data, including 2935849 shop-item-date combinations\n","*  *items* contains 22170 \"unique\" item_id values\n","\n","In the *sales_train* data, we therefore have:\n","*  on average, 132 rows with a given shop-item pair for a given item_id\n","*  on average, 4 rows with a given shop-item pair for a given item_id in a given month\n","*  on average, 0.23 rows with a given shop-item pair for a given item_id in all months named 'November'\n","\n","If we wish to improve our model predictions for the following month of November, it behooves us to use monthly grouping of sales, or, even better, November grouping of sales.  This smooths out day-to-day variations in sales for a better monthly prediction.  However, the sparse number of available rows in the *sales_train* data will contribute to inaccuracy in our model training and predictions.\n","\n","Imagine if we could reduce the number of item_id values from 22170 to perhaps half that or even less.  Given that the number of rows for training (per item, on a monthly or a November basis) is so small, then such a reduction in the number of item_id values would have a big impact.  (The same is true for creating features to supplement \"shop_id\" so as to group and reduce the individuality of each shop - and thus effectively create, on average, more rows of training data for each shop-item pair."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZfgOx-0_KXg"},"source":["###2.5.1) **Translate and Ruminate**\n","We will start by translating the Russian text in the dataframe, and add our ruminations on possible new features we can generate.\n","\n","The dataframe *items_transl* (equivalent to *items* plus a column for English translation) is saved as a .csv file so we do not have to repeat the translation process the next time we open a Google Colab runtime."]},{"cell_type":"code","metadata":{"id":"FhHSfXNxsKxQ","colab_type":"code","outputId":"89b05884-b180-4704-e8d2-18ed1bc480a5","executionInfo":{"status":"ok","timestamp":1589075364446,"user_tz":240,"elapsed":37642,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["print(items_transl.info())\n","print(\"\\n\")\n","print(items_transl.tail(10))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 22170 entries, 0 to 22169\n","Data columns (total 4 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   item_name         22170 non-null  object\n"," 1   item_id           22170 non-null  int64 \n"," 2   item_category_id  22170 non-null  int64 \n"," 3   en_item_name      22170 non-null  object\n","dtypes: int64(2), object(2)\n","memory usage: 692.9+ KB\n","None\n","\n","\n","                                                   item_name  item_id  item_category_id                                           en_item_name\n","22160                             ЯРМАРКА ТЩЕСЛАВИЯ (Регион)    22160                40                                   Vanity Fair (Region)\n","22161                       ЯРОСЛАВ. ТЫСЯЧУ ЛЕТ НАЗАД э (BD)    22161                37                YAROSLAV. Thousands of years ago e (BD)\n","22162                                                 ЯРОСТЬ    22162                40                                                   FURY\n","22163                                       ЯРОСТЬ ( регион)    22163                40                                          FURY (region)\n","22164                                            ЯРОСТЬ (BD)    22164                37                                              FURY (BD)\n","22165                 Ядерный титбит 2 [PC, Цифровая версия]    22165                31                 Nuclear titbit 2 [PC, Digital Version]\n","22166        Язык запросов 1С:Предприятия  [Цифровая версия]    22166                54     Language 1C queries: Enterprises [Digital Version]\n","22167  Язык запросов 1С:Предприятия 8 (+CD). Хрусталева Е.Ю.    22167                49  1C query language: Enterprise 8 (+ CD). Khrustalev EY\n","22168                                    Яйцо для Little Inu    22168                62                                     Egg for Little Inu\n","22169                          Яйцо дракона (Игра престолов)    22169                69                           Dragon egg (Game of Thrones)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9oSMeRVd7dvZ"},"source":["###2.5.2) **NLP for feature generation from items data**\n","Automate the search for commonality among items, and create new categorical feature to prevent overfitting from close similarity between many item names"]},{"cell_type":"markdown","metadata":{"id":"CwfXcHsMJ0Yg","colab_type":"text"},"source":["####**Delimited Groups of Words**\n","\n","Investigating \"special\" delimited word groups (like this) or [here] or /hobbitville/ that are present in item names, and may be particularly important in creating n>1 n-grams for uniquely identifying items so that we can tell if two items are the same or nearly the same"]},{"cell_type":"markdown","metadata":{"id":"V7QY11R1QmlN","colab_type":"text"},"source":["#####Some Details on The Approach..."]},{"cell_type":"code","metadata":{"id":"jac_TColdsMf","colab_type":"code","colab":{}},"source":["# explanation of regex string I'm using to parse the item_name\n","'''\n","\n","^\\s+|\\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*|\\s+$\n","\n","gm\n","1st Alternative ^\\s+\n","^ asserts position at start of a line\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","2nd Alternative \\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","Match a single character present in the list below [,\\\"\\/\\(\\)\\[\\]]+\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n",", matches the character , literally (case sensitive)\n","\\\" matches the character \" literally (case sensitive)\n","\\/ matches the character / literally (case sensitive)\n","\\( matches the character ( literally (case sensitive)\n","\\) matches the character ) literally (case sensitive)\n","\\[ matches the character [ literally (case sensitive)\n","\\] matches the character ] literally (case sensitive)\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","3rd Alternative \\s+$\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","$ asserts position at the end of a line\n","\n","Global pattern flags\n","g modifier: global. All matches (don't return after first match)\n","m modifier: multi line. Causes ^ and $ to match the begin/end of each line (not only begin/end of string)\n","'''\n","commented_cell = True  # prevent Jupyter from printing triple-quoted comments"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rsc0yYJkiRBY","colab_type":"code","colab":{}},"source":["# This cell contains no code to run; it is simply a record of some inspections that were done on the items database\n","\n","# before removing undesirable characters / punctuation from the item name,\n","#   let's see if we can find n-grams or useful describers or common abbreviations by looking between the nasty characters\n","# first, let's see what characters are present in the en_item_name column\n","'''\n","nasty_symbols = re.compile('[^0-9a-zA-Z ]')\n","nasties = set()\n","for i in range(len(items_transl)):\n","  n = nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  nasties = nasties.union(set(n))\n","print(nasties)\n","{'[', '\\u200b', 'ñ', '(', ')', '.', 'à', '`', 'ó', '®', 'Á', \n","'\\\\', 'è', '&', '-', ':', 'ë', '_', 'û', '»', '=', '+', ']', ',', \n","'«', 'ú', \"'\", 'ö', '#', 'ä', ';', 'ü', '\"', 'ô', '/', '№', 'é', \n","'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","'''\n","# From the above set of nasty characters, it looks like slashes, single quotes, double quotes, parentheses, and square brackets might enclose relevant n-grams\n","# Let's pull everything from en_item_name that is inside ' ', \" \", (), or [] and see how many unique values we get, and if they are n-grams or abbreviations, for example\n","# It also seems that many of the item names end in a single character \"D\" for example, which should be converted to DVD\n","\n","# ignore the :&+' stuff for now...\n","# Let's set up columns for ()[]-grams, for last string in the name, and for first string in name, and for text that precedes \":\", and for text that surrounds \"&\" or \"+\"\n","#   but first, we will strip out every nasty character except ()[]:&+'\"/ and replace the nasties with spaces, then eliminating double spaces\n","\n","'''\n","# sanity check:\n","really_nasty_symbols = re.compile('[^0-9a-zA-Z \\(\\)\\[\\]:&+\\'\"/]')\n","really_nasties = set()\n","for i in range(len(items_transl)):\n","  rn = really_nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  really_nasties = really_nasties.union(set(rn))\n","print(really_nasties)\n","{'\\u200b', 'ñ', '.', 'à', '`', 'ó', '®', 'Á', '\\\\', 'è', '-', 'ë', '_', 'û', '»', '=', ',', '«', 'ú', 'ö', '#', 'ä', ';', 'ü', 'ô', '№', 'é', 'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","OK, looks good\n","'''\n","commented_cell = True  # prevent Jupyter from printing triple-quoted comments"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvzvPqmCQ2ZM","colab_type":"text"},"source":["#####Add 'delimited' and 'cleaned' data columns; shorten the titles of other columns so dataframe fits better on the screen"]},{"cell_type":"code","metadata":{"id":"Z35pqOYCtyZ7","colab_type":"code","outputId":"8ca84da1-a066-4314-ac26-482c0400a450","executionInfo":{"status":"ok","timestamp":1589075368646,"user_tz":240,"elapsed":41830,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":666}},"source":["%%time\n","items_delimited = items_transl.copy(deep=True)\n","# delete the wide \"item_name\" column so we can read more of the data table width-wise\n","items_delimited = items_delimited.drop(\"item_name\", axis=1).rename(columns = {'en_item_name':'item_name','item_category_id':'i_cat_id'})\n","items_in_test_set = test.item_id.unique()\n","items_delimited[\"i_tested\"] = False\n","for i in items_in_test_set:\n","  items_delimited.at[i,\"i_tested\"] = True\n","\n","# stopwords to remove from item names\n","stop_words = \"a,the,an,only,more,are,any,on,your,just,it,its,has,with,for,by,from\".split(\",\")\n","\n","nasty_symbols_re = re.compile(r'[^0-9a-zA-Z ]')  # remove all punctuation\n","really_nasty_symbols_re = re.compile(r'[^0-9a-zA-Z ,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@]')  # remove nasties, but leave behind the delimiters\n","delimiters_re = re.compile(r'[,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@\\u00AB\\u00BB~<>]')  # unicodes are << and >> thingies\n","delim_pattern_re = re.compile(r'^\\s+|\\s*[,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@\\u00AB\\u00BB~<>]+\\s*|\\s+$') # special symbols indicating a delimiter --> a space at start or end of item name will be removed at split time, along with ,;/()[]:\"-@~<<>><>\n","multiple_whitespace_re = re.compile(r'[ ]{2,}')\n","\n","cleanup_text = {}\n","cleanup_text[' dvd'] = re.compile(r'\\s+d$')  #several item names end in \"d\" -- which actually seems to indicate dvd (because the items I see are in category 40: Movies-DVD)... standardize so d --> dvd\n","cleanup_text['digital version'] = re.compile(r'digital in$') # several items seem to end in \"digital in\"... maybe in = internet?, but looking at nearby items/categories, 'digital version' looks standard\n","cleanup_text['bluray dvd'] = re.compile(r'\\bbd\\b|\\bblu\\s+ray\\b|\\bblu\\-ray\\b|\\bblueray\\b|\\bblue\\s+ray\\b|\\bblue\\-ray\\b')\n","cleanup_text['007 : james bond : skyfall'] = re.compile(r'\\bskyfall\\b|\\bskayfoll\\b')\n","cleanup_text[' and '] = re.compile(r'[\\&\\+]')\n","def maid_service(text):\n","    text = text.lower()\n","    for repl_text, pattern in cleanup_text.items():\n","        text = pattern.sub(repl_text, text)\n","    #r = re.compile(r'\\bskayfoll\\b')\n","    #text = r.sub('skyfall',text)  \n","    return text\n","\n","# ?remove dupes in cleaned text\n","\n","\n","def text_total_clean(text):\n","    #text: the original en_item_name\n","    #return: en_item_name made lowercase, stripped of \"really_nasties\" and multiple spaces\n","    text = maid_service(text)\n","    text = delimiters_re.sub(\" \", text)  # replace all delimiters with a space; other nasties get simply deleted\n","    text = nasty_symbols_re.sub(\"\", text)  # delete anything other than letters, numbers, and spaces\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = text.strip() # remove whitespace around string\n","    # lemmatize each word\n","    text = \" \".join([lemmatizer.lemmatize(w) for w in text.split(\" \") if w not in stop_words])\n","    return text\n","\n","def text_clean_delimited(text):\n","    #text: the original en_item_name\n","    #return: en_item_name made lowercase, stripped of \"really_nasties\" and multiple spaces, in a list of strings that had been separated by one of the above \"delimiters\"\n","    text = maid_service(text)\n","    text = really_nasty_symbols_re.sub(\"\", text)  # just delete the nasty symbols\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = delim_pattern_re.split(text)           # split item_name at all delimiters, irrespective of number of spaces before or after the string or delimiter\n","    text = [x.strip() for x in text if x != \"\"]           # remove empty strings \"\" from the list of split items in text, and remove whitespace outside text n-gram\n","    # lemmatize each word\n","    lemtext = []\n","    for ngram in text:\n","        lemtext.append(\" \".join([lemmatizer.lemmatize(w) for w in ngram.split(\" \") if w not in stop_words]))\n","    return lemtext\n","\n","# add item_category name with delimiter to the item_name, as this will be useful info for grouping similar items\n","items_delimited['item_name'] = items_delimited.apply(lambda x: item_categories_augmented.at[x.i_cat_id,'en_cat_name'] + \" : \" + x.item_name, axis=1)\n","\n","# add a column of simply cleaned text without any undesired punctuation or delimiters\n","items_delimited['clean_item_name'] = items_delimited['item_name'].apply(text_total_clean)\n","\n","# now add a column of lists of delimited (cleaned) text\n","items_delimited['delim_name_list'] = items_delimited['item_name'].apply(text_clean_delimited)\n","\n","# have a look at what we got with our delimited text globs\n","def maxgram(gramlist):\n","    maxg = 0\n","    for g in gramlist:\n","        maxg = max(maxg,len(g.split()))\n","    return maxg\n","items_delimited['d_len'] = items_delimited.delim_name_list.apply(lambda x: len(x))\n","items_delimited['d_maxgram'] = items_delimited.delim_name_list.apply(maxgram)\n","print(items_delimited.head())\n","print(\"\\n\")\n","print(items_delimited.describe())\n","print(\"\\n\")\n","print(items_delimited.iloc[31][:])\n","#items_delimited.to_csv(\"data_output/items_delimited.csv\", index=False)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["   item_id  i_cat_id                                                                                                  item_name  i_tested                                                                                   clean_item_name  \\\n","0        0        40                                                                 Movie - DVD : ! POWER IN glamor (PLAST.) D     False                                                               movie dvd power in glamor plast dvd   \n","1        1        76  Program - Home & Office (Digital) : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]     False  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2        2        40                                                                     Movie - DVD : *** In the glory (UNV) D     False                                                                        movie dvd in glory unv dvd   \n","3        3        40                                                                       Movie - DVD : *** BLUE WAVE (Univ) D     False                                                                      movie dvd blue wave univ dvd   \n","4        4        40                                                                            Movie - DVD : *** BOX (GLASS) D     False                                                                           movie dvd box glass dvd   \n","\n","                                                                                           delim_name_list  d_len  d_maxgram  \n","0                                                                [movie, dvd, power in glamor, plast, dvd]      5          3  \n","1  [program, home and office, digital, abbyy finereader 12 professional edition full, pc, digital version]      6          6  \n","2                                                                         [movie, dvd, in glory, unv, dvd]      5          2  \n","3                                                                       [movie, dvd, blue wave, univ, dvd]      5          2  \n","4                                                                            [movie, dvd, box, glass, dvd]      5          1  \n","\n","\n","         item_id  i_cat_id  d_len  d_maxgram\n","count      22170     22170  22170      22170\n","mean  11,084.500    46.291  4.502      4.249\n","std    6,400.072    15.941  1.448      2.144\n","min            0         0      2          1\n","25%    5,542.250        37      3          3\n","50%   11,084.500        40      4          4\n","75%   16,626.750        58      5          5\n","max        22169        83     15         17\n","\n","\n","item_id                                                                                    31\n","i_cat_id                                                                                   37\n","item_name                                  Movie - Blu-Ray : 007: COORDINATES \"SKAYFOLL» (BD)\n","i_tested                                                                                 True\n","clean_item_name             movie bluray dvd 007 coordinate 007 james bond skyfall bluray dvd\n","delim_name_list    [movie, bluray dvd, 007, coordinate, 007, james bond, skyfall, bluray dvd]\n","d_len                                                                                       8\n","d_maxgram                                                                                   2\n","Name: 31, dtype: object\n","CPU times: user 4.11 s, sys: 62.1 ms, total: 4.18 s\n","Wall time: 4.18 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IUA58xQZ72NC","colab_type":"code","colab":{}},"source":["# make item df easier to read for the following stuff\n","items_clean_delimited = items_delimited.copy(deep=True).drop(\"item_name\", axis=1).rename(columns = {'clean_item_name':'item_name'})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"USoJANp3U9Ei","colab":{}},"source":["# %%time\n","# # Inspect the delimited 4-grams (4.64sec to run this cell without GPU, 4.01sec with GPU)\n","\n","# items_delimited_4gram = items_clean_delimited.copy(deep=True)\n","# items_delimited_4gram[\"d_4grams\"] = items_delimited_4gram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == 4]) # column contains all \"delimited\" 4-grams in the translation\n","\n","# g4 = items_delimited_4gram.d_4grams.apply(pd.Series,1).stack()\n","# g4.index = g4.index.droplevel(-1)\n","# g4.name = 'd_4grams'\n","# del items_delimited_4gram['d_4grams']\n","# items_delimited_4gram = items_delimited_4gram.join(g4)\n","\n","# print(items_delimited_4gram.tail())\n","# print(\"\\n\")\n","# freq_4grams = items_delimited_4gram.d_4grams.value_counts()\n","# print(f'Number of unique delimited 4-grams: {len(freq_4grams)}')\n","# print(f'Number of unique delimited 4-grams that are duplicated at least once: {len(freq_4grams[freq_4grams > 1])}')\n","# print(freq_4grams[1:12])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Uvreedy4YRVd"},"source":["#####Gather all info for duplicated n-grams in our delimited set"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"0d6c05fb-77c3-40e4-a4a7-12bc493a184d","executionInfo":{"status":"ok","timestamp":1589075436355,"user_tz":240,"elapsed":109531,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"7H2VxLddVqlJ","colab":{"base_uri":"https://localhost:8080/","height":952}},"source":["%%time\n","# Get all of the delimited n-grams that are duplicated at least once in item names (1min 24sec no gpu vs. 1min 11sec with gpu)\n","#  range of sizes of delimited phrases (number of 'words'):\n","min_gram = items_delimited.d_maxgram.min()\n","max_gram = items_delimited.d_maxgram.max()\n","\n","total_dupe_grams = 0\n","gram_freqs = {}   # dict will hold elements that are pd.Series with index = phrase, value = number of repeats in items database item names\n","for n in range(min_gram,max_gram+1):\n","    item_ngram = items_clean_delimited.copy(deep=True)\n","    item_ngram['delim_ngrams'] = item_ngram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == n])\n","\n","    grams = item_ngram.delim_ngrams.apply(pd.Series,1).stack()\n","    grams.index = grams.index.droplevel(-1)\n","    grams.name = 'delim_ngrams'\n","    del item_ngram['delim_ngrams']\n","    item_ngram = item_ngram.join(grams)\n","\n","    freq_grams = item_ngram.delim_ngrams.value_counts()\n","    print(f'Number of unique delimited {n}-grams: {len(freq_grams)}')\n","    grams_dupe = len(freq_grams[freq_grams > 1])\n","    print(f'Number of unique delimited {n}-grams that are duplicated at least once: {grams_dupe}\\n')\n","    if grams_dupe > 0:\n","        gram_freqs[n] = freq_grams[freq_grams > 1].copy(deep=True)\n","        total_dupe_grams += grams_dupe\n","print(f'\\nTotal number of unique, delimited, duplicated n-grams for all n: {total_dupe_grams}')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Number of unique delimited 1-grams: 2906\n","Number of unique delimited 1-grams that are duplicated at least once: 1256\n","\n","Number of unique delimited 2-grams: 4293\n","Number of unique delimited 2-grams that are duplicated at least once: 1213\n","\n","Number of unique delimited 3-grams: 3910\n","Number of unique delimited 3-grams that are duplicated at least once: 728\n","\n","Number of unique delimited 4-grams: 3543\n","Number of unique delimited 4-grams that are duplicated at least once: 368\n","\n","Number of unique delimited 5-grams: 2812\n","Number of unique delimited 5-grams that are duplicated at least once: 223\n","\n","Number of unique delimited 6-grams: 1910\n","Number of unique delimited 6-grams that are duplicated at least once: 130\n","\n","Number of unique delimited 7-grams: 1235\n","Number of unique delimited 7-grams that are duplicated at least once: 65\n","\n","Number of unique delimited 8-grams: 772\n","Number of unique delimited 8-grams that are duplicated at least once: 26\n","\n","Number of unique delimited 9-grams: 488\n","Number of unique delimited 9-grams that are duplicated at least once: 14\n","\n","Number of unique delimited 10-grams: 247\n","Number of unique delimited 10-grams that are duplicated at least once: 10\n","\n","Number of unique delimited 11-grams: 136\n","Number of unique delimited 11-grams that are duplicated at least once: 6\n","\n","Number of unique delimited 12-grams: 62\n","Number of unique delimited 12-grams that are duplicated at least once: 1\n","\n","Number of unique delimited 13-grams: 25\n","Number of unique delimited 13-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 14-grams: 9\n","Number of unique delimited 14-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 15-grams: 10\n","Number of unique delimited 15-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 16-grams: 2\n","Number of unique delimited 16-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 17-grams: 1\n","Number of unique delimited 17-grams that are duplicated at least once: 0\n","\n","\n","Total number of unique, delimited, duplicated n-grams for all n: 4040\n","CPU times: user 1min 7s, sys: 992 ms, total: 1min 8s\n","Wall time: 1min 7s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sw9OnsLGv5P0","colab_type":"code","outputId":"6ec2adfd-591c-442a-9bc1-d4b9e677cc93","executionInfo":{"status":"ok","timestamp":1589075436361,"user_tz":240,"elapsed":109526,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# format data for feeding into word vector creator\n","\n","count_bins = [0, 2, 4, 8, 16, 32, 128, 1024, 32768]\n","idf_weights = [8,7,6,5,4,3,2,1]  # more weight for ngrams with lower counts\n","\n","notfirst = False\n","for n,s in gram_freqs.items():\n","    a=len(s)\n","    n_array = np.ones(a,dtype=np.int32)*n\n","    gram_count = s.values.astype(np.int32)\n","    gram_string0 = s.index.to_numpy(dtype='str')\n","    gram_string = [re.compile(r'\\b' + gs + r'\\b') for gs in gram_string0]  # I'm not looking for partial words; n-grams must match at word boundaries\n","    weight_bin = pd.cut(s,count_bins,labels=idf_weights,retbins=False).astype(np.int32)\n","\n","    if notfirst:\n","        n_arrays = np.concatenate((n_arrays,n_array))\n","        gram_counts = np.concatenate((gram_counts,gram_count))\n","        gram_strings = np.concatenate((gram_strings,gram_string))\n","        weight_bins = np.concatenate((weight_bins,weight_bin))\n","    else:\n","        n_arrays = n_array\n","        gram_counts = gram_count\n","        gram_strings = gram_string\n","        weight_bins = weight_bin\n","        notfirst = True\n","\n","print(n_arrays[:5],gram_counts[:5],gram_strings[:5],weight_bins[:5])\n","print(len(n_arrays),len(gram_counts),len(gram_strings),len(weight_bins))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[1 1 1 1 1] [7138 5319 4333 3530 2746] [re.compile('\\\\bmovie\\\\b') re.compile('\\\\bdvd\\\\b')\n"," re.compile('\\\\bmusic\\\\b') re.compile('\\\\bgift\\\\b') re.compile('\\\\bpc\\\\b')] [1 1 1 1 1]\n","4040 4040 4040 4040\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EdrBr4rN7wr8","colab_type":"code","colab":{}},"source":["# use np matrix storage to speed this up... takes about 3 min vs. 8 min with pandas dataframe calculations\n","def make_word_vecs(item_names, ngram_re_patterns, ngram_ns, ngram_weights):\n","    \"\"\"Output is word vectors for input containing item names (english transl)\"\"\"\n","\n","    # create np zeros array of size (number of items, word vector length)\n","    n_items = len(item_names)\n","    wv_len = len(ngram_ns)\n","    item_vec_array = np.zeros((n_items, wv_len), dtype = np.int32)\n","\n","    for g in range(wv_len):\n","        gram_pattern = ngram_re_patterns[g] \n","        gram_len = ngram_ns[g]\n","        gram_weight = ngram_weights[g]\n","        for i in range(n_items):\n","            if gram_pattern.search(item_names[i]):\n","                item_vec_array[i,g] = 2 * gram_len * gram_weight  # use weighting function 2 * (n= length of ngram) * (idf weight from binning above)\n","    return item_vec_array\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrNVLGs1nLmR","colab_type":"code","outputId":"3fad5664-e4c0-4ec2-c073-0f6c621e4a89","executionInfo":{"status":"ok","timestamp":1589075578986,"user_tz":240,"elapsed":252138,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","item_word_vectors = make_word_vecs(items_clean_delimited.loc[:,'item_name'].to_numpy(dtype='str'), gram_strings,n_arrays,weight_bins)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["CPU times: user 2min 22s, sys: 99.6 ms, total: 2min 22s\n","Wall time: 2min 22s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_p1QepqQp1kv","colab_type":"code","colab":{}},"source":["# # intermediate point: can save word vectors here for the 21700 items\n","# np.savez_compressed('data_output/item_word_vectors.npz', arrayname = item_word_vectors)\n","# # ...\n","# iwv = np.load(\"data_output/item_word_vectors.npz\")\n","# item_word_vectors = iwv['arrayname']\n","# print(item_word_vectors.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_B2ZZPQigJmh"},"source":["#####Use scipy sparse matrices instead of pandas... faster, and less memory use"]},{"cell_type":"code","metadata":{"id":"oWtui0wwF1Yp","colab_type":"code","colab":{}},"source":["item_vec_matrix = sparse.csr_matrix(item_word_vectors)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQEF0onOUtxz","colab_type":"code","outputId":"bbeff6c3-cd5e-4b7e-81f5-4fc0e977b767","executionInfo":{"status":"ok","timestamp":1589075637095,"user_tz":240,"elapsed":2693,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","# <2sec for 21,700 items x 4000+ ngrams; output is a csr matrix of type int64\n","dots = item_vec_matrix.dot(item_vec_matrix.transpose()) "],"execution_count":19,"outputs":[{"output_type":"stream","text":["CPU times: user 1.41 s, sys: 206 ms, total: 1.61 s\n","Wall time: 1.62 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yFPX5PouV_aw","colab_type":"code","colab":{}},"source":["# wicked fast way to get top K # of items by dot product value (i.e., closest K items to the item of interest)\n","# https://stackoverflow.com/questions/31790819/scipy-sparse-csr-matrix-how-to-get-top-ten-values-and-indices\n","# also, great reference for speeding up python here: https://colab.research.google.com/drive/1nMDtWcVZCT9q1VWen5rXL8ZHVlxn2KnL\n","\n","@jit(cache=True)\n","def row_topk_csr(data, indices, indptr, K):\n","    \"\"\"Take a sparse scipy csr matrix, and for each column, find the K largest \n","    values in that column (like argmax or argsort[:K]).  Return the row indices \n","    and associated values for each column as two separate np arrays of \n","    length = number of columns in sparse matrix.  Inputs are data/indices/indptr\n","    of csr matrix, and integer K.  Call function like this:\n","    rows, vals = row_topk_csr(csr_name.data, csr_name.indices, csr_name.indptr, K)\n","    Use jit by importing jit and prange from numba, and decorating with\n","    @jit(cache=True) immediately before this function definition\n","    (adopted from https://stackoverflow.com/users/3924566/deepak-saini ) \"\"\"\n","\n","    m = indptr.shape[0] - 1\n","    max_indices = np.zeros((m, K), dtype=indices.dtype)\n","    max_values = np.zeros((m, K), dtype=data.dtype)\n","\n","    for i in prange(m):\n","        top_inds = np.argsort(data[indptr[i] : indptr[i + 1]])[::-1][:K]\n","        max_indices[i] = indices[indptr[i] : indptr[i + 1]][top_inds]\n","        max_values[i] = data[indptr[i] : indptr[i + 1]][top_inds]\n","\n","    return max_indices, max_values\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3D9OVzjXDlGm","colab_type":"code","outputId":"104fdff5-4c07-45e4-f781-ab725a015526","executionInfo":{"status":"ok","timestamp":1589075649229,"user_tz":240,"elapsed":7110,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","dots.setdiag(0)\n","closest10_indices, highest_values = row_topk_csr(dots.data, dots.indices, dots.indptr, K=10)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["CPU times: user 6.24 s, sys: 13.9 ms, total: 6.25 s\n","Wall time: 6.3 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-xjDRQcM9odF","colab_type":"code","outputId":"b58eecf7-bf23-4d8c-e20b-816bff7a7960","executionInfo":{"status":"ok","timestamp":1589075652497,"user_tz":240,"elapsed":869,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["print(closest10_indices.shape)\n","print(closest10_indices[:10,:])"],"execution_count":22,"outputs":[{"output_type":"stream","text":["(22170, 10)\n","[[14329 19060  8995 19062 11949 11015 18529 18530 18531 18532]\n"," [ 1155  1154  1156  1152  1153  3876  3873  3878  3874  3875]\n"," [17361 16692 16916 16917 16973 17125 17191 17212 17213 17251]\n"," [19630  9029 20027 10463  9633 10462  2486  4662 12427 13115]\n"," [11360  9451  9525 10190 10264 10573 10590 11287 11303 11374]\n"," [10468 12642  9856     6     7  9908     9 21903  8964 10344]\n"," [    7 10468 12642     5  9856  9908     9 21903  8964 10344]\n"," [    6 14894 17179  8964 12338 12642 21903 10468 10344  9908]\n"," [17992 17182 17181 17180 17179 17178 17177 17176 17175 17173]\n"," [10468 12642     5     6     7  9856  9908 21903  8964 10344]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OfOzk_xWnF7q","colab_type":"code","outputId":"ff6114de-703c-4103-8a06-48c474b08e29","executionInfo":{"status":"ok","timestamp":1589075657004,"user_tz":240,"elapsed":1862,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["similar_items = pd.DataFrame({'item_id':range(22170)}) #,'close_item_idx':closest10_indices,'close_item_dot':highest_values})\n","similar_items['close_item_idx'] = [closest10_indices[x] for x in range(22170)]\n","similar_items['close_item_dot'] = [highest_values[x] for x in range(22170)]\n","similar_items = similar_items.merge(items_clean_delimited[['item_id','i_tested','i_cat_id']], on='item_id')\n","similar_items['close_item_cat'] = similar_items.close_item_idx.apply(lambda x: [items.at[i,'item_category_id'] for i in x])\n","print(similar_items.head())\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["   item_id                                                          close_item_idx                                            close_item_dot  i_tested  i_cat_id                            close_item_cat\n","0        0   [14329, 19060, 8995, 19062, 11949, 11015, 18529, 18530, 18531, 18532]        [264, 264, 264, 264, 264, 264, 264, 264, 264, 264]     False        40  [40, 40, 37, 40, 37, 40, 37, 40, 40, 40]\n","1        1            [1155, 1154, 1156, 1152, 1153, 3876, 3873, 3878, 3874, 3875]  [10520, 1328, 1328, 1264, 1240, 932, 932, 928, 928, 928]     False        76  [75, 76, 76, 76, 75, 28, 29, 23, 19, 23]\n","2        2  [17361, 16692, 16916, 16917, 16973, 17125, 17191, 17212, 17213, 17251]        [264, 264, 264, 264, 264, 264, 264, 264, 264, 264]     False        40  [38, 40, 38, 38, 40, 40, 37, 40, 37, 40]\n","3        3      [19630, 9029, 20027, 10463, 9633, 10462, 2486, 4662, 12427, 13115]        [108, 108, 108, 108, 108, 108, 104, 104, 104, 104]     False        40  [40, 37, 40, 40, 40, 37, 56, 59, 55, 43]\n","4        4    [11360, 9451, 9525, 10190, 10264, 10573, 10590, 11287, 11303, 11374]        [804, 804, 804, 804, 804, 804, 804, 804, 804, 804]     False        40  [43, 43, 28, 23, 43, 43, 30, 43, 43, 43]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OWuI-nw27XFH","colab_type":"code","outputId":"ab09c1b1-8c5b-4f67-8131-5773a1de5e27","executionInfo":{"status":"ok","timestamp":1589075690149,"user_tz":240,"elapsed":1054,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["# create a graph with nodes = item ids in test set, and edge weights = dot product values\n","\n","# we will use the \"community\" algorithms to determine useful groupings of other items around/including the test items\n","# ##### start with a graph containing the 5100 items in the test set as starter nodes, and add in the 10 highest-match wordvector items if dot product > threshold\n","# TAKING A LEAP... gonna try with 21700 full items dataset / top 10 matches\n","\n","edge_threshold = 100  # dot product (edge weight) must be greater than this for two item_ids to be connected in the graph\n","\n","graph_items = similar_items[['item_id','close_item_idx']].copy(deep=True).explode('close_item_idx').reset_index(drop=True)\n","graph_weights = similar_items[['item_id','close_item_dot']].copy(deep=True).explode('close_item_dot').reset_index(drop=True)\n","graph_items['weight'] = graph_weights.loc[:]['close_item_dot']\n","graph_items.columns = ['item1_id','item2_id','weight']\n","\n","print(len(graph_items))\n","graph_items = graph_items[graph_items.weight > edge_threshold]\n","print(len(graph_items))\n","graph_items.head()\n","# depending on threshold, we may end up dropping some of the test items (for example, we lose item 22154 if threshold = 150, but not if threshold = 100)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["221700\n","175303\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item1_id</th>\n","      <th>item2_id</th>\n","      <th>weight</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>14329</td>\n","      <td>264</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>19060</td>\n","      <td>264</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>8995</td>\n","      <td>264</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>19062</td>\n","      <td>264</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>11949</td>\n","      <td>264</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item1_id item2_id weight\n","0         0    14329    264\n","1         0    19060    264\n","2         0     8995    264\n","3         0    19062    264\n","4         0    11949    264"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"cNTMKGOREZGL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"d95a1d38-34d2-40a9-b37f-9f534b92f21c","executionInfo":{"status":"ok","timestamp":1589075702974,"user_tz":240,"elapsed":2664,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["%%time\n","# import pandas df into weighted-edge graph:\n","G = nx.from_pandas_edgelist(graph_items, 'item1_id', 'item2_id', ['weight'])"],"execution_count":26,"outputs":[{"output_type":"stream","text":["CPU times: user 1.8 s, sys: 19 ms, total: 1.81 s\n","Wall time: 1.82 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iR2U0g8iY0AN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"2b59970c-6648-4218-8d8e-4c3629a38f7f","executionInfo":{"status":"ok","timestamp":1589075728376,"user_tz":240,"elapsed":23075,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["%%time\n","# employ a clustering method that utilizes the edge weights\n","communities2 = community.asyn_lpa_communities(G, weight='weight', seed=42)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["CPU times: user 22.2 s, sys: 5.68 ms, total: 22.2 s\n","Wall time: 22.3 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dtbKGa2CcjJa","colab_type":"code","outputId":"bd7bd5d7-a27b-44a6-c18a-c2f5f4597ee3","executionInfo":{"status":"ok","timestamp":1589075763646,"user_tz":240,"elapsed":2288,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["num_communities = 0\n","community_items = set()\n","cluster_nodes = []\n","n_nodes = []\n","weight_avgs = []\n","weight_sums = []\n","weight_maxs = []\n","weight_mins = []\n","weight_stds = []\n","for i,c in enumerate(communities2):\n","    num_communities += 1\n","    community_items = community_items | set(c)\n","    nodelist = list(c)\n","    n_nodes.append(len(nodelist))\n","    edgeweights = []\n","    for m in range(n_nodes[-1]-1):\n","        for n in range(m+1,n_nodes[-1]):\n","            try:\n","                edgeweights.append(G.edges[nodelist[m], nodelist[n]]['weight'])\n","            except:\n","                pass\n","    cluster_nodes.append(nodelist)\n","    weight_avgs.append(np.mean(edgeweights))\n","    weight_sums.append(np.sum(edgeweights))\n","    weight_maxs.append(np.max(edgeweights))\n","    weight_mins.append(np.min(edgeweights))\n","    weight_stds.append(np.std(edgeweights))\n","\n","print(num_communities)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["2121\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vw2SkxaFd1qm","colab_type":"code","outputId":"bb2abc83-7673-43a8-9402-115b44ac2c39","executionInfo":{"status":"ok","timestamp":1589075774970,"user_tz":240,"elapsed":831,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["weight_avgs = [round(x) for x in weight_avgs]\n","community_df = pd.DataFrame({'n_nodes':n_nodes,'w_avg':weight_avgs,'w_sum':weight_sums,'w_max':weight_maxs,'w_min':weight_mins,'w_std':weight_stds,'item_id':cluster_nodes})\n","print(community_df.head())\n","print(\"\\n\")\n","print(community_df.describe())"],"execution_count":29,"outputs":[{"output_type":"stream","text":["   n_nodes  w_avg   w_sum  w_max  w_min     w_std  \\\n","0       10    364    8724   2572    264   460.506   \n","1       56    313   83032   4984    144   396.113   \n","2       24    558   51368   3480    144   746.886   \n","3       55   1521  480660  13880    104 3,075.009   \n","4        6    584    8760    584    584         0   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                   item_id  \n","0                                                                                                                                                                                                                                                                                                                          [0, 18531, 18532, 9027, 9932, 10833, 9490, 17265, 17910, 19062]  \n","1  [13057, 2, 13058, 14597, 21512, 9489, 16916, 16917, 6295, 19611, 14366, 14243, 11940, 14885, 17191, 9512, 17706, 11950, 9519, 9520, 16691, 16692, 6269, 14522, 10811, 17212, 17213, 14523, 19520, 10052, 8008, 13257, 11209, 8136, 10442, 16973, 11723, 15819, 20043, 12883, 12884, 341, 20308, 17251, 10083, 17125, 13795, 3816, 9065, 17518, 19060, 18805, 14329, 6268, 16893, 16895]  \n","2                                                                                                                                                                                                                  [16520, 21902, 21903, 21904, 20756, 20757, 21908, 21911, 21912, 21913, 21915, 21916, 12829, 12828, 12832, 12834, 8995, 12707, 12709, 12708, 12836, 12845, 12846, 16511]  \n","3                      [17667, 19347, 16153, 13341, 13342, 13343, 18090, 299, 11949, 15986, 15027, 15028, 15029, 15030, 15031, 15032, 15033, 2235, 15036, 15037, 17604, 838, 12232, 10697, 471, 472, 473, 20186, 18010, 20188, 20189, 20187, 479, 480, 20193, 481, 20194, 20195, 20196, 20197, 21484, 21485, 21486, 492, 497, 9458, 9459, 9460, 9461, 9462, 9463, 9464, 501, 16250, 15988]  \n","4                                                                                                                                                                                                                                                                                                                                               [11013, 11014, 11015, 11016, 11017, 11018]  \n","\n","\n","       n_nodes     w_avg       w_sum     w_max     w_min      w_std\n","count     2121      2121        2121      2121      2121       2121\n","mean     8.922 2,021.130  36,393.086 3,475.166 1,641.081    473.097\n","std     16.449 3,241.651 134,976.158 4,713.930 3,268.934  1,043.148\n","min          2       104         104       104       104          0\n","25%          2       342        2400       792       196          0\n","50%          4      1032        7304      1852       280     30.847\n","75%         10      2312       27812      4316      1352    505.360\n","max        412     32300     3643844     41540     32300 12,217.862\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bmZVij-EgUww","colab_type":"code","outputId":"28c77659-f589-468a-8988-61ab8ff19148","executionInfo":{"status":"ok","timestamp":1589075797609,"user_tz":240,"elapsed":834,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["community_df.w_avg.nunique()\n","# can't use this as a category code because not unique among clusters,\n","# but I want to use the average cluster weights property to encode the cluster category\n","# (higher numbers for category code --> stronger clustering; may be useful to have this correlation instead of random generation of category codes)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1234"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"OhgonOqnhH5s","colab_type":"code","outputId":"134a702c-535a-4b74-c909-97bd2916fa6d","executionInfo":{"status":"ok","timestamp":1589075804790,"user_tz":240,"elapsed":880,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# so, I will sort on w_avg, then on number of nodes as perhaps the next most important defining characteristic of a given cluster\n","#  and, to make the categorization unique, I will take the w_avg value and sum it with the index (row number)...\n","#     (with the sorting, this favors even more the clusters with high average item-to-item similarity)\n","community_df = community_df.sort_values(['w_avg','n_nodes']).reset_index(drop=True)\n","community_df['item_cluster_id'] = community_df.index + community_df['w_avg']\n","community_df.head()"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>n_nodes</th>\n","      <th>w_avg</th>\n","      <th>w_sum</th>\n","      <th>w_max</th>\n","      <th>w_min</th>\n","      <th>w_std</th>\n","      <th>item_id</th>\n","      <th>item_cluster_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>[8491, 21478]</td>\n","      <td>104</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>[16040, 16178]</td>\n","      <td>105</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>104</td>\n","      <td>312</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>[14938, 17619, 8188]</td>\n","      <td>106</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>104</td>\n","      <td>1040</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>[5841, 5842, 5843, 5844, 5845]</td>\n","      <td>107</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>22</td>\n","      <td>104</td>\n","      <td>9776</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>[388, 401, 405, 409, 281, 282, 414, 418, 293, 294, 295, 298, 427, 303, 310, 311, 442, 451, 328, 335, 375, 253]</td>\n","      <td>108</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   n_nodes  w_avg  w_sum  w_max  w_min  w_std                                                                                                         item_id  item_cluster_id\n","0        2    104    104    104    104      0                                                                                                   [8491, 21478]              104\n","1        2    104    104    104    104      0                                                                                                  [16040, 16178]              105\n","2        3    104    312    104    104      0                                                                                            [14938, 17619, 8188]              106\n","3        5    104   1040    104    104      0                                                                                  [5841, 5842, 5843, 5844, 5845]              107\n","4       22    104   9776    104    104      0  [388, 401, 405, 409, 281, 282, 414, 418, 293, 294, 295, 298, 427, 303, 310, 311, 442, 451, 328, 335, 375, 253]              108"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"fuEH3MFyjuNH","colab_type":"code","outputId":"603762dc-37e3-4141-cd1b-4adfa01f4b09","executionInfo":{"status":"ok","timestamp":1589075820086,"user_tz":240,"elapsed":753,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# unravel / explode the cluster node lists... we know this will not duplicate item ids, from the counting we did above\n","item_clusters = community_df.copy(deep=True).explode('item_id').reset_index().rename(columns = {'index':'cluster_number'})\n","item_clusters.head()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cluster_number</th>\n","      <th>n_nodes</th>\n","      <th>w_avg</th>\n","      <th>w_sum</th>\n","      <th>w_max</th>\n","      <th>w_min</th>\n","      <th>w_std</th>\n","      <th>item_id</th>\n","      <th>item_cluster_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>8491</td>\n","      <td>104</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>21478</td>\n","      <td>104</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>16040</td>\n","      <td>105</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>16178</td>\n","      <td>105</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>104</td>\n","      <td>312</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>14938</td>\n","      <td>106</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   cluster_number  n_nodes  w_avg  w_sum  w_max  w_min  w_std item_id  item_cluster_id\n","0               0        2    104    104    104    104      0    8491              104\n","1               0        2    104    104    104    104      0   21478              104\n","2               1        2    104    104    104    104      0   16040              105\n","3               1        2    104    104    104    104      0   16178              105\n","4               2        3    104    312    104    104      0   14938              106"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"OSnYLCamXr2S","colab_type":"code","outputId":"925c984b-5a6e-48bb-e188-c24030a9b258","executionInfo":{"status":"ok","timestamp":1589075831317,"user_tz":240,"elapsed":1025,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["items_clustered = items_clean_delimited[['item_id','i_cat_id','i_tested','item_name']].merge(item_clusters,on='item_id',how='left')\n","items_clustered = items_clustered[['item_id','i_cat_id','item_cluster_id','i_tested','cluster_number','n_nodes','w_avg','w_sum','w_max','w_min','w_std','item_name']]\n","items_clustered.columns = ['item_id','item_category_id','item_cluster_id','item_tested','cluster_number','n_items_in_cluster','w_avg','w_sum','w_max','w_min','w_std','item_name']\n","print(items_clustered.head())"],"execution_count":33,"outputs":[{"output_type":"stream","text":["  item_id  item_category_id  item_cluster_id  item_tested  cluster_number  n_items_in_cluster  w_avg  w_sum  w_max  w_min     w_std                                                                                         item_name\n","0       0                40              920        False             556                  10    364   8724   2572    264   460.506                                                               movie dvd power in glamor plast dvd\n","1       1                76             2600        False            1322                  12   1278  61348  10520    404 1,497.166  program home and office digital abbyy finereader 12 professional edition full pc digital version\n","2       2                40              802        False             489                  56    313  83032   4984    144   396.113                                                                        movie dvd in glory unv dvd\n","3       3                40              330        False             129                   6    201   2212   1132    108   294.379                                                                      movie dvd blue wave univ dvd\n","4       4                40             1686        False             886                  56    800 264120   2664    104   240.272                                                                           movie dvd box glass dvd\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KtvuccfmrWJy","colab_type":"code","outputId":"542751e3-2da0-4827-d5db-8c852c3b017a","executionInfo":{"status":"ok","timestamp":1589075844045,"user_tz":240,"elapsed":839,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["# how many test items are represented by clusters?\n","tested_clustered = items_clustered[items_clustered.item_tested==True][['item_id','item_category_id','item_cluster_id','item_name']]\n","tested_clustered['unclustered'] = tested_clustered.apply(lambda x: np.NaN if x.item_cluster_id > 0  else x.item_id, axis = 1)\n","print(tested_clustered.head(10))\n","print('\\n')\n","print(tested_clustered.item_id.nunique())\n","unclustered = tested_clustered.unclustered.unique()\n","unclustered = [x for x in unclustered if x > 0]\n","print(len(unclustered))\n","train_items = sales_train.item_id.unique()\n","print(len(train_items))\n","print(len(items))\n","untrained = [x for x in unclustered if x not in train_items]\n","print(len(untrained))\n","print(len(items) - len(train_items) - len(untrained))"],"execution_count":34,"outputs":[{"output_type":"stream","text":["   item_id  item_category_id  item_cluster_id                                                            item_name  unclustered\n","30      30                40             1109                      movie dvd 007 coordinate 007 james bond skyfall          nan\n","31      31                37             1109    movie bluray dvd 007 coordinate 007 james bond skyfall bluray dvd          nan\n","32      32                40             4055                                                    movie dvd 1 and 1          nan\n","33      33                37             4055                                  movie bluray dvd 1 and 1 bluray dvd          nan\n","38      38                41              nan  cinema collector 10 most popular comedy twentieth century 10dvd rem           38\n","42      42                57              231                   music mp3 100 best romantic melody mp3 cd digipack          nan\n","45      45                57              231                   music mp3 100 of best folk song mp3 cd cd digipack          nan\n","51      51                57              231                    music mp3 100 best classical work mp3 cd digipack          nan\n","53      53                57              231                   music mp3 100 best russian song mp3 cd cd digipack          nan\n","57      57                57             1146                music mp3 100 pound of multhitov given y mp3 cd jewel          nan\n","\n","\n","5100\n","640\n","21807\n","22170\n","53\n","310\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nuWZ7bPJ4bR2","colab_type":"code","outputId":"fe99c547-79d0-4672-f1eb-1edd458c763a","executionInfo":{"status":"ok","timestamp":1589075909379,"user_tz":240,"elapsed":1078,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# revert to original item_category_id if item is not in clustered items\n","items_clustered['cluster_code'] = items_clustered.apply(lambda x: x.item_cluster_id if x.item_cluster_id > 0 else x.item_category_id, axis = 1)\n","items_clustered.head()"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>item_category_id</th>\n","      <th>item_cluster_id</th>\n","      <th>item_tested</th>\n","      <th>cluster_number</th>\n","      <th>n_items_in_cluster</th>\n","      <th>w_avg</th>\n","      <th>w_sum</th>\n","      <th>w_max</th>\n","      <th>w_min</th>\n","      <th>w_std</th>\n","      <th>item_name</th>\n","      <th>cluster_code</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>920</td>\n","      <td>False</td>\n","      <td>556</td>\n","      <td>10</td>\n","      <td>364</td>\n","      <td>8724</td>\n","      <td>2572</td>\n","      <td>264</td>\n","      <td>460.506</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>920</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>2600</td>\n","      <td>False</td>\n","      <td>1322</td>\n","      <td>12</td>\n","      <td>1278</td>\n","      <td>61348</td>\n","      <td>10520</td>\n","      <td>404</td>\n","      <td>1,497.166</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>2600</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>40</td>\n","      <td>802</td>\n","      <td>False</td>\n","      <td>489</td>\n","      <td>56</td>\n","      <td>313</td>\n","      <td>83032</td>\n","      <td>4984</td>\n","      <td>144</td>\n","      <td>396.113</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>802</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>40</td>\n","      <td>330</td>\n","      <td>False</td>\n","      <td>129</td>\n","      <td>6</td>\n","      <td>201</td>\n","      <td>2212</td>\n","      <td>1132</td>\n","      <td>108</td>\n","      <td>294.379</td>\n","      <td>movie dvd blue wave univ dvd</td>\n","      <td>330</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>40</td>\n","      <td>1686</td>\n","      <td>False</td>\n","      <td>886</td>\n","      <td>56</td>\n","      <td>800</td>\n","      <td>264120</td>\n","      <td>2664</td>\n","      <td>104</td>\n","      <td>240.272</td>\n","      <td>movie dvd box glass dvd</td>\n","      <td>1686</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  item_id  item_category_id  item_cluster_id  item_tested  cluster_number  n_items_in_cluster  w_avg  w_sum  w_max  w_min     w_std                                                                                         item_name  cluster_code\n","0       0                40              920        False             556                  10    364   8724   2572    264   460.506                                                               movie dvd power in glamor plast dvd           920\n","1       1                76             2600        False            1322                  12   1278  61348  10520    404 1,497.166  program home and office digital abbyy finereader 12 professional edition full pc digital version          2600\n","2       2                40              802        False             489                  56    313  83032   4984    144   396.113                                                                        movie dvd in glory unv dvd           802\n","3       3                40              330        False             129                   6    201   2212   1132    108   294.379                                                                      movie dvd blue wave univ dvd           330\n","4       4                40             1686        False             886                  56    800 264120   2664    104   240.272                                                                           movie dvd box glass dvd          1686"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"aFs7GZWs6bkF","colab_type":"code","colab":{}},"source":["# save what we have; maybe refine later\n","\n","compression_opts = dict(method='gzip',\n","                        archive_name='items_clustered_21700.csv')  \n","items_clustered.to_csv('data_output/items_clustered_21700.csv.gz', index=False, compression=compression_opts)"],"execution_count":0,"outputs":[]}]}