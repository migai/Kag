{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MG_EDA_items only v2.ipynb","provenance":[{"file_id":"14t_SkT4SYL-JAcrbJIJ60cbNgCrJlvIN","timestamp":1589069755220},{"file_id":"1mFtJLElc2hyopq6yrPAoi0zQVUegsAP5","timestamp":1589018631041},{"file_id":"1y04qp_hoyBnsJQwkX67pk4iZsKGQIqNy","timestamp":1588805435380},{"file_id":"1b_K0QD9U6dofQ7VtTAtzUrqbKJMdj64l","timestamp":1588785261238},{"file_id":"1gcbeu-d1GUUzznZwTzfqYYbaD6cJ7EQ4","timestamp":1588238522691},{"file_id":"1pSGNRDJGzdeI69bw1zWefzPifBq-rv9H","timestamp":1588151557805},{"file_id":"1hq-ivO1BBtc5IC5xd-JdH8HNAQ81bkRf","timestamp":1587386702728},{"file_id":"1I7DWo2B7q7g9Ne2khD11YGTq_gD2FoaT","timestamp":1587321559573},{"file_id":"1fyZv-jgb8twsCBQwPxjgk_XSYA6dOa2t","timestamp":1587303588700},{"file_id":"1iKsplqpLQQZqdr3Trflk7TapksgkQXjX","timestamp":1587145642564},{"file_id":"https://github.com/migai/Kag/blob/master/Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587076517706}],"collapsed_sections":["s0J5l5H98Xsh","lZfgOx-0_KXg","V7QY11R1QmlN"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn","colab_type":"text"},"source":["#**Investigation of *items* database and correlations between items**\n","\n","**EDA, NLP, Feature Generation**\n","\n","Andreas Theodoulou and Michael Gaidis (May, 2020)"]},{"cell_type":"markdown","metadata":{"id":"L6ErXnphuc2t","colab_type":"text"},"source":["##**summary so far (May 9, 2020):**\n","\n","I refined the \"delimiter\" characters, and did a bit more cleaning on some stuff I noticed with \"blu-ray\" vs. \"bluray\" vs. \"bd\"...\n","--> generated a new set of unique n-grams for n=1 to highest n, and filtered to include only where there are at least 2 item names containing that n-gram\n","\n","--> created \"word vector\" representations of the item names in items dataset, including roughly 4000 elements (all of the delimited n-grams mentioned above)\n","\n","--> for each of the 21700 items, they were encoded as word vectors, and then I used dot-product to identify which items were similar to others.  In the encoding of the word vectors, I did some \"weighting\" such that I didn't just have a word vector that was all 0's except for 1's in locations representing the particular n-grams that are found in the item name string (English translation).  I used 2 types of weighting: \n","1. like TF-IDF, I counted the number of occurrences of each of the roughly 4000 n-grams in the set of all item names, and I binned the n-grams to more heavily weight n-grams that have less representation in the item names. (For example, \"klompferstietnitz\" is more valuable than \"dvd\", because so many item names contain \"dvd\".  So, the former 1-gram gets a larger integer inserted into its location in the word vector.)  I used binning rather than strict TF-IDF \"continuous\" weighting because I believe there is a cutoff at which a term no longer holds much weight at all.\n","2. longer n-grams get heavier weight as well... if two item names have the same 10-word string (10-gram), it is much more relevant than if they have the same 1-word string (1-gram).\n","\n","--> after running the dot products between items in the 21700 x 4000 size matrix containing word vectors for each item, I end up with a 21700 x 21700 matrix containing integers = dot products of the word vectors i,j for cell at i,j coming from item i and item j.\n","\n","--> then, I found a nifty jit-accelerated function (reference below) to pick out the top K largest dot-product values for a given item.  The function gives me the top K items and their dot-product values with the item of interest.  I run this function on all 21700 items, and pick (at first the top 3, but now...) the top 10 highest dot-product values for a given item.\n","\n","**To date** I have only then taken the 5100 items that we know are in the test set (I was afraid of overwhelming the computing power or memory allocation in Colab).  \n","--> so, now I have a dataset with 5100 rows (each test item), and columns for the top-10 matching item ids as per the dot product, and for the actual top-10 dot products also.\n","\n","--> I \"explode\" (unravel) the list of 10 matching items and dot product values so now I have 51000 rows and columns indicating item id of interest (one of the 5100 in the test set), the top-10 matching item ids, and a column for the dot product values between the two.\n","\n","**Now we need to create features from this**\n","\n","**First, look for clusters of tightly-matched item-item pairs**\n","\n","I decided to use the networkX package to map these item pairs into an undirected graph with nodes = item ids, and edges weighted by the value of the dot product.  Then, I can utilize some of the pre-made algorithms that can automatically identify strongly-clustered groups.\n","\n","Before feeding the 51000 row matrix into the graph, I applied a threshold so only dot products above a certain value would be allowed as nodes/edges in the graph.  (This is to filter out \"matches\" where both items have some common term like \"dvd\" but nothing else.  But, as some of the item names are short an nondescriptive, even this \"dvd\" match can place the item-item pair in the top 10.  Of course, you could have 1000 items like this that match the subject item with the same dot-product value, but the aforementioned algorithm just picks the first 10.)\n","\n","Ok, so it goes into the graph, and I apply a \"community\" algorithm that takes into account the weighted edge values (preferentially grouping together items that have higher dot-product values).\n","I didn't find a way to get much control over how many clusters are identified by the algorithm.  It seems to go up roughly linearly in the number of nodes(items) in the graph.  Anyhow, I get about 1400 clusters for my 5100 input items.  These clusters contain item_ids both in and not in the test set, as the graph was made with 51000 edges (minus about 10000 from thresholding) that used top-10 matches with the 5100 test items.  These top-10 matches may or may not be in the test set.\n","\n","These 1400 clusters have anywhere from 2 items (one edge) to perhaps 100 items.  I computed an average dot product value between all elements in a cluster, and used that to estimate the overall \"strength\" of clustering.  This provides a natural way to do category encoding.  I simply use the integer average of cluster dot-products as the \"cluster category code\".  (One minor complication is that some clusters have identical averages... I gave a small boost to the clusters with greater number of elements, so n=2, avg=300 might get a category value of 300, whereas n=5 avg=300 might get category value = 320)\n","\n","I assign all items in a given cluster the same \"cluster category code\".\n","\n","Any items that do not belong to a cluster (either because they didn't make the top-10 list for any of the test item matches, or because the thresholding eliminated them from inclusion in the graph) were assigned a \"cluster category code\" equal to their original item_category_code.  The cluster category code is a minimum of 2x or 3x larger than the largest original item category code (83), and the cluster category code can be quite a bit larger ... 100x or more, for the strongest-matching clusters.\n","\n","</br>\n","\n","**This was all done with the v1.1 items EDA ipynb on GitHub, and the dataset containing the \"cluster category codes\" is saved as csv.gz in the data_output directory.  You can just load in that dataset and use the cluster category code column (alongside the item_id column) as a feature in the model.  It shouldn't need further category encoding.**\n","\n","I'm now working on v2.0 of this items EDA (this file), to remove unnecessary code stragglers, and I hope to try a graph/clustering with all 21700 items rather than just the 5100 test items.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ruw_WyRxhqpx"},"source":["#0. Configure Environment\n","**NOT OPTIONAL**"]},{"cell_type":"code","metadata":{"id":"sTVAxnMnenrB","colab_type":"code","colab":{}},"source":["# General python libraries/modules used throughout the notebook\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import MultipleLocator, FormatStrFormatter, AutoMinorLocator\n","import numpy as np\n","from scipy import sparse\n","import seaborn as sns\n","from numba import jit, prange\n","import networkx as nx\n","from networkx.algorithms import community, cluster\n","\n","import os\n","from itertools import product\n","import re\n","import json\n","import time\n","from time import sleep, localtime, strftime\n","import pickle\n","\n","\n","# Magics\n","%matplotlib inline\n","\n","\n","# NLP packages\n","import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer \n","lemmatizer = WordNetLemmatizer() \n","\n","# # ML packages\n","# from sklearn.linear_model import LinearRegression\n","\n","# !pip install catboost\n","# from catboost import CatBoostRegressor \n","\n","# %tensorflow_version 2.x\n","# import tensorflow as tf\n","# import keras as K\n","\n","# # List of the modules we need to version-track for reference\n","modules = ['pandas','matplotlib','numpy','scipy','numba','seaborn','sklearn','tensorflow','keras','catboost','pip','nltk','networkx']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NoOf7oi8Oe-Q","colab_type":"code","colab":{}},"source":["# Notebook formatting\n","# Adjust as per your preferences.  I'm using a FHD monitor with a full-screen browser window containing my IPynb notebook\n","\n","# format pandas output so we can see all the columns we care about (instead of \"col1  col2  ........ col8 col9\", we will see \"col1 col2 col3 col4 col5 col6 col7 col8 col9\" if it fits inside display.width parameter)\n","pd.set_option(\"display.max_columns\",30)  \n","pd.set_option(\"display.max_rows\",100)     # Override pandas choice of how many rows to show, so, for example, we can see the full 84-row item_category dataframe instead of the first few rows, then ...., then the last few rows\n","pd.set_option(\"display.width\", 300)       # Similar to the above for showing more rows than pandas defaults to, we can show more columns than default, if we tune this to our monitor window size\n","pd.set_option(\"max_colwidth\", None)\n","\n","#pd.set_option(\"display.precision\", 3)  # Nah, this is helpful, but below is even better\n","#Try to convince pandas to print without decimal places if a number is actually an integer (helps keep column width down, and highlights data types)\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"miCS3XqUhDXz"},"source":["#1. Load Data Files\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kjWzoXizEa5O","colab_type":"text"},"source":["##1.1) Enter Data File Names and Paths\n","\n","**NOT Optional**"]},{"cell_type":"code","metadata":{"id":"p9vsd3EynZLO","colab_type":"code","colab":{}},"source":["#  FYI, data is coming from a public repo on GitHub at github.com/migai/Kag\n","# List of the data files (path relative to GitHub master), to be loaded into pandas DataFrames\n","data_files = [  \"readonly/final_project_data/items.csv\",\n","                \"readonly/final_project_data/item_categories.csv\",\n","                #\"readonly/final_project_data/shops.csv\",\n","                #\"readonly/final_project_data/sample_submission.csv.gz\",\n","                \"readonly/final_project_data/sales_train.csv.gz\",\n","                \"readonly/final_project_data/test.csv.gz\",\n","                #\"data_output/shops_transl.csv\",\n","                #\"data_output/shops_augmented.csv\",\n","                \"data_output/item_categories_transl.csv\",\n","                \"data_output/item_categories_augmented.csv\",\n","                \"data_output/items_transl.csv\",\n","                #\"data_output/items_clustered.csv.gz\",\n","                #\"readonly/en_50k.csv\"\n","              ]\n","\n","\n","# Dict of helper code files, to be loaded and imported {filepath : import_as}\n","code_files = {}  # not used at this time; example dict = {\"helper_code/kaggle_utils_at_mg.py\" : \"kag_utils\"}\n","\n","\n","# GitHub file location info\n","git_hub_url = \"https://raw.githubusercontent.com/migai\"\n","repo_name = 'Kag'\n","branch_name = 'master'\n","base_url = os.path.join(git_hub_url, repo_name, branch_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPrPvh7sorJd","colab_type":"text"},"source":["##1.2) Load Data Files"]},{"cell_type":"code","metadata":{"id":"CUIE1PVjSAmg","colab_type":"code","outputId":"d6361ede-7d3d-4b67-80df-5b80f824c357","executionInfo":{"status":"ok","timestamp":1589031771248,"user_tz":240,"elapsed":264863,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, then paste it into the input box and hit 'enter' to complete mounting of the drive\n","from google.colab import drive  \n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dy5i7jl00oX-","colab_type":"code","colab":{}},"source":["'''\n","############################################################\n","############################################################\n","'''\n","# Replace this path with the path on *your* Google Drive where the repo master branch is stored\n","#   (on GitHub, the remote repo is located at github.com/migai/Kag --> below is my cloned repo location)\n","GDRIVE_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\"\n","'''\n","############################################################\n","############################################################\n","'''\n","\n","%cd \"{GDRIVE_REPO_PATH}\"\n","\n","print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","\n","# Loop to load the data files into appropriately-named pandas DataFrames\n","for path_name in data_files:\n","    filename = path_name.rsplit(\"/\")[-1]\n","    data_frame_name = filename.split(\".\")[0]\n","    exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","    if data_frame_name == 'sales_train':\n","        sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","    print(\"Data Frame: \" + data_frame_name)\n","    print(eval(data_frame_name).head(2))\n","    print(\"\\n\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P99yjzAasJva"},"source":["#2. Explore Data (EDA), Clean Data, and Generate Features"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xr0FDeno_EUQ"},"source":["#2.5) ***items*** Dataset: EDA, Cleaning, Correlations, and Feature Generation\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s0J5l5H98Xsh"},"source":["###Thoughts regarding items dataframe\n","Let's first look at how many training examples we have to work with..."]},{"cell_type":"markdown","metadata":{"id":"1lg7NbEchkuM","colab_type":"text"},"source":["Many of the items have similar names, but slightly different punctuation, or only very slightly different version numbers or types.  (e.g., 'Call of Duty III' vs. 'Call of Duty III DVD')\n","\n","One can expect that these two items would have similar sales in general, and by grouping them into a single feature category, we can eliminate some of the overfitting that might come as a result of the relatively small ratio of (training set shop-item-date combinations = 2935849)/(total number of unique items = 22170).  (This is an average of about 132 rows in the sales_train data for each shop-item-date combination that we are using to train our model.  Our task is to produce a monthly estimate of sales (for November 2015), so it is relevant to consider training our model based on how many sales in a month vs. how many sales in the entire training set.  Given that the sales_train dataset covers the time period from January 2013 to October 2015 (34 months), we have on average fewer than 4 shop-item combinations in our training set for a given item in any given month.  Furthermore, as we are trying to predict for a particular month (*November* 2015), it is relevant to consider how many rows in our training set occur in the month of November.  The sales_train dataset contains data for two 'November' months out of the total 34 months of data.  Another simple calculation gives us an estimate that our training set contains on average 0.23 shop-item combinations per item for November months.\n","\n","To summarize:\n","\n","*  *sales_train* contains 34 months of data, including 2935849 shop-item-date combinations\n","*  *items* contains 22170 \"unique\" item_id values\n","\n","In the *sales_train* data, we therefore have:\n","*  on average, 132 rows with a given shop-item pair for a given item_id\n","*  on average, 4 rows with a given shop-item pair for a given item_id in a given month\n","*  on average, 0.23 rows with a given shop-item pair for a given item_id in all months named 'November'\n","\n","If we wish to improve our model predictions for the following month of November, it behooves us to use monthly grouping of sales, or, even better, November grouping of sales.  This smooths out day-to-day variations in sales for a better monthly prediction.  However, the sparse number of available rows in the *sales_train* data will contribute to inaccuracy in our model training and predictions.\n","\n","Imagine if we could reduce the number of item_id values from 22170 to perhaps half that or even less.  Given that the number of rows for training (per item, on a monthly or a November basis) is so small, then such a reduction in the number of item_id values would have a big impact.  (The same is true for creating features to supplement \"shop_id\" so as to group and reduce the individuality of each shop - and thus effectively create, on average, more rows of training data for each shop-item pair."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZfgOx-0_KXg"},"source":["###2.5.1) **Translate and Ruminate**\n","We will start by translating the Russian text in the dataframe, and add our ruminations on possible new features we can generate.\n","\n","The dataframe *items_transl* (equivalent to *items* plus a column for English translation) is saved as a .csv file so we do not have to repeat the translation process the next time we open a Google Colab runtime."]},{"cell_type":"code","metadata":{"id":"FhHSfXNxsKxQ","colab_type":"code","outputId":"c391796f-5713-4e76-8ebd-fed3114d0494","executionInfo":{"status":"ok","timestamp":1589031787468,"user_tz":240,"elapsed":281072,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["print(items_transl.info())\n","print(\"\\n\")\n","print(items_transl.tail(10))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 22170 entries, 0 to 22169\n","Data columns (total 4 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   item_name         22170 non-null  object\n"," 1   item_id           22170 non-null  int64 \n"," 2   item_category_id  22170 non-null  int64 \n"," 3   en_item_name      22170 non-null  object\n","dtypes: int64(2), object(2)\n","memory usage: 692.9+ KB\n","None\n","\n","\n","                                                   item_name  item_id  item_category_id                                           en_item_name\n","22160                             ЯРМАРКА ТЩЕСЛАВИЯ (Регион)    22160                40                                   Vanity Fair (Region)\n","22161                       ЯРОСЛАВ. ТЫСЯЧУ ЛЕТ НАЗАД э (BD)    22161                37                YAROSLAV. Thousands of years ago e (BD)\n","22162                                                 ЯРОСТЬ    22162                40                                                   FURY\n","22163                                       ЯРОСТЬ ( регион)    22163                40                                          FURY (region)\n","22164                                            ЯРОСТЬ (BD)    22164                37                                              FURY (BD)\n","22165                 Ядерный титбит 2 [PC, Цифровая версия]    22165                31                 Nuclear titbit 2 [PC, Digital Version]\n","22166        Язык запросов 1С:Предприятия  [Цифровая версия]    22166                54     Language 1C queries: Enterprises [Digital Version]\n","22167  Язык запросов 1С:Предприятия 8 (+CD). Хрусталева Е.Ю.    22167                49  1C query language: Enterprise 8 (+ CD). Khrustalev EY\n","22168                                    Яйцо для Little Inu    22168                62                                     Egg for Little Inu\n","22169                          Яйцо дракона (Игра престолов)    22169                69                           Dragon egg (Game of Thrones)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9oSMeRVd7dvZ"},"source":["###2.5.2) **NLP for feature generation from items data**\n","Automate the search for commonality among items, and create new categorical feature to prevent overfitting from close similarity between many item names"]},{"cell_type":"markdown","metadata":{"id":"CwfXcHsMJ0Yg","colab_type":"text"},"source":["####**Delimited Groups of Words**\n","\n","Investigating \"special\" delimited word groups (like this) or [here] or /hobbitville/ that are present in item names, and may be particularly important in creating n>1 n-grams for uniquely identifying items so that we can tell if two items are the same or nearly the same"]},{"cell_type":"markdown","metadata":{"id":"V7QY11R1QmlN","colab_type":"text"},"source":["#####Some Details on The Approach..."]},{"cell_type":"code","metadata":{"id":"jac_TColdsMf","colab_type":"code","colab":{}},"source":["# explanation of regex string I'm using to parse the item_name\n","'''\n","\n","^\\s+|\\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*|\\s+$\n","\n","gm\n","1st Alternative ^\\s+\n","^ asserts position at start of a line\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","2nd Alternative \\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","Match a single character present in the list below [,\\\"\\/\\(\\)\\[\\]]+\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n",", matches the character , literally (case sensitive)\n","\\\" matches the character \" literally (case sensitive)\n","\\/ matches the character / literally (case sensitive)\n","\\( matches the character ( literally (case sensitive)\n","\\) matches the character ) literally (case sensitive)\n","\\[ matches the character [ literally (case sensitive)\n","\\] matches the character ] literally (case sensitive)\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","3rd Alternative \\s+$\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","$ asserts position at the end of a line\n","\n","Global pattern flags\n","g modifier: global. All matches (don't return after first match)\n","m modifier: multi line. Causes ^ and $ to match the begin/end of each line (not only begin/end of string)\n","'''\n","commented_cell = True  # prevent Jupyter from printing triple-quoted comments"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rsc0yYJkiRBY","colab_type":"code","colab":{}},"source":["# This cell contains no code to run; it is simply a record of some inspections that were done on the items database\n","\n","# before removing undesirable characters / punctuation from the item name,\n","#   let's see if we can find n-grams or useful describers or common abbreviations by looking between the nasty characters\n","# first, let's see what characters are present in the en_item_name column\n","'''\n","nasty_symbols = re.compile('[^0-9a-zA-Z ]')\n","nasties = set()\n","for i in range(len(items_transl)):\n","  n = nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  nasties = nasties.union(set(n))\n","print(nasties)\n","{'[', '\\u200b', 'ñ', '(', ')', '.', 'à', '`', 'ó', '®', 'Á', \n","'\\\\', 'è', '&', '-', ':', 'ë', '_', 'û', '»', '=', '+', ']', ',', \n","'«', 'ú', \"'\", 'ö', '#', 'ä', ';', 'ü', '\"', 'ô', '/', '№', 'é', \n","'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","'''\n","# From the above set of nasty characters, it looks like slashes, single quotes, double quotes, parentheses, and square brackets might enclose relevant n-grams\n","# Let's pull everything from en_item_name that is inside ' ', \" \", (), or [] and see how many unique values we get, and if they are n-grams or abbreviations, for example\n","# It also seems that many of the item names end in a single character \"D\" for example, which should be converted to DVD\n","\n","# ignore the :&+' stuff for now...\n","# Let's set up columns for ()[]-grams, for last string in the name, and for first string in name, and for text that precedes \":\", and for text that surrounds \"&\" or \"+\"\n","#   but first, we will strip out every nasty character except ()[]:&+'\"/ and replace the nasties with spaces, then eliminating double spaces\n","\n","'''\n","# sanity check:\n","really_nasty_symbols = re.compile('[^0-9a-zA-Z \\(\\)\\[\\]:&+\\'\"/]')\n","really_nasties = set()\n","for i in range(len(items_transl)):\n","  rn = really_nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  really_nasties = really_nasties.union(set(rn))\n","print(really_nasties)\n","{'\\u200b', 'ñ', '.', 'à', '`', 'ó', '®', 'Á', '\\\\', 'è', '-', 'ë', '_', 'û', '»', '=', ',', '«', 'ú', 'ö', '#', 'ä', ';', 'ü', 'ô', '№', 'é', 'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","OK, looks good\n","'''\n","commented_cell = True  # prevent Jupyter from printing triple-quoted comments"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvzvPqmCQ2ZM","colab_type":"text"},"source":["#####Add 'delimited' and 'cleaned' data columns; shorten the titles of other columns so dataframe fits better on the screen"]},{"cell_type":"code","metadata":{"id":"Z35pqOYCtyZ7","colab_type":"code","outputId":"35bc8f05-0ff1-402f-dbab-5a8d4fa6aa5d","executionInfo":{"status":"ok","timestamp":1589049129836,"user_tz":240,"elapsed":3590,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":632}},"source":["%%time\n","items_delimited = items_transl.copy(deep=True)\n","# delete the wide \"item_name\" column so we can read more of the data table width-wise\n","items_delimited = items_delimited.drop(\"item_name\", axis=1).rename(columns = {'en_item_name':'item_name','item_category_id':'i_cat_id'})\n","items_in_test_set = test.item_id.unique()\n","items_delimited[\"i_tested\"] = False\n","for i in items_in_test_set:\n","  items_delimited.at[i,\"i_tested\"] = True\n","\n","# stopwords to remove from item names\n","stop_words = \"a,the,an,only,more,are,any,on,your,just,it,its,has,with,for,by,from\".split(\",\")\n","\n","nasty_symbols_re = re.compile(r'[^0-9a-zA-Z ]')  # remove all punctuation\n","really_nasty_symbols_re = re.compile(r'[^0-9a-zA-Z ,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@]')  # remove nasties, but leave behind the delimiters\n","delimiters_re = re.compile(r'[,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@\\u00AB\\u00BB~<>]')  # unicodes are << and >> thingies\n","delim_pattern_re = re.compile(r'^\\s+|\\s*[,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@\\u00AB\\u00BB~<>]+\\s*|\\s+$') # special symbols indicating a delimiter --> a space at start or end of item name will be removed at split time, along with ,;/()[]:\"-@~<<>><>\n","multiple_whitespace_re = re.compile(r'[ ]{2,}')\n","\n","cleanup_text = {}\n","cleanup_text[' dvd'] = re.compile(r'\\s+d$')  #several item names end in \"d\" -- which actually seems to indicate dvd (because the items I see are in category 40: Movies-DVD)... standardize so d --> dvd\n","cleanup_text['digital version'] = re.compile(r'digital in$') # several items seem to end in \"digital in\"... maybe in = internet?, but looking at nearby items/categories, 'digital version' looks standard\n","cleanup_text['bluray dvd'] = re.compile(r'\\bbd\\b|\\bblu\\s+ray\\b|\\bblu\\-ray\\b|\\bblueray\\b|\\bblue\\s+ray\\b|\\bblue\\-ray\\b')\n","cleanup_text['007 : james bond : skyfall'] = re.compile(r'\\bskyfall\\b|\\bskayfoll\\b')\n","cleanup_text[' and '] = re.compile(r'[\\&\\+]')\n","def maid_service(text):\n","    text = text.lower()\n","    for repl_text, pattern in cleanup_text.items():\n","        text = pattern.sub(repl_text, text)\n","    #r = re.compile(r'\\bskayfoll\\b')\n","    #text = r.sub('skyfall',text)  \n","    return text\n","\n","# ?remove dupes in cleaned text\n","\n","\n","def text_total_clean(text):\n","    #text: the original en_item_name\n","    #return: en_item_name made lowercase, stripped of \"really_nasties\" and multiple spaces\n","    text = maid_service(text)\n","    text = delimiters_re.sub(\" \", text)  # replace all delimiters with a space; other nasties get simply deleted\n","    text = nasty_symbols_re.sub(\"\", text)  # delete anything other than letters, numbers, and spaces\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = text.strip() # remove whitespace around string\n","    # lemmatize each word\n","    text = \" \".join([lemmatizer.lemmatize(w) for w in text.split(\" \") if w not in stop_words])\n","    return text\n","\n","def text_clean_delimited(text):\n","    #text: the original en_item_name\n","    #return: en_item_name made lowercase, stripped of \"really_nasties\" and multiple spaces, in a list of strings that had been separated by one of the above \"delimiters\"\n","    text = maid_service(text)\n","    text = really_nasty_symbols_re.sub(\"\", text)  # just delete the nasty symbols\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = delim_pattern_re.split(text)           # split item_name at all delimiters, irrespective of number of spaces before or after the string or delimiter\n","    text = [x.strip() for x in text if x != \"\"]           # remove empty strings \"\" from the list of split items in text, and remove whitespace outside text n-gram\n","    # lemmatize each word\n","    lemtext = []\n","    for ngram in text:\n","        lemtext.append(\" \".join([lemmatizer.lemmatize(w) for w in ngram.split(\" \") if w not in stop_words]))\n","    return lemtext\n","\n","# add item_category name with delimiter to the item_name, as this will be useful info for grouping similar items\n","items_delimited['item_name'] = items_delimited.apply(lambda x: item_categories_augmented.at[x.i_cat_id,'en_cat_name'] + \" : \" + x.item_name, axis=1)\n","\n","# add a column of simply cleaned text without any undesired punctuation or delimiters\n","items_delimited['clean_item_name'] = items_delimited['item_name'].apply(text_total_clean)\n","\n","# now add a column of lists of delimited (cleaned) text\n","items_delimited['delim_name_list'] = items_delimited['item_name'].apply(text_clean_delimited)\n","\n","# have a look at what we got with our delimited text globs\n","def maxgram(gramlist):\n","    maxg = 0\n","    for g in gramlist:\n","        maxg = max(maxg,len(g.split()))\n","    return maxg\n","items_delimited['d_len'] = items_delimited.delim_name_list.apply(lambda x: len(x))\n","items_delimited['d_maxgram'] = items_delimited.delim_name_list.apply(maxgram)\n","print(items_delimited.head())\n","print(\"\\n\")\n","print(items_delimited.describe())\n","print(\"\\n\")\n","print(items_delimited.iloc[31][:])\n","#items_delimited.to_csv(\"data_output/items_delimited.csv\", index=False)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   item_id  i_cat_id                                                                                                  item_name  i_tested                                                                                   clean_item_name  \\\n","0        0        40                                                                 Movie - DVD : ! POWER IN glamor (PLAST.) D     False                                                               movie dvd power in glamor plast dvd   \n","1        1        76  Program - Home & Office (Digital) : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]     False  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2        2        40                                                                     Movie - DVD : *** In the glory (UNV) D     False                                                                        movie dvd in glory unv dvd   \n","3        3        40                                                                       Movie - DVD : *** BLUE WAVE (Univ) D     False                                                                      movie dvd blue wave univ dvd   \n","4        4        40                                                                            Movie - DVD : *** BOX (GLASS) D     False                                                                           movie dvd box glass dvd   \n","\n","                                                                                           delim_name_list  d_len  d_maxgram  \n","0                                                                [movie, dvd, power in glamor, plast, dvd]      5          3  \n","1  [program, home and office, digital, abbyy finereader 12 professional edition full, pc, digital version]      6          6  \n","2                                                                         [movie, dvd, in glory, unv, dvd]      5          2  \n","3                                                                       [movie, dvd, blue wave, univ, dvd]      5          2  \n","4                                                                            [movie, dvd, box, glass, dvd]      5          1  \n","\n","\n","         item_id  i_cat_id  d_len  d_maxgram\n","count      22170     22170  22170      22170\n","mean  11,084.500    46.291  4.502      4.249\n","std    6,400.072    15.941  1.448      2.144\n","min            0         0      2          1\n","25%    5,542.250        37      3          3\n","50%   11,084.500        40      4          4\n","75%   16,626.750        58      5          5\n","max        22169        83     15         17\n","item_id                                                                                    31\n","i_cat_id                                                                                   37\n","item_name                                  Movie - Blu-Ray : 007: COORDINATES \"SKAYFOLL» (BD)\n","i_tested                                                                                 True\n","clean_item_name             movie bluray dvd 007 coordinate 007 james bond skyfall bluray dvd\n","delim_name_list    [movie, bluray dvd, 007, coordinate, 007, james bond, skyfall, bluray dvd]\n","d_len                                                                                       8\n","d_maxgram                                                                                   2\n","Name: 31, dtype: object\n","CPU times: user 3.25 s, sys: 977 µs, total: 3.25 s\n","Wall time: 3.25 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IUA58xQZ72NC","colab_type":"code","colab":{}},"source":["# make item df easier to read for the following stuff\n","items_clean_delimited = items_delimited.copy(deep=True).drop(\"item_name\", axis=1).rename(columns = {'clean_item_name':'item_name'})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"USoJANp3U9Ei","colab":{}},"source":["# %%time\n","# # Inspect the delimited 4-grams (4.64sec to run this cell without GPU, 4.01sec with GPU)\n","\n","# items_delimited_4gram = items_clean_delimited.copy(deep=True)\n","# items_delimited_4gram[\"d_4grams\"] = items_delimited_4gram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == 4]) # column contains all \"delimited\" 4-grams in the translation\n","\n","# g4 = items_delimited_4gram.d_4grams.apply(pd.Series,1).stack()\n","# g4.index = g4.index.droplevel(-1)\n","# g4.name = 'd_4grams'\n","# del items_delimited_4gram['d_4grams']\n","# items_delimited_4gram = items_delimited_4gram.join(g4)\n","\n","# print(items_delimited_4gram.tail())\n","# print(\"\\n\")\n","# freq_4grams = items_delimited_4gram.d_4grams.value_counts()\n","# print(f'Number of unique delimited 4-grams: {len(freq_4grams)}')\n","# print(f'Number of unique delimited 4-grams that are duplicated at least once: {len(freq_4grams[freq_4grams > 1])}')\n","# print(freq_4grams[1:12])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Uvreedy4YRVd"},"source":["#####Gather all info for duplicated n-grams in our delimited set"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7H2VxLddVqlJ","colab":{}},"source":["%%time\n","# Get all of the delimited n-grams that are duplicated at least once in item names (1min 24sec no gpu vs. 1min 11sec with gpu)\n","#  range of sizes of delimited phrases (number of 'words'):\n","min_gram = items_delimited.d_maxgram.min()\n","max_gram = items_delimited.d_maxgram.max()\n","\n","total_dupe_grams = 0\n","gram_freqs = {}   # dict will hold elements that are pd.Series with index = phrase, value = number of repeats in items database item names\n","for n in range(min_gram,max_gram+1):\n","    item_ngram = items_clean_delimited.copy(deep=True)\n","    item_ngram['delim_ngrams'] = item_ngram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == n])\n","\n","    grams = item_ngram.delim_ngrams.apply(pd.Series,1).stack()\n","    grams.index = grams.index.droplevel(-1)\n","    grams.name = 'delim_ngrams'\n","    del item_ngram['delim_ngrams']\n","    item_ngram = item_ngram.join(grams)\n","\n","    freq_grams = item_ngram.delim_ngrams.value_counts()\n","    print(f'Number of unique delimited {n}-grams: {len(freq_grams)}')\n","    grams_dupe = len(freq_grams[freq_grams > 1])\n","    print(f'Number of unique delimited {n}-grams that are duplicated at least once: {grams_dupe}\\n')\n","    if grams_dupe > 0:\n","        gram_freqs[n] = freq_grams[freq_grams > 1].copy(deep=True)\n","        total_dupe_grams += grams_dupe\n","print(f'\\nTotal number of unique, delimited, duplicated n-grams for all n: {total_dupe_grams}')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sw9OnsLGv5P0","colab_type":"code","outputId":"d00382b2-d258-4f10-f132-ec43575c9e24","executionInfo":{"status":"ok","timestamp":1589049454826,"user_tz":240,"elapsed":634,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# format data for feeding into word vector creator\n","\n","count_bins = [0, 2, 4, 8, 16, 32, 128, 1024, 32768]\n","idf_weights = [8,7,6,5,4,3,2,1]  # more weight for ngrams with lower counts\n","\n","notfirst = False\n","for n,s in gram_freqs.items():\n","    a=len(s)\n","    n_array = np.ones(a,dtype=np.int32)*n\n","    gram_count = s.values.astype(np.int32)\n","    gram_string0 = s.index.to_numpy(dtype='str')\n","    gram_string = [re.compile(r'\\b' + gs + r'\\b') for gs in gram_string0]  # I'm not looking for partial words; n-grams must match at word boundaries\n","    weight_bin = pd.cut(s,count_bins,labels=idf_weights,retbins=False).astype(np.int32)\n","\n","    if notfirst:\n","        n_arrays = np.concatenate((n_arrays,n_array))\n","        gram_counts = np.concatenate((gram_counts,gram_count))\n","        gram_strings = np.concatenate((gram_strings,gram_string))\n","        weight_bins = np.concatenate((weight_bins,weight_bin))\n","    else:\n","        n_arrays = n_array\n","        gram_counts = gram_count\n","        gram_strings = gram_string\n","        weight_bins = weight_bin\n","        notfirst = True\n","\n","print(n_arrays[:5],gram_counts[:5],gram_strings[:5],weight_bins[:5])\n","print(len(n_arrays),len(gram_counts),len(gram_strings),len(weight_bins))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1 1 1 1 1] [7138 5319 4333 3530 2746] [re.compile('\\\\bmovie\\\\b') re.compile('\\\\bdvd\\\\b')\n"," re.compile('\\\\bmusic\\\\b') re.compile('\\\\bgift\\\\b') re.compile('\\\\bpc\\\\b')] [1 1 1 1 1]\n","4040 4040 4040 4040\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EdrBr4rN7wr8","colab_type":"code","colab":{}},"source":["# use np matrix storage to speed this up... takes about 3 min vs. 8 min with pandas dataframe calculations\n","def make_word_vecs(item_names, ngram_re_patterns, ngram_ns, ngram_weights):\n","    \"\"\"Output is word vectors for input containing item names (english transl)\"\"\"\n","\n","    # create np zeros array of size (number of items, word vector length)\n","    n_items = len(item_names)\n","    wv_len = len(ngram_ns)\n","    item_vec_array = np.zeros((n_items, wv_len), dtype = np.int32)\n","\n","    for g in range(wv_len):\n","        gram_pattern = ngram_re_patterns[g] \n","        gram_len = ngram_ns[g]\n","        gram_weight = ngram_weights[g]\n","        for i in range(n_items):\n","            if gram_pattern.search(item_names[i]):\n","                item_vec_array[i,g] = 2 * gram_len * gram_weight  # use weighting function 2 * (n= length of ngram) * (idf weight from binning above)\n","    return item_vec_array\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrNVLGs1nLmR","colab_type":"code","outputId":"12c48190-4102-4a51-e65b-4963c82fb649","executionInfo":{"status":"ok","timestamp":1589049638242,"user_tz":240,"elapsed":163592,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","item_word_vectors = make_word_vecs(items_clean_delimited.loc[:,'item_name'].to_numpy(dtype='str'), gram_strings,n_arrays,weight_bins)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 2min 43s, sys: 42.6 ms, total: 2min 43s\n","Wall time: 2min 43s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_p1QepqQp1kv","colab_type":"code","colab":{}},"source":["# # intermediate point: can save word vectors here for the 21700 items\n","# np.savez_compressed('data_output/item_word_vectors.npz', arrayname = item_word_vectors)\n","# # ...\n","# iwv = np.load(\"data_output/item_word_vectors.npz\")\n","# item_word_vectors = iwv['arrayname']\n","# print(item_word_vectors.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_B2ZZPQigJmh"},"source":["#####Use scipy sparse matrices instead of pandas... faster, and less memory use"]},{"cell_type":"code","metadata":{"id":"oWtui0wwF1Yp","colab_type":"code","colab":{}},"source":["item_vec_matrix = sparse.csr_matrix(iwv2) #item_vectors2.values)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQEF0onOUtxz","colab_type":"code","outputId":"f421fbee-606d-43e4-bc82-ada49af45347","executionInfo":{"status":"ok","timestamp":1589049763665,"user_tz":240,"elapsed":5700,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","# <2sec for 21,700 items x 4000+ ngrams; output is a csr matrix of type int64\n","dots = item_vec_matrix.dot(item_vec_matrix.transpose()) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 1.58 s, sys: 500 ms, total: 2.08 s\n","Wall time: 2.08 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yFPX5PouV_aw","colab_type":"code","colab":{}},"source":["# wicked fast way to get top K # of items by dot product value (i.e., closest K items to the item of interest)\n","# https://stackoverflow.com/questions/31790819/scipy-sparse-csr-matrix-how-to-get-top-ten-values-and-indices\n","# also, great reference for speeding up python here: https://colab.research.google.com/drive/1nMDtWcVZCT9q1VWen5rXL8ZHVlxn2KnL\n","\n","@jit(cache=True)\n","def row_topk_csr(data, indices, indptr, K):\n","    \"\"\"Take a sparse scipy csr matrix, and for each column, find the K largest \n","    values in that column (like argmax or argsort[:K]).  Return the row indices \n","    and associated values for each column as two separate np arrays of \n","    length = number of columns in sparse matrix.  Inputs are data/indices/indptr\n","    of csr matrix, and integer K.  Call function like this:\n","    rows, vals = row_topk_csr(csr_name.data, csr_name.indices, csr_name.indptr, K)\n","    Use jit by importing jit and prange from numba, and decorating with\n","    @jit(cache=True) immediately before this function definition\n","    (adopted from https://stackoverflow.com/users/3924566/deepak-saini ) \"\"\"\n","\n","    m = indptr.shape[0] - 1\n","    max_indices = np.zeros((m, K), dtype=indices.dtype)\n","    max_values = np.zeros((m, K), dtype=data.dtype)\n","\n","    for i in prange(m):\n","        top_inds = np.argsort(data[indptr[i] : indptr[i + 1]])[::-1][:K]\n","        max_indices[i] = indices[indptr[i] : indptr[i + 1]][top_inds]\n","        max_values[i] = data[indptr[i] : indptr[i + 1]][top_inds]\n","\n","    return max_indices, max_values\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3D9OVzjXDlGm","colab_type":"code","outputId":"b4d880c8-8167-481b-8235-249703b19659","executionInfo":{"status":"ok","timestamp":1589049810005,"user_tz":240,"elapsed":16466,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","dots.setdiag(0)\n","closest10_indices, highest_values = row_topk_csr(dots.data, dots.indices, dots.indptr, K=10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 5.52 s, sys: 4.97 ms, total: 5.52 s\n","Wall time: 5.52 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-xjDRQcM9odF","colab_type":"code","outputId":"e1aba30d-8f8f-4630-ce11-1c7347cfda26","executionInfo":{"status":"ok","timestamp":1589049811784,"user_tz":240,"elapsed":233,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["print(closest10_indices.shape)\n","print(closest10_indices[:10,:])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(22170, 10)\n","[[14329 19060  8995 19062 11949 11015 18529 18530 18531 18532]\n"," [ 1155  1154  1156  1152  1153  3876  3873  3878  3874  3875]\n"," [17361 16692 16916 16917 16973 17125 17191 17212 17213 17251]\n"," [19630  9029 20027 10463  9633 10462  2486  4662 12427 13115]\n"," [11360  9451  9525 10190 10264 10573 10590 11287 11303 11374]\n"," [10468 12642  9856     6     7  9908     9 21903  8964 10344]\n"," [    7 10468 12642     5  9856  9908     9 21903  8964 10344]\n"," [    6 14894 17179  8964 12338 12642 21903 10468 10344  9908]\n"," [17992 17182 17181 17180 17179 17178 17177 17176 17175 17173]\n"," [10468 12642     5     6     7  9856  9908 21903  8964 10344]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OfOzk_xWnF7q","colab_type":"code","outputId":"9c2296ed-2954-466a-89b2-2a48b36eea2a","executionInfo":{"status":"ok","timestamp":1589049823924,"user_tz":240,"elapsed":1382,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["similar_items = pd.DataFrame({'item_id':range(22170)}) #,'close_item_idx':closest10_indices,'close_item_dot':highest_values})\n","similar_items['close_item_idx'] = [closest10_indices[x] for x in range(22170)]\n","similar_items['close_item_dot'] = [highest_values[x] for x in range(22170)]\n","similar_items = similar_items.merge(items_clean_delimited[['item_id','i_tested','i_cat_id']], on='item_id')\n","similar_items['close_item_cat'] = similar_items.close_item_idx.apply(lambda x: [items.at[i,'item_category_id'] for i in x])\n","print(similar_items.head())\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   item_id                                                          close_item_idx                                            close_item_dot  i_tested  i_cat_id                            close_item_cat\n","0        0   [14329, 19060, 8995, 19062, 11949, 11015, 18529, 18530, 18531, 18532]        [264, 264, 264, 264, 264, 264, 264, 264, 264, 264]     False        40  [40, 40, 37, 40, 37, 40, 37, 40, 40, 40]\n","1        1            [1155, 1154, 1156, 1152, 1153, 3876, 3873, 3878, 3874, 3875]  [10520, 1328, 1328, 1264, 1240, 932, 932, 928, 928, 928]     False        76  [75, 76, 76, 76, 75, 28, 29, 23, 19, 23]\n","2        2  [17361, 16692, 16916, 16917, 16973, 17125, 17191, 17212, 17213, 17251]        [264, 264, 264, 264, 264, 264, 264, 264, 264, 264]     False        40  [38, 40, 38, 38, 40, 40, 37, 40, 37, 40]\n","3        3      [19630, 9029, 20027, 10463, 9633, 10462, 2486, 4662, 12427, 13115]        [108, 108, 108, 108, 108, 108, 104, 104, 104, 104]     False        40  [40, 37, 40, 40, 40, 37, 56, 59, 55, 43]\n","4        4    [11360, 9451, 9525, 10190, 10264, 10573, 10590, 11287, 11303, 11374]        [804, 804, 804, 804, 804, 804, 804, 804, 804, 804]     False        40  [43, 43, 28, 23, 43, 43, 30, 43, 43, 43]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OWuI-nw27XFH","colab_type":"code","colab":{}},"source":["# create a graph with nodes = item ids in test set, and edge weights = dot product values\n","\n","# we will use the \"community\" algorithms to determine useful groupings of other items around/including the test items\n","# ##### start with a graph containing the 5100 items in the test set as starter nodes, and add in the 10 highest-match wordvector items if dot product > threshold\n","# TAKING A LEAP... gonna try with 21700 full items dataset / top 10 matches\n","\n","edge_threshold = 100  # dot product (edge weight) must be greater than this for two item_ids to be connected in the graph\n","\n","graph_items = similar_items[['item_id','close_item_idx']].copy(deep=True).explode('close_item_idx').reset_index(drop=True)\n","graph_weights = similar_items[['item_id','close_item_dot']].copy(deep=True).explode('close_item_dot').reset_index(drop=True)\n","graph_items['weight'] = testweights.loc[:]['close_item_dot']\n","graph_items.columns = ['item1_id','item2_id','weight']\n","\n","print(len(graph_items))\n","graph_items = graph_items[graph_items.weight > edge_threshold]\n","print(len(graph_items))\n","graph_items.head()\n","# depending on threshold, we may end up dropping some of the test items (for example, we lose item 22154 if threshold = 150, but not if threshold = 100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cNTMKGOREZGL","colab_type":"code","colab":{}},"source":["%%time\n","# import pandas df into weighted-edge graph:\n","G = nx.from_pandas_edgelist(graph_items, 'item1_id', 'item2_id', ['weight'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iR2U0g8iY0AN","colab_type":"code","colab":{}},"source":["%%time\n","# employ a clustering method that utilizes the edge weights\n","communities2 = community.asyn_lpa_communities(G, weight='weight', seed=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtbKGa2CcjJa","colab_type":"code","colab":{}},"source":["num_communities = 0\n","community_items = set()\n","cluster_nodes = []\n","n_nodes = []\n","weight_avgs = []\n","weight_sums = []\n","weight_maxs = []\n","weight_mins = []\n","weight_stds = []\n","for i,c in enumerate(communities2):\n","    num_communities += 1\n","    community_items = community_items | set(c)\n","    nodelist = list(c)\n","    n_nodes.append(len(nodelist))\n","    edgeweights = []\n","    for m in range(n_nodes[-1]-1):\n","        for n in range(m+1,n_nodes[-1]):\n","            try:\n","                edgeweights.append(G.edges[nodelist[m], nodelist[n]]['weight'])\n","            except:\n","                pass\n","    cluster_nodes.append(nodelist)\n","    weight_avgs.append(np.mean(edgeweights))\n","    weight_sums.append(np.sum(edgeweights))\n","    weight_maxs.append(np.max(edgeweights))\n","    weight_mins.append(np.min(edgeweights))\n","    weight_stds.append(np.std(edgeweights))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vw2SkxaFd1qm","colab_type":"code","colab":{}},"source":["weight_avgs = [round(x) for x in weight_avgs]\n","community_df = pd.DataFrame({'n_nodes':n_nodes,'w_avg':weight_avgs,'w_sum':weight_sums,'w_max':weight_maxs,'w_min':weight_mins,'w_std':weight_stds,'item_id':cluster_nodes})\n","print(community_df.head())\n","print(\"\\n\")\n","print(community_df.describe())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmZVij-EgUww","colab_type":"code","colab":{}},"source":["community_df.w_avg.nunique()\n","# can't use this as a category code because not unique among clusters,\n","# but I want to use the average cluster weights property to encode the cluster category\n","# (higher numbers for category code --> stronger clustering; may be useful to have this correlation instead of random generation of category codes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OhgonOqnhH5s","colab_type":"code","colab":{}},"source":["# so, I will sort on w_avg, then on number of nodes as perhaps the next most important defining characteristic of a given cluster\n","#  and, to make the categorization unique, I will take the w_avg value and sum it with the index (row number)...\n","#     (with the sorting, this favors even more the clusters with high average item-to-item similarity)\n","community_df = community_df.sort_values(['w_avg','n_nodes']).reset_index(drop=True)\n","community_df['item_cluster_id'] = community_df.index + community_df['w_avg']\n","community_df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuEH3MFyjuNH","colab_type":"code","colab":{}},"source":["# unravel / explode the cluster node lists... we know this will not duplicate item ids, from the counting we did above\n","item_clusters = community_df.copy(deep=True).explode('item_id').reset_index().rename(columns = {'index':'cluster_number'})\n","item_clusters.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OSnYLCamXr2S","colab_type":"code","colab":{}},"source":["items_clustered = items_clean_delimited[['item_id','i_cat_id','i_tested','item_name']].merge(item_clusters,on='item_id',how='left')\n","items_clustered = items_clustered[['item_id','i_cat_id','item_cluster_id','i_tested','cluster_number','n_nodes','w_avg','w_sum','w_max','w_min','w_std','item_name']]\n","items_clustered.columns = ['item_id','item_category_id','item_cluster_id','item_tested','cluster_number','n_items_in_cluster','w_avg','w_sum','w_max','w_min','w_std','item_name']\n","print(items_clustered.head())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KtvuccfmrWJy","colab_type":"code","colab":{}},"source":["# how many test items are represented by clusters?\n","tested_clustered = items_clustered[items_clustered.item_tested==True][['item_id','item_category_id','item_cluster_id','item_name']]\n","tested_clustered['unclustered'] = tested_clustered.apply(lambda x: np.NaN if x.item_cluster_id > 0  else x.item_id, axis = 1)\n","print(tested_clustered.head(10))\n","print('\\n')\n","print(tested_clustered.item_id.nunique())\n","unclustered = tested_clustered.unclustered.unique()\n","unclustered = [x for x in unclustered if x > 0]\n","print(len(unclustered))\n","train_items = sales_train.item_id.unique()\n","print(len(train_items))\n","print(len(items))\n","untrained = [x for x in unclustered if x not in train_items]\n","print(len(untrained))\n","print(len(items) - len(train_items) - len(untrained))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nuWZ7bPJ4bR2","colab_type":"code","colab":{}},"source":["# revert to original item_category_id if item is not in clustered items\n","items_clustered['cluster_code'] = items_clustered.apply(lambda x: x.item_cluster_id if x.item_cluster_id > 0 else x.item_category_id, axis = 1)\n","items_clustered.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFs7GZWs6bkF","colab_type":"code","colab":{}},"source":["# save what we have; maybe refine later\n","\n","compression_opts = dict(method='gzip',\n","                        archive_name='items_clustered_21700.csv')  \n","items_clustered.to_csv('data_output/items_clustered_21700.csv.gz', index=False, compression=compression_opts)"],"execution_count":0,"outputs":[]}]}