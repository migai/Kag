{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MG_EDA_v3.0 readable.ipynb","provenance":[{"file_id":"1gcbeu-d1GUUzznZwTzfqYYbaD6cJ7EQ4","timestamp":1588238522691},{"file_id":"1pSGNRDJGzdeI69bw1zWefzPifBq-rv9H","timestamp":1588151557805},{"file_id":"1hq-ivO1BBtc5IC5xd-JdH8HNAQ81bkRf","timestamp":1587386702728},{"file_id":"1I7DWo2B7q7g9Ne2khD11YGTq_gD2FoaT","timestamp":1587321559573},{"file_id":"1fyZv-jgb8twsCBQwPxjgk_XSYA6dOa2t","timestamp":1587303588700},{"file_id":"1iKsplqpLQQZqdr3Trflk7TapksgkQXjX","timestamp":1587145642564},{"file_id":"https://github.com/migai/Kag/blob/master/Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587076517706}],"collapsed_sections":["YPp_Nesy2yxn","r_Oe76PW3aoN","UKQVGfU2s_Ga","ybaxHFNbJDH9","LyLQLqBcOnLt","NdR16Fu7tt6S","CH3WGLDS0gNL","ZezbXafHXOkd","3Ir4z6y488C0","4-EZjbv-8HJi","xkkzLCB387kR","jgA5Z4IeXmOb","uD3G2taWX9pq","bOd1bPrNh5Kw","nSRNAtTYV3mW","xUIfCz4U4sVF"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn","colab_type":"text"},"source":["#**Final Project for Coursera's 'How to Win a Data Science Competition'**\n","\n","**Optional Section - Description of Project Objectives, Inputs, Outputs**\n","\n","Andreas Theodoulou and Michael Gaidis (April - May, 2020)"]},{"cell_type":"markdown","metadata":{"id":"r_Oe76PW3aoN","colab_type":"text"},"source":["##**About this Competition**\n","\n","You are provided with **daily** historical sales data. The task is to forecast the total amount of products (irrespective of product type;  we just want the sum of all products) sold in every shop for the test set (the **month** of November, 2015). Note that the list of shops(!) and products slightly *changes every month*. Creating a robust model that can handle such situations is part of the challenge.\n"]},{"cell_type":"markdown","metadata":{"id":"1sWwRJmtIs0C","colab_type":"text"},"source":["###**File descriptions**\n","\n","***sales_train.csv*** - the training set. Daily historical data from January 2013 to October 2015.\n","\n","***test.csv*** - the test set. You need to forecast the sales for these shops and products for November 2015.\n","\n","***sample_submission.csv*** - a sample submission file in the correct format (two columns: \"shop ID number\" and \"total number of products sold in Nov. 2015\")\n","\n","***items.csv*** - item names, their corresponding item_categories IDs, and item IDs to link with the other files\n","\n","***item_categories.csv***  - item category names and corresponding IDs to link with the other files\n","\n","***shops.csv***- shop names and corresponding IDs to link with the other files"]},{"cell_type":"markdown","metadata":{"id":"sENf0d8HImag","colab_type":"text"},"source":["###**Data fields**\n","\n","***ID*** - an Id that represents a (Shop, Item) tuple within the test set\n","\n","***shop_id*** - unique identifier of a shop\n","\n","***item_id*** - unique identifier of a product\n","\n","***item_category_id*** - unique identifier of item category\n","\n","***item_cnt_day*** - number of products sold. You are predicting a monthly amount of this measure\n","\n","***item_price*** - current price of an item\n","\n","***date*** - date in format dd/mm/yyyy\n","\n","***date_block_num*** - a consecutive month number. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n","\n","***item_name*** - name of item\n","\n","***shop_name*** - name of shop\n","\n","***item_category_name*** - name of item category"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UKQVGfU2s_Ga"},"source":["##**About This IPynb Notebook**\n","If you are a reviewer or plan to use this notebook for your own learning, here are some tips to help you understand the layout of this notebook, and the rationale behind it.  We used Google Colab to develop our code, so you may notice certain aspects of the notebook that are not conventional in a standard Jupyter framework.  Hopefully any such deviations do not cause issues for you."]},{"cell_type":"markdown","metadata":{"id":"F6AfWZW0JRHg","colab_type":"text"},"source":["###**For Jupyter Notebook Readability:**\n","Many sections are grouped so they may be collapsed for easier navigation to the code of interest.  (For example, the code to create new features and save them to a csv file exists in this notebook, but after that is done, a simple csv import is all that is needed, and we keep the code in the notebook just for future reference -- not to re-run every time we start a Google Colab runtime.)  Unfortunately, I haven't found a way in Colab to set cell metadata to disable running these unnecessary cells when selecting the \"Run All\" or \"Run Before\" menu options for the notebook.  Apparently this can be done in a standard (non-Colab) Jupyter notebook, or maybe using a plug-in like the one [here](https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tree/master/src/jupyter_contrib_nbextensions/nbextensions/freeze).\n"]},{"cell_type":"markdown","metadata":{"id":"p37LVopvJJP2","colab_type":"text"},"source":["###**For The Development Team:**\n","**An effort has been made to comment-out non-essential cells from the development team's viewpoint** (especially for long-running code, or code that might overwrite datasets).  In this way, when starting a new Google Colab runtime, the development team can choose a cell far down in the IPynb notebook, and use the menu option \"run before\" to run all code cells necessary to import the relevant data into our Colab environment to support ongoing code development beneath that chosen cell."]},{"cell_type":"markdown","metadata":{"id":"Pq2h8-InJEN8","colab_type":"text"},"source":["###**For People Not On The Development Team:**\n","If someone *not* on the development team wishes to recreate the operations in this notebook, that person may expand sections and uncomment code cells as desired.  If a code block is commented out, we will try to indicate if the danger is from overwriting an existing data file, or if there is inconvenience from spending substantial time waiting for that code to execute."]},{"cell_type":"markdown","metadata":{"id":"ufy-J0xC2efV","colab_type":"text"},"source":["###**If You Are Using The Google Colab VM To Execute This Notebook**\n","* Click **File -> Save a copy in Drive** and accordingly save your progress in your own personal Google Drive.\n","* Click **Runtime -> Change runtime type** and select **GPU** hardware acceleration if required / desired."]},{"cell_type":"markdown","metadata":{"id":"sqDRwjZKWR_w","colab_type":"text"},"source":["#FOR THE DEVELOPMENT TEAM BEFORE FINAL PROJECT SUBMISSION:\n","   -- ***To Do* Items** (from Coursera/Kaggle class info) --\n","\n","**Optional Section - Intended for Project Development Team to compare with actual work done, and verify we have met the course requirements.  This code may be deleted when project is complete.**"]},{"cell_type":"markdown","metadata":{"id":"H_wYp7WMGXxy","colab_type":"text"},"source":["**Course Instructors' Requirements**\n","\n","Follow these guidelines to simplify life for you and for the fellow learner. This is a must.\n","\n","1. The solution runs without errors. \n","\n","2. Specify required libraries and their versions in the first notebook cell or in requirements.txt -- https://pip.readthedocs.io/en/1.1/requirements.html This will save a lot of time for other students, assessing your project.\n","\n","3. Serialize the trained model to disk. This enables code to use the trained model to make predictions on the test data without re-training the model (which is typically much more time-intensive)"]},{"cell_type":"markdown","metadata":{"id":"82JJ4czPBVtr","colab_type":"text"},"source":["**Review Criteria for Coursera Peer Review**\n","\n","Pay attention to the following criteria. Try to complete most of them and present results in a form that can be easily assessed.\n","\n","1. Clarity\n","\n",">* Clear step-by-step instructions on how to produce the final submit file are provided.\n","\n",">* Code has comments where it is needed and meaningful function names\n","\n","2. Feature preprocessing and generation with respect to models\n","\n",">* Several simple features are generated\n","\n",">* For non-tree-based models, preprocessing is used or the absence of it is explained\n","\n","3. Feature extraction from text and images\n","\n",">* Features from text are extracted\n","\n",">* Special preprocessings for text are utilized (TF-IDF, stemming, levenshtening...)\n","\n","4. EDA\n","\n",">* Several interesting observations about data are discovered and explained\n","\n",">* Target distribution is visualized, time trend is assessed\n","\n","5. Validation\n","\n",">* Type of train/test split is identified and used for validation\n","\n",">* Type of public/private split is identified\n","\n","6. Data leakages\n","\n",">* Data is investigated for data leakages and investigation process is described\n","\n",">* Found data leakages are utilized\n","\n","7. Metrics optimization\n","\n",">* Correct metric is optimized\n","\n","8. Advanced Features I: mean encodings\n","\n",">* Mean-encoding is applied\n","\n",">* Mean-encoding is set up correctly, i.e. KFold or expanding scheme are utilized correctly\n","\n","9. Advanced Features II\n","\n",">* At least one feature from this topic is introduced (Statistics & Distance-Based Features, Matrix Factorizations, Feature Interactions, tSNE)\n","\n","10. Hyperparameter tuning\n","\n",">* Parameters of models are roughly optimal\n","\n","11. Ensembles\n","\n",">* Ensembling is utilized (linear combination counts)\n","\n",">* Validation with ensembling scheme is set up correctly, i.e. KFold or Holdout is utilized\n","\n",">* Models from different classes are utilized (at least two from the following: KNN, linear models, RF, GBDT, NN)"]},{"cell_type":"markdown","metadata":{"id":"ybaxHFNbJDH9","colab_type":"text"},"source":["#**Workflow**"]},{"cell_type":"markdown","metadata":{"id":"e0A9t1ISJX0t","colab_type":"text"},"source":["##0. Configure Environment\\*\n","\n","\n","*   Install packages not present in our Google Colab environment\n","*   Import python libraries / modules needed for EDA / Feature Gen / Modeling\n","\n","</br>\n","\n","\\*Our competition team utilized Google Colab for our computations.  We used GitHub for remote file storage / git coordination with each other.  We each have local repositories linking GitHub remote to our individual Google Drives.  As such, you will find below some code for enabling this file manipulation and use of Colab.  We have made every attempt to also provide the reader of this notebook with alternatives, such as Colab without git, and local machine hosting without Colab or git.  (Code is provided for all alternatives to access any \"augmented\" or newly-created files that we have stored in our public GitHub repository.  You will not need to have your own local copy of any data.  You will only need the IPynb notebook(s) containing our code, and will need to ensure your python and package versions are consistent with running this code. -- ref. Colab versions of packages as of April 30, 2020)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sIZAhpmSA-hV"},"source":["##1. Load Data\n","\n","*   Load competition data files\\*\n","*   Load any utility \"helper\" code files\n","\n","</br>\n","\n",">\\* We also load \"augmented\" data files that are essentially the original competition datafiles, after subjected to our EDA, Cleanup, and Feature Generation.  In this way, the entire EDA / Cleanup / Feature Generation code does not have to be executed each time we open this IPynb notebook.  However, the original data files and (commented-out) code exists below if you wish to step through each code cell to generate your own \"augmented\" files.  **NOTE:** some of the \"augmented\" files contain manually-entered information, so for complete conformity with our feature generation and model inputs, you will need to rely on these \"augmented\" files to some extent. (*note to self* - to do: set up code cells to essentially perform these manual entries, so nobody will be dependent on anything but the original data files and our IPynb code)"]},{"cell_type":"markdown","metadata":{"id":"4oD5KLMaKNaX","colab_type":"text"},"source":["##2. Explore Data / Clean Data / Generate Features\n","\n","\n","*   Data formatting and translating\n","*   Observations about the provided competition data\n","*   Grouping and statistical descriptions of the provided competition data\n","*   Data visualizations and correlations\n","*   Cleaning (outliers, missing entries, NaNs, irrelevant data, ...)\n","*   Signs of data leakage --> Data Manipulation and Model Adjustments\n","*   Feature Generation (pre-processing for initial training; to be revised based on model performance and choice of model/ensemble)\n","*   **Save Generated Features** to GitHub repo so we do not have to recompute\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bhJFCP5_MwA6","colab_type":"text"},"source":["##3. Quick Modeling (set up framework for more complex model improvement)\n","\n","\n","*   Choose and implement a fast and simple approach for train/val data splitting\n","*   Choose a simple and fast evaluation metric (comparable to Kaggle's metric)\n","*   Choose a simple, but appropriate, model to use (minimal \\# hyperparameters)\n","*   Train the model, check for major issues\n","*   Save the model parameters, etc., along with version control\n","*   Submit model to Kaggle/Coursera to verify proper formatting of entry and evaluate how much improvement is required\n","*   Verify that Kaggle/Coursera test performance is reasonably close to validation metric\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8qpxM8ICOjN8","colab_type":"text"},"source":["##4. Refine the Model and the Features\n","\n","*   Evaluate suitability of inital model.  Determine if we continue with this type of model and/or what other models to include if we utilize ensemble methods.\n","*   Consider alternative metrics for training and validation (speed, accuracy)\n","*   Explore hyperparameter tuning (vs. resources and time limitations)\n","*   Adjust methods of train/val splitting if desirable and timely\n","*   **Version control** using git and GitHub remote repo\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VYVy_LhOSAZR","colab_type":"text"},"source":["##5. Finalize Model and Documentation\n","\n","\n","*   Restart kernel, clean any possible lingering variables\n","*   Train and tune hyperparamers until you run out of time\n","*   Clean up code for readability by reviewers\n","*   Submit model\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5hdrirUjS0H6","colab_type":"text"},"source":["#0. Configure Environment\n","\n","*  **Section 0.1 is optional**\n","\n","*  **Section 0.2 is NOT optional**"]},{"cell_type":"markdown","metadata":{"id":"LyLQLqBcOnLt","colab_type":"text"},"source":["##0.1) Install Packages (for Google Colab)\n","**Optional Section**\n","\n","To run this IPynb in its entirety, you will need to use some packages not found in Google Colab.  We install them here."]},{"cell_type":"code","metadata":{"id":"ubh9NBYfPcyC","colab_type":"code","colab":{}},"source":["'''\n","############################################################\n","\n","# Run this cell only if you plan to redo all preprocessing and feature generation\n","#   (otherwise, you can use existing, already saved data files, and eliminate many hours of runtime)\n","#   ** If you are not running in Google Colab, remove the !pip statement, and instead make sure googletrans 2.4.0 is installed on your machine\n","\n","############################################################\n","\n","# Translating Package using Google API\n","#   used for translating Russian text in dataframes, so we can better understand potential features, data leaks, or outliers\n","!pip install googletrans  # version 2.4.0\n","\n","# Assuming you are planning to use this package (because you ran this cell and imported the googletrans package),\n","#  we will go ahead and import the library and instantiate a Translator class\n","# TIP: Google limits the rate and size of queries to its free translation service.  There is minimal information\n","#   online about what these limits are. (I believe they change depending on other workload.)\n","#   In a code cell far, far below, you will find us using a 2 second delay between requests to the translation\n","#   service.  This was found necessary for arbitrarily large numbers of requests (e.g., when translating\n","#   21,700 rows of the \"items\" dataframe).  Smaller numbers of requests to the service require no delay.\n","#   (The 60-row \"shops\" and 84-row \"item_categories\" dataframes did not require any such throttling.)\n","from googletrans import Translator\n","translator = Translator()\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1_RXcGOyq_s","colab_type":"code","colab":{}},"source":["'''\n","############################################################\n","\n","# Run this cell only if you plan to redo preprocessing and feature generation related to shop location\n","#   (otherwise, you can use existing, already saved data files, and eliminate many hours of runtime)\n","#   ** If you are not running in Google Colab, remove the !pip statement, and instead make sure geopy 1.17.0 is installed on your machine\n","\n","############################################################\n","\n","# Geocoding library \n","#   used for creating features from shop location\n","!pip install geopy   # version 1.17.0\n","\n","# Assuming you are planning to use this package (because you ran this cell and imported the geopy package),\n","#  we will go ahead and import the library elements and instantiate two rate-limited geocoders\n","from geopy.geocoders import Nominatim\n","from geopy.geocoders import GeoNames\n","from geopy.extra.rate_limiter import RateLimiter\n","\n","# Utilize \"RateLimiter\" to limit location queries to one per second, as the free services tend to throttle rate of use\n","# We will use Nominatim for location, and GeoNames for population\n","# Note that for Nominatum, the mgaidis@yahoo.com is an arbitrary identifier to help \n","#   the service detect the rate at which a particular user is querying the service (and throttle as necessary)\n","# For GeoNames, you do need to set up a (free) account at their site before using the python package.  \n","#   The user_agent in GeoNames must correspond to the email you used to create that GeoNames account.\n","nominatum_service = Nominatim(timeout=10, user_agent = \"mgaidis@yahoo.com\", format_string=\"%s, Russia\")\n","nominatum_geocode = RateLimiter(nominatum_service.geocode, min_delay_seconds=1)\n","geonames_service = GeoNames(username='gaidis', timeout=10, user_agent=\"mgaidis@yahoo.com\")  # be sure to enable free web services when creating geonames account\n","geonames_geocode = RateLimiter(geonames_service.geocode, min_delay_seconds=1)\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ruw_WyRxhqpx"},"source":["##0.2) Import Libraries/Modules\n","**NOT OPTIONAL**"]},{"cell_type":"code","metadata":{"id":"sTVAxnMnenrB","colab_type":"code","colab":{}},"source":["# General python libraries/modules used throughout the notebook\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","\n","import os\n","from itertools import product\n","import re\n","import json\n","import time\n","from time import sleep, localtime, strftime\n","import pickle\n","\n","'''\n","# NLP packages\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","'''\n","\n","'''\n","# ML packages\n","from sklearn.linear_model import LinearRegression\n","\n","!pip install catboost\n","from catboost import CatBoostRegressor \n","'''\n","\n","# Magics\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NoOf7oi8Oe-Q","colab_type":"code","colab":{}},"source":["# Notebook formatting\n","# Adjust as per your preferences.  I'm using a FHD monitor with a full-screen browser window containing my IPynb notebook\n","\n","# format pandas output so we can see all the columns we care about (instead of \"col1  col2  ........ col8 col9\", we will see \"col1 col2 col3 col4 col5 col6 col7 col8 col9\" if it fits inside display.width parameter)\n","pd.set_option(\"display.max_columns\",30)  \n","pd.set_option(\"display.max_rows\",100)     # Override pandas choice of how many rows to show, so, for example, we can see the full 84-row item_category dataframe instead of the first few rows, then ...., then the last few rows\n","pd.set_option(\"display.width\", 200)       # Similar to the above for showing more rows than pandas defaults to, we can show more columns than default, if we tune this to our monitor window size\n","\n","#pd.set_option(\"display.precision\", 3)  # Nah, this is helpful, but below is even better\n","#Try to convince pandas to print without decimal places if a number is actually an integer (helps keep column width down, and highlights data types)\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"miCS3XqUhDXz"},"source":["#1. Load Data and Code Utility Files\n","\n","*  **Section 1.1 is NOT optional**\n","\n","*  **Section 1.2 is NOT optional** (but, choose only 1 of three file load methods)\n","\n","The list of data files contains the original files provided by Kaggle, which can be run through this notebook to generate the new features used in our model.\n","\n","As the data set preprocessing and feature generation can take several hours, this notebook is set up to also read data files that contain the features we created.  This allows you to skip execution of the code cells that take a long time to generate features. (They will be highlighted, so you will know which ones to skip.)\n","\n","The code cells below therefore load the original and the augmented data files, and allow you to choose whether or not to execute \"re-creation\" of the augmented data files.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kjWzoXizEa5O","colab_type":"text"},"source":["##1.1) Enter Data File Names and Paths\n","\n","**NOT Optional**"]},{"cell_type":"code","metadata":{"id":"p9vsd3EynZLO","colab_type":"code","colab":{}},"source":["#  FYI, data is coming from a public repo on GitHub at github.com/migai/Kag\n","# List of the data files (path relative to GitHub master), to be loaded into pandas DataFrames\n","data_files = [  \"readonly/final_project_data/items.csv\",\n","                \"readonly/final_project_data/item_categories.csv\",\n","                \"readonly/final_project_data/shops.csv\",\n","                \"readonly/final_project_data/sample_submission.csv.gz\",\n","                \"readonly/final_project_data/sales_train.csv.gz\",\n","                \"readonly/final_project_data/test.csv.gz\",\n","                \"data_output/shops_transl.csv\",\n","                \"data_output/shops_augmented.csv\",\n","                \"data_output/item_categories_transl.csv\",\n","                \"data_output/item_categories_augmented.csv\",\n","                \"data_output/items_transl.csv\"  ]\n","\n","\n","# Dict of helper code files, to be loaded and imported {filepath : import_as}\n","code_files = {}  # not used at this time; example dict = {\"helper_code/kaggle_utils_at_mg.py\" : \"kag_utils\"}\n","\n","\n","# GitHub file location info\n","git_hub_url = \"https://raw.githubusercontent.com/migai\"\n","repo_name = 'Kag'\n","branch_name = 'master'\n","base_url = os.path.join(git_hub_url, repo_name, branch_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPrPvh7sorJd","colab_type":"text"},"source":["##1.2) Load Data Files\n","\n","**NOT Optional** (but, choose only one of the 3 methods below)\n","</br></br>\n","Three options are provided for loading the source data files and similar data files augmented with additional preprocessing material.  The files are located in GitHub, within a public repo [migai/Kag](https://github.com/migai/Kag)\n","\n","1. **For Development Team:**  If you are running in Google Colab with git (and have cloned the repo from GitHub to your local Google Drive already)... contact me if you wish to do this, and I can get you set up with an appropriate git token, etc.)\n","\n","2. If you are running in Google Colab without git\n","\n","3. If you are running on a local machine / not using Colab\n","\n","**Expand the appropriate ipynb section for your needs, and use those code cells to load the data**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YtZIsQbFugpc"},"source":["####**Option 1:  You are running in Colab with local git repo on Google Drive**\n","(e.g., you are a code developer on this team --> **USE THIS**)"]},{"cell_type":"code","metadata":{"id":"CUIE1PVjSAmg","colab_type":"code","outputId":"2c60a05e-3b87-43e4-b95b-868022dbfbba","executionInfo":{"status":"ok","timestamp":1588287141523,"user_tz":240,"elapsed":21581,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, then paste it into the input box and hit 'enter' to complete mounting of the drive\n","from google.colab import drive  \n","drive.mount('/content/drive')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dy5i7jl00oX-","colab_type":"code","outputId":"ffc1e065-464a-4e0b-a57e-4e8c9a913b45","executionInfo":{"status":"ok","timestamp":1588287638983,"user_tz":240,"elapsed":2645,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["'''\n","############################################################\n","############################################################\n","'''\n","# Replace this path with the path on *your* Google Drive where the repo master branch is stored\n","#   (on GitHub, the remote repo is located at github.com/migai/Kag --> below is my cloned repo location)\n","GDRIVE_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\"\n","'''\n","############################################################\n","############################################################\n","'''\n","\n","%cd \"{GDRIVE_REPO_PATH}\"\n","\n","print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","\n","# Loop to load the data files into appropriately-named pandas DataFrames\n","for path_name in data_files:\n","  filename = path_name.rsplit(\"/\")[-1]\n","  data_frame_name = filename.split(\".\")[0]\n","  exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","  if data_frame_name == 'sales_train':\n","    sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","  print(\"Data Frame: \" + data_frame_name)\n","  print(eval(data_frame_name).head(2))\n","  print(\"\\n\")\n","\n","\"\"\"\n","unused at this time...\n","\n","# Load in any helper functions from the code_files dictionary\n","#    dictionary key is the path (replace \"/\"\" with \".\" when using Google Drive + Colab), \n","#      and dictionary value is the module reference name\n","#    note that the directory chain on GitHub (and local repo) from current directory down to the .py file\n","#      must include a \"__init__.py\" file (it can be empty) in each of the directories\n","for filepath, module in code_files.items():\n","  path_name = filepath.replace(\"/\",\".\")[:-3]  # Google Drive reference does not use .py, and uses a \".\" instead of \"/\" for directory delineation\n","  exec(\"import \" + path_name + \" as \" + module)\n","\n","# Sanity check test\n","test1 = kag_utils.add_one(2)\n","print(test1)\n","\"\"\""],"execution_count":13,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","Loading Files from Google Drive repo into Colab...\n","\n","Data Frame: items\n","                                           item_name  item_id  item_category_id\n","0          ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0                40\n","1  !ABBYY FineReader 12 Professional Edition Full...        1                76\n","\n","\n","Data Frame: item_categories\n","        item_category_name  item_category_id\n","0  PC - Гарнитуры/Наушники                 0\n","1         Аксессуары - PS2                 1\n","\n","\n","Data Frame: shops\n","                       shop_name  shop_id\n","0  !Якутск Орджоникидзе, 56 фран        0\n","1  !Якутск ТЦ \"Центральный\" фран        1\n","\n","\n","Data Frame: sample_submission\n","   ID  item_cnt_month\n","0   0           0.500\n","1   1           0.500\n","\n","\n","Data Frame: sales_train\n","        date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0 2013-01-02               0       59    22154         999             1\n","1 2013-01-03               0       25     2552         899             1\n","\n","\n","Data Frame: test\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","\n","\n","Data Frame: shops_transl\n","                       shop_name  shop_id                       en_shop_name\n","0  !Якутск Орджоникидзе, 56 фран        0  ! Yakutsk Ordzhonikidze, 56 Franc\n","1  !Якутск ТЦ \"Центральный\" фран        1       ! Yakutsk TC \"Central\" Franc\n","\n","\n","Data Frame: shops_augmented\n","                       shop_name  shop_id                       en_shop_name shop_city shop_category shop_federal_district  shop_city_population  shop_tested\n","0  !Якутск Орджоникидзе, 56 фран        0  ! Yakutsk Ordzhonikidze, 56 Franc   Yakutsk          Shop               Eastern                235600        False\n","1  !Якутск ТЦ \"Центральный\" фран        1       ! Yakutsk TC \"Central\" Franc   Yakutsk          Mall               Eastern                235600        False\n","\n","\n","Data Frame: item_categories_transl\n","        item_category_name  item_category_id                 en_cat_name\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones\n","1         Аксессуары - PS2                 1           Accessories - PS2\n","\n","\n","Data Frame: item_categories_augmented\n","        item_category_name  item_category_id                 en_cat_name item_category1 item_category2 item_category3 item_category4  item_cat_tested\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones          Audio             PC    Accessories             PC             True\n","1         Аксессуары - PS2                 1           Accessories - PS2    Accessories    PlayStation    Accessories    PlayStation            False\n","\n","\n","Data Frame: items_transl\n","                                           item_name  item_id  item_category_id                                       en_item_name\n","0          ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0                40                       ! POWER IN glamor (PLAST.) D\n","1  !ABBYY FineReader 12 Professional Edition Full...        1                76  ! ABBYY FineReader 12 Professional Edition Ful...\n","\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\nunused at this time...\\n\\n# Load in any helper functions from the code_files dictionary\\n#    dictionary key is the path (replace \"/\"\" with \".\" when using Google Drive + Colab), \\n#      and dictionary value is the module reference name\\n#    note that the directory chain on GitHub (and local repo) from current directory down to the .py file\\n#      must include a \"__init__.py\" file (it can be empty) in each of the directories\\nfor filepath, module in code_files.items():\\n  path_name = filepath.replace(\"/\",\".\")[:-3]  # Google Drive reference does not use .py, and uses a \".\" instead of \"/\" for directory delineation\\n  exec(\"import \" + path_name + \" as \" + module)\\n\\n# Sanity check test\\ntest1 = kag_utils.add_one(2)\\nprint(test1)\\n'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"NdR16Fu7tt6S","colab_type":"text"},"source":["####**Option 2:  You are running in Colab and are not integrating git with Google Drive**"]},{"cell_type":"code","metadata":{"id":"jAAYsKvIs_hg","colab_type":"code","colab":{}},"source":["'''\n","############################################################\n","\n","# Run this cell if you are executing this notebook in Google Colab, but you\n","#   are not using git to integrate Colab with GitHub and Google Drive \n","#   (e.g., you are not a code developer on this team, but you run in Colab)\n","\n","############################################################\n","\n","def xfer_github_to_colab(path, filename):\n","    os.system(\"wget \" + base_url + \"/{} -O {}\".format(path, filename))\n","    print(base_url + \"/\" + path + \" ---> loaded into ---> \" + filename)\n","    return\n","\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False\n","if IN_COLAB:\n","    print(\"Loading Files from GitHub to Colab...\\n\")\n","\n","    # Loop to load the above data files into appropriately-named pandas DataFrames\n","    for path_name in data_files:\n","      filename = path_name.rsplit(\"/\")[-1]\n","      xfer_github_to_colab(path_name, filename)\n","      data_frame_name = path_name.rsplit(\"/\")[-1].split(\".\")[0]\n","      exec(data_frame_name + \" = pd.read_csv(filename)\")\n","      if data_frame_name == 'sales_train':\n","        sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","      print(\"Data Frame: \" + data_frame_name)\n","      print(eval(data_frame_name).head(2))\n","      print(\"\\n\")\n","\n","\n","    # Load in any helper functions from the code_files dictionary\n","    for code_path, import_as in code_files.items():\n","      code_filename = code_path.rsplit(\"/\")[-1]\n","      xfer_github_to_colab(code_path, code_filename)\n","      exec(\"import \" + code_filename[:-3] + \" as \" + import_as)  # no \".py\" on the filepath for import\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CH3WGLDS0gNL"},"source":["####**Option 3:  You are running this code on a local machine**"]},{"cell_type":"code","metadata":{"id":"X3dcoEITSnvR","colab_type":"code","colab":{}},"source":["'''\n","############################################################\n","\n","# Execute this cell if you are executing this notebook on your local machine, and\n","#   therefore need no special accommodations for Colab integration\n","\n","############################################################\n","\n","print(\"Loading files from GitHub...\\n\")\n","\n","# Loop to load the data files into appropriately-named pandas DataFrames\n","for path_name in data_files:\n","    full_url = os.path.join(base_url, path_name)\n","    data_frame_name = path_name.rsplit(\"/\")[-1].split(\".\")[0]\n","    exec(data_frame_name + \" = pd.read_csv(full_url)\")\n","    if data_frame_name == 'sales_train':\n","      sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","    print(\"Data Frame: \" + data_frame_name)\n","    print(eval(data_frame_name).head(2))\n","    print(\"\\n\")\n","\n","# Load in any helper functions from the code_files dictionary\n","#############################\n","# You need to do this manually, but lucky for you, we aren't using any helper functions at this time ;)\n","#############################\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P99yjzAasJva"},"source":["#2. Explore Data (EDA), Clean Data, and Generate Features\n","\n","**Optional Section**\n","\n","(The following code in \"Section 2\" has all been stored in \"augmented\" .csv data files that were loaded above in \"Section 1.\"  You are welcome to work through the cells in the notebook below, to re-generate the augmented data files, but it is sufficient if you simply look at the notes/comments and not run any of the code in this section.)"]},{"cell_type":"markdown","metadata":{"id":"B30VRGdz9Z6o","colab_type":"text"},"source":["**Organization of the Following EDA/Cleaning/Feature Generation Subsections:**\n","\n","*  2.1) *item_categories* dataset investigation (which produced several of the above recommendations)\n","*  2.2) *shops* dataset investigation (which produced the several of the above recommendations)\n","*  2.3) *shops* - *items* dataset correlations (suggesting potential elimination of shops 9 and 13, and highlighting the adustment of shop #40 category to be \"online\")\n","*  2.4) *shops* - *item_categories* dataset correlations (suggesting use of item_category3 rather than original item_category_id as a feature)\n","*  2.5) *items* dataset EDA, Cleaning, Feature Creation (tbd; ongoing)\n","*  2.6) *sales_train* dataset EDA, outliers - cleaning\n","*  2.7) *test* dataset EDA and considerations concerning overlap (or partial overlap) of *test* dataset inputs and *sales_train* training data"]},{"cell_type":"markdown","metadata":{"id":"sgAY5jIb-htX","colab_type":"text"},"source":["**Spoiler Alert... Recommendations from the EDA computations to come below:**\n","(TL;DR)\n","\n","*sales_train* dataset outliers:\n","1.\tDelete row 1163158 as in: sales_train.drop(1163158) or sales_train = sales_train[sales_train.item_price<300000] to handle the one high price outlier\n","2.\tDelete rows 2257299, 2909401, and 2326930 to handle high item_count outlier, and hard-wire the output test prediction for shop12-item20949 to be 0 sales in Nov. 2015\n","3.\tTo handle the worst of the high item_count outliers, delete row 2909818, and use remaining *sales_train* data to do any predictions related to shop12-item11373\n","4.\tTo handle the one negative-price item in *sales_train*, delete row 484683  (shop 32, item 2973, on 2013-05-15)\n"," \n","*shops* dataset: create 2 or 3 features from this (**shop_id** and **shop_category**), and adjust *sales_train* dataset as below:\n","1.\tFor now, add shop “**shop_category**” (it is a column in *shops_augmented* csv file) as one of your categorical features, along with **shop_id** (also categorical)\n","2.\tCombine shop 11 into shop 10 (i.e., in *sales_train* dataset, everywhere you see shop_id = 11, change it to shop_id = 10)\n","3.\tCombine shop 0 into shop 57 (change shop_id = 0 rows in training set so they now say shop_id = 57)\n","4.\tCombine shop 1 into shop 58 (1 changes to 58 in shop_id in training set)\n","5.  *If time permits*, check predictive power of shop 9 vs. other shops using correlation of time-lag data.  It seems that at least for no-lag data, **shop 9 could be dropped** from the training set alltogether. (It is not in the test set.)\n","6.  *If time permits*, consider odd behavior of shop 13, and given that it is a \"supermarket\" rather than a store of \"entertainment\" items/accessories like the other stores, **possibly drop shop 13** as well. (It is not in the test set.)\n","7.  *If data/features for training are not too large*, also use shop \"**district**\" as a categorical feature (along with shop_id and \"shop_category\")\n"," \n","*item_categories* dataset:  create 1 new feature (**item_category3**) from this\n","1.\tDo not use **item_category_id** as a feature!  It is likely too specific, and will overfit the data.\n","2.\tUse column “**item_category3**” from the *item_categories_augmented* csv file, and merge it into *sales_train* according to **item_id**, and use this as a categorical feature"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4tqIxzAUsCir"},"source":["#2.x Auxiliary Data Sets:  EDA, Cleaning, Feature Generation\n","\n","**Optional Section**\n","\n","*  Russian-to-English Translation and Geocoding\n","*  2.1) *item_categories* dataset investigation (which produced several of the above recommendations)\n","*  2.2) *shops* dataset investigation (which produced the several of the above recommendations)\n","*  2.3) *shops* - *items* dataset correlations (suggesting potential elimination of shops 9 and 13, and highlighting the adustment of shop #40 category to be \"online\")\n","*  2.4) *shops* - *item_categories* dataset correlations (suggesting use of item_category3 rather than original item_category_id as a feature)\n","*  2.5) *items* dataset EDA, Cleaning, Feature Creation (tbd; ongoing)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wVIybiQ33wVy"},"source":["##2.1) ***item_categories*** Dataset: EDA, Cleaning, and Feature *Generation*\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qxAp8KM44TUp"},"source":["###2.1.1) **Translate and Ruminate**\n","We will start by translating the Russian text in the dataframe, and add our ruminations on possible new features we can generate.\n","\n","The dataframe *item_categories_transl* (equivalent to item_categories plus a column for English translation) is saved as a .csv file so we do not have to repeat the translation process the next time we open a Google Colab runtime."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sYm2ewbs6S2w"},"source":["**Tip**\n","</br>\n","If you want to run this code to generate translated features for the item categories, be sure to install the googletrans package and import Translator as in code above\n","\n","If you have already created and saved this data, save time by importing the modified csv datafile and you won't have to re-run the translating.  (This can be a big deal, because Google Translate API restricts amount of usage and/or rate of usage for calls to the translator.)"]},{"cell_type":"code","metadata":{"id":"nhOC233wZNPt","colab_type":"code","colab":{}},"source":["item_categories.describe"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"omWZusxeO4Fp","colab_type":"code","colab":{}},"source":["'''\n","#################################################\n","#  Do NOT run unless recreating from beginning\n","#    This computation already stored in data file\n","#################################################\n","\n","item_categories_transl = item_categories.copy(deep=True)\n","item_categories_transl['en_cat_name'] = item_categories_transl.item_category_name.apply(lambda x: translator.translate(x, src='ru', dest='en').text)\n","\n","# Save the translated data\n","item_categories_transl.to_csv(\"data_output/item_categories_transl.csv\", index=False)\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0bEjJr2vQPa6","colab_type":"code","colab":{}},"source":["item_categories_transl.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdhI7FCM6QkV","colab_type":"text"},"source":["Observations...\n","There is clearly an overlap in item_category types that can be made into a new feature (e.g., all accessories for PlayStation, all accessories for XBox, ...)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pMin_e1a7fUJ"},"source":["###2.1.2) ***item_categories* With Manually-Augmented Features**\n","Since there are only 84 categories, offline hand-coding new categorical features into a csv file isn't difficult.  We will add column \"item_category1\" and column \"item_category2\" which focus on (1) type of product (console, software, etc.), and (2) platform of product (playstation, xbox, pc, etc.).\n","</br>\n","These two new columns are added to the *item_categories_transl.csv* file using an external spreadsheet editor, and are saved as file *item_categories_augmented.csv* \n","\n","As the features were manually added, there is no code to generate the file *item_categories_augmented.csv*.  We therefore use the .csv file loaded from the repo in the first part of this notebook.\n"]},{"cell_type":"code","metadata":{"id":"zAsG4m5-9ZfD","colab_type":"code","colab":{}},"source":["# These are the categories presently being used (hand-coded) in the two extra item_categories feature columns:\n","print(item_categories_augmented.item_category1.unique())\n","print(item_categories_augmented.item_category2.unique())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GKVEFOUGk659","colab_type":"code","colab":{}},"source":["item_categories_augmented.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"guNfS02NlPYw","colab_type":"code","colab":{}},"source":["item_categories_augmented.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZsGXSO99u2TB","colab_type":"text"},"source":["##2.2) ***shops*** Dataset: EDA, Cleaning, and Feature Generation\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WHeMPLFk0rOB"},"source":["###2.2.1) **Translate and Ruminate**\n","We will start by translating the Russian text in the dataframe, and add our ruminations on possible new features we can generate.\n","\n","The dataframe shops_transl (equivalent to shops + 'column for English translation') is saved as a .csv file so we do not have to repeat the translation process the next time we open a Google Colab runtime."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CHYWpvTt0EbW"},"source":["**Tip**\n","</br>\n","If you want to run this code to generate translated features for the shops, be sure to install the googletrans package and import Translator as in code above\n","\n","If you have already created and saved this data, save time by importing the modified csv datafile and you won't have to re-run the translating.  (This can be a big deal, because Google Translate API restricts amount of usage and/or rate of usage for calls to the translator.)"]},{"cell_type":"code","metadata":{"id":"aXAwthz5Z4yf","colab_type":"code","colab":{}},"source":["'''\n","#################################################\n","#  Do NOT run unless recreating from beginning\n","#    This computation already stored in data file\n","#################################################\n","\n","shops_transl = shops.copy(deep=True)\n","shops_transl['en_shop_name'] = shops_transl.shop_name.apply(lambda x: translator.translate(x, src='ru', dest='en').text)\n","shops_transl.to_csv(\"data_output/shops_transl.csv\", index=False)\n","print(len(shops_transl))\n","'''\n","shops_transl.head(15)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7KcpTfwYh22j","colab_type":"text"},"source":["Observations:\n","\n","\n","1.  The number of shops is only 60, so manual feature generation is not out of the question.\n","2.  Most shops have a city associated with their name, so it's reasonable that we can do some feature generation based on shop location.\n","3.  (After Googling several of the shops, we realized that...) The shop type may be categorized.  We noticed that \"Mega\" shops were located inside shopping malls that were anchored (and managed) by Ikea stores.  \"SEC\" acronym implies the shop is part of a shopping and entertainment center (like a large shopping mall with a cinema or other activities).  SC, TC, TRK, and TRC acronyms generally imply the shop is in a standard shopping mall, but careful inspection on the world wide interweb shows that some of these shops are actually in SECs.  We will try assigning a shop_category to each store from the following:\n","\n","**['Online', 'Itinerant', 'Shop', 'Mall', 'Mega', 'SEC']**\n","\n","\n","\n",">*   Online (like Amazon or eBay)\n","*   Itinerant (traveling salesman)\n","*   Shop (small, isolated store)\n","*   Mall (store is based in a shopping mall)\n","*   Mega (store is based in an Ikea-managed mall)\n","*   SEC (store is located in a shopping-entertainment complex)\n","\n","\n","\n","\n","Once we have extracted city information from the shop names, we can use a geolocator package to help categorize the location of the store, and even the population of the city in which the store is located.\n"]},{"cell_type":"markdown","metadata":{"id":"euTwvnkG7c2M","colab_type":"text"},"source":["The geopy package seems pretty good for performing the location-based categorization.  Free services Nominatum and GeoNames can work with geopy to give us longitude, latitude, federal district, and population, for example.\n","</br></br>\n","Latitude and longitude of the shops are likely too fine-grained to prevent overfitting with our model.  Instead, we can generate a feature based on Russian **Federal District**, as retrieved with geocode Nominatum service.  Due to religious preferences, for example, there may be a bias for a certain region to have higher sales in November (before Christmas) or not.  The map below shows roughly how Nominatum would categorize the Russian Federal Districts (the red text in the image):\n","\n","<img src=\"https://www.worldatlas.com/r/w728-h425-c728x425/upload/4c/4b/0f/shutterstock-183567236-1.jpg\">\n","\n","</br>\n","\n","We have no shops in the dataframe that come from North Caucasia or Crimea, so the category types for district are as follows ('None' indicates online or itinerant shops):\n","\n","**['Central', 'Northwestern', 'Siberian', 'Ural', 'Volga', 'South', 'Eastern', 'None']**\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S7cTM5qJ1NhY"},"source":["###2.2.2) **Geocoding New Features**\n","We now apply geocoding to the shop locations to create features including Russian Federal District and population of the shop's city.\n","\n","*shops_augmented.csv* file is saved to contain the original *shops.csv* data plus the English translation plus a feature column for Federal District and a feature column for population."]},{"cell_type":"markdown","metadata":{"id":"gXTJW2dKzG_V","colab_type":"text"},"source":["**Tip**\n","</br>\n","If you want to run this code to generate geo features for the shops, be sure to install the geopy package and import Nominatum and GeoNames as in code above\n","\n","If you have already created and saved this data, save time by importing the modified csv datafile and you won't have to re-run the geocoding."]},{"cell_type":"code","metadata":{"id":"E5EaVl2QRog8","colab_type":"code","colab":{}},"source":["'''\n","#################################################\n","#  Do NOT run unless recreating from beginning\n","#    This computation already stored in data file\n","#################################################\n","\n","# Add 'shop_federal_district' column to the shops_augmented dataframe using Nominatim\n","shops_augmented['shop_federal_district'] = shops_augmented.shop_city.apply(lambda x:  'None' if (x == 'None') else re.search(r'[,\\s](\\w*)\\sFederal District', str(nominatum_geocode(x, language='en'))).group(1))\n","\n","# Add 'shop_city_population' column to the shops_augmented dataframe using GeoNames\n","shops_augmented['shop_city_population'] = shops_augmented.shop_city.apply(lambda x:  'None' if (x == 'None') else geonames_geocode2(x, timeout=10).raw['population'])\n","\n","# GeoNames had some trouble with Zhukovsky and Checkov... insert values from Wikipedia\n","shops_augmented.at[56,'shop_city_population'] = 61000\n","shops_augmented.at[10,'shop_city_population'] = 105000\n","shops_augmented.at[11,'shop_city_population'] = 105000\n","\n","# for 'Itinerant' (traveling salesman) shop, set the population to 100,000 (roughly the number of people the salesman might have access to in day)\n","shops_augmented.at[9,'shop_city_population'] = 100000\n","\n","# for 'Online' shops, set the population to 20,000,000 (estimate of the number of people who have internet and would place an online order from Russia)\n","shops_augmented.at[12,'shop_city_population'] = 20000000\n","shops_augmented.at[20,'shop_city_population'] = 20000000\n","shops_augmented.at[55,'shop_city_population'] = 20000000\n","\n","shops_augmented.to_csv(\"data_output/shops_augmented.csv\", index=False)\n","\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nD18D7fctMtT","colab_type":"text"},"source":["###2.2.3) **Shop Inclusion in Test Dataset**\n","Add a column to the 60-row *shops_augmented* dataframe with boolean value indicating if that row's shop is contained in the set of 42 unique shops present in the *test* dataset"]},{"cell_type":"code","metadata":{"id":"x258EMq2tcSA","colab_type":"code","colab":{}},"source":["'''\n","#################################################\n","#  Do NOT run unless recreating from beginning\n","#    This computation already stored in data file\n","#################################################\n","\n","test_shops = test.shop_id.unique()\n","shops_augmented[\"shop_tested\"] = False\n","for i in test_shops:\n","  shops_augmented.at[i,\"shop_tested\"] = True\n","\n","shops_augmented.to_csv(\"data_output/shops_augmented.csv\", index=False)\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9YeV5ENdz26R","colab_type":"text"},"source":["###2.2.4) **Discussion of manually-augmented *shops* data**"]},{"cell_type":"code","metadata":{"id":"0KsyFSOT-Q11","colab_type":"code","colab":{}},"source":["shops_augmented"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uc0W6tm4xg5L","colab_type":"text"},"source":["A few things pop out when comparing the full *shops_augmented* dataframe with only the shop_id values included in the *test* dataframe:\n","\n","*  shop_id #9 is not in the *test* set.  This particular \"shop\" is the only \"itinerant\" shop out of all 60, which makes it somewhat unique.  This gives some confidence that we should discard any training involving shop_id == 9\n","\n","*  shop_id #10 and #11 are located at the same place, and web search does not distinguish between the two.  Shop #11 is not included in the *test* set.  Thus, there is a good chance we can either discard shop #11 or combine it into shop #10 when training our model.\n","\n","*  shop_id #12, 20, and 55 are the only \"online\" shops in the database.  Shop #20 is not included in the *test* set.  Because these three shops are set apart from the others as being online, they may behave quite differently than the others.  And, because there are only 3 to look at, we can analyze rather quickly to see the characteristics of 12, 20, and 55 to see if 20 is a good supplement for training, or if 20 should be discarded during training.\n","\n","*  shop_id #0 and 1 are not included in the *test* set, leaving shops 57 and 58 as the only shops to be tested that come from the Eastern Federal District.  We might want to consider treating these 4 shops differently from the other shops when training, as they might be considered as \"outliers.\"  Interestingly, shops 0 and 1 might actually be the same as 57 and 58 (the names are strikingly similar).  Maybe we combine data from 0 into 57 and from 1 into 58 and train our model without 0 and 1?\n","\n","*  shop_id #5, 42, and 43 are the only shops in the Northwestern Federal District, and shop 42 is not in the *test* set.  We need to watch closely at these shops too, to see if this makes any of the 3 to be outliers that we treat differently during training.\n","\n","*  shop #40 is an \"island\" -- I belive this indicates it is \"isolated\" as in 'no contact with people' as in 'online' store.   Use 'online' for the type of shop for #40.  I've changed it as such, on 4/28/20.\n","\n","*  shop #0 is a street address; it should be type 'shop' rather than 'mall' as I was using earlier.  I've changed it 4/28/20.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZezbXafHXOkd"},"source":["###2.2.5) **A Look at Some Individual Shops of Particular Interest**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3Ir4z6y488C0"},"source":["####2.2.5.1) **A Typical Shop**\n","\n","(tbd)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JMkxlDjP4qNx","colab":{}},"source":["shops_sales_train = sales_train.groupby(\"shop_id\").count()['item_cnt_day']\n","shops_sales_train.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"52Owl-nz88t_","colab_type":"code","colab":{}},"source":["shops_sales_train.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4-EZjbv-8HJi"},"source":["####2.2.5.2) **Shop 9 - Analysis**\n","\n","**Tentative Conclusion (TL;DR): We should dump shop \\#9 from any model training**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-wf8k35xuX59","colab":{}},"source":["# starting with shop #9, let's look at the behavior in the training dataset\n","shop9_sales = sales_train.loc[sales_train['shop_id']==9]\n","print(shop9_sales.item_id.nunique())\n","shop9_sales.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wpy2n4rY27-M","colab_type":"code","colab":{}},"source":["shop9_sales.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDlAwJmP4-2L","colab_type":"code","colab":{}},"source":["print(shop9_sales.item_cnt_day.sum())\n","shop9_sales.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lcp_lA1x6G-p","colab_type":"code","colab":{}},"source":["shop9_many_sales = shop9_sales[shop9_sales.item_cnt_day > 75]\n","shop9_many_sales"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oEbh_lkHy6l","colab_type":"code","colab":{}},"source":["print(shop9_sales.date.nunique())\n","shop9_sales.date.unique()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kikz5FP33pJk","colab_type":"text"},"source":["After a bunch of stumbling around and looking at shop \\#9 in different ways, we found that the total number of days this shop had sales was only 14 (out of 34 months of training data), with a total of 1404 different items sold, and 3751 rows in the *sales_train* dataset.  So, it looks like this shop is indeed an outlier in that the sales take place on a few select days, and they don't happen to include November.\n","\n","**We need to consider eliminating shop \\#9 from any model training unless it has some time-lag benefit for prediction**"]},{"cell_type":"markdown","metadata":{"id":"YS8_H42a8YIv","colab_type":"text"},"source":["The \"typical\" shop had roughly 42000 rows in the *sales_train* dataset, as opposed to 3751 rows for shop #9."]},{"cell_type":"code","metadata":{"id":"fkgZs0BJ-1gP","colab_type":"code","colab":{}},"source":["#shop9_many_sales = shop9_sales[shop9_sales.item_cnt_day == 03.10.2014]\n","print(dates_sales_train[dates_sales_train.index == pd.Timestamp(year=2014, month=3, day=10)])\n","print(dates_sales_train[pd.Timestamp(year=2014, month=3, day=10)])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xkkzLCB387kR"},"source":["####2.2.5.3) **Shops 10 and 11 - Analysis**\n","\n","**Conclusion (TL;DR): Merge Shop 11 into Shop 10 -- change** *sales_train* **dataset so (if shop_id == 11, then  shop_id = 10)**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vt5cmg8F9PUG","colab":{}},"source":["# let's look at the behavior in the training dataset for shops 10 and 11\n","shop10_sales = sales_train.loc[sales_train['shop_id']==10]\n","print(shop10_sales.item_id.nunique())\n","print(shop10_sales.info())\n","print(\"\\n\")\n","shop11_sales = sales_train.loc[sales_train['shop_id']==11]\n","print(shop11_sales.item_id.nunique())\n","print(shop11_sales.info())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7N2KR08E94Rx","colab_type":"text"},"source":["Shop 10 (the one included in the *test* dataset) has far more transactions than Shop 11.  Should we just dump Shop 11, or should we merge it into shop 10?\n","\n","Let's look at a comparison of the items sold, and see if perhaps it is really the same shop, and we should merge the data. (tbd)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jgA5Z4IeXmOb"},"source":["####2.2.5.4) **Shops 0 and 1 vs. Shops 57 and 58 - Analysis**\n","\n","**Conclusion (TL;DR): change** *sales_train* **dataset so (if shop_id == 0, then  shop_id = 57) and (if shop_id == 1, then shop_id = 58)**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gZ30t_XRXmOb","colab":{}},"source":["# analysis tbd"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xrl8_0enXmOe"},"source":["TBD: look at the behavior in the training dataset for shops 0, 1, 57, 58... verify that 0/57 and 1/58 have similar distribution of sales over item_id\n","\n","*  note shops 0 and 1 are not in the test set\n","*  note shops 0 and 1 have essentially the same names as 57 and 58\n","\n","for now, do the merge; detailed analysis can follow if time allows"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1tWT6TnP3Si9"},"source":["##2.3) **SHOP-ITEM** pairing and related correlations:\n","\n","Correlate shops by sales in the different item_id options\n","\n","*  How many rows in the *sales_train* dataset for each item (and, same info, but grouped by shop)?  (i.e., how many training examples do we have to work with)\n","\n","*  From the *sales_train* dataset, how many total units of items were sold for each shop (i.e., how much traffic / importance should we assign to a particular item and shop)"]},{"cell_type":"markdown","metadata":{"id":"xdQUBtRHYiEJ","colab_type":"text"},"source":["###2.3.1) shop-item distributions\n","It appears as though only 21807 out of 22170 possible item_ids are present in the training set.  Let's look at some distributions... first, the number of total sales of each individual item_id for the entire train set:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"c22wqk94ijwh","colab":{}},"source":["# How many rows in the sales_train set for each item?  How many total units of each item were sold (per the sales_train dataset)?\n","items_sales = sales_train.groupby(\"item_id\", as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","items_sales.columns = ['item_id','item_total_train_rows','item_total_units_sold']\n","print(items_sales.describe())\n","items_sales.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUNGG95ZhU5z","colab_type":"code","colab":{}},"source":["# How many rows in the sales_train set for each shop?  How many total items sold for each shop?\n","shops_sales = sales_train.groupby(\"shop_id\", as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","shops_sales.columns = ['shop_id','shop_total_train_rows','shop_total_units_sold']\n","print(shops_sales.describe())\n","shops_sales.head(15)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wIR9l8pUbyBV","colab_type":"code","colab":{}},"source":["# indeed every row has a nonzero value in item_cnt_day\n","nonzeros = sales_train[sales_train['item_cnt_day'].fillna(0).astype(bool) == False] #.sum(axis=0)\n","nonzeros"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3Xem_ULOM-L","colab_type":"code","colab":{}},"source":["# Compute the number of times a shop_id-item_id pair appears in the sales_train dataset\n","# Also, for each shop_id-item_id pair, compute the number of those particular items sold at that shop (in the entire sales_train dataset)\n","# Finally, for each shop_id-item_id pair, compute the percentage of the above two items with respect to the total numbers for the shop\n","#   and with respect to the total numbers for the item\n","shop_item_sales = sales_train.groupby(['shop_id','item_id'], as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","shop_item_sales.columns = ['shop_id','item_id','n_train_rows','n_units_sold']\n","shop_item_sales['n_units_sold'] = shop_item_sales['n_units_sold'].astype(int)\n","shop_item_sales = shop_item_sales.merge(items_sales, on = 'item_id')\n","shop_item_sales = shop_item_sales.merge(shops_sales, on = 'shop_id')\n","shop_item_sales['shop_row_pct'] = 100 * shop_item_sales.n_train_rows / shop_item_sales.shop_total_train_rows\n","shop_item_sales['shop_units_pct'] = 100 * shop_item_sales.n_units_sold / shop_item_sales.shop_total_units_sold\n","shop_item_sales['item_row_pct'] = 100 * shop_item_sales.n_train_rows / shop_item_sales.item_total_train_rows\n","shop_item_sales['item_units_pct'] = 100 * shop_item_sales.n_units_sold / shop_item_sales.item_total_units_sold\n","# clean up rows where total number of units sold = 0 (item_units_fraction = +/- inf)... note that we don't have to do this for shops column because total is never 0\n","shop_item_sales['item_units_pct'] = shop_item_sales['item_units_pct'].replace([np.inf, -np.inf], 0)\n","# drop the unneeded columns for easier readability\n","shop_item_sales.drop(['item_total_train_rows','item_total_units_sold','shop_total_train_rows','shop_total_units_sold'], axis=1, inplace=True)\n","\n","print(shop_item_sales.describe())\n","shop_item_sales.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aMRMRlZm4njN"},"source":["###2.3.2) shop-item correlations \n","Investigate correlation between shops using item sales as the correlation variable"]},{"cell_type":"code","metadata":{"id":"6NugKXkbHBzz","colab_type":"code","colab":{}},"source":["# I would like to look at correlations between shops, between items, and between shop-item pairs\n","# For this, I need to fill out the shop_item_sales table to include all possible shop-item pairs, using itertools.product\n","\n","l1=shops['shop_id'].unique().tolist()\n","l2=items['item_id'].unique().tolist()\n","pairs_df =  pd.DataFrame(list(product(l1,l2))).rename(columns={0:'shop_id',1:'item_id'})\n","all_shop_item = pd.merge(pairs_df, shop_item_sales, on=['shop_id', 'item_id'], how='left').fillna(0)\n","print(len(all_shop_item))\n","all_shop_item.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPbL605QWy3v","colab_type":"code","colab":{}},"source":["shop_shop_units = all_shop_item.pivot(index='item_id',columns='shop_id',values='shop_units_pct')\n","print(len(shop_shop_units))\n","shop_shop_units.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffKZnfV_9rjH","colab_type":"code","colab":{}},"source":["corr_matrix = shop_shop_units.corr()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"op0eO70KFzCS","colab_type":"code","colab":{}},"source":["corr_matrix = corr_matrix.applymap(lambda x: 0 if abs(x) < 0.01 else x)  # stop the heatmap from printing -0.0 in some places instead of always 0.0, and no funky formatting from \":.1g\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4uh81adGGxU-","colab_type":"code","colab":{}},"source":["corr_matrix.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQSc3_5qU-ud","colab_type":"code","colab":{}},"source":["plt.rcParams[\"figure.figsize\"] = [50,50]\n","from IPython.display import Javascript\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 5000})'''))\n","\n","h = sns.heatmap(corr_matrix,\n","                cmap= sns.color_palette(\"RdBu_r\", 11), #sns.diverging_palette( 30, 50, s=90, l=60, n=11, center=\"dark\"), #'coolwarm',\n","                annot=True,\n","                fmt= '.0g', #\".1g\", #\".1f\",\n","                annot_kws={'size':12},\n","                cbar=True,\n","                cbar_kws={\"shrink\": 0.7},\n","                square=True)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PriEBJpu-jO6","colab_type":"text"},"source":["###Observations:\n","From the above heatmap, we can see that certain shops have very little overall correlation with any other shop.  A few issues with this analysis:\n","\n","*  The correlation is computed based on all sales in the *sales_train* dataset, and has no accommodation for time-lag influences (e.g., one shop may predict a month ahead of another shop)\n","\n","*  This calculation uses very fine discretization of 21,700 item_id values... if a shop calls an item by a slightly different name, it could reduce correlation to other shops to zero, for example.\n","\n","Let's address the second of these issues first, by using item categories instead of item ids when comparing shop-to-shop."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ltmKnRMnoJ2Q"},"source":["##2.4) **SHOP-CATEGORY** pairing and related correlations:\n","\n","Correlate shops by sales in the different item_categories options, unlike the above where we correlated shops by sales in the different item_id options\n","\n","*  How many rows in the *sales_train* dataset for each category (and, same info, but grouped by shop)?  (i.e., how many training examples do we have to work with)\n","\n","*  From the *sales_train* dataset, how many total units of items were sold for each category (and, same info, but grouped by shop)?  (i.e., how much traffic / importance should we assign to a particular category)"]},{"cell_type":"markdown","metadata":{"id":"Eqd2okGt24Lk","colab_type":"text"},"source":["###2.4.1) dataframe merging/augmenting as setup for the EDA"]},{"cell_type":"code","metadata":{"id":"439v8msroxU0","colab_type":"code","colab":{}},"source":["# Recall the augmented item_categories data set:\n","item_categories_augmented.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JUvb31CdoJ2W","colab":{}},"source":["# How many rows in the sales_train set for each category?  How many total units of each category were sold (per the sales_train dataset)?\n","# Let's do this for the original \"item_category_id\" as well as \"item_category1\" and \"item_category2\"\n","#    --> start by giving the 2 augmented subcategories their own associated id numbers for simpler plot labeling, etc.\n","item_categories_augmented.item_category1 = pd.Categorical(item_categories_augmented.item_category1)\n","item_categories_augmented.item_category2 = pd.Categorical(item_categories_augmented.item_category2)\n","item_categories_augmented['cat1_id'] = item_categories_augmented.item_category1.cat.codes\n","item_categories_augmented['cat2_id'] = item_categories_augmented.item_category2.cat.codes\n","\n","# Adding categories 3 and 4 due to earlier analysis of 1 and 2:\n","item_categories_augmented.item_category3 = pd.Categorical(item_categories_augmented.item_category3)\n","item_categories_augmented.item_category4 = pd.Categorical(item_categories_augmented.item_category4)\n","item_categories_augmented['cat3_id'] = item_categories_augmented.item_category3.cat.codes\n","item_categories_augmented['cat4_id'] = item_categories_augmented.item_category4.cat.codes\n","\n","\n","item_categories_augmented.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lsUA6RMSrL3w","colab_type":"code","colab":{}},"source":["# Merge the item category information with the sales_train data\n","train_cats = sales_train.merge(items[['item_id','item_category_id']], on = 'item_id')  \n","# sort on shops so we can get a look at several different items to verify all is ok\n","train_cats = train_cats.merge(item_categories_augmented[['item_category_id','cat1_id','cat2_id','cat3_id','cat4_id']], on = 'item_category_id').sort_values('shop_id')\n","train_cats.head(7)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSLv0Si4dGIB","colab_type":"text"},"source":["###2.4.2) Grouping shop/item info by item category \n","5 different choices of item categorization (the original item_category_id, and then 4 modified versions that group similar category IDs into a single category, thus reducing the total number of categories)"]},{"cell_type":"markdown","metadata":{"id":"-vO9ZkSsiAWl","colab_type":"text"},"source":["####2.4.2.1) by item_category_id"]},{"cell_type":"code","metadata":{"id":"5pn5IcS5wwI4","colab_type":"code","colab":{}},"source":["# Sales by category for the 84 unique categories in \"item_category_id\" (the original dataset categorization):\n","cat0_sales = train_cats.groupby(\"item_category_id\", as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","cat0_sales.columns = ['item_category_id','cat0_total_train_rows','cat0_total_units_sold']\n","cat0_sales = cat0_sales.merge(item_categories_augmented[['item_category_id','en_cat_name']], on = 'item_category_id')\n","print(cat0_sales.describe())\n","print(cat0_sales.head())\n","\n","plt.rcParams[\"figure.figsize\"] = [15,5]\n","cat0_sales.plot(kind='bar',x='item_category_id',y='cat0_total_units_sold')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"daK85yPTD5cj","colab_type":"code","colab":{}},"source":["# Let's get a better look at the categories with small total unit sales, by using log y axis\n","cat0_sales.plot(kind='bar',x='item_category_id',y='cat0_total_units_sold', logy=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IGDp98OOCQEK","colab_type":"text"},"source":["There looks to be a large variance by category when using original category IDs.\n","\n","For \"total units sold within a category for the entire sales_train dataset\" vs. item_category_id:\n","\n","*  Standard deviation is roughly 2x the mean value, and 10x the median value\n","\n","*  Minimum category has just 1 item sold in entire *sales_train* dataset, and from the bar plot, you can see that there are several categories with irrelevant numbers of sales."]},{"cell_type":"markdown","metadata":{"id":"PNWkM4jJiKa8","colab_type":"text"},"source":["####2.4.2.2) by manually-created item_category1"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mqLK7oXEygfF","colab":{}},"source":["# Let's look at the data with the augmented category options.\n","# Sales by item_category1 (categories grouped into 13 unique values, relating to item \"type\")\n","print(item_categories_augmented.item_category1.unique().to_list())\n","cat1_sales = train_cats.groupby(\"cat1_id\", as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","cat1_sales.columns = ['cat1_id','cat1_total_train_rows','cat1_total_units_sold']\n","cat1_sales = cat1_sales.merge(item_categories_augmented.groupby('cat1_id').first().reset_index()[['cat1_id','item_category1']], on = 'cat1_id')\n","print(cat1_sales.describe())\n","print(cat1_sales)\n","\n","#plt.rcParams[\"figure.figsize\"] = [15,5]\n","cat1_sales.plot(kind='bar',x='cat1_id',y='cat1_total_units_sold')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qCacFG4wFN_x","colab":{}},"source":["# Let's get a better look at the categories with small total unit sales, by using log y axis\n","cat1_sales.plot(kind='bar',x='cat1_id',y='cat1_total_units_sold', logy=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KU_JqqbQFe6f"},"source":["There is a much more balanced distribution of total units sold by category with the 13 manually-determined item_category1 categories, roughly set by item \"type\" without nod to branding.\n","\n","For \"total units sold within a category for the entire sales_train dataset\" vs. item_category1:\n","\n","*  Standard deviation has relative decrease to be 1.5x the mean value, and 6x the median value (instead of 2x and 10x)\n","\n","*  Minimum category has just 3 items sold in entire *sales_train* dataset, and from the bar plot, you can see that this is now the only category with an irrelevant number of unit sales.  (item_category1 = 'audio', such as PC headsets/headphones, which comes directly from the one original item_category_id == 0, and does not group any other of the original categories with it.)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tGwQH8daiXfN"},"source":["####2.4.2.3) by manually-created item_category2"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rqu9KLXI4NBf","colab":{}},"source":["# Sales by item_category2:  (categories grouped into 10 unique values, relating to item \"brand\" then \"type\")\n","print(item_categories_augmented.item_category2.unique().to_list())\n","cat2_sales = train_cats.groupby(\"cat2_id\", as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","cat2_sales.columns = ['cat2_id','cat2_total_train_rows','cat2_total_units_sold']\n","cat2_sales = cat2_sales.merge(item_categories_augmented.groupby('cat2_id').first().reset_index()[['cat2_id','item_category2']], on = 'cat2_id')\n","print(cat2_sales.describe())\n","print(cat2_sales)\n","\n","#plt.rcParams[\"figure.figsize\"] = [15,5]\n","cat2_sales.plot(kind='bar',x='cat2_id',y='cat2_total_units_sold')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x5c8vYnHHxsG","colab":{}},"source":["# Let's get a better look at the categories with small total unit sales, by using log y axis\n","cat2_sales.plot(kind='bar',x='cat2_id',y='cat2_total_units_sold', logy=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5288B925v7x","colab_type":"text"},"source":["It is probable that the items in (item_category1 == 'audio') and in (item_category2 == 'phone' or item_category2 == 'other') should not be simply dispensed with, yet the very small numbers of units involved for these categories in the *sales_train* dataset make these to be poor choices of categories to get good model fitting.\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","So, let's revisit the manual category grouping that we did before, and see if these low-traffic item subcategories could be grouped with one of the larger subcategories so the low-traffic item subcategories can gain some benefit during training by sharing information with other item_category_id rows in the *sales_train* dataset.\n","\n","---\n","OK, below we have modified cat1 --> cat3 and cat2 --> cat4.  Let's see how they look:\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"h49_7BVxif7B"},"source":["####2.4.2.4) by manually-created item_category3"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pCplqQXKyCIl","colab":{}},"source":["# Sales by item_category3:  (categories grouped into 12 unique values, relating to item \"type\")\n","print(item_categories_augmented.item_category3.unique().to_list())\n","cat3_sales = train_cats.groupby(\"cat3_id\", as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","cat3_sales.columns = ['cat3_id','cat3_total_train_rows','cat3_total_units_sold']\n","cat3_sales = cat3_sales.merge(item_categories_augmented.groupby('cat3_id').first().reset_index()[['cat3_id','item_category3']], on = 'cat3_id')\n","print(cat3_sales.describe())\n","print(cat3_sales)\n","\n","#plt.rcParams[\"figure.figsize\"] = [15,5]\n","cat3_sales.plot(kind='bar',x='cat3_id',y='cat3_total_units_sold')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mzgWx3BzyGcN","colab":{}},"source":["# Let's get a better look at the categories with small total unit sales, by using log y axis\n","cat3_sales.plot(kind='bar',x='cat3_id',y='cat3_total_units_sold', logy=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Fl9Y5u1DinVo"},"source":["####2.4.2.5) by manually-created item_category4"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JiBnCAzHyArH","colab":{}},"source":["# OK, Cat3 looks better to use than Cat1 for subgrouping the 84 original categories\n","#\n","# Now check Cat4 (vs. Cat2)\n","# Sales by item_category4:  (categories grouped into 8 unique values, relating to item \"brand\" then \"type\")\n","print(item_categories_augmented.item_category4.unique().to_list())\n","cat4_sales = train_cats.groupby(\"cat4_id\", as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","cat4_sales.columns = ['cat4_id','cat4_total_train_rows','cat4_total_units_sold']\n","cat4_sales = cat4_sales.merge(item_categories_augmented.groupby('cat4_id').first().reset_index()[['cat4_id','item_category4']], on = 'cat4_id')\n","print(cat4_sales.describe())\n","print(cat4_sales)\n","\n","#plt.rcParams[\"figure.figsize\"] = [15,5]\n","cat4_sales.plot(kind='bar',x='cat4_id',y='cat4_total_units_sold')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pWFCFiTO3SD7","colab_type":"text"},"source":["###Observation / Summary Up to now:\n","It appears as if item_category3 is indeed an improvement over item_category1 grouping, and similarly, item_category4 is an improvement over item_category2.  And, all seem to be potentially more relevant (and generalizable) than the original item_category_id grouping.\n","\n","So, let's proceed with the analysis of shop-shop correlations using the original item_category_id (84 values), the item_category3 (12 values), and the item_category4 (8 values)...."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FwNIqeKE4_cg"},"source":["###2.4.3) shop-item_category correlations \n","Investigate correlation between shops using item sales in a given item category as the correlation variable"]},{"cell_type":"markdown","metadata":{"id":"RtvVby66e3Nu","colab_type":"text"},"source":["####2.4.3.1) using original item_category_id categorization"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5WcgRYLAHr1F","colab":{}},"source":["# Compute the number of times a shop_id-category_id pair appears in the sales_train dataset\n","# Also, for each shop_id-category_id pair, compute the number of the particular items in that category sold at that shop (in the entire sales_train dataset)\n","# Finally, for each shop_id-category_id pair, compute the percentage of the above two items with respect to the total numbers for the shop\n","#   and with respect to the total numbers for the item category\n","shop_cat0_sales = train_cats.groupby(['shop_id','item_category_id'], as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","shop_cat0_sales.columns = ['shop_id','item_category_id','n_train_rows','n_units_sold']\n","shop_cat0_sales['n_units_sold'] = shop_cat0_sales['n_units_sold'].astype(int)\n","shop_cat0_sales = shop_cat0_sales.merge(cat0_sales, on = 'item_category_id')\n","shop_cat0_sales = shop_cat0_sales.merge(shops_sales, on = 'shop_id')\n","shop_cat0_sales['shop_row_pct'] = 100 * shop_cat0_sales.n_train_rows / shop_cat0_sales.shop_total_train_rows\n","shop_cat0_sales['shop_units_pct'] = 100 * shop_cat0_sales.n_units_sold / shop_cat0_sales.shop_total_units_sold\n","shop_cat0_sales['cat0_row_pct'] = 100 * shop_cat0_sales.n_train_rows / shop_cat0_sales.cat0_total_train_rows\n","shop_cat0_sales['cat0_units_pct'] = 100 * shop_cat0_sales.n_units_sold / shop_cat0_sales.cat0_total_units_sold\n","# clean up rows where total number of units sold = 0 (item_units_fraction = +/- inf)... note that we don't have to do this for shops column because total is never 0\n","shop_cat0_sales['cat0_units_pct'] = shop_cat0_sales['cat0_units_pct'].replace([np.inf, -np.inf], 0)\n","# drop the unneeded columns for easier readability\n","shop_cat0_sales.drop(['cat0_total_train_rows','cat0_total_units_sold','shop_total_train_rows','shop_total_units_sold'], axis=1, inplace=True)\n","\n","print(shop_cat0_sales.describe())\n","shop_cat0_sales.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q8AWVBxzoJ2f","colab":{}},"source":["# I would like to look at correlations between shops, between item categories, and between shop-item_category pairs\n","# For this, I need to fill out the shop_catX_sales table to include all possible shop-item_category pairs, using itertools.product\n","\n","l1=shops['shop_id'].unique().tolist()\n","l2=items['item_category_id'].unique().tolist()\n","pairs_df =  pd.DataFrame(list(product(l1,l2))).rename(columns={0:'shop_id',1:'item_category_id'})\n","all_shop_cat0 = pd.merge(pairs_df, shop_cat0_sales, on=['shop_id', 'item_category_id'], how='left').fillna(0)\n","print(\"df rows = \" + str(len(all_shop_cat0)) + \"  and 84 x 60 = \" + str(84*60))\n","all_shop_cat0.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iyW1-frXoJ2i","colab":{}},"source":["shop_shop_units0 = all_shop_cat0.pivot(index='item_category_id',columns='shop_id',values='shop_units_pct')\n","print(len(shop_shop_units0))\n","shop_shop_units0.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U-EWL6PsoJ2j","colab":{}},"source":["corr_matrix_cat0 = shop_shop_units0.corr()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e1znmmLEoJ2l","colab":{}},"source":["corr_matrix_cat0 = corr_matrix_cat0.applymap(lambda x: 0 if abs(x) < 0.01 else x)  # stop the heatmap from printing -0.0 in some places instead of always 0.0, and no funky formatting from \":.1g\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FEbhUpzIoJ2m","colab":{}},"source":["corr_matrix_cat0.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"24RZlSsZoJ2n","colab":{}},"source":["plt.rcParams[\"figure.figsize\"] = [50,50]\n","from IPython.display import Javascript\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 5000})'''))\n","\n","h = sns.heatmap(corr_matrix_cat0,\n","                cmap= sns.color_palette(\"RdBu_r\", 11), #sns.diverging_palette( 30, 50, s=90, l=60, n=11, center=\"dark\"), #'coolwarm',\n","                annot=True,\n","                fmt= '.0g', #\".1g\", #\".1f\",\n","                annot_kws={'size':12},\n","                cbar=True,\n","                cbar_kws={\"shrink\": 0.7},\n","                square=True)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"igmtMP4xoJ2p"},"source":["Observations:\n","Shops 20 and 55 not so don't correlate well with the other shops at all, yet both of these shops are in the test set.  We may want to train these two shops separately(?).\n","\n","But, first, let's look at the correlation results from the grouped categories item_category3 and item_category4:"]},{"cell_type":"markdown","metadata":{"id":"oy7MYEAhfDhw","colab_type":"text"},"source":["####2.4.3.2) using manually-generated item_category3 categorization"]},{"cell_type":"code","metadata":{"id":"SVtr2nSUjYa0","colab_type":"code","colab":{}},"source":["# Compute the number of times a shop_id-category_id pair appears in the sales_train dataset\n","# Also, for each shop_id-category_id pair, compute the number of the particular items in that category sold at that shop (in the entire sales_train dataset)\n","# Finally, for each shop_id-category_id pair, compute the percentage of the above two items with respect to the total numbers for the shop\n","#   and with respect to the total numbers for the item category\n","shop_cat3_sales = train_cats.groupby(['shop_id','cat3_id'], as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","shop_cat3_sales.columns = ['shop_id','cat3_id','n_train_rows','n_units_sold']\n","shop_cat3_sales['n_units_sold'] = shop_cat3_sales['n_units_sold'].astype(int)\n","shop_cat3_sales = shop_cat3_sales.merge(cat3_sales, on = 'cat3_id')\n","shop_cat3_sales = shop_cat3_sales.merge(shops_sales, on = 'shop_id')\n","shop_cat3_sales['shop_row_pct'] = 100 * shop_cat3_sales.n_train_rows / shop_cat3_sales.shop_total_train_rows\n","shop_cat3_sales['shop_units_pct'] = 100 * shop_cat3_sales.n_units_sold / shop_cat3_sales.shop_total_units_sold\n","shop_cat3_sales['cat3_row_pct'] = 100 * shop_cat3_sales.n_train_rows / shop_cat3_sales.cat3_total_train_rows\n","shop_cat3_sales['cat3_units_pct'] = 100 * shop_cat3_sales.n_units_sold / shop_cat3_sales.cat3_total_units_sold\n","# clean up rows where total number of units sold = 0 (item_units_fraction = +/- inf)... note that we don't have to do this for shops column because total is never 0\n","shop_cat3_sales['cat3_units_pct'] = shop_cat3_sales['cat3_units_pct'].replace([np.inf, -np.inf], 0)\n","# drop the unneeded columns for easier readability\n","shop_cat3_sales.drop(['cat3_total_train_rows','cat3_total_units_sold','shop_total_train_rows','shop_total_units_sold'], axis=1, inplace=True)\n","\n","print(shop_cat3_sales.describe())\n","print(shop_cat3_sales.head())\n","\n","# I would like to look at correlations between shops, between item categories, and between shop-item_category pairs\n","# For this, I need to fill out the shop_catX_sales table to include all possible shop-item_category pairs, using itertools.product\n","\n","l1=shops['shop_id'].unique().tolist()\n","l2=cat3_sales['cat3_id'].unique().tolist()\n","pairs_df =  pd.DataFrame(list(product(l1,l2))).rename(columns={0:'shop_id',1:'cat3_id'})\n","shop_cat3_sales['item_category3'] = shop_cat3_sales['item_category3'].cat.add_categories(0) # need to do this to prevent error when using fillna(0)\n","all_shop_cat3 = pd.merge(pairs_df, shop_cat3_sales, on=['shop_id', 'cat3_id'], how='left').fillna(0)\n","print(\"df rows = \" + str(len(all_shop_cat3)) + \"  and 12 x 60 = \" + str(12*60))\n","print(all_shop_cat3.head())\n","\n","shop_shop_units3 = all_shop_cat3.pivot(index='cat3_id',columns='shop_id',values='shop_units_pct')\n","print(len(shop_shop_units3))\n","print(shop_shop_units3.head())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KmPc8BX6jbi_","colab_type":"code","colab":{}},"source":["corr_matrix_cat3 = shop_shop_units3.corr()\n","corr_matrix_cat3 = corr_matrix_cat3.applymap(lambda x: 0 if abs(x) < 0.01 else x)  # stop the heatmap from printing -0.0 in some places instead of always 0.0, and no funky formatting from \":.1g\"\n","print(corr_matrix_cat3.head())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CwuQw6MWjb3G","colab_type":"code","colab":{}},"source":["plt.rcParams[\"figure.figsize\"] = [50,50]\n","from IPython.display import Javascript\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 5000})'''))\n","\n","h = sns.heatmap(corr_matrix_cat3,\n","                cmap= sns.color_palette(\"RdBu_r\", 11), #sns.diverging_palette( 30, 50, s=90, l=60, n=11, center=\"dark\"), #'coolwarm',\n","                annot=True,\n","                fmt= '.0g', #\".1g\", #\".1f\",\n","                annot_kws={'size':12},\n","                cbar=True,\n","                cbar_kws={\"shrink\": 0.7},\n","                square=True)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_Xo3AWgUv_8s"},"source":["Observations:\n","Shop 55 correlation with other shops is much improved with the change in item categorization.\n","Shop 20 continues to be poorly correlated with everything except shop 40.  (And shop 40 correlates strongly only with shop 20.)\n","\n","Shop 9 is not extremely well correlated with other shops, but appears better than shops 12, 13, 20, 40, 55\n","\n","Shops 10 and 11 correlate very well with each other, and behave very much the same in correlations with other shops.\n","\n","Note that shops 12, 20, and 55 are explicitly named as online shops (the only 3 explicit online shops).  Shop 40 has the exact same name as shop 39, except that the word \"island\" is appended to shop 39's name.  I'm thinking this may be a translation issue, and \"island\" indicates it is a mail-order branch of shop 39 (which is confirmed by web search to be in an SEC).\n","I was curious about the similarity in (poor) correlation with other shops as shown by shops 12 and 13.  It turns out that shop 13 is \"Behetle,\" which is a supermarket chain (i.e., mostly selling food, and probably not many of the electronics-type items in the original item_category list).  \n","\n","Need to dig deeper on shop 40 == online ??\n","Need to dig deeper on shop types, possibly adding \"supermarket\" to the list of possible shop categories.\n","Need to see if online shops (especially 20 and 55, and perhaps 40, and even #12) have a time-lag correlation with the brick-and-mortar type shops.  Interesting that #55 seems to have *negative* correlation with many other shops when using the original item_category_id values.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"T0O516lDe4UC","colab":{}},"source":["# Do the above analysis for item_category4\n","\n","# dig deeper on shop 40 --> online?\n","# possibly add \"supermarket\" to list of categories of shops (= shop 13, Behetle;  maybe other shops too)\n","\n","# verify shops 10 and 11 correlate with each other strongly and have similar correlations with other shops, so we can potentially combine (untested) shop 11 into shop 10 for training\n","\n","# need to do time-lag correlations... have to automate it so we can run through a bunch of different time lags.  Try to figure out what to do with shop 9.\n","#     look especially at the online stores for their predictive potential\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n_Fct16CfOLf","colab_type":"text"},"source":["####2.4.3.3) using manually-generated item_category4 categorization"]},{"cell_type":"code","metadata":{"id":"ppYa4rr8CKry","colab_type":"code","colab":{}},"source":["# item_category4\n","\n","# Compute the number of times a shop_id-category_id pair appears in the sales_train dataset\n","# Also, for each shop_id-category_id pair, compute the number of the particular items in that category sold at that shop (in the entire sales_train dataset)\n","# Finally, for each shop_id-category_id pair, compute the percentage of the above two items with respect to the total numbers for the shop\n","#   and with respect to the total numbers for the item category\n","shop_cat4_sales = train_cats.groupby(['shop_id','cat4_id'], as_index=False).agg({'item_cnt_day': ['count', 'sum']})\n","shop_cat4_sales.columns = ['shop_id','cat4_id','n_train_rows','n_units_sold']\n","shop_cat4_sales['n_units_sold'] = shop_cat4_sales['n_units_sold'].astype(int)\n","shop_cat4_sales = shop_cat4_sales.merge(cat4_sales, on = 'cat4_id')\n","shop_cat4_sales = shop_cat4_sales.merge(shops_sales, on = 'shop_id')\n","shop_cat4_sales['shop_row_pct'] = 100 * shop_cat4_sales.n_train_rows / shop_cat4_sales.shop_total_train_rows\n","shop_cat4_sales['shop_units_pct'] = 100 * shop_cat4_sales.n_units_sold / shop_cat4_sales.shop_total_units_sold\n","shop_cat4_sales['cat4_row_pct'] = 100 * shop_cat4_sales.n_train_rows / shop_cat4_sales.cat4_total_train_rows\n","shop_cat4_sales['cat4_units_pct'] = 100 * shop_cat4_sales.n_units_sold / shop_cat4_sales.cat4_total_units_sold\n","# clean up rows where total number of units sold = 0 (item_units_fraction = +/- inf)... note that we don't have to do this for shops column because total is never 0\n","shop_cat4_sales['cat4_units_pct'] = shop_cat4_sales['cat4_units_pct'].replace([np.inf, -np.inf], 0)\n","# drop the unneeded columns for easier readability\n","shop_cat4_sales.drop(['cat4_total_train_rows','cat4_total_units_sold','shop_total_train_rows','shop_total_units_sold'], axis=1, inplace=True)\n","\n","print(shop_cat4_sales.describe())\n","print(shop_cat4_sales.head())\n","\n","# I would like to look at correlations between shops, between item categories, and between shop-item_category pairs\n","# For this, I need to fill out the shop_catX_sales table to include all possible shop-item_category pairs, using itertools.product\n","\n","l1=shops['shop_id'].unique().tolist()\n","l2=cat4_sales['cat4_id'].unique().tolist()\n","pairs_df =  pd.DataFrame(list(product(l1,l2))).rename(columns={0:'shop_id',1:'cat4_id'})\n","shop_cat4_sales['item_category4'] = shop_cat4_sales['item_category4'].cat.add_categories(0) # need to do this to prevent error when using fillna(0)\n","all_shop_cat4 = pd.merge(pairs_df, shop_cat4_sales, on=['shop_id', 'cat4_id'], how='left').fillna(0)\n","print(\"df rows = \" + str(len(all_shop_cat4)) + \"  and 8 x 60 = \" + str(8*60))\n","print(all_shop_cat4.head())\n","\n","shop_shop_units4 = all_shop_cat4.pivot(index='cat4_id',columns='shop_id',values='shop_units_pct')\n","print(len(shop_shop_units4))\n","print(shop_shop_units4.head())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eGZC5nCMCT5k","colab":{}},"source":["corr_matrix_cat4 = shop_shop_units4.corr()\n","corr_matrix_cat4 = corr_matrix_cat4.applymap(lambda x: 0 if abs(x) < 0.01 else x)  # stop the heatmap from printing -0.0 in some places instead of always 0.0, and no funky formatting from \":.1g\"\n","print(corr_matrix_cat4.head())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k3SUbNbZCgen","colab_type":"code","colab":{}},"source":["plt.rcParams[\"figure.figsize\"] = [50,50]\n","from IPython.display import Javascript\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 5000})'''))\n","\n","h = sns.heatmap(corr_matrix_cat4,\n","                cmap= sns.color_palette(\"RdBu_r\", 11), #sns.diverging_palette( 30, 50, s=90, l=60, n=11, center=\"dark\"), #'coolwarm',\n","                annot=True,\n","                fmt= '.0g', #\".1g\", #\".1f\",\n","                annot_kws={'size':12},\n","                cbar=True,\n","                cbar_kws={\"shrink\": 0.7},\n","                square=True)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TV0uo3hdHLtd","colab_type":"text"},"source":["###Observations:  \n","Correlation differences are less striking with fewer categories (as expected).  Shops 9, 12, 13, 20, 40, 55 stand out.\n","\n","Shops 10 and 11 are close enough to each other that we can combine them for training.\n","\n","item_category3 seems to be a good balance between underfitting and overfitting\n","\n","</br>\n","\n","**Recommendations:**\n","* Merge shops 10 and 11\n","* Use item_category3 as a feature, rather than item_category_id\n","* Further investigation needed on shops 9, 12, 13, 20, 40, 55"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xr0FDeno_EUQ"},"source":["##2.5) ***items*** Dataset: EDA, Cleaning, and Feature Generation\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uD3G2taWX9pq"},"source":["####Quick look at original *items* dataset"]},{"cell_type":"code","metadata":{"id":"EbFjNekLZNI0","colab_type":"code","colab":{}},"source":["items.describe"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gj66wn48_mrf","colab_type":"code","colab":{}},"source":["items.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANpMF_Je_muq","colab_type":"code","colab":{}},"source":["items.info()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZfgOx-0_KXg"},"source":["###2.5.1) **Translate and Ruminate**\n","We will start by translating the Russian text in the dataframe, and add our ruminations on possible new features we can generate.\n","\n","The dataframe *items_transl* (equivalent to *items* plus a column for English translation) is saved as a .csv file so we do not have to repeat the translation process the next time we open a Google Colab runtime."]},{"cell_type":"markdown","metadata":{"id":"9fW3q2PF632z","colab_type":"text"},"source":["####Translation Code for 22,170-row *items* data set\n","Skip this section if you have already loaded the items_transl dataframe from previous translation efforts (save yourself about 15 hours)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mWQ_xzQg4nxr","colab":{}},"source":["#use this if you don't already have items_transl loaded:\n","temp_store = []\n","items_transl = items.copy(deep=True)\n","items_transl['en_item_name']= \"\"  # initialize an empty column"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s3tgcUE74tfC","colab":{}},"source":["# google translate API is reliable for me only if I submit fewer than about 1 request every 2 seconds\n","#   below is a loop to translate all item_name cells for the 22,170 rows of the items dataframe\n","\n","translator = Translator()\n","\n","progress_counter = 0\n","progress_interval = 500  # we will print out a row number every 500 translations, for confirmation things are working OK (it takes several hours at 2sec per translation)\n","for i in range(len(items_transl)):\n","  items_transl.at[i,'en_item_name'] = translator.translate(items_transl.at[i,'item_name'],src='ru',dest='en').text\n","  if i//progress_interval > progress_counter:\n","    progress_counter += 1\n","    print(\"Translation completed for row number: \" + str(i) + \" at \" + strftime(\"%H:%M\",localtime()))\n","  sleep(2)\n","\n","items_transl.to_csv(\"data_output/items_transl.csv\", index = False)\n","items_transl.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s0J5l5H98Xsh"},"source":["###Thoughts regarding items dataframe\n","Let's first look at how many training examples we have to work with..."]},{"cell_type":"markdown","metadata":{"id":"1lg7NbEchkuM","colab_type":"text"},"source":["Many of the items have similar names, but slightly different punctuation, or only very slightly different version numbers or types.  (e.g., 'Call of Duty III' vs. 'Call of Duty III DVD')\n","\n","One can expect that these two items would have similar sales in general, and by grouping them into a single feature category, we can eliminate some of the overfitting that might come as a result of the relatively small ratio of (training set shop-item-date combinations = 2935849)/(total number of unique items = 22170).  (This is an average of about 132 rows in the sales_train data for each shop-item-date combination that we are using to train our model.  Our task is to produce a monthly estimate of sales (for November 2015), so it is relevant to consider training our model based on how many sales in a month vs. how many sales in the entire training set.  Given that the sales_train dataset covers the time period from January 2013 to October 2015 (34 months), we have on average fewer than 4 shop-item combinations in our training set for a given item in any given month.  Furthermore, as we are trying to predict for a particular month (*November* 2015), it is relevant to consider how many rows in our training set occur in the month of November.  The sales_train dataset contains data for two 'November' months out of the total 34 months of data.  Another simple calculation gives us an estimate that our training set contains on average 0.23 shop-item combinations per item for November months.\n","\n","To summarize:\n","\n","*  *sales_train* contains 34 months of data, including 2935849 shop-item-date combinations\n","*  *items* contains 22170 \"unique\" item_id values\n","\n","In the *sales_train* data, we therefore have:\n","*  on average, 132 rows with a given shop-item pair for a given item_id\n","*  on average, 4 rows with a given shop-item pair for a given item_id in a given month\n","*  on average, 0.23 rows with a given shop-item pair for a given item_id in all months named 'November'\n","\n","If we wish to improve our model predictions for the following month of November, it behooves us to use monthly grouping of sales, or, even better, November grouping of sales.  This smooths out day-to-day variations in sales for a better monthly prediction.  However, the sparse number of available rows in the *sales_train* data will contribute to inaccuracy in our model training and predictions.\n","\n","Imagine if we could reduce the number of item_id values from 22170 to perhaps half that or even less.  Given that the number of rows for training (per item, on a monthly or a November basis) is so small, then such a reduction in the number of item_id values would have a big impact.  (The same is true for creating features to supplement \"shop_id\" so as to group and reduce the individuality of each shop - and thus effectively create, on average, more rows of training data for each shop-item pair."]},{"cell_type":"code","metadata":{"id":"FhHSfXNxsKxQ","colab_type":"code","colab":{}},"source":["items_transl.info()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9oSMeRVd7dvZ"},"source":["####NLP for feature generation from items dataframe\n","Automate the search for commonality among items, and create new categorical feature to prevent overfitting from close similarity between many item names"]},{"cell_type":"markdown","metadata":{"id":"EIn77EZzoxkY","colab_type":"text"},"source":["### start from here\n","\n","---\n","\n","\n","\n","---\n","\n","start from below when adding more rows to the items datafile translation\n"]},{"cell_type":"code","metadata":{"id":"bY5u83Wmw5-m","colab_type":"code","colab":{}},"source":["REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","MULTIPLE_WHITESPACE_RE = re.compile('[ ]{2,}')\n","STOPWORDS = set(stopwords.words('english'))  #using \"set\" speeds things up a little; note all stopwords are in lowercase\n","#print(\".\" in STOPWORDS)\n","def text_prepare(text):\n","    \"\"\"\n","        text: a string\n","        \n","        return: modified initial string\n","    \"\"\"\n","    text = text.lower() # lowercase text... need to do this before removing stopwords because stopwords are all lowercase\n","    text = REPLACE_BY_SPACE_RE.sub(' ',text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n","    text = BAD_SYMBOLS_RE.sub('',text) # delete symbols which are in BAD_SYMBOLS_RE from text\n","    text = MULTIPLE_WHITESPACE_RE.sub(' ',text)\n","    text = \" \".join([word for word in text.split(\" \") if word not in stopwords.words('english')]) # delete stopwords from text\n"," \n","    return text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZuzKi0D9g7j","colab_type":"code","colab":{}},"source":["sales_train.info()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uj6xalXKgxez","colab_type":"text"},"source":["#2.6 Core Data Sets:  EDA, Cleaning, Feature Generation\n","\n","**Optional Section**\n","\n","*  2.6) *sales_train* dataset EDA, outliers - cleaning\n","*  Include considerations with *test* dataset concerning overlap (or partial overlap) of *test* dataset inputs and *sales_train* training data"]},{"cell_type":"markdown","metadata":{"id":"0gz9vZkye8OX","colab_type":"text"},"source":["###2.6.1) *sales_train* data check for outliers or unusual trends"]},{"cell_type":"code","metadata":{"id":"ZwIu7XOVa_DL","colab_type":"code","colab":{}},"source":["# Rough examination of sales_train data -- Plot histograms and bar/box plots of the features/values\n","\n","fig = plt.figure(figsize=(25,15))\n","plt.subplots_adjust(hspace=.5)\n","\n","# amount of training data vs. shop_id\n","plt.subplot2grid((5,2), (0,0), colspan = 2, rowspan = 2)\n","sales_train['shop_id'].value_counts(normalize=False).plot(kind='bar')\n","plt.title('by SHOP_ID, number of rows in sales_train')\n","plt.grid(b=True, which='major', axis='y', color='#666666', linestyle='-')\n","plt.minorticks_on()\n","plt.grid(b=True, which='minor', axis='y', color='#999999', linestyle='-', alpha=0.3)\n","\n","# amount of training data vs. month\n","plt.subplot2grid((5,2), (2,0), colspan = 2)\n","sales_train['date_block_num'].value_counts(normalize=False).plot(kind='bar')\n","plt.title('by MONTH (date_block_num), number of rows in sales_train')\n","plt.grid(b=True, which='major', axis='y', color='#666666', linestyle='-')\n","\n","# amount of training data vs. item_id\n","n_items = items.item_id.nunique()\n","binwidth = 100\n","plt.subplot2grid((5,2), (3,0), colspan = 2)\n","sales_train['item_id'].plot(kind='hist', bins=np.arange(0, n_items+binwidth, binwidth))\n","plt.title('by ITEM_ID, number of rows in sales_train (histogram)')\n","plt.grid(b=True, which='major', color='#666666', linestyle='-')\n","plt.minorticks_on()\n","plt.grid(b=True, which='minor', axis='x', color='#999999', linestyle='-', alpha=0.3)\n","#plt.grid(b=None, which='minor', axis='y')\n","\n","# item_cnt_day boxplot to look at outliers\n","plt.subplot2grid((5,2), (4,0))\n","plt.xlim(min(0,sales_train.item_cnt_day.min()*1.1), sales_train.item_cnt_day.max()*1.1)  # scale so we can see negative values\n","#plt.xscale(\"log\")\n","plt.title('item_cnt_day values in sales_train')\n","sns.boxplot(x = sales_train.item_cnt_day)\n","\n","# item prices boxplot to look at outliers\n","plt.subplot2grid((5,2), (4,1))\n","plt.xlim(min(0, sales_train.item_price.min()*1.1), sales_train.item_price.max()*1.1)  # scale so we can see negative values\n","#plt.xscale(\"log\")\n","plt.title('item_price values in sales_train')\n","sns.boxplot(x = sales_train.item_price)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gHvoKr2fHlJ","colab_type":"text"},"source":["####2.1.1.1) *sales_train* data handling of outlier with negative price"]},{"cell_type":"code","metadata":{"id":"e3jHRTiiWAqu","colab_type":"code","colab":{}},"source":["# look more closely at the items with very low price:\n","low_price = sales_train.sort_values('item_price')[:100]\n","print(low_price.describe())\n","print(\"\\n\")\n","print(low_price.head())\n","\n","# seems to be one particular bad player... item 2973 from shop 6 in row 484683\n","#   look more at this item, shop, and item-shop combo:\n","print(\"\\n\")\n","print(2973 in test.item_id.to_list())\n","# this item is not in the test set... does it make up a significant amount of the train set?\n","\n","print(f\"Shop 32 = {shops_augmented.at[32,'en_shop_name']}\", end=\"\")\n","print(f\", Item 2973 = {items_transl.at[2973,'en_item_name']}\\n\")\n","\n","item2973 = sales_train[sales_train.item_id == 2973]\n","print(item2973.describe())\n","print(\"\\n\")\n","print(item2973.sort_values('item_price').head(10))\n","# only one sales_train row entry with price < 1000, and it is this negative outlier\n","\n","print(\"\\n\")\n","item2973shop32 = item2973[item2973.shop_id == 32]\n","print(item2973shop32.describe())\n","print(\"\\n\")\n","print(item2973shop32.sort_values('date').head(15))\n","# it looks like perhaps this shop had a discount clearance sale in summer 2013, and\n","#  then never sold the item again\n","\n","fig = plt.figure(figsize=(20,10))\n","#item2973.sort_values('date_block_num')['date_block_num'].value_counts().plot(kind='bar')\n","item2973.date_block_num.value_counts().reset_index().drop('index',axis=1).rename(columns={'date_block_num':'n_train_rows'}).plot(kind='bar')\n","plt.title('by MONTH (date_block_num), number of rows in sales_train for item 2973')\n","plt.grid(b=True, which='major', axis='y', color='#666666', linestyle='-')\n","\n","plt.show()\n","\n","# looks like this item has little bearing on sales in Nov. 2015, as its sales\n","#  died off (among all shops) by January 2015\n","# and, the one entry at row 484683 in sales_train with the negative price can be safely deleted"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_HvV-7C4fTIo","colab_type":"text"},"source":["####2.1.1.2) *sales_train* data ouliers with high item_price and high item_cnt_day"]},{"cell_type":"code","metadata":{"id":"1xMq27hRtz2T","colab_type":"code","colab":{}},"source":["# What are the outliers?:\n","outliers = pd.concat( [sales_train[sales_train.item_cnt_day > 800],\n","                       sales_train[sales_train.item_price > 1e5]],\n","                      ignore_index=True ) \n","print(outliers)\n","print(\"\\n\")\n","outlier_items = outliers.item_id.to_list()\n","outlier_shops = outliers.shop_id.to_list()\n","for row in range(len(outliers)):\n","  print(f\"Shop {outlier_shops[row]} = {shops_augmented.at[outlier_shops[row],'en_shop_name']}\", end=\"\")\n","  print(f\", Item {outlier_items[row]} = {items_transl.at[outlier_items[row],'en_item_name']}\")\n","  print(f\"Number of rows in sales_train for item {outlier_items[row]} = {len(sales_train[sales_train.item_id == outlier_items[row]])}\\n\")\n","\n","# 300,000 rubles is about $400.  1 ruble is $0.014, so it looks like perhaps item_price is not in rubles\n","# is item 6066 even in the test set?  \n","print(6066 in test.item_id.to_list())\n","print(sales_train[sales_train.item_id == 6066])\n","# no it is not... so, let's just delete that row from the training data\n","\n","# the other two items have a good number of rows in the sales_train dataset... let's look at their distributions\n","item20949 = sales_train[sales_train.item_id == 20949]\n","print(item20949.describe())\n","print(item20949.sort_values('item_cnt_day').tail(10))\n","item20949shop12 = item20949[item20949.shop_id == 12]\n","print(item20949shop12.describe())\n","print(item20949shop12.sort_values('item_cnt_day').tail(10))\n","print(20949 in test.item_id.to_list())\n","# Not good... shop12-item20949 is in our test set, but this pair only has three training rows, \n","#   and the rows don't seem correlated by date in a way that they would help us predict sales for November 2015\n","#   However, we do see a rather large sale in October 2015, and the sparseness of the other sales would\n","#   suggest that shop12 won't sell another load of these items in the following month.\n","# I would recommend we remove the three rows from the training data, and set the test prediction = 0 sales for shop12-item20949\n","#   as a 'hard-wired' entry after model training and predictions are done.\n","\n","item11373 = sales_train[sales_train.item_id == 11373]\n","print(item11373.describe())\n","print(item11373.sort_values('item_cnt_day').tail(10))\n","# only shop 12 sells item 11373; no other shops do\n","print(11373 in test.item_id.to_list())\n","fig = plt.figure(figsize=(25,9))\n","item11373.drop(2909818).sort_values('date_block_num').groupby('date_block_num')['item_cnt_day'].sum().plot(kind='bar')\n","plt.title('by MONTH (date_block_num), number of units of item 11373 sold by shop 12')\n","plt.grid(b=True, which='major', axis='y', color='#666666', linestyle='-')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhpVzlReIYyE","colab_type":"code","colab":{}},"source":["price11373 = item11373.drop(2909818).sort_values('date')\n","\n","fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n","fig.suptitle('by day, price of units of item 11373 sold by shop 12')\n","ax1.scatter(price11373['date'], price11373['item_price'],color='red')\n","ax1.grid(b=True, which='major', axis='y', color='#666666', linestyle='-')\n","plt.ylim(0,100)\n","ax2.scatter(price11373['date'], price11373['item_price'],color='red')\n","ax2.grid(b=True, which='major', axis='y', color='#666666', linestyle='-')\n","\n","plt.show()\n","\n","# the price drops below 40 only on one day, and not by much\n","#  so, the sales_train row showing price of 0.9 and count of > 2000 seems to be a mistake\n","#  and, the best way I see of correcting it is to remove this row from sales_train before training"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LmQWKGYIfimL","colab_type":"text"},"source":["###2.6.2) Closer look at *sales_train* in conjunction with *test* dataset characteristics"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F6xg0EGFrN9N"},"source":["####2.6.2.1) Some info on *sales_train* and *test* datasets"]},{"cell_type":"code","metadata":{"id":"8pUF-6OkN28o","colab_type":"code","colab":{}},"source":["n_item_ids = items.item_id.nunique()\n","n_shop_ids = shops.shop_id.nunique()\n","\n","print(\"------------------------------------------------------------------\")\n","print(\"               SALES_TRAIN DATA SET\")\n","print(\"------------------------------------------------------------------\")\n","n_train_item_ids = sales_train.item_id.nunique()\n","n_train_shop_ids = sales_train.shop_id.nunique()\n","n_train_rows = len(sales_train)\n","train_shop_item_pairs = sales_train.groupby(['shop_id','item_id']).size().reset_index().rename(columns={0:'n_train_rows'})\n","train_shop_item_pairs['pair'] = list(zip(train_shop_item_pairs.shop_id, train_shop_item_pairs.item_id))  # make a column of (shop, item) tuples to compare with test set\n","n_train_shop_item_pairs = train_shop_item_pairs.pair.nunique()\n","\n","print(f\"Total number of rows in sales_train dataset: {n_train_rows:,d}\")\n","print(\"\\nWe are evaluated on predictions for a given shop-item pair, so let's group and explore:\")\n","print(\" _ _ _ _ _  grouping of sales_train dataset to show shop-item pairs _ _ _ _ _ _\")\n","print(train_shop_item_pairs.head())\n","print(\"\\nDataframe description of sales_train data grouped by shop_id and item_id:\")\n","print(train_shop_item_pairs.describe())\n","\n","print(\"\\nThere are some apparent deficiencies in *sales_train* data:\")\n","print(\"  1. SMALL AMOUNT OF TRAINING DATA:  Most of the data we are able to train on, for a given shop-item pair,\")\n","print(\"         has relatively few rows present in the sales_train set, for most of the available shop-item pairs\")\n","print(\"         (median is only 3 rows of training data for shop-item pairs, and 75th pctile is only 7 rows).\")\n","print(\"  2. MISSING ITEMS: sales_train data does not contain rows for each and every item_id present in the items dataset:\")\n","print(f\"        Only {n_train_item_ids:,d} of {n_item_ids:,d} possible item_ids are in sales_train.\", end=\" \")\n","print(f\" {n_item_ids - n_train_item_ids} are missing.\")\n","print(\"        Good news: the maximum value of item_id in sales_train indicates the items dataset contains all items_ids in sales_train.\")\n","print(\"  3. SHOPS (good news): sales_train data does contain transactions for each and every shop_id present in the shops dataset:\")\n","print(f\"        {n_train_shop_ids} of {n_shop_ids} possible shop_ids are in sales_train.\")\n","print(\"  4. MISSING SHOP-ITEM PAIRS: sales_train data does not contain rows for a large number of the possible shop-item pairs:\")\n","print(f\"        Number of unique shop_id-item_id pairs in sales_train dataset: {n_train_shop_item_pairs:,d}.\")\n","print(f\"        Possible combinations of shop_ids and item_ids present in sales_train: {n_train_item_ids:,d} x {n_train_shop_ids} = {n_train_item_ids * n_train_shop_ids:,d}\", end=\" \")\n","print(f\" ({n_train_item_ids * n_train_shop_ids - n_train_shop_item_pairs:,d} are not present in sales_train data.)\")\n","print(f\"        Possible combinations *all* available shop_ids and item_ids: {n_item_ids:,d} x {n_shop_ids} = {n_item_ids * n_shop_ids:,d}.\", end=\" \")\n","print(f\" ({n_item_ids * n_shop_ids - n_train_shop_item_pairs:,d} are not present in sales_train data.)\")\n","print(f\"        Reminder FYI:  Total number of rows present in the given sales_train dataset: {n_train_rows:,d}\")\n","print(\"\\nLet's see the implications when we compare with the test dataset characteristics...\")\n","print(\"\\n\\n\")\n","\n","\n","print(\"------------------------------------------------------------------\")\n","print(\"               TEST DATA SET\")\n","print(\"------------------------------------------------------------------\")\n","n_test_item_ids = test.item_id.nunique()\n","n_test_shop_ids = test.shop_id.nunique()\n","n_test_rows = len(test)\n","print(f\"Total number of rows in test dataset: {n_test_rows:,d}\")\n","print(f\"Number of unique item_ids in test set: {n_test_item_ids:,d} out of {n_item_ids:,d} possible.\")\n","# get category info on the 5100 items in the test data set:\n","items_transl_with_cat = items_transl.merge(item_categories_augmented[['item_category_id','en_cat_name','item_category1','item_category2','item_category3','item_category4']], \n","                                           how='left', on='item_category_id').rename(columns={'en_cat_name':'category'})\n","test_with_cat = test.merge(items_transl_with_cat, how='left', on='item_id').drop(['item_name'], axis=1).rename(columns={'en_item_name':'item_name'})\n","test_category_ids = test_with_cat.item_category_id.unique()\n","test_category_names = test_with_cat.category.unique()\n","test_item_category3_names = test_with_cat.item_category3.unique()\n","test_item_category4_names = test_with_cat.item_category4.unique()\n","original_category_names = item_categories_augmented.en_cat_name.unique()\n","item_category3_names = item_categories_augmented.item_category3.unique()\n","item_category4_names = item_categories_augmented.item_category4.unique()\n","# which categories don't get tested?\n","untested_category_names = [x for x in original_category_names if x not in test_category_names]\n","untested_item_category3_names = [x for x in item_category3_names if x not in test_item_category3_names]\n","untested_item_category4_names = [x for x in item_category4_names if x not in test_item_category4_names]\n","print(f\"Number of unique item_category_ids in test set: {len(test_category_ids)} out of {len(item_categories)} possible.\")\n","print(f\"Untested category names: {untested_category_names}\")\n","print(\"(all categories explicitly involving 'PS2' or 'tickets' or 'Net carriers' are not in the test set)\")\n","print(f\"Number of unique item_category3 values in test set: {len(test_item_category3_names)} out of {len(item_category3_names)} possible.\")\n","print(f\"Untested item_category3 names: {untested_item_category3_names}\")\n","print(\"(these untested item_category3 names correspond directly with 'tickets' and 'Net carriers' from the original category names)\")\n","print(f\"Number of unique item_category4 values in test set: {len(test_item_category4_names)} out of {len(item_category4_names)} possible.\")\n","print(f\"Untested item_category4 names: {untested_item_category4_names if untested_item_category4_names else 'None'}\")\n","print(\"\\nI'm liking item_category3 for use as a training feature, moreso than the original item_category designations\\n\")\n","print(f\"Number of unique shop_ids in test set: {n_test_shop_ids} out of {n_shop_ids} possible.\")\n","print(f\"Number of possible unique combinations from item and shop ids present in the test set: {n_test_item_ids:,d} x {n_test_shop_ids} = {n_test_item_ids * n_test_shop_ids:,d}\")\n","print(\"\\nIt's apparent that the test set uses only select item_ids and shop_ids, yet creates all possible\")\n","print(\"pairings of these select items and shops for us to predict, and every row in test set has a unique shop-item pair.\")\n","print(\"   (because N rows in test set == N possible unique combinations of item and shop ids in test set)\")\n","print(\"\\nAs the test set uses fewer item_ids and fewer shop_ids than the train set, it is apparent that the train set\")\n","print(\"contains rows with shop-item pairs that are not present in the test set.\")\n","print(\"\\nTherefore, we either have extranneous train data to discard, or we will need to generalize\")\n","print(\"these non-corresponding train rows to help train the shop-item pairs that *are* relevant to test.\")\n","print(\"\\nNote:  In the EDA code further below, we will look more closely at shops, items, shop-item pairs,\")\n","print(\"and item_categories to develop a plan for discarding or generalizing train data, so that we make\")\n","print(\"best use of the sales_train data without overfitting or being influenced by outliers.\")\n","print(\"\\nLet's group the test dataset into shop-item pairs like we did above with the sales_train dataset,\")\n","print(\"for a closer look at overlap between the two datasets:\")\n","test_shop_item_pairs = test.groupby(['shop_id','item_id']).size().reset_index().rename(columns={0:'n_test_rows'})\n","test_shop_item_pairs['pair'] = list(zip(test_shop_item_pairs.shop_id, test_shop_item_pairs.item_id))  # make a column of (shop, item) tuples to compare with train set\n","test_shop_item_pairs['pair_in_train'] = test_shop_item_pairs.pair.isin(train_shop_item_pairs.pair)  # Boolean list to see if a given test shop-item pair is present in the train set\n","test_shop_item_pairs['item_in_train'] = test_shop_item_pairs.item_id.isin(train_shop_item_pairs.item_id)  # Boolean list to see if a given test shop-item pair is present in the train set\n","n_pairs_in_test_and_train = test_shop_item_pairs['pair_in_train'].sum()\n","n_rows_items_in_test_and_train = test_shop_item_pairs['item_in_train'].sum()\n","\n","# add a column showing the number of rows in the sales_train set associated with each pair\n","test_shop_item_pairs = test_shop_item_pairs.merge(train_shop_item_pairs[['n_train_rows','pair']], how='left', on='pair').fillna(0)\n","# compute how many rows in the sales_train dataset deal with a test pair, a test item, and a test shop\n","n_rows_in_train_for_test_pairs = test_shop_item_pairs['n_train_rows'].sum()\n","n_rows_in_train_for_test_items = train_shop_item_pairs[train_shop_item_pairs['item_id'].isin(test_shop_item_pairs.item_id)]['n_train_rows'].sum()\n","n_rows_in_train_for_test_shops = train_shop_item_pairs[train_shop_item_pairs['shop_id'].isin(test_shop_item_pairs.shop_id)]['n_train_rows'].sum()\n","\n","test_items = test.groupby(['item_id']).size().reset_index().rename(columns={0:'n_test_rows'})\n","test_items['item_in_train'] = test_items.item_id.isin(train_shop_item_pairs.item_id)  # Boolean list to see if a given test shop-item pair is present in the train set\n","n_items_in_test_and_train = test_items['item_in_train'].sum()\n","\n","\n","print(\"\\n _ _ _ _ _  grouping of test dataset to show shop-item pairs _ _ _ _ _ _\")\n","print(test_shop_item_pairs.head())\n","print(\"\\nDataframe description of test data grouped by shop_id and item_id:\")\n","print(test_shop_item_pairs.describe())\n","print(f\"\\nTotal number of rows in test dataset: {n_test_rows:,d}\")\n","print(f\"Number of 'True' entries in the pair_in_train column: {n_pairs_in_test_and_train:,d}\")\n","print(f\"Number of 'True' entries in the item_in_train column: {n_rows_items_in_test_and_train:,d}\")\n","print(f\"Number of unique item_ids with 'True' entries in the item_in_train column: {n_items_in_test_and_train:,d}\")\n","\n","print(\"\\nLook at how many sales_train rows directly correlate with either the shops or items in the test set:\")\n","print(f\"Reminder FYI:  Total number of rows present in the given sales_train dataset: {n_train_rows:,d}\")\n","print(f\"Number of rows in sales_train dataset that have shop-item pair found in test: {int(n_rows_in_train_for_test_pairs):,d} or {100*n_rows_in_train_for_test_pairs/n_train_rows:.0f}%\")\n","print(f\"Number of rows in sales_train dataset that have item_id found in test: {int(n_rows_in_train_for_test_items):,d} or {100*n_rows_in_train_for_test_items/n_train_rows:.0f}%\")\n","print(f\"Number of rows in sales_train dataset that have shop_id found in test: {int(n_rows_in_train_for_test_shops):,d} or {100*n_rows_in_train_for_test_shops/n_train_rows:.0f}%\")\n","\n","print(\"\\nThis further exposes deficiencies in the *sales_train* dataset:\")\n","print(f\"    1. SHOP-ITEM PAIR OVERLAP:  Only {n_pairs_in_test_and_train:,d} shop-item pairs are present in both test and train datasets.\")\n","print(f\"       Therefore, {n_test_rows - n_pairs_in_test_and_train:,d} pairs are present only in the test dataset.\")\n","print(f\"    2. ITEM OVERLAP:  Only {n_items_in_test_and_train:,d} items are present in both test and train datasets.\")\n","print(f\"       Therefore, {n_test_item_ids - n_items_in_test_and_train:,d} items are present only in the test dataset.\")\n","print(\"       Good news: the maximum value of item_id in test set indicates the items dataset contains all items_ids in test.\")\n","print(f\"    3. More good news: sales_train contains rows for all {n_shop_ids} possible shops, thus it contains data for all {n_test_shop_ids} shops in the test set.\")\n","print(\"\\nRoughly half of our test predictions will be on shop-item pairs for which we have no previous data.\")\n","print(f\"Almost 15% of those {n_test_rows - n_pairs_in_test_and_train:,d} 'non-trained' shop-item pairs in the test set involve *item_id* for which we have no training data.\")\n","print(\"\\nSlightly less than half of our training data is directly associated with a shop-item pair present in our test set.\")\n","print(\"\\n------------------------------------------------------------------\")\n","print(\"\\nConclusion:  As important as it is for our model to generalize in time (to predict sales in the future month),\")\n","print(\"our model will also need to generalize substantially in terms of item and in terms of shop-item pair.\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MIJD-tol-l1l","colab":{}},"source":["'''\n","#################################################\n","#  Do NOT run unless recreating from beginning\n","#    This computation already stored in data file\n","#################################################\n","\n","# add an extra column to the item_categories_augmented data file, indicating T/F if that row is in the test set\n","\n","test_cats = test_with_cat.item_category_id.unique()\n","item_categories_augmented[\"item_cat_tested\"] = False\n","for i in test_cats:\n","  item_categories_augmented.at[i,\"item_cat_tested\"] = True\n","\n","item_categories_augmented.to_csv(\"data_output/item_categories_augmented.csv\", index=False)\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YRBuVmRzrhi1"},"source":["####2.6.2.2) Analysis and treatment of 'nasty' shop_id - item_id pairs \n","(those pairs present in the test set, but with little or no training data)\n","\n","tbd... ongoing; not implemented in cleaning or feature generation yet"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4H321OGTgQio","colab":{}},"source":["# Let's take a closer look at the bad pairs, shops, items (present in test but not in train, and/or present in both test and train, but with very few train rows)\n","print(\"\\nNasty Pairs / Items / Shops are those present in the test set, but have little representation in the train set\")\n","nasty_pairs = test_shop_item_pairs[test_shop_item_pairs.n_train_rows < 2]\n","print(f\"\\nNumber of test set rows with shop-item pairs found in only 0 or 1 training rows = {len(nasty_pairs)}\")\n","print(f\"Number of shops present in nasty pairs (fewer than 2 training examples) = {nasty_pairs.shop_id.nunique()}\")\n","print(f\"Number of items present in nasty pairs (fewer than 2 training examples) = {nasty_pairs.item_id.nunique()}\\n\")\n","#print(nasty_pairs.head())\n","print(\"\\n\")\n","#print(nasty_pairs.describe())\n","nasty_shops_train = sales_train.groupby('shop_id').count().reset_index().rename(columns={'item_id':'n_training_rows'})[['shop_id','n_training_rows']]\n","nasty_shops_train = nasty_shops_train[nasty_shops_train.shop_id.isin(nasty_pairs.shop_id)]\n","\n","print(\"By shop in the test set, how many rows do we have in the train set?  (NOTE LOG Y Axis):\")\n","print(\"\\nLooking only at the 42 shops present in the test set, and not including any item information, we find:\")\n","print(f\"Shop 36 has the least training data, with a total of {sales_train[sales_train.shop_id == 36]['shop_id'].count()} rows in the sales_train dataset (paired with *any* item).\")\n","print(f\"  Note that Novosibirsk SEC 'Gallery Novosibirsk' is shop 36, and is similar to 37 in name.\")\n","print(f\"Shop 34 has a total of {sales_train[sales_train.shop_id == 34]['shop_id'].count()} rows in the sales_train dataset (paired with *any* item).\\n\")\n","plt.rcParams[\"figure.figsize\"] = [15,4]\n","nasty_shops_train.plot(kind='bar',x='shop_id',y='n_training_rows', logy=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"amvBdxvbzcZZ","colab_type":"code","colab":{}},"source":["nasty_items_train = sales_train.groupby('item_id').count().reset_index().rename(columns={'shop_id':'n_training_rows'})[['item_id','n_training_rows']]\n","nasty_items_train = nasty_items_train[nasty_items_train.item_id.isin(nasty_pairs.item_id)]  # keep only items that are in the set of 5100 test set items\n","nasty_items_train = nasty_items_train.merge(test_items, how='right', on='item_id').fillna(0)  # include the items that are only found in the test set\n","nasty_items_train = nasty_items_train[nasty_items_train.n_training_rows < 2][['item_id','n_training_rows']]\n","print(nasty_items_train.head())\n","print(nasty_items_train.describe())\n","\n","print(\"\\nA total of 83 of the items in the test set have only 1 row in the train set.\")\n","print(\"And, as determined earlier, 363 items in the test set have 0 rows in the train set\")\n","print(f\"Total items with 0 or 1 rows in train set: {len(nasty_items_train)}\")\n","print(\"Let's have a look at some of their item names...\\n\")\n","nasty_items_train2 = nasty_items_train.merge(items_transl_with_cat, how='left', on='item_id').drop(['item_name'], axis=1).rename(columns={'en_item_name':'item_name'})\n","print(nasty_items_train2[['item_name','category']].head())\n","\n","print(\"\\nThere are too many items to deal with manually in some fashion at this time,\")\n","print(\"so let's reduce the info by grouping into item categories.\")\n","nasty_categories = nasty_items_train2.groupby(['item_category_id','category']).count().reset_index()[['item_id','item_category_id','category']].rename(columns={'item_id':'n_nasty_items'})\n","print(f\"Total number of unique item_category_id values present in 'nasty items' is: {nasty_categories.category.nunique()}\\n\")\n","print(nasty_categories.head())\n","nastiest_cat_row = nasty_categories.loc[nasty_categories.n_nasty_items.argmax()]\n","print(f\"\\nThe nastiest category is #{nastiest_cat_row.item_category_id}, '{nastiest_cat_row.category}'\\n\")\n","\n","plt.rcParams[\"figure.figsize\"] = [15,4]\n","nasty_categories.plot(kind='bar',x='item_category_id',y='n_nasty_items', logy=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_gyL17RQzWWY","colab_type":"code","colab":{}},"source":["print(\"\\nAnd let's have a look at item_category1 to see if there is a tighter distribution:\")\n","nasty_Subcategories1 = nasty_items_train2.groupby('item_category1').count().reset_index()[['item_id','item_category1']].rename(columns={'item_id':'n_nasty_items'})\n","print(f\"Total number of unique item_category1 values present in 'nasty items' is: {nasty_Subcategories1.item_category1.nunique()}\\n\")\n","nasty_Subcategories1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wWaSS9TafNKm","colab_type":"code","colab":{}},"source":["# to be continued..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bOd1bPrNh5Kw","colab_type":"text"},"source":["##**Summary of sales_train vs. test datasets:**"]},{"cell_type":"markdown","metadata":{"id":"nSRNAtTYV3mW","colab_type":"text"},"source":["###**Numerical Recap:**\n"]},{"cell_type":"markdown","metadata":{"id":"WlPB7cHaWDdy","colab_type":"text"},"source":["*  There are 424,124 unique pairs of \"shop_id\" with \"item_id\" in the 2,935,849 rows of the *sales_train* dataset\n","\n","*  Half the shop-item pairs have 3 or fewer rows in the entire *sales_train* dataset, and at least 25% of the shop-item pairs have only 1 row in the *sales_train* dataset.\n","\n","*  There are 22,170 unique item_id values in the *items* dataset, of which only 21,807 are present in the *sales_train* dataset.  Only 5,100 of the 22,170 items are present in the *test* dataset.  The 363 items present in the *items* dataset but not in the *sales_train* dataset are ***all present*** in the *test* set.  Therefore, of the 5,100 unique items in the *test* set, only 5,100 - 373 = 4737 items are also present in the *sales_train* set.  Our model will have to make predictions on sales of 363 items for which we have no historical record of ever being sold.\n","\n","*  There are 60 unique shop_id values in the *shops* dataset, all of which are present in the *sales_train* dataset, but only 42 of which are present in the *test* dataset.\n","\n","*  The *test* dataset has 214,200 rows.  This happens to be equal to 5,100 \\* 42, which is the number of possible unique pairings for the shop_ids and item_ids present in the *test* dataset.  There are no duplicate rows in the *test* dataset, so we know that the *test* set contains every possible pairing of the 42 shops with the 5100 items.\n","\n","*  Of the 214,200 unique shop-item pairs in the *test* set, only 111,404 of them are present in the *sales_train* data set.  Therfore, our model will need to predict sales for a very large number of items at shops that have no recorded history of selling that item previously.  This type of \"novel\" prediction makes up roughly half of the total predictions we need to make.\n","\n","*  All of the 42 unique *test* set shop_ids are present in both the set of shop-item pairs common to *test* and *sales_train* as well as the set of shop-item pairs found only in the *test*\n","\n","*  All 5,100 unique item_id values in the *test* set are found in the set of shop-item pairs found in *test* but not in *sales_train*.  Only 4,716 unique item_id values are in the *test* set corresponding to shop-item pairs that are present in both *test* and *sales_train*."]},{"cell_type":"markdown","metadata":{"id":"iw4i9S8ZVSiZ","colab_type":"text"},"source":["###**Issues:**\n","\n","1.  LACK OF TRAINING DATA: For most shop-item pairs, the sales_train training data has relatively few rows describing any day's transactions.  Statstical analysis indicates that at least half the shop-item pairs have 3 or fewer rows in the sales_train dataset.  (*Caveat*:  this could be a manifestation of unnecessary rows in the items dataset.  It appears as though the items dataset does not truly describe 21,700 very different objects, but rather has multiple rows referring to the same (or nearly the same) object, and are in separate rows because one shop may have slightly different nomenclature from another shop.  We will look at this below, and see if it is feasible to compress the items dataset.)\n","\n","2.  NO TRAINING EXAMPLES FOR HALF OF TEST QUERIES:  Roughly half the shop-item pairs present in the test dataset have no training data present in the sales_train dataset.  Almost 15% of these 'no-train-data' shop-item pairs in the test set come from item_ids which are not present at all in the sales_train dataset."]},{"cell_type":"markdown","metadata":{"id":"yJ5D5-vcVnLf","colab_type":"text"},"source":["###**Observations:**\n","\n","1.  As per the explicit guidelines for the competition, our model must be able to generalize in time -- such that it can predict an unknown future (i.e., a test set that has dates which are not present in the training set).  From the above EDA, it is apparent that our model also needs to be powerful in generalizing for item_id and for shop_id-item_id pairs (i.e., the test set includes item_ids and shop_id-item_id pairs which are not present in the training set).  There are a substantial number of these non-overlapping train/test items and shop-item pairs.\n","\n","2.  For feature generation and model inputs, one might focus on the competition guidelines and create such things based on time (e.g., only looking at November sales, or looking at the predictive power of a previous month's sales (or previous day/week/season/year)) by creating time-lag inputs to the model.  We know from the above EDA that it is also going to be important to create features based on item_id and shop_id-item_id pair.  For example, do some item_ids have predictive power for other item_ids in time (not just for the same item_id, but generalized to other item_ids as well)?  Or, do some item_ids behave similarly to other item_ids such that we can group them and essentially have more training data for a given item_id?  (And, can we gather information from the items dataset or item_categories dataset that will help us generalize further to predict the test queries with items that have no training data.)  Similarly, shop-item pairs need to be considered."]},{"cell_type":"markdown","metadata":{"id":"nAq21FiktJzI","colab_type":"text"},"source":["###**A few questions to think about:**\n","\n","1.  At one extreme, we could train our model for the 111,404 shop-item pairs that are in both the *test* and the *sales_train* sets, and predict 0 sales for the shop-item pairs that historically have not existed?  </br>\n","Or, perhaps instead of setting predicted sales to 0 for those shop-item pairs with no historical information, we assign weights so that in some way the *test* set's novel pairs are treated differently?\n","\n","2.  More optimistically, we might think it is possible that certain shops lag other shops in obtaining items for sale.  For the novel *test* set pairs we should look to predict from our model by extrapolating *sales_train* data from similar shops and items in previous months.</br>\n","It could help to have a \"similarity\" metric between each of the shop-item pairs in the *test* set and each of the shop-item pairs in the *sales_train* set, and use this metric to restrict which shop-item pairs in the *sales_train* set are useful for predicting the sales of novel shop-item pairs in the *test* set.\n","\n","3.  Do we use the full *sales_train* dataset to train our model, or do we eliminate certain rows that pertain to irrelevant shop_id or item_id or shop-item pairs?</br>\n","The *shops* dataframe is only 60 rows, so it shouldn't be too hard to analyze manually and see if we can find some sort of data leak that allows us to use only a subset of the shops when training our model.  We should certainly keep all *sales_train* data pertaining to the 42 shops in the *test* set, but how much of the data from the other 18 shops should we use to train our model?\n","\n","4.  Because we will be predicting future sales of 363 items of which we have no record of ever being sold, we will need to rely heavily on item category or other similarities with items that we *do* have in our *sales_train* set.  We need to establish correlations between item_ids and shop_ids that we can exploit to predict future sales of shop-item pairs that have no historical record."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xUIfCz4U4sVF"},"source":["#XX) Below this markdown cell can be ignored\n","\n","These are just code snippets I used to debug how to accomplish some of the tasks above.  I may want to revisit them some day, so I am too scared to just delete them. :)"]},{"cell_type":"code","metadata":{"id":"z9NktFNZZNeC","colab_type":"code","colab":{}},"source":["shops_augmented.to_csv(\"data_output/shops_augmented.csv\", index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F0gfZR3Fnsr5","colab_type":"code","colab":{}},"source":["item_categories_transl.to_csv(\"data_output/item_categories_transl.csv\", index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P3VmOjdJy6YQ","colab_type":"code","colab":{}},"source":["# Utilize \"RateLimiter\" to limit location queries to one per second, as the free services tend to throttle rate of use\n","# We will use Nominatim for location, and GeoNames for population\n","nominatum_service = Nominatim(timeout=10, user_agent = \"mgaidis@yahoo.com\", format_string=\"%s, Russia\")\n","nominatum_geocode = RateLimiter(nominatum_service.geocode, min_delay_seconds=1)\n","geonames_service = GeoNames(country_bias='ru', username='gaidis', timeout=10, user_agent=\"mgaidis@yahoo.com\", format_string=\"%s, Russia\")  # be sure to enable free web services when creating geonames account\n","geonames_geocode = RateLimiter(geonames_service.geocode, min_delay_seconds=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OqA25J0-1n9p","colab_type":"code","outputId":"d870cef3-02f9-4073-913e-67ca7a8fc097","executionInfo":{"status":"ok","timestamp":1587315403295,"user_tz":240,"elapsed":697,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# Example use of Nominatum\n","location = nominatum_geocode('Adygea', language=\"en\")\n","print(json.dumps(location.raw))\n","print(location.latitude, location.longitude)\n","print(location.address.split(\",\")[-2].strip())\n","location"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{\"place_id\": 234832475, \"licence\": \"Data \\u00a9 OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright\", \"osm_type\": \"relation\", \"osm_id\": 253256, \"boundingbox\": [\"43.7601459\", \"45.2171133\", \"38.6840182\", \"40.7776469\"], \"lat\": \"44.6939006\", \"lon\": \"40.1520421\", \"display_name\": \"Republic of Adygea, South Federal District, Russia\", \"class\": \"boundary\", \"type\": \"administrative\", \"importance\": 0.7686671937378127, \"icon\": \"https://nominatim.openstreetmap.org/images/mapicons/poi_boundary_administrative.p.20.png\"}\n","44.6939006 40.1520421\n","South Federal District\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["Location(Republic of Adygea, South Federal District, Russia, (44.6939006, 40.1520421, 0.0))"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"OfNppvfs8AL6","colab_type":"code","colab":{}},"source":["# Check on how Nominatum characterizes each federal district in Russia.  Below is a list of names of the biggest cities in each district\n","big_cities = ['Moscow','St. Petersburg','Novosibirsk','Yekaterinburg','Nizhny Novgorod','Rostov-on-Don','Makhachkala', 'Vladivostok','Sevastopol']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HcjMEa8DIlyi","colab_type":"code","outputId":"55499e03-7efd-49a9-f193-4745442124d4","executionInfo":{"status":"ok","timestamp":1587313148533,"user_tz":240,"elapsed":12501,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["for bc in big_cities:\n","  location = nominatum_geocode(bc, language='en')\n","  print(location)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Moscow, Central Federal District, Russia\n","Saint Petersburg, Northwestern Federal District, 190000, Russia\n","Novosibirsk, Novosibirsk Oblast, Siberian Federal District, 630000, Russia\n","Yekaterinburg, Yekaterinburg Municipality, Sverdlovsk Oblast, Ural Federal District, Russia\n","Nizhny Novgorod, Nizhny Novgorod Oblast, Volga Federal District, Russia\n","Rostov-on-Don, Rostov Oblast, South Federal District, Russia\n","Makhachkala, Makhachkala Urban Okrug, Republic of Dagestan, North Caucasus Federal District, 367000, Russia\n","Vladivostok, Владивостокский городской округ, Primorsky Krai, Far Eastern Federal District, 690000, Russia\n","Sevastopol, Ленинский район, Sevastopol, South Federal District, 299000-299699, Russia\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kg7PAE8PJStB","colab_type":"code","outputId":"69949386-f56f-4b7a-e486-9bed766746f5","executionInfo":{"status":"ok","timestamp":1587313236740,"user_tz":240,"elapsed":325,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# It appears as though Crimea does not qualify as a district (Sevastapol falls into the South category, according to Nominatum)\n","# Here is a list of the districts as Nominatum reports them:\n","russian_districts = ['Central','Northwestern','Siberian','Ural','Volga','South','North Caucasus','Far Eastern']\n","# and, since Nominatum returns unpredictable presence of \"zip codes\", we will use a regex to make use of the fact that Nominatum always returns \"xxx Federal District\"\n","example_loc = 'Sevastopol, Ленинский район, Sevastopol, South Federal District, 299000-299699, Russia 299000-299699'\n","district_in_location = re.search(r'[,\\s](\\w*)\\sFederal District', example_loc)\n","print(district_in_location.group(1))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["South\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gyS9dPeXQv4X","colab_type":"code","outputId":"1bff1b19-a3e8-47d2-cb92-1947dd8800bd","executionInfo":{"status":"ok","timestamp":1587313643023,"user_tz":240,"elapsed":11851,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# Check on how well GeoNames does with retrieving populations...\n","# Note that GeoNames doesn't consider Sevastapol (Crimea) to be part of Russia, so I did not use country bias or format string to force GeoNames to only look for Russian cities\n","#     Results below are close to Wikipedia.  We're good, at least for the big cities.\n","for bc in big_cities:\n","  g = geonames_geocode(bc,timeout=10)\n","  print(g.raw[\"population\"])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10381222\n","5028000\n","1419007\n","1349772\n","1284164\n","1074482\n","497959\n","587022\n","416263\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-87zUUs0_uIt","colab_type":"code","colab":{}},"source":["g = geonames_geocode('Yakutsk',timeout=10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7U4st22ONTCO","colab_type":"code","outputId":"20c072ad-49f5-4e0f-a713-7ab44e093440","executionInfo":{"status":"ok","timestamp":1587262590471,"user_tz":240,"elapsed":321,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(json.dumps(g.raw))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{\"adminCode1\": \"63\", \"lng\": \"129.73306\", \"geonameId\": 2013159, \"toponymName\": \"Yakutsk\", \"countryId\": \"2017370\", \"fcl\": \"P\", \"population\": 235600, \"countryCode\": \"RU\", \"name\": \"Yakutsk\", \"fclName\": \"city, village,...\", \"adminCodes1\": {\"ISO3166_2\": \"SA\"}, \"countryName\": \"Russia\", \"fcodeName\": \"seat of a first-order administrative division\", \"adminName1\": \"Sakha\", \"lat\": \"62.03389\", \"fcode\": \"PPLA\"}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9U_O7I909tZX","colab_type":"code","outputId":"8aa5077a-ddc2-4bab-d0f6-b4d03457c14a","executionInfo":{"status":"ok","timestamp":1587262593003,"user_tz":240,"elapsed":365,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["g.raw[\"population\"]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["235600"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"code","metadata":{"id":"MK9UCSM38bJh","colab_type":"code","colab":{}},"source":["path_name = \"data_output/item_categories_augmented.csv\"\n","filename = path_name.rsplit(\"/\")[-1]\n","data_frame_name = filename.split(\".\")[0]\n","exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","print(\"Data Frame: \" + data_frame_name)\n","print(eval(data_frame_name).head(2))\n","print(\"\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2jRqKt-te653","colab_type":"code","colab":{}},"source":["#nasty_pairs = test_shop_item_pairs2[test_shop_item_pairs.n_train_rows < 2]\n","#print(f\"Number of shops present in nasty pairs = {nasty_pairs.shop_id.nunique()}\")\n","#print(f\"Number of items present in nasty pairs = {nasty_pairs.item_id.nunique()}\")\n","#print(no_test_train_pair_overlap.head())\n","#nttpo_shop = no_test_train_pair_overlap.groupby('shop_id').sum()\n","#print(nttpo_shop.describe())\n","#print(nttpo_shop[nttpo_shop['n_test_rows'] > 2625])"],"execution_count":0,"outputs":[]}]}