{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_mg_v1p0.ipynb","provenance":[{"file_id":"1zoSvdwl9M_wq03QHaMQuQdIcdEN0JRSY","timestamp":1594910950502},{"file_id":"12KA-rL8-rk29NOHJN8-DbZ8fGUESsfgV","timestamp":1589287670669},{"file_id":"1nzPRIdf4UB-3biwx8fCJlTnx8bL126aO","timestamp":1588242465890},{"file_id":"https://github.com/migai/Kag/blob/master/template_Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587141076973}],"collapsed_sections":["YPp_Nesy2yxn","-YA__znazThG","i_NGdlH8zbz8","r_Oe76PW3aoN","ufy-J0xC2efV","WLdAjg45wEne","UlH3NopEv1Ha","DP8AZkYQvtaj","9TJk9bzeCjqF","HprRItOZV8o6"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"twk8LgLIzEAI"},"source":["# **Intro and Setup**"]},{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn"},"source":["##**Final Project for Coursera's 'How to Win a Data Science Competition'**\n","April, 2020;  Andreas Theodoulou and Michael Gaidis;  (Competition Info last updated:  3 years ago)"]},{"cell_type":"markdown","metadata":{"id":"-YA__znazThG"},"source":["###**About this Competition**\n","\n","You are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n","\n","Evaluation: root mean squared error (RMSE). True target values are clipped into [0,20] range."]},{"cell_type":"markdown","metadata":{"id":"i_NGdlH8zbz8"},"source":["###**File descriptions**\n","\n","***sales_train.csv*** - the training set. Daily historical data from January 2013 to October 2015.\n","\n","***test.csv*** - the test set. You need to forecast the sales for these shops and products for November 2015.\n","\n","***sample_submission.csv*** - a sample submission file in the correct format.\n","\n","***items.csv*** - supplemental information about the items/products.\n","\n","***item_categories.csv***  - supplemental information about the items categories.\n","\n","***shops.csv***- supplemental information about the shops."]},{"cell_type":"markdown","metadata":{"id":"r_Oe76PW3aoN"},"source":["###**Data fields**\n","\n","***ID*** - an Id that represents a (Shop, Item) tuple within the test set\n","\n","***shop_id*** - unique identifier of a shop\n","\n","***item_id*** - unique identifier of a product\n","\n","***item_category_id*** - unique identifier of item category\n","\n","***item_cnt_day*** - number of products sold. You are predicting a monthly amount of this measure\n","\n","***item_price*** - current price of an item\n","\n","***date*** - date in format dd/mm/yyyy\n","\n","***month*** - a consecutive month number. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n","\n","***item_name*** - name of item\n","\n","***shop_name*** - name of shop\n","\n","***item_category_name*** - name of item category"]},{"cell_type":"markdown","metadata":{"id":"ufy-J0xC2efV"},"source":["## **Colab Prep Tips** for those using Google Colab\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WLdAjg45wEne"},"source":["### **Save Previous Work**\n","* Click **File -> Save a copy in Drive** and click **Open in new tab** in the pop-up window to save your progress in Google Drive. (This places the copy at the top level of Colab directory.)\n","* Or, in Google Drive before opening this notebook, right-click on this ipynb and select ***make a copy***, then with the copy in the same directory, right-click and select ***rename*** to update the version number.  Finally, right-click on the new version and ***open in colab***."]},{"cell_type":"markdown","metadata":{"id":"UlH3NopEv1Ha"},"source":["### **Select Runtime Type** *before* running notebook:\n","* Click **Runtime -> Change runtime type** and select **GPU** or **TPU** in Hardware accelerator box to enable faster training."]},{"cell_type":"markdown","metadata":{"id":"DP8AZkYQvtaj"},"source":["### **Keep Colab Active**\n","* To keep Colab connected by clicking on Colab window once every minute, go to Chrome Dev Tools --> Console Tab --> run the following code (April 2020):\n","</br>Take note that this should prevent disconnecting after each 1.5 hours of inactivity, but each runtime, if you don't have Colab Pro, will be terminated after 12 hours. (Pro = 24 hours) (Interval below is in millisec.)\n","```\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"#ok\").click()\n","}\n","setInterval(ClickConnect,60000)\n","```\n","Note that it will throw an error, its ok, it means that the Disconnection notification is not shown. Once it appear it will be clicked to reconnect.\n","\n","* If that doesn't work, try this in the console:\n","```\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"colab-connect-button\").click()\n","}\n","setInterval(ClickConnect,60000)\n","```\n","* Lastly, can try this (older):\n","```\n","function KeepClicking(){\n","   console.log(\"Clicking\");\n","   document.querySelector(\"colab-toolbar-button#connect\").click()\n","}setInterval(KeepClicking,600000)\n","```"]},{"cell_type":"markdown","metadata":{"id":"SNR_OjCZcfxf"},"source":["### **Save Previous Work**\n","* Click **File -> Save a copy in Drive** and click **Open in new tab** in the pop-up window to save your progress in Google Drive. (This places the copy at the top level of Colab directory.)\n","* Or, in Google Drive before opening this notebook, right-click on this ipynb and select ***make a copy***, then with the copy in the same directory, right-click and select ***rename*** to update the version number.  Finally, right-click on the new version and ***open in colab***.\n","```\n","from datetime import datetime\n","from pytz import timezone\n","amsterdam = timezone('Europe/Amsterdam')\n","ams_time = amsterdam.localize(datetime(2002, 10, 27, 6, 0, 0))\n","print(ams_time)\n","# 2002-10-27 06:00:00+01:00\n","# It will also know when it's Summer Time\n","# in Amsterdam (similar to Daylight Savings Time):\n","ams_time = amsterdam.localize(datetime(2002, 6, 27, 6, 0, 0))\n","print(ams_time)\n","# 2002-06-27 06:00:00+02:00\n","```"]},{"cell_type":"markdown","metadata":{"id":"LyLQLqBcOnLt"},"source":["##**Set Up Environment**"]},{"cell_type":"markdown","metadata":{"id":"Ind-TjBOu6gs"},"source":["### **Import Python Packages; Set Environment Options; Identify Input Data Files**\n","\n"]},{"cell_type":"code","metadata":{"id":"naC94KtXOnLt","cellView":"both","executionInfo":{"status":"ok","timestamp":1601460577525,"user_tz":240,"elapsed":1556,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"8b3ad535-c397-4b21-8d82-a2d0951d0916","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["''' ################################################################################################################################################\n","Import Python Packages, and Document Version Numbers\n","''' ################################################################################################################################################\n","\n","global RUN_n, MEMORY_STATS, ALL_exploded_shape, OUTPUTS_df, GDRIVE_REPO_PATH, OUT_OF_REPO_PATH\n","\n","from google.colab import drive  \n","\n","########## General python libraries/packages used throughout the notebook ######################################\n","from   itertools import product\n","from   collections import OrderedDict\n","import pprint\n","import re\n","import os\n","import sys\n","from   pathlib import Path\n","import platform                         # determine the active version of python\n","import pkg_resources                    # determine the active versions of imported packages\n","import psutil                           # from psutil import virtual_memory   # find how much RAM you have left in Colab VM\n","import gc                               # garbage collection... ok with np arrays, not useful with python objects\n","import multiprocessing                  # help to delete unnecessary pandas dataframes when you are done with them\n","from contextlib import ContextDecorator # , contextmanager ## create helper function that can be used as a decorator (for timing, e.g.)\n","\n","######### timing ################################################################################################\n","import time\n","from   time import strftime, tzset, perf_counter\n","from   timeit import default_timer\n","from   datetime import datetime\n","os.environ['TZ'] = 'EST+05EDT,M4.1.0,M10.5.0'   # allows formatted version of the local date and time; track of what cells were run, and when\n","tzset()                                         # set the time zone\n","\n","########## Helpful packages for EDA, cleaning, data manipulation #################################################\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","########### ML packages ##########################################################################################\n","import lightgbm as lgb  # LGBMRegressor\n","import sklearn\n","from   sklearn.preprocessing import MinMaxScaler, RobustScaler\n","from   sklearn.experimental import enable_hist_gradient_boosting  #noqa #explicitly require expt feature before import HistGradientBoostingRegressor\n","from   sklearn.ensemble import HistGradientBoostingRegressor\n","from   sklearn.metrics import mean_squared_error\n","from   sklearn.metrics import r2_score\n","# %tensorflow_version 2.x\n","# import tensorflow as tf\n","\n","''' ################################################################################################################################################\n","Package Version Control: For finding the versions of packages used in this notebook, list the relevant packages here\n","''' ################################################################################################################################################\n","packages = ['pandas','matplotlib','numpy','scikit-learn','lightgbm']  # ,'keras','catboost','seaborn','nltk','graphx','tensorflow'\n","print(f'Package Imports Complete: {strftime(\"%a %X %x\")}')\n","\n","''' ################################################################################################################################################\n","Adjust Pandas Setup Options for enhancements and for desired formatting in this ipynb\n","''' ################################################################################################################################################\n","pd.set_option('compute.use_bottleneck', False)  # speed up operation when using NaNs\n","pd.set_option('compute.use_numexpr', False)     # speed up bool ops, large dfs; DataFrame.query() and pandas.eval() will evaluate subexpr by numexpr\n","pd.set_option(\"display.max_rows\",60)            # Override pandas choice of how many rows to show, to see, e.g., full 84-row item_category df\n","pd.set_option(\"display.max_columns\",60)         # Similar to row code above, we can show more columns than default\n","pd.set_option(\"display.width\", 220)             # Tune this to monitor window size to avoid horiz scroll bars in output windows (but may get text wrap)\n","pd.set_option(\"max_colwidth\", None)             # This is done, for example, so we can see full item name and not '...' in the middle\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)  # no decimals if integer; use 3 dec float\n","print(f'Pandas Setup Complete: {strftime(\"%a %X %x\")}')\n","\n","\n","''' ################################################################################################################################################\n","Identify Input Data and File Path Info (path relative to local Google Drive Repo Head = GDRIVE_REPO_PATH)\n","''' ################################################################################################################################################\n","GDRIVE_HOME = Path(\"/content\" + \"/drive\")                               # initial directory when mounting Google Drive in Colab\n","COLAB_DIR = GDRIVE_HOME / \"My Drive/Colab Notebooks\"                    # default Colab directory\n","GDRIVE_REPO_PATH = COLAB_DIR / \"NRUHSE_2_Kaggle_Coursera/final/Kag\"     # content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","OUT_OF_REPO_PATH = COLAB_DIR / \"NRUHSE_2_Kaggle_Coursera/final\"         # place >100MB files here, because they won't sync with GitHub (eg, ftr files)\n","data_files = [  \"data_output/items_enc.csv\",                # pandas df name will be \"items_enc\" (see list comprehension below)\n","                \"data_output/shops_enc.csv\",\n","                \"data_output/date_scaling.csv\",\n","                \"data_output/stt.csv.gz\",                   # stt is short for sales-train-test (contains all data for months 0 to 34)\n","                \"readonly/final_project_data/test.csv.gz\"]\n","\n","data_df_names = [path_name.rsplit('/')[-1].split('.')[0] for path_name in data_files] # root name of csv files will name the respective pandas dfs\n","data_sources = list(zip(data_df_names, data_files))\n","print(f'Data File Sources Identified: {strftime(\"%a %X %x\")}\\n{data_sources}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Package Imports Complete: Wed 06:09:37 09/30/20\n","Pandas Setup Complete: Wed 06:09:37 09/30/20\n","Data File Sources Identified: Wed 06:09:37 09/30/20\n","[('items_enc', 'data_output/items_enc.csv'), ('shops_enc', 'data_output/shops_enc.csv'), ('date_scaling', 'data_output/date_scaling.csv'), ('stt', 'data_output/stt.csv.gz'), ('test', 'readonly/final_project_data/test.csv.gz')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8i0uUt5rmSa1","executionInfo":{"status":"ok","timestamp":1601334639283,"user_tz":240,"elapsed":626,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"84bddf14-6353-4b83-b381-e54a564588c2","colab":{"base_uri":"https://localhost:8080/"}},"source":["class MemoryStats:\n","    \"\"\"Gather relevant memory consumption info into one object.\n","        1) get RAM usage as specified by psutil.Process(pid) and psutil.virtual_memory()\n","        2) get the number of active children per the multiprocessing configuration being used \n","            ( = empty list [] when process is properly completed / closed).\"\"\"\n","\n","    def __init__(self, program_location='Start'):\n","        self.program_location = program_location\n","        self.pid = os.getpid()\n","        self.py = psutil.Process(pid)\n","        self.pid_mem_use_GB = self.py.memory_info()[0] / 2. ** 30\n","        self.vm_used_GB = psutil.virtual_memory().used/ 1e9\n","        self.vm_total_GB = psutil.virtual_memory().total/ 1e9\n","        self.vm_available_GB = self.vm_total - self.vm_used\n","        self.active_proc = multiprocessing.active_children()\n","        self.date_time = f'{strftime(\"%a %X %x\")}'\n","\n","\n","class ProgramMgr:\n","    \"\"\"Hold information about computer system and files being used in this notebook.\"\"\"\n","\n","    def __init__(self, packages):\n","        self.packages = packages  # list of imports, for version control\n","        self.package_versions = None\n","        self.GDRIVE_HOME = Path(\"/content\" + \"/drive\")                               # initial directory when mounting Google Drive in Colab\n","        self.COLAB_DIR = self.GDRIVE_HOME / \"My Drive/Colab Notebooks\"               # default Colab directory\n","        self.GDRIVE_REPO_PATH = self.COLAB_DIR / \"NRUHSE_2_Kaggle_Coursera/final/Kag\"     # content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","        self.OUT_OF_REPO_PATH = COLAB_DIR / \"NRUHSE_2_Kaggle_Coursera/final\"         # place >100MB files here, because they won't sync with GitHub (eg, ftr files)\n","        self.data_files = [\n","                           \"data_output/items_enc.csv\",                # pandas df name will be \"items_enc\" (see list comprehension below)\n","                           \"data_output/shops_enc.csv\",\n","                           \"data_output/date_scaling.csv\",\n","                           \"data_output/stt.csv.gz\",                   # stt is short for sales-train-test (contains all data for months 0 to 34)\n","                           \"readonly/final_project_data/test.csv.gz\"\n","                           ]\n","        # Initialize MEMORY_STATS tracking list; throughout program; usage like: MEMORY_STATS.append(get_memory_stats(\"After Merge\", printout=False))\n","        self.memory_stats = []\n","        self.runtime_type = 'Unknown runtime type'\n","        self.init_datetime = f'{strftime(\"%a %X %x\")}'\n","        self.n_cpu = 0\n","        self.time = OrderedDict([('start': default_timer())])\n","        self.get_memory_stats(\"At Notebook start\", printout=True)\n","        self.get_runtime_type(printout=True)\n","\n","    def get_df_names(self):\n","        \"\"\"Return a list of names intended for pandas dataframes, based on the prefix of the data source files.\"\"\"\n","        return [path_name.rsplit('/')[-1].split('.')[0] for path_name in self.data_files] # root name of csv files will name the respective pandas dfs\n","\n","    def get_data_sources(self):\n","        \"\"\"Return a zipped list of datafile names and disk locations, to be used for multiprocessing map functions.\"\"\"\n","        return list(zip(self.get_df_names(), self.data_files))\n","\n","    def get_memory_stats(self, program_location=\"0\", print_status='location'):\n","        \"\"\"Provide memory consumption information.  print_status = one of [None, 'location', 'all'].\"\"\"\n","        ms = MemoryStats(program_location)\n","        self.memory_stats.append(ms)\n","        # print either all the stats gathered throughout the program, or just the present program location, or nothing\n","        if print_status == 'all':\n","            max_meas_str_len = 0\n","            for ms in self.memory_stats:  # find longest string to accommodate during printout\n","                if len(ms.program_location) > max_meas_str_len:\n","                    max_meas_str_len = len(ms.program_location) \n","            print(f\"{' ':<21}   {' ':<{max_meas_str_len}} |  pid   |               vm               {' ':<14}|\")\n","            print(f\"{'  Time and Date':<21} | {'  Measurement Point':<{max_meas_str_len}} | pid-GB | used-GB | avail-GB | total-GB | {'Active Procs':<12} |\")\n","            for ms in self.memory_stats:\n","                print(f\"{ms.date_time:<21} | {ms.program_location:<{max_meas_str_len}} | {ms.pid_mem_use_GB:>6.2f} | \"\n","                      f\"{ms.vm_used_GB:>7.2f} | {ms.vm_available_GB:>8.2f} | {ms.vm_total_GB:>8.2f} | {ms.active_proc}\")\n","        elif print_status == 'location':\n","            print(f'{program_location}:               Multiprocessing Active Children = {ms.active_proc}')\n","            print(f'Memory Use: | {ms.pid_mem_use_GB:.2f} | {ms.vm_used_GB:.2f} | {ms.vm_available_GB:.2f} | {ms.vm_total_GB:.2f} | GB:'\n","                  f'  pid, vm used / available / total.  {ms.date_time}.')\n","\n","    def get_runtime_type(self, printout=True):  \n","        \"\"\"Check if connected to a CPU vs. GPU-enabled runtime, or possibly a TPU: (relevant for people using Google Colab).\"\"\"\n","        try: \n","            gpu_device_name = tf.test.gpu_device_name()\n","            if gpu_device_name != '/device:GPU:0':  # ' ' means CPU whereas '/device:G:0' means GPU\n","                try: \n","                    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","                    self.runtime_type = f'Colab using TPU at: {tpu.cluster_spec().as_dict()[\"worker\"]}'\n","                except ValueError: self.runtime_type = 'Colab using CPU'\n","            else: self.runtime_type = f'Colab using GPU at: {gpu_device_name}'\n","        except: pass  # self.runtime_type = 'Unknown runtime type'\n","        if printout:\n","            print(f'Runtime Type: {self.runtime_type}')\n","\n","    def get_n_cpu(printout=True):   \n","        \"\"\"Find the number of CPUs available for use.\"\"\"\n","        try: \n","            self.n_cpu = multiprocessing.cpu_count()\n","        except: pass  # self.n_cpu = 0\n","        if printout:\n","            print(f'Number of available CPUs: {self.n_cpu}')\n","\n","    def get_package_versions(self, printout=True):  \n","        \"\"\"Find the versions of python packages used in this notebook, to assist with re-creation of notebook at some future date.\n","        Could also use the following methodology: print(pandas.__version__).\"\"\"\n","        self.package_versions = OrderedDict([(\"Python\", platform.python_version())])\n","        for pkg in sorted(self.packages): \n","            try: \n","                self.package_versions[pkg] = pkg_resources.get_distribution(pkg).version\n","            except: pass\n","        if printout:\n","            for k,v in self.package_versions.items():\n","                print(f'{k} version: {v}')\n","\n","    def get_elapsed_time(self, program_location='start', reference='start'):\n","        time_now = default_timer()\n","        if program_location == reference:\n","            program_location = f'{len(self.time)}'\n","        self.time[program_location] = time_now\n","        return f'{datetime.utcfromtimestamp(self.time[program_location] - self.time[reference]).strftime('%H:%M:%S')} H:M:S'\n","\n","o = ProgramMgr()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["At Notebook start:               Multiprocessing Active Children = []\n","Memory Use: | 0.15 | 0.61 | 26.78 | 27.39 | GB:  pid, vm used / available / total.  Mon 19:10:38 09/28/20.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9uV9AzFbtdJZ"},"source":["### **Analysis and Descriptive (Helper) Functions**\n","\n"]},{"cell_type":"code","metadata":{"id":"duCrNE7ZpnzG","executionInfo":{"status":"ok","timestamp":1599636975407,"user_tz":240,"elapsed":1792,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"d9aa4bfb-73a8-4564-8091-cea24c1e60b3","colab":{"base_uri":"https://localhost:8080/"}},"source":["# ############################################################################################\n","# def get_memory_stats(txt=\"0\",printout=True):\n","#     \"\"\"\n","#     1) get RAM usage as specified by psutil.Process(pid) and psutil.virtual_memory()\n","#     2) get the number of active children per the multiprocessing configuration being used (= empty list [] when process is properly completed / closed)\n","#     3) allow option of printing the single data point immediately, and/or append data to a variable for formatted printout summary\n","#     \"\"\"\n","#     pid = os.getpid()\n","#     py = psutil.Process(pid)\n","#     pid_memory_use = py.memory_info()[0] / 2. ** 30\n","#     vm_used = psutil.virtual_memory().used/ 1e9\n","#     vm_total = psutil.virtual_memory().total/ 1e9\n","#     vm_available = vm_total - vm_used\n","#     active_proc = multiprocessing.active_children()\n","#     if printout:\n","#         print(f'{txt}:               Multiprocessing Active Children = {active_proc}')\n","#         print(f'Memory Use: | {pid_memory_use:.2f} | {vm_used:.2f} | {vm_available:.2f} | {vm_total:.2f} | GB:',end='')\n","#         print(f'  pid, vm used / available / total.  {strftime(\"%a %X %x\")}.')\n","#     return {'datetime':f'{strftime(\"%a %X %x\")}','measure_point':txt,'pid_mem_use_GB':pid_memory_use,\n","#             'vm_used_GB':vm_used,'vm_available_GB':vm_available,'vm_total_GB':vm_total,'active_proc':active_proc}\n","\n","# # Initialize MEMORY_STATS tracking list; throughout program; usage like: MEMORY_STATS.append(get_memory_stats(\"After Merge\", printout=False))\n","# MEMORY_STATS = [get_memory_stats(\"At Memory Stats Func Def\", printout=True)]\n","\n","# ############################################################################################\n","# def display_all_memory_stats(list_of_dicts):   \n","#     \"\"\"\n","#     formatted printout of MEMORY_STATS collection of RAM use as tracked throughout the computations\n","#     \"\"\"\n","#     max_meas_str_len = 0\n","#     for dicts in list_of_dicts:\n","#         if len(dicts['measure_point']) > max_meas_str_len:\n","#             max_meas_str_len = len(dicts['measure_point']) \n","#     print(f\"{' ':<21}   {' ':<{max_meas_str_len}} |  pid   |               vm               {' ':<14}|\")\n","#     print(f\"{'  Time and Date':<21} | {'  Measurement Point':<{max_meas_str_len}} | pid-GB | used-GB | avail-GB | total-GB | {'Active Procs':<12} |\")\n","#     for dicts in list_of_dicts:\n","#         print(f\"{dicts['datetime']:<21} | {dicts['measure_point']:<{max_meas_str_len}} | {dicts['pid_mem_use_GB']:>6.2f} | \",end='')\n","#         print(f\"{dicts['vm_used_GB']:>7.2f} | {dicts['vm_available_GB']:>8.2f} | {dicts['vm_total_GB']:>8.2f} | {dicts['active_proc']}\")\n","#     return\n","\n","# ############################################################################################\n","# def get_runtime_type(do_printout=True):  \n","#     \"\"\"\n","#     Check if connected to a CPU vs. GPU-enabled runtime, or possibly a TPU: (relevant for people using Google Colab)\n","#     \"\"\"\n","#     runtime_type_dict = {}\n","#     try: \n","#         gpu_device_name = tf.test.gpu_device_name()\n","#         if gpu_device_name != '/device:GPU:0':  #' ' means CPU whereas '/device:G:0' means GPU\n","#             try: \n","#                 tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","#                 run_type = f'Colab using TPU at: {tpu.cluster_spec().as_dict()[\"worker\"]}'\n","#             except ValueError: run_type = 'Colab using CPU'\n","#         else: run_type = f'Colab using GPU at: {gpu_device_name}'\n","#     except: run_type = 'Unknown runtime type'\n","#     if do_printout:\n","#         print(run_type)\n","#     return run_type\n","\n","# ############################################################################################\n","# def get_n_cpu(do_printout=True):   \n","#     \"\"\"\n","#     Find the number of CPUs available for use\n","#     \"\"\"\n","#     try: \n","#         n_cpu = multiprocessing.cpu_count()\n","#     except: n_cpu = 0\n","#     if do_printout:\n","#         print(f'Number of available CPUs: {n_cpu}')\n","#     return n_cpu\n","\n","# ############################################################################################\n","# def get_package_versions(package_list=packages, do_printout=True):  \n","#     \"\"\"\n","#     Find the versions of python packages used in this notebook, to assist with re-creation of notebook at some future date\n","#     could also use: print(pandas.__version__)\n","#     \"\"\"\n","#     package_version_dict = OrderedDict([(\"Python\",platform.python_version())])\n","#     for mod in sorted(package_list): \n","#         try: \n","#             package_version_dict[mod] = pkg_resources.get_distribution(mod).version\n","#         except: pass\n","#     if do_printout:\n","#         for k,v in package_version_dict.items():\n","#             print(f'{k} version: {v}')\n","#     return package_version_dict\n","\n","############################################################################################\n","def print_col_info(df,nrows=5,SPACE_BETWEEN_COLS = 6,COLUMN_PADDING = 3):   # formatted print of column datatypes and memory usage\n","    \"\"\"\n","    instead of the usual single column (plus index) series printout of dtypes and memory use in a dataframe, \n","    this function combines dtypes and memory use so they use the same index, and then prints out multiple columns of length \"nrows\", \n","    where each column is like: \"column_dtype \\t column_memory_use(MB) \\t column_name\"; finishes with a printout of total df mem usage\n","        df = dataframe of interest\n","        nrows = int, tells how many rows of (type/mem/name) to print before moving to a new printout column for the next triplet (type/mem/name)\n","    \"\"\"\n","    col_mem = df.memory_usage(deep=True) /1e6  #change to MB\n","    print(f'DataFrame shape: {df.shape}\\nDataFrame total memory usage: {col_mem.sum():.0f} MB\\nDataFrame Column Names: {df.columns.to_list()}')\n","    # df.memory_usage includes Index, but df.dtypes does not include Index, so we have to add it\n","    col_dtypes = pd.concat([pd.Series([df.index.dtype], index = ['Index']),df.dtypes], axis=0)  \n","    col_info_df = pd.concat([col_dtypes, col_mem], axis=1).reset_index().rename(columns={'index':'Column Name', 0:'DType', 1:'MBytes'})\n","    if nrows == 0:  # if nrows == 0, print all triplets in just one column, with no \"wrapping\"\n","        print(col_info_df)\n","    else:\n","        col_info_df.MBytes = col_info_df.MBytes.apply(lambda x: str(f'{x:.1f}'))\n","        n_print_cols, stragglers = divmod(col_info_df.shape[0], nrows)  # adjust n rows such that we don't have nasty column with just a few rows\n","        if (stragglers > 0):  # add an extra column if we have lots of stragglers; else make n rows a bit higher so we don't have stragglers\n","            n_print_cols += (stragglers > nrows/2)\n","            nrows = col_info_df.shape[0] // n_print_cols + (col_info_df.shape[0] % n_print_cols > 0)\n","        # to make dataframe of where each column is shifted by number of elements to print per column (will truncate below)\n","        df_print = pd.concat([col_info_df.shift(periods= -nrows * pc) for pc in range(n_print_cols)], axis = 1)  \n","        # truncate \"over-shifted\" rows so only one copy of each element in the df; make first row = column names (easier for printing)\n","        df_print = df_print.iloc[:nrows][:].fillna(\" \").T.reset_index().T.reset_index(drop=True)   \n","        columnStrLengths = np.vectorize(len) \n","        # find max string length in each column, add COLUMN_PADDING (roughly = 3) to create nice/clean column look\n","        col_widths = np.add(columnStrLengths(df_print.values.astype(str)).max(axis=0), COLUMN_PADDING)  \n","        for r in range(nrows):  # create strings for each row to print sequentially, containing all columns in each row string\n","            print_row = ''\n","            for c in range(len(df_print.columns)):\n","                print_row = print_row + f'{str(df_print.iloc[r][c]):>{col_widths[c]}} '  \n","                # format string for right alignment in column; padding to fit column width of 'col_widths'\n","                print_row = print_row + \" \" * SPACE_BETWEEN_COLS * ((c+1)%col_info_df.shape[1] == 0)  \n","                # when finished formatting one column of data; add extra spaces and move to the next column\n","            print(print_row)\n","    # MEMORY_STATS.append(get_memory_stats(\"print_col_info Completed\",printout=True))\n","    return\n","\n","# ############################################################################################\n","# class elapsed_timer(ContextDecorator): # base class enables a contextmanager to also be used as a decorator\n","#     def __init__(self):   \n","#         self.function_init_time   = f'{strftime(\"%a %X %x\")}'\n","#         self.start_time           = default_timer()\n","#         self.function_total_time  = 0\n","#     def __enter__(self):  \n","#         self.start_time = default_timer()\n","#         return datetime.utcfromtimestamp(default_timer() - self.start_time).strftime('%H:%M:%S')\n","#     def __exit__(self, exc_type, exc_value, exc_traceback):\n","#         self.function_total_time = datetime.utcfromtimestamp(default_timer() - self.start_time).strftime('%H:%M:%S')\n","#         return True # ignores errors in while loop\n","#     def restart_time(self):\n","#         self.start_time = default_timer()\n","#     def get_elapsed_time(self):\n","#         return datetime.utcfromtimestamp(default_timer() - self.start_time).strftime('%H:%M:%S')\n","\n","print(f'Helper Functions Defined: {strftime(\"%a %X %x\")}')\n","\n","''' ################################################################################################################################################\n","Gather and print system information, including the version numbers of python packages being used (to assist others in replicating this work)\n","''' ################################################################################################################################################\n","runtime_type_cpu_gpu_tpu = get_runtime_type(do_printout=True)                   #####    Runtime Type     #####\n","n_available_cpus = get_n_cpu(do_printout=True)                                  #####   Number of CPUs    #####\n","available_vm_ram_gb = psutil.virtual_memory().total/ 1e9                        ##### Colab Available RAM #####\n","print(f'Available VM RAM (GB): {available_vm_ram_gb:.1f}\\n')\n","package_versions = get_package_versions(packages, do_printout=True)             ##### Py Package Versions #####\n","print(f'\\nSystem Info Gathered: {strftime(\"%a %X %x\")}')\n","display_all_memory_stats(MEMORY_STATS)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["At Memory Stats Func Def:               Multiprocessing Active Children = []\n","Memory Use: | 0.14 | 0.62 | 26.77 | 27.39 | GB:  pid, vm used / available / total.  Wed 03:36:14 09/09/20.\n","Helper Functions Defined: Wed 03:36:14 09/09/20\n","Unknown runtime type\n","Number of available CPUs: 4\n","Available VM RAM (GB): 27.4\n","\n","Python version: 3.6.9\n","lightgbm version: 2.2.3\n","matplotlib version: 3.2.2\n","numpy version: 1.18.5\n","pandas version: 1.0.5\n","scikit-learn version: 0.22.2.post1\n","\n","System Info Gathered: Wed 03:36:14 09/09/20\n","                                                 |  pid   |               vm                             |\n","  Time and Date       |   Measurement Point      | pid-GB | used-GB | avail-GB | total-GB | Active Procs |\n","Wed 03:36:14 09/09/20 | At Memory Stats Func Def |   0.14 |    0.62 |    26.77 |    27.39 | []\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OJ0PUuSovTN7"},"source":["##**Identify (Manually) Splits to Be Performed and Features to Use**"]},{"cell_type":"markdown","metadata":{"id":"ZA5YyPKB7QAt"},"source":["###**Define Feature Columns, Statistics, and Lags**"]},{"cell_type":"code","metadata":{"id":"u8gkHCNow8xq","executionInfo":{"status":"ok","timestamp":1599636975408,"user_tz":240,"elapsed":1784,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"3bcbe9b5-aeb8-40fb-a789-f480e2f1bf53","colab":{"base_uri":"https://localhost:8080/"}},"source":["### for reference, column names of the loaded dataframes:\n","# items_enc_cols  = [ 'item_id', 'item_tested', 'item_cluster', 'item_category_id', 'item_cat_tested', 'item_group', \n","#                     'item_category1', 'item_category2', 'item_category3', 'item_category4']\n","# shops_enc_cols  = ['shop_id','shop_tested','shop_group','shop_type','s_type_broad','shop_federal_district','fd_popdens','fd_gdp','shop_city']\n","# date_adj_cols   = ['month', 'year', 'season', 'MoY', 'days_in_M', 'weekday_weight', 'retail_sales', 'week_retail_weight']\n","# stt_cols        = ['day', 'week', 'qtr', 'season', 'month', 'price', 'sales', 'shop_id', 'item_id']\n","# test_cols       = ['ID', 'shop_id', 'item_id']\n","\n","########\n","#  Allow looping over variations in choices of categories to keep, statistics to use, and lag months (and stats to lag for each month)\n","#    However, instead of \"cartesian product\" type looping of all possible combinations of [categories, stats, lags], \n","#    set these columns together as one unit for each iteration, like  [[[cats1,stats1,lags1],[cats2,stats2,lags2],...]]] \n","#    instead of                                                       [[[cats1,stats1,lags1],[cats1,stats1,lags2],[cats1,stats2,lags1],...]]\n","########\n","n_categories_stats_lags_splits = 1\n","feature_params = []                                 # each dict within the \"feature_params\" list is an iteration split (another run) \n","for csls in range(n_categories_stats_lags_splits):\n","    cats_stats_lags = OrderedDict()\n","    # columns to keep for this round of modeling (dropping some of the less important features to save memory):\n","    items_enc_keep =    ['item_id', 'item_category_id', 'item_group', 'item_cluster'] \n","    shops_enc_keep =    ['shop_id', 'shop_group']\n","    date_scaling_keep = ['month', 'week_retail_weight'] \n","    stt_keep =          ['month', 'sales', 'price', 'shop_id', 'item_id'] \n","    test_keep =         ['ID', 'shop_id', 'item_id']\n","    # if you wish to run more than one combination of these included columns (make sure they align with lags/stats as mentioned above)\n","    cats_stats_lags[\"keep_columns\"] = OrderedDict()\n","    for ds in data_sources:\n","        cats_stats_lags[\"keep_columns\"][ds[0]] = eval(ds[0]+'_keep')\n","        \n","    cats_stats_lags['aggregate_stats'] = OrderedDict([ (\"sales\", ['sum', 'median', 'count']), (\"revenue\", ['sum']) ])  # , (\"price\", ['median']) \n","\n","    # lag specs for the csls'th split (lag months = list(lag_split.keys())    or     \"for step_number,months_lagged in enumerate(lag_split):\" )\n","    lag_split       = OrderedDict()  # single iteration choices for lag elements, to be matched with a single row of categories/stats from above\n","    lag_split[1]    = OrderedDict()  # key = lag month number; value = ODict with details on stats/grouping to include for this month number's lag\n","    lag_split[1]['shop_id_x_item_id']           = {'group':['shop_id', 'item_id'],          'stats':{\"sales\":['sum','median','count'],\"revenue\":['sum']}}\n","    lag_split[1]['shop_id_x_item_category_id']  = {'group':['shop_id', 'item_category_id'], 'stats':{\"sales\":['sum','median','count']}}\n","    lag_split[1]['shop_id_x_item_cluster']      = {'group':['shop_id', 'item_cluster'],     'stats':{\"sales\":['sum','median']}}\n","    lag_split[1]['shop_id']                     = {'group':['shop_id'],                     'stats':{\"sales\":['sum','count']}}\n","    lag_split[1]['item_id']                     = {'group':['item_id'],                     'stats':{\"sales\":['sum','median','count'],\"revenue\":['sum']}}\n","    lag_split[1]['shop_group']                  = {'group':['shop_group'],                  'stats':{\"revenue\":['sum']}}\n","    lag_split[1]['item_category_id']            = {'group':['item_category_id'],            'stats':{\"sales\":['sum', 'count'],\"revenue\":['sum']}}\n","    lag_split[1]['item_group']                  = {'group':['item_group'],                  'stats':{\"sales\":['sum'],\"revenue\":['sum']}}\n","    lag_split[1]['item_cluster']                = {'group':['item_cluster'],                'stats':{\"sales\":['sum', 'count'],\"revenue\":['sum']}}\n","    lag_split[2]    = OrderedDict()  # key = lag month number; value = ODict with details on stats/grouping to include for this month number's lag\n","    lag_split[2]['shop_id_x_item_id']           = {'group':['shop_id', 'item_id'],          'stats':{\"sales\":['sum','count'],\"revenue\":['sum']}}\n","    lag_split[2]['shop_id_x_item_category_id']  = {'group':['shop_id', 'item_category_id'], 'stats':{\"sales\":['count'],\"revenue\":['sum']}}\n","    lag_split[2]['shop_id']                     = {'group':['shop_id'],                     'stats':{\"sales\":['sum']}}\n","    lag_split[2]['item_id']                     = {'group':['item_id'],                     'stats':{\"sales\":['sum','count'],\"revenue\":['sum']}}\n","    lag_split[2]['item_category_id']            = {'group':['item_category_id'],            'stats':{\"sales\":['sum', 'count']}}\n","    lag_split[2]['item_cluster']                = {'group':['item_cluster'],                'stats':{\"sales\":['sum', 'count'],\"revenue\":['sum']}}\n","    lag_split[3]    = OrderedDict()  # key = lag month number; value = ODict with details on stats/grouping to include for this month number's lag\n","    lag_split[3]['shop_id_x_item_id']           = {'group':['shop_id', 'item_id'],          'stats':{\"sales\":['sum']}}\n","    lag_split[3]['shop_id']                     = {'group':['shop_id'],                     'stats':{\"sales\":['sum']}}\n","    lag_split[3]['item_id']                     = {'group':['item_id'],                     'stats':{\"sales\":['sum','count'],\"revenue\":['sum']}}\n","    lag_split[3]['item_category_id']            = {'group':['item_category_id'],            'stats':{\"sales\":['sum', 'count']}}\n","    lag_split[3]['item_cluster']                = {'group':['item_cluster'],                'stats':{\"sales\":['count']}}\n","    lag_split[4]    = OrderedDict()  # key = lag month number; value = ODict with details on stats/grouping to include for this month number's lag\n","    lag_split[4]['item_id']                     = {'group':['item_id'],                     'stats':{\"sales\":['sum']}}\n","    lag_split[5]    = OrderedDict()  # key = lag month number; value = ODict with details on stats/grouping to include for this month number's lag\n","    lag_split[5]['shop_id_x_item_id']           = {'group':['shop_id', 'item_id'],          'stats':{\"sales\":['sum']}}\n","    lag_split[6]    = OrderedDict()  # key = lag month number; value = ODict with details on stats/grouping to include for this month number's lag\n","    lag_split[6]['shop_id_x_item_id']           = {'group':['shop_id', 'item_id'],          'stats':{\"sales\":['sum']}}\n","    lag_split[7]    = OrderedDict()  # key = lag month number; value = ODict with details on stats/grouping to include for this month number's lag\n","    lag_split[7]['item_id']                     = {'group':['item_id'],                     'stats':{\"sales\":['sum']}}\n","    lag_split[8]    = OrderedDict()  # key = lag month number; value = ODict with details on stats/grouping to include for this month number's lag\n","    lag_split[8]['shop_id_x_item_id']           = {'group':['shop_id', 'item_id'],          'stats':{\"sales\":['sum']}}\n","\n","    # Now get the minimal list of stats that need to be calculated, and get the feature names with lag months appended as text\n","    #   - stats_set is SET of all agg statistics columns for all lags (allows us to shed the other stats, keeping memory requirements lower)\n","    #   - first monthly group merge is on month/shop_id/item_id; keep other categorical features in this merge by using agg_stat = 'first'\n","    #        (none of these features get lagged, so don't include them in lag_split dict or in stats_set_feature_names, but do include in stats_set)\n","    first_cols = cats_stats_lags[\"keep_columns\"][\"shops_enc\"][1:] + cats_stats_lags[\"keep_columns\"][\"items_enc\"][1:]\n","    first_stats = dict(zip(first_cols,[['first']]*len(first_cols)))\n","    stats_set = OrderedDict( [ ('shop_id_x_item_id', {'group':['shop_id', 'item_id'], 'stats':first_stats} ) ] )                  \n","    stats_set_feature_names = []   #stats_set_feature_names = first_cols.copy()\n","\n","    all_lag_feature_names = []\n","    for lag_mo, lag_stats in lag_split.items():\n","        feature_root_list = []\n","        lagged_feature_name_dict = OrderedDict()\n","        for lag_group_name, agg_details in lag_stats.items():\n","            for stat_target, stats in agg_details['stats'].items():\n","                if lag_group_name not in list(stats_set.keys()):\n","                    stats_set[lag_group_name] = agg_details\n","                else:\n","                    if stat_target not in list(stats_set[lag_group_name]['stats'].keys()):\n","                        stats_set[lag_group_name]['stats'][stat_target] = stats\n","                    else:\n","                        stats_set[lag_group_name]['stats'][stat_target] = list( set( stats_set[lag_group_name]['stats'][stat_target] + stats ) )\n","                for stat in stats:\n","                    root_name = f'{lag_group_name}_{stat_target}_{stat}'\n","                    lag_feature_name = f'{lag_group_name}_{stat_target}_{stat}_L{lag_mo}'\n","                    stats_set_feature_names.append(root_name)\n","                    feature_root_list.append(root_name)\n","                    lagged_feature_name_dict[root_name] = lag_feature_name\n","                    all_lag_feature_names.append(lag_feature_name)\n","        lag_split[lag_mo]['feature_root'] = feature_root_list\n","        lag_split[lag_mo]['lagged_feature_name'] = lagged_feature_name_dict\n","    stats_set_feature_names = list(OrderedDict.fromkeys(stats_set_feature_names))  #keep only first occurrence, remove any following duplicates\n","    \n","    for agg_item, agg_params in stats_set.items():\n","        #agg_names = first_cols if agg_item == 'shop_id_x_item_id' else []\n","        agg_names = []\n","        for stat_target, stat_type in agg_params['stats'].items():\n","            if stat_type == ['first'] or stat_type == 'first':\n","                agg_names.append(stat_target)\n","            else:\n","                for st in stat_type:\n","                    agg_names.append(f'{agg_item}_{stat_target}_{st}')\n","        stats_set[agg_item]['agg_names'] = agg_names\n","\n","    # stt_final = columns in train/val/test set, before monthly grouping and calculation of statistics, (columns in order desired)\n","    cats_stats_lags['stt_final']    = ['month'] + list(cats_stats_lags['aggregate_stats'].keys()) + ['shop_id', 'item_id'] + first_cols\n","    cats_stats_lags['categorical']  = cats_stats_lags[\"keep_columns\"][\"shops_enc\"] + cats_stats_lags[\"keep_columns\"][\"items_enc\"]\n","    cats_stats_lags['integer']      = ['month'] + cats_stats_lags[\"categorical\"] # int dtype uses less memory, and can speed model fitting     \n","    cats_stats_lags['lag_splits']   = { 'months_list':list(lag_split.keys()), 'params':lag_split, 'stats_set':stats_set, \n","                                        'stats_set_feature_names':stats_set_feature_names }\n","    cats_stats_lags['all_feature_names'] = cats_stats_lags['integer'] + all_lag_feature_names\n","    cats_stats_lags['n_features']   = len(cats_stats_lags['all_feature_names'])\n","\n","    feature_params.append(cats_stats_lags)\n","\n","print(f'Feature Columns, Statistic, and Lags Identified: {strftime(\"%a %X %x\")}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Feature Columns, Statistic, and Lags Identified: Wed 03:36:14 09/09/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gFbWag7-zoDU"},"source":["###**Define Dictionaries / Dataframes to Enable Looping Grid Search for Optimal Parameters**"]},{"cell_type":"code","metadata":{"id":"1c8M1ONK7txo","cellView":"both","executionInfo":{"status":"ok","timestamp":1599636976252,"user_tz":240,"elapsed":2621,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"79560c2b-391d-4650-ff44-2b4e614a1e34","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Define various constants that drive the attributes of the various features\n","# Define hyperparameters for modeling and feature generation, including those that we might want to loop over multiple choices\n","# Put in a DataFrame so we can \"explode\" and generate rows for every possible combination of variable inputs\n","\n","filename_root = 'v1p0_test1'\n","ALL_PARAMS = pd.DataFrame({\n","        ###### below are model/file name specifications ########  ... set model_filename_base to False if you want user to input it during run\n","        'model_filename_base':      f\"{strftime('%Y%m%d-%H%M')}_{filename_root}\", # fname like 'model_type'+'20200812-0740+decription'+RUN_n+suffix.xxx\n","        'model_type':               [['LGBM']],         # 'HGBR' for SKLearn version of GBDT\n","        'feature_params':           [feature_params],   # details on cats to use, agg stats and lag features, (list of dicts, 1 dict per iter)\n","        'data_sources':             [[data_sources]],   # (pd_df_name_string, filepath_string) tuple list load csv files from google drive\n","        ###### below are eda parameters ########               \n","        'eda_delete_shops':         [[[9,20]]],    # [0,1,8,9,11,13,17,20,23,27,29,30,32,33,40,43,51,54] [8, 9, 13, 20, 23, 32, 33, 40] False\n","        'eda_delete_item_cats':     [[[8,10,32,59,80,81,82]]], # [1,4,8,10,13,14,17,18,32,39,46,48,50,51,52,53,59,66,68,80,81,82] [8, 80, 81, 82] False\n","        'eda_scale_month':          [['week_retail_weight']],  # False # scale sales by days in month, n of each wkday, Russian recessn retail sales idx\n","        'feather_stt':              [[True]],      # if True, save STT dataframe to disk as .ftr file to conserve RAM\n","        ###### below are data conditioning parameters (monthly merging, scaling, cartesian product filling, lagged statistics) ########\n","        'cartprod_fillna0':         [[True]],      # fill n/a cartesian additions with zeros (not good for price-based stats, however)\n","        'cartprod_first_month':     [[13]],        # month no + max lag to start adding Cart Prod rows (eg, maxlag=6mo and cart_first_mon=10 fills 4-33)\n","        'cartprod_test_pairs':      [[False]],     # include all of test set 'shop-item pairs' with each month's normal cartesian product fill\n","        'clip_train_H':             [[20]],        # this clips sales after doing monthly groupings (monthly_stt dataframe)\n","        'clip_train_L':             [[0]],         # this clips sales after doing monthly groupings (monthly_stt dataframe)\n","        'feather_monthly_stt':      [[True]],      # if True, save final monthly_stt dataframe to disk as .ftr file to conserve RAM\n","        'feature_data_type':        [[np.int16]],  # np.float32 np.uint16 # if df contains np.NaNs, int type cannot represent, and we must use float32\n","        'minmax_scaler_range':      [[(0,16000)]], # int16=(0,32700); uint16=(0,65500) positive range for best LGBM results; smaller nums=faster fit\n","        'robust_scaler_quantiles':  [[(20,80)]],   # parameter determines how sklearn robust scaler will do it's bizness  \n","        'use_cartprod_fill':        [[True]],      # use cartesian fill, or not\n","        'use_categorical':          [[True]],      # relevant dataframe columns are changed to categorical dtype just before model fitting/creation\n","        'use_minmax_scaler':        [[True]],      # scale features linearly to use large range of np.int16 (NOTE: only use positive integer output)\n","        'use_robust_scaler':        [[True]],      # scale features using quantiles to reduce influence of outliers\n","        ###### below are train/val/test splitting parameters ######\n","        'feather_tvt_split':        [[False]],     # if True, save Train/Val/Test dataframes to disk as .ftr files to conserve RAM\n","        'test_month':               [[34]],\n","        'train_start_month':        [[13]],        # == 24 ==> less than a year of data, but avoids December 'outlier' of 2014\n","        'train_final_month':        [[29]],        # [29,32] #,30,32]\n","        'validate_months':          [[999]],       # 1 # 2 # 999 val= all months after training, incl 33; else val= n months after train_final_month\n","        ###### below are regresssor setup parameters ########\n","        'boosting_type':            'gbdt',\n","        'metric':                   [['rmse']],\n","        'learning_rate':            [[0.05]],     # default = 0.1\n","        'n_estimators':             [[200]],\n","        'colsample_bytree':         [[0.4]],      # = feature_fraction; default 1 for LGBM, 0 for HGBR (models use inverse forms of regularization)\n","        'random_state':             [[42]],\n","        'subsample_for_bin':        [[200000,800000]],\n","        'num_leaves':               [[31]],\n","        'max_depth':                [[-1]],\n","        'min_split_gain':           [[0.0]],\n","        'min_child_weight':         [[0.001]],\n","        'min_child_samples':        [[20]],\n","        'silent':                   [[False]],\n","        'importance_type':          [['split']],  # 'split' = num times the feature is used in model; 'gain' = total gains of splits using the feature\n","        'reg_alpha':                [[0.0]],\n","        'reg_lambda':               [[0.0]],\n","        'n_jobs':                   -1,\n","        'subsample':                1.0,\n","        'subsample_freq':           0,\n","        'objective':                'regression', \n","        ## **kwargs is not supported in sklearn, it may cause unexpected issues.\n","        ###### below are regressor fitting parameters ########\n","        'early_stopping_rounds':    [[20]],\n","        'eval_metric':              [['rmse']], # if multiple metrics, use [['rmse',['rmse','l2']]] to get 1 run w/ 'rmse' and 1 w/ ['rmse','l2']\n","        'init_score':               None,\n","        'eval_init_score':          None,\n","        'verbose':                  [[True]],   # (int=4 prints every 4th iteration); True is every iteration; False= no print except best and last\n","        'feature_name':             [['auto']], # (list of strings or ‘auto’) if 'auto' and data is from pandas df, data column names are used\n","        'categorical_feature':      [['auto']], # If ‘auto’ and data is pandas DataFrame, pandas unordered categorical columns are used\n","        'callbacks':                None,\n","        ###### output processing parameters below ######  (also need inverse scaling transformers)\n","        'clip_predict_H':           [[20]],        # this clips the final result before submission to Coursera/Kaggle\n","        'clip_predict_L':           [[0]],         # this clips the final result before submission to Coursera/Kaggle\n","        ###### below are output results slots ########\n","        'model_filename':           filename_root,   # will be overwritten with actual filename inside control loop\n","        'best_iteration_':          0,                                      \n","        'best_score_':              0.0,\n","        'feature_name_':            [[[\"\"]]],\n","        'feature_importances_':     [[[0.0]]],\n","        'model_params':             [[{}]],\n","        'time_cumulative':          \"\",\n","        'time_data_manip':          \"\",\n","        'time_dataset_splits':      \"\",\n","        'time_eda':                 \"\",\n","        'time_full_iteration':      \"\",\n","        'time_model_fit':           \"\",\n","        'time_model_predict':       \"\",\n","        'tr_rmse':                  0.0,\n","        'tr_R2':                    0.0,\n","        'val_rmse':                 0.0,\n","        'val_R2':                   0.0,\n","        'runtime_type':             runtime_type_cpu_gpu_tpu,\n","        'n_cpus':                   n_available_cpus,\n","        'ram_gb':                   available_vm_ram_gb,\n","        'package_versions':         [package_versions]\n","        })\n","\n","model_cols          = ['model_filename_base','model_type','feature_params','data_sources']\n","eda_cols            = ['eda_delete_shops','eda_delete_item_cats','eda_scale_month','feather_stt']\n","data_cols           = ['cartprod_fillna0','cartprod_first_month','cartprod_test_pairs','clip_train_H','clip_train_L','feather_monthly_stt',\n","                        'feature_data_type','minmax_scaler_range','robust_scaler_quantiles','use_cartprod_fill','use_categorical',\n","                        'use_minmax_scaler','use_robust_scaler']\n","tvt_split_cols            = ['feather_tvt_split','test_month','train_start_month','train_final_month','validate_months']\n","lgbm_setup_cols     = ['boosting_type','metric','learning_rate','n_estimators','colsample_bytree','random_state','subsample_for_bin','num_leaves',\n","                        'max_depth','min_split_gain','min_child_weight','min_child_samples','silent','importance_type','reg_alpha','reg_lambda',\n","                        'n_jobs','subsample','subsample_freq','objective']\n","lgbm_fit_cols       = ['eval_metric','early_stopping_rounds','init_score','eval_init_score','verbose','feature_name','categorical_feature','callbacks']\n","processing_cols     = ['clip_predict_H','clip_predict_L']\n","OUTPUT_cols         = ['model_filename','best_iteration_','best_score_','feature_importances_','feature_name_','model_params','time_cumulative',\n","                        'time_data_manip','time_dataset_splits','time_eda','time_full_iteration','time_model_fit','time_model_predict',\n","                        'tr_R2','tr_rmse','val_R2','val_rmse','runtime_type','n_cpus','ram_gb','package_versions']\n","\n","# One large df to explode all parameter variants and give us a sense of run size; other dfs are smaller portions of the input parameters set\n","# Parameters governing chunks of similar computations are collected into each of these different small dataframes,\n","#     so that in the main control section, there is one 'for loop' per small dataframe / objective (we won't need to use ['ALL'] later in the code)\n","SPLIT_PARAMS_module_dfs = OrderedDict()\n","SPLIT_PARAMS_module_dfs['ALL']          =   ALL_PARAMS.drop(OUTPUT_cols, axis=1).copy(deep=True) # don't explode OUTPUT_cols for input splits\n","SPLIT_PARAMS_module_dfs['MODEL']        =   ALL_PARAMS[model_cols].copy(deep=True)\n","SPLIT_PARAMS_module_dfs['EDA']          =   ALL_PARAMS[eda_cols].copy(deep=True)\n","SPLIT_PARAMS_module_dfs['DATA']         =   ALL_PARAMS[data_cols].copy(deep=True)\n","SPLIT_PARAMS_module_dfs['TVT_SPLIT']    =   ALL_PARAMS[tvt_split_cols].copy(deep=True)\n","SPLIT_PARAMS_module_dfs['LGBM_SETUP']   =   ALL_PARAMS[lgbm_setup_cols].copy(deep=True)\n","SPLIT_PARAMS_module_dfs['LGBM_FIT']     =   ALL_PARAMS[lgbm_fit_cols].copy(deep=True)\n","SPLIT_PARAMS_module_dfs['PROCESSING']   =   ALL_PARAMS[processing_cols].copy(deep=True)\n","OUTPUTS_df                              =   ALL_PARAMS[OUTPUT_cols].copy(deep=True)\n","\n","# find the parameters that have splits in them, and print out to highlight; ignore feature_params because too unwieldy for summary here\n","parameter_splits = {}\n","for col_name, param in SPLIT_PARAMS_module_dfs['ALL'].to_dict('index', into=OrderedDict)[0].items(): # before exploding df\n","    if type(param) is list:  # this level of list gets removed during \"explode\" operation\n","        if len(param) > 1:\n","            if col_name == 'feature_params':  # this parameter has so many items in it, it is impractical for quick split summary\n","                parameter_splits[col_name] = f\"[{len(param)} splits exist; printout below]\"\n","            else:\n","                parameter_splits[col_name] = param\n","\n","# pretty print dictionary-style dataframe info before exploding\n","for df_name, param_df in SPLIT_PARAMS_module_dfs.items():\n","    if 'feature_params' in param_df.columns:\n","        param_df = param_df.drop('feature_params', axis=1)  # will print this out below; it is a huge dict of a dict of a dict\n","    print(f'\\n{df_name} Parameters DataFrame:\\nColumns = {list(param_df.columns)}\\n{param_df.to_dict(orient=\"list\")}\\n')      \n","print(f'\\nOutput Results Dataframe:\\nColumns = {list(OUTPUTS_df.columns)}\\n{OUTPUTS_df.to_dict(orient=\"list\")}\\n')\n","\n","# Explode the dataframes so each row is one iteration of modeling parameters\n","for df_name, param_df in SPLIT_PARAMS_module_dfs.items():\n","    for col in param_df.columns:\n","        param_df = param_df.explode(col)\n","    SPLIT_PARAMS_module_dfs[df_name] = param_df.reset_index(drop=True)\n","\n","ALL_exploded_shape = SPLIT_PARAMS_module_dfs['ALL'].shape\n","print(f'Shape of Exploded [\"ALL\"] Parameters/Splits DataFrame (n_runs, n_parameters): {ALL_exploded_shape}')\n","print(f'N train models: {ALL_exploded_shape[0]}')  \n","print(f'Splits in this run (excluding features/lags): {parameter_splits}')\n","\n","# for df_name, param_df in SPLIT_PARAMS_module_dfs.items():\n","#     print(f'\\nExploded {df_name} Parameters DataFrame:\\n{param_df}\\n')  # tabular format\n","\n","print(f'\\nExploded SPLIT_PARAMS_module_dfs[\"ALL\"] df, as a dict:\\n{SPLIT_PARAMS_module_dfs[\"ALL\"].to_dict(orient=\"list\")}') # simple format\n","\n","print(\"\\nFeature Column Info:\") \n","for i, split_feature_dict in enumerate(feature_params):\n","    print(f'Iteration {i}:')\n","    for k, v in split_feature_dict.items():\n","        if k != 'lag_splits':\n","            print(f'  {k}: {v}')\n","        else:\n","            print(f'  Lag Split Info:  ',end='')\n","            for k1, v1 in v.items():\n","                print(f'    {k1}: {v1}')\n","print('\\n')\n","# Make placeholder rows in the OUTPUTS_df to handle same number of runs as is equal to number of rows in exploded 'ALL' dataframe\n","#   Concatenate 'ALL' onto 'OUTPUTS_df' so we only have to write one log file for entire run (= OUTPUTS_df)\n","OUTPUTS_df = pd.DataFrame(np.repeat(OUTPUTS_df.values, ALL_exploded_shape[0], axis=0), columns=OUTPUTS_df.columns).reset_index(drop=True)\n","OUTPUTS_df = pd.concat([OUTPUTS_df, SPLIT_PARAMS_module_dfs['ALL']], axis=1)\n","\n","time_cumulative         = elapsed_timer()\n","time_data_manip         = elapsed_timer()\n","time_dataset_splits     = elapsed_timer()\n","time_eda                = elapsed_timer()\n","time_full_iteration     = elapsed_timer()\n","time_model_fit          = elapsed_timer()\n","time_model_predict      = elapsed_timer()\n","block_time              = elapsed_timer() # general timer for various code blocks below; start it up right now with other initializations\n","MEMORY_STATS.append(get_memory_stats(\"Iteration Parameters Defined\",printout=True))\n","\n","print(f'\\nRun Splits/Parameters Identified: {strftime(\"%a %X %x\")}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","ALL Parameters DataFrame:\n","Columns = ['model_filename_base', 'model_type', 'data_sources', 'eda_delete_shops', 'eda_delete_item_cats', 'eda_scale_month', 'feather_stt', 'cartprod_fillna0', 'cartprod_first_month', 'cartprod_test_pairs', 'clip_train_H', 'clip_train_L', 'feather_monthly_stt', 'feature_data_type', 'minmax_scaler_range', 'robust_scaler_quantiles', 'use_cartprod_fill', 'use_categorical', 'use_minmax_scaler', 'use_robust_scaler', 'feather_tvt_split', 'test_month', 'train_start_month', 'train_final_month', 'validate_months', 'boosting_type', 'metric', 'learning_rate', 'n_estimators', 'colsample_bytree', 'random_state', 'subsample_for_bin', 'num_leaves', 'max_depth', 'min_split_gain', 'min_child_weight', 'min_child_samples', 'silent', 'importance_type', 'reg_alpha', 'reg_lambda', 'n_jobs', 'subsample', 'subsample_freq', 'objective', 'early_stopping_rounds', 'eval_metric', 'init_score', 'eval_init_score', 'verbose', 'feature_name', 'categorical_feature', 'callbacks', 'clip_predict_H', 'clip_predict_L']\n","{'model_filename_base': ['20200909-0336_v1p0_test1'], 'model_type': [['LGBM']], 'data_sources': [[[('items_enc', 'data_output/items_enc.csv'), ('shops_enc', 'data_output/shops_enc.csv'), ('date_scaling', 'data_output/date_scaling.csv'), ('stt', 'data_output/stt.csv.gz'), ('test', 'readonly/final_project_data/test.csv.gz')]]], 'eda_delete_shops': [[[9, 20]]], 'eda_delete_item_cats': [[[8, 10, 32, 59, 80, 81, 82]]], 'eda_scale_month': [['week_retail_weight']], 'feather_stt': [[True]], 'cartprod_fillna0': [[True]], 'cartprod_first_month': [[13]], 'cartprod_test_pairs': [[False]], 'clip_train_H': [[20]], 'clip_train_L': [[0]], 'feather_monthly_stt': [[True]], 'feature_data_type': [[<class 'numpy.int16'>]], 'minmax_scaler_range': [[(0, 16000)]], 'robust_scaler_quantiles': [[(20, 80)]], 'use_cartprod_fill': [[True]], 'use_categorical': [[True]], 'use_minmax_scaler': [[True]], 'use_robust_scaler': [[True]], 'feather_tvt_split': [[False]], 'test_month': [[34]], 'train_start_month': [[13]], 'train_final_month': [[29]], 'validate_months': [[999]], 'boosting_type': ['gbdt'], 'metric': [['rmse']], 'learning_rate': [[0.05]], 'n_estimators': [[200]], 'colsample_bytree': [[0.4]], 'random_state': [[42]], 'subsample_for_bin': [[200000, 800000]], 'num_leaves': [[31]], 'max_depth': [[-1]], 'min_split_gain': [[0.0]], 'min_child_weight': [[0.001]], 'min_child_samples': [[20]], 'silent': [[False]], 'importance_type': [['split']], 'reg_alpha': [[0.0]], 'reg_lambda': [[0.0]], 'n_jobs': [-1], 'subsample': [1.0], 'subsample_freq': [0], 'objective': ['regression'], 'early_stopping_rounds': [[20]], 'eval_metric': [['rmse']], 'init_score': [None], 'eval_init_score': [None], 'verbose': [[True]], 'feature_name': [['auto']], 'categorical_feature': [['auto']], 'callbacks': [None], 'clip_predict_H': [[20]], 'clip_predict_L': [[0]]}\n","\n","\n","MODEL Parameters DataFrame:\n","Columns = ['model_filename_base', 'model_type', 'data_sources']\n","{'model_filename_base': ['20200909-0336_v1p0_test1'], 'model_type': [['LGBM']], 'data_sources': [[[('items_enc', 'data_output/items_enc.csv'), ('shops_enc', 'data_output/shops_enc.csv'), ('date_scaling', 'data_output/date_scaling.csv'), ('stt', 'data_output/stt.csv.gz'), ('test', 'readonly/final_project_data/test.csv.gz')]]]}\n","\n","\n","EDA Parameters DataFrame:\n","Columns = ['eda_delete_shops', 'eda_delete_item_cats', 'eda_scale_month', 'feather_stt']\n","{'eda_delete_shops': [[[9, 20]]], 'eda_delete_item_cats': [[[8, 10, 32, 59, 80, 81, 82]]], 'eda_scale_month': [['week_retail_weight']], 'feather_stt': [[True]]}\n","\n","\n","DATA Parameters DataFrame:\n","Columns = ['cartprod_fillna0', 'cartprod_first_month', 'cartprod_test_pairs', 'clip_train_H', 'clip_train_L', 'feather_monthly_stt', 'feature_data_type', 'minmax_scaler_range', 'robust_scaler_quantiles', 'use_cartprod_fill', 'use_categorical', 'use_minmax_scaler', 'use_robust_scaler']\n","{'cartprod_fillna0': [[True]], 'cartprod_first_month': [[13]], 'cartprod_test_pairs': [[False]], 'clip_train_H': [[20]], 'clip_train_L': [[0]], 'feather_monthly_stt': [[True]], 'feature_data_type': [[<class 'numpy.int16'>]], 'minmax_scaler_range': [[(0, 16000)]], 'robust_scaler_quantiles': [[(20, 80)]], 'use_cartprod_fill': [[True]], 'use_categorical': [[True]], 'use_minmax_scaler': [[True]], 'use_robust_scaler': [[True]]}\n","\n","\n","TVT_SPLIT Parameters DataFrame:\n","Columns = ['feather_tvt_split', 'test_month', 'train_start_month', 'train_final_month', 'validate_months']\n","{'feather_tvt_split': [[False]], 'test_month': [[34]], 'train_start_month': [[13]], 'train_final_month': [[29]], 'validate_months': [[999]]}\n","\n","\n","LGBM_SETUP Parameters DataFrame:\n","Columns = ['boosting_type', 'metric', 'learning_rate', 'n_estimators', 'colsample_bytree', 'random_state', 'subsample_for_bin', 'num_leaves', 'max_depth', 'min_split_gain', 'min_child_weight', 'min_child_samples', 'silent', 'importance_type', 'reg_alpha', 'reg_lambda', 'n_jobs', 'subsample', 'subsample_freq', 'objective']\n","{'boosting_type': ['gbdt'], 'metric': [['rmse']], 'learning_rate': [[0.05]], 'n_estimators': [[200]], 'colsample_bytree': [[0.4]], 'random_state': [[42]], 'subsample_for_bin': [[200000, 800000]], 'num_leaves': [[31]], 'max_depth': [[-1]], 'min_split_gain': [[0.0]], 'min_child_weight': [[0.001]], 'min_child_samples': [[20]], 'silent': [[False]], 'importance_type': [['split']], 'reg_alpha': [[0.0]], 'reg_lambda': [[0.0]], 'n_jobs': [-1], 'subsample': [1.0], 'subsample_freq': [0], 'objective': ['regression']}\n","\n","\n","LGBM_FIT Parameters DataFrame:\n","Columns = ['eval_metric', 'early_stopping_rounds', 'init_score', 'eval_init_score', 'verbose', 'feature_name', 'categorical_feature', 'callbacks']\n","{'eval_metric': [['rmse']], 'early_stopping_rounds': [[20]], 'init_score': [None], 'eval_init_score': [None], 'verbose': [[True]], 'feature_name': [['auto']], 'categorical_feature': [['auto']], 'callbacks': [None]}\n","\n","\n","PROCESSING Parameters DataFrame:\n","Columns = ['clip_predict_H', 'clip_predict_L']\n","{'clip_predict_H': [[20]], 'clip_predict_L': [[0]]}\n","\n","\n","Output Results Dataframe:\n","Columns = ['model_filename', 'best_iteration_', 'best_score_', 'feature_importances_', 'feature_name_', 'model_params', 'time_cumulative', 'time_data_manip', 'time_dataset_splits', 'time_eda', 'time_full_iteration', 'time_model_fit', 'time_model_predict', 'tr_R2', 'tr_rmse', 'val_R2', 'val_rmse', 'runtime_type', 'n_cpus', 'ram_gb', 'package_versions']\n","{'model_filename': ['v1p0_test1'], 'best_iteration_': [0], 'best_score_': [0.0], 'feature_importances_': [[[0.0]]], 'feature_name_': [[['']]], 'model_params': [[{}]], 'time_cumulative': [''], 'time_data_manip': [''], 'time_dataset_splits': [''], 'time_eda': [''], 'time_full_iteration': [''], 'time_model_fit': [''], 'time_model_predict': [''], 'tr_R2': [0.0], 'tr_rmse': [0.0], 'val_R2': [0.0], 'val_rmse': [0.0], 'runtime_type': ['Unknown runtime type'], 'n_cpus': [4], 'ram_gb': [27.393728512], 'package_versions': [OrderedDict([('Python', '3.6.9'), ('lightgbm', '2.2.3'), ('matplotlib', '3.2.2'), ('numpy', '1.18.5'), ('pandas', '1.0.5'), ('scikit-learn', '0.22.2.post1')])]}\n","\n","Shape of Exploded [\"ALL\"] Parameters/Splits DataFrame (n_runs, n_parameters): (2, 56)\n","N train models: 2\n","Splits in this run (excluding features/lags): {'subsample_for_bin': [200000, 800000]}\n","\n","Exploded SPLIT_PARAMS_module_dfs[\"ALL\"] df, as a dict:\n","{'model_filename_base': ['20200909-0336_v1p0_test1', '20200909-0336_v1p0_test1'], 'model_type': ['LGBM', 'LGBM'], 'feature_params': [OrderedDict([('keep_columns', OrderedDict([('items_enc', ['item_id', 'item_category_id', 'item_group', 'item_cluster']), ('shops_enc', ['shop_id', 'shop_group']), ('date_scaling', ['month', 'week_retail_weight']), ('stt', ['month', 'sales', 'price', 'shop_id', 'item_id']), ('test', ['ID', 'shop_id', 'item_id'])])), ('aggregate_stats', OrderedDict([('sales', ['sum', 'median', 'count']), ('revenue', ['sum'])])), ('stt_final', ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']), ('categorical', ['shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster']), ('integer', ['month', 'shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster']), ('lag_splits', {'months_list': [1, 2, 3, 4, 5, 6, 7, 8], 'params': OrderedDict([(1, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum', 'median', 'count'], 'revenue': ['sum']}}), ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_revenue_sum']}), ('shop_id_x_item_cluster', {'group': ['shop_id', 'item_cluster'], 'stats': {'sales': ['sum', 'median']}, 'agg_names': ['shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median']}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['count', 'sum']}, 'agg_names': ['shop_id_sales_count', 'shop_id_sales_sum']}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['item_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_revenue_sum']}), ('shop_group', {'group': ['shop_group'], 'stats': {'revenue': ['sum']}, 'agg_names': ['shop_group_revenue_sum']}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_category_id_sales_count', 'item_category_id_sales_sum', 'item_category_id_revenue_sum']}), ('item_group', {'group': ['item_group'], 'stats': {'sales': ['sum'], 'revenue': ['sum']}, 'agg_names': ['item_group_sales_sum', 'item_group_revenue_sum']}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_cluster_sales_count', 'item_cluster_sales_sum', 'item_cluster_revenue_sum']}), ('feature_root', ['shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', 'shop_id_sales_sum', 'shop_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', 'shop_group_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', 'item_group_sales_sum', 'item_group_revenue_sum', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L1'), ('shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_median_L1'), ('shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_count_L1'), ('shop_id_x_item_id_revenue_sum', 'shop_id_x_item_id_revenue_sum_L1'), ('shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_sum_L1'), ('shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_median_L1'), ('shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_count_L1'), ('shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_sum_L1'), ('shop_id_x_item_cluster_sales_median', 'shop_id_x_item_cluster_sales_median_L1'), ('shop_id_sales_sum', 'shop_id_sales_sum_L1'), ('shop_id_sales_count', 'shop_id_sales_count_L1'), ('item_id_sales_sum', 'item_id_sales_sum_L1'), ('item_id_sales_median', 'item_id_sales_median_L1'), ('item_id_sales_count', 'item_id_sales_count_L1'), ('item_id_revenue_sum', 'item_id_revenue_sum_L1'), ('shop_group_revenue_sum', 'shop_group_revenue_sum_L1'), ('item_category_id_sales_sum', 'item_category_id_sales_sum_L1'), ('item_category_id_sales_count', 'item_category_id_sales_count_L1'), ('item_category_id_revenue_sum', 'item_category_id_revenue_sum_L1'), ('item_group_sales_sum', 'item_group_sales_sum_L1'), ('item_group_revenue_sum', 'item_group_revenue_sum_L1'), ('item_cluster_sales_sum', 'item_cluster_sales_sum_L1'), ('item_cluster_sales_count', 'item_cluster_sales_count_L1'), ('item_cluster_revenue_sum', 'item_cluster_revenue_sum_L1')]))])), (2, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count'], 'revenue': ['sum']}}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['sum']}}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['sum', 'count']}}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_revenue_sum', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L2'), ('shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_count_L2'), ('shop_id_x_item_id_revenue_sum', 'shop_id_x_item_id_revenue_sum_L2'), ('shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_count_L2'), ('shop_id_x_item_category_id_revenue_sum', 'shop_id_x_item_category_id_revenue_sum_L2'), ('shop_id_sales_sum', 'shop_id_sales_sum_L2'), ('item_id_sales_sum', 'item_id_sales_sum_L2'), ('item_id_sales_count', 'item_id_sales_count_L2'), ('item_id_revenue_sum', 'item_id_revenue_sum_L2'), ('item_category_id_sales_sum', 'item_category_id_sales_sum_L2'), ('item_category_id_sales_count', 'item_category_id_sales_count_L2'), ('item_cluster_sales_sum', 'item_cluster_sales_sum_L2'), ('item_cluster_sales_count', 'item_cluster_sales_count_L2'), ('item_cluster_revenue_sum', 'item_cluster_revenue_sum_L2')]))])), (3, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['sum']}}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['sum', 'count']}}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['count']}}), ('feature_root', ['shop_id_x_item_id_sales_sum', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_cluster_sales_count']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L3'), ('shop_id_sales_sum', 'shop_id_sales_sum_L3'), ('item_id_sales_sum', 'item_id_sales_sum_L3'), ('item_id_sales_count', 'item_id_sales_count_L3'), ('item_id_revenue_sum', 'item_id_revenue_sum_L3'), ('item_category_id_sales_sum', 'item_category_id_sales_sum_L3'), ('item_category_id_sales_count', 'item_category_id_sales_count_L3'), ('item_cluster_sales_count', 'item_cluster_sales_count_L3')]))])), (4, OrderedDict([('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('item_id_sales_sum', 'item_id_sales_sum_L4')]))])), (5, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L5')]))])), (6, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L6')]))])), (7, OrderedDict([('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('item_id_sales_sum', 'item_id_sales_sum_L7')]))])), (8, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L8')]))]))]), 'stats_set': OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'shop_group': ['first'], 'item_category_id': ['first'], 'item_group': ['first'], 'item_cluster': ['first'], 'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_group', 'item_category_id', 'item_group', 'item_cluster', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_revenue_sum']}), ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_revenue_sum']}), ('shop_id_x_item_cluster', {'group': ['shop_id', 'item_cluster'], 'stats': {'sales': ['sum', 'median']}, 'agg_names': ['shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median']}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['count', 'sum']}, 'agg_names': ['shop_id_sales_count', 'shop_id_sales_sum']}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['item_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_revenue_sum']}), ('shop_group', {'group': ['shop_group'], 'stats': {'revenue': ['sum']}, 'agg_names': ['shop_group_revenue_sum']}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_category_id_sales_count', 'item_category_id_sales_sum', 'item_category_id_revenue_sum']}), ('item_group', {'group': ['item_group'], 'stats': {'sales': ['sum'], 'revenue': ['sum']}, 'agg_names': ['item_group_sales_sum', 'item_group_revenue_sum']}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_cluster_sales_count', 'item_cluster_sales_sum', 'item_cluster_revenue_sum']})]), 'stats_set_feature_names': ['shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', 'shop_id_sales_sum', 'shop_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', 'shop_group_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', 'item_group_sales_sum', 'item_group_revenue_sum', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum', 'shop_id_x_item_category_id_revenue_sum']}), ('all_feature_names', ['month', 'shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster', 'shop_id_x_item_id_sales_sum_L1', 'shop_id_x_item_id_sales_median_L1', 'shop_id_x_item_id_sales_count_L1', 'shop_id_x_item_id_revenue_sum_L1', 'shop_id_x_item_category_id_sales_sum_L1', 'shop_id_x_item_category_id_sales_median_L1', 'shop_id_x_item_category_id_sales_count_L1', 'shop_id_x_item_cluster_sales_sum_L1', 'shop_id_x_item_cluster_sales_median_L1', 'shop_id_sales_sum_L1', 'shop_id_sales_count_L1', 'item_id_sales_sum_L1', 'item_id_sales_median_L1', 'item_id_sales_count_L1', 'item_id_revenue_sum_L1', 'shop_group_revenue_sum_L1', 'item_category_id_sales_sum_L1', 'item_category_id_sales_count_L1', 'item_category_id_revenue_sum_L1', 'item_group_sales_sum_L1', 'item_group_revenue_sum_L1', 'item_cluster_sales_sum_L1', 'item_cluster_sales_count_L1', 'item_cluster_revenue_sum_L1', 'shop_id_x_item_id_sales_sum_L2', 'shop_id_x_item_id_sales_count_L2', 'shop_id_x_item_id_revenue_sum_L2', 'shop_id_x_item_category_id_sales_count_L2', 'shop_id_x_item_category_id_revenue_sum_L2', 'shop_id_sales_sum_L2', 'item_id_sales_sum_L2', 'item_id_sales_count_L2', 'item_id_revenue_sum_L2', 'item_category_id_sales_sum_L2', 'item_category_id_sales_count_L2', 'item_cluster_sales_sum_L2', 'item_cluster_sales_count_L2', 'item_cluster_revenue_sum_L2', 'shop_id_x_item_id_sales_sum_L3', 'shop_id_sales_sum_L3', 'item_id_sales_sum_L3', 'item_id_sales_count_L3', 'item_id_revenue_sum_L3', 'item_category_id_sales_sum_L3', 'item_category_id_sales_count_L3', 'item_cluster_sales_count_L3', 'item_id_sales_sum_L4', 'shop_id_x_item_id_sales_sum_L5', 'shop_id_x_item_id_sales_sum_L6', 'item_id_sales_sum_L7', 'shop_id_x_item_id_sales_sum_L8']), ('n_features', 58)]), OrderedDict([('keep_columns', OrderedDict([('items_enc', ['item_id', 'item_category_id', 'item_group', 'item_cluster']), ('shops_enc', ['shop_id', 'shop_group']), ('date_scaling', ['month', 'week_retail_weight']), ('stt', ['month', 'sales', 'price', 'shop_id', 'item_id']), ('test', ['ID', 'shop_id', 'item_id'])])), ('aggregate_stats', OrderedDict([('sales', ['sum', 'median', 'count']), ('revenue', ['sum'])])), ('stt_final', ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']), ('categorical', ['shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster']), ('integer', ['month', 'shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster']), ('lag_splits', {'months_list': [1, 2, 3, 4, 5, 6, 7, 8], 'params': OrderedDict([(1, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum', 'median', 'count'], 'revenue': ['sum']}}), ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_revenue_sum']}), ('shop_id_x_item_cluster', {'group': ['shop_id', 'item_cluster'], 'stats': {'sales': ['sum', 'median']}, 'agg_names': ['shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median']}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['count', 'sum']}, 'agg_names': ['shop_id_sales_count', 'shop_id_sales_sum']}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['item_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_revenue_sum']}), ('shop_group', {'group': ['shop_group'], 'stats': {'revenue': ['sum']}, 'agg_names': ['shop_group_revenue_sum']}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_category_id_sales_count', 'item_category_id_sales_sum', 'item_category_id_revenue_sum']}), ('item_group', {'group': ['item_group'], 'stats': {'sales': ['sum'], 'revenue': ['sum']}, 'agg_names': ['item_group_sales_sum', 'item_group_revenue_sum']}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_cluster_sales_count', 'item_cluster_sales_sum', 'item_cluster_revenue_sum']}), ('feature_root', ['shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', 'shop_id_sales_sum', 'shop_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', 'shop_group_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', 'item_group_sales_sum', 'item_group_revenue_sum', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L1'), ('shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_median_L1'), ('shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_count_L1'), ('shop_id_x_item_id_revenue_sum', 'shop_id_x_item_id_revenue_sum_L1'), ('shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_sum_L1'), ('shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_median_L1'), ('shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_count_L1'), ('shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_sum_L1'), ('shop_id_x_item_cluster_sales_median', 'shop_id_x_item_cluster_sales_median_L1'), ('shop_id_sales_sum', 'shop_id_sales_sum_L1'), ('shop_id_sales_count', 'shop_id_sales_count_L1'), ('item_id_sales_sum', 'item_id_sales_sum_L1'), ('item_id_sales_median', 'item_id_sales_median_L1'), ('item_id_sales_count', 'item_id_sales_count_L1'), ('item_id_revenue_sum', 'item_id_revenue_sum_L1'), ('shop_group_revenue_sum', 'shop_group_revenue_sum_L1'), ('item_category_id_sales_sum', 'item_category_id_sales_sum_L1'), ('item_category_id_sales_count', 'item_category_id_sales_count_L1'), ('item_category_id_revenue_sum', 'item_category_id_revenue_sum_L1'), ('item_group_sales_sum', 'item_group_sales_sum_L1'), ('item_group_revenue_sum', 'item_group_revenue_sum_L1'), ('item_cluster_sales_sum', 'item_cluster_sales_sum_L1'), ('item_cluster_sales_count', 'item_cluster_sales_count_L1'), ('item_cluster_revenue_sum', 'item_cluster_revenue_sum_L1')]))])), (2, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count'], 'revenue': ['sum']}}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['sum']}}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['sum', 'count']}}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_revenue_sum', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L2'), ('shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_count_L2'), ('shop_id_x_item_id_revenue_sum', 'shop_id_x_item_id_revenue_sum_L2'), ('shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_count_L2'), ('shop_id_x_item_category_id_revenue_sum', 'shop_id_x_item_category_id_revenue_sum_L2'), ('shop_id_sales_sum', 'shop_id_sales_sum_L2'), ('item_id_sales_sum', 'item_id_sales_sum_L2'), ('item_id_sales_count', 'item_id_sales_count_L2'), ('item_id_revenue_sum', 'item_id_revenue_sum_L2'), ('item_category_id_sales_sum', 'item_category_id_sales_sum_L2'), ('item_category_id_sales_count', 'item_category_id_sales_count_L2'), ('item_cluster_sales_sum', 'item_cluster_sales_sum_L2'), ('item_cluster_sales_count', 'item_cluster_sales_count_L2'), ('item_cluster_revenue_sum', 'item_cluster_revenue_sum_L2')]))])), (3, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['sum']}}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['sum', 'count']}}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['count']}}), ('feature_root', ['shop_id_x_item_id_sales_sum', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_cluster_sales_count']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L3'), ('shop_id_sales_sum', 'shop_id_sales_sum_L3'), ('item_id_sales_sum', 'item_id_sales_sum_L3'), ('item_id_sales_count', 'item_id_sales_count_L3'), ('item_id_revenue_sum', 'item_id_revenue_sum_L3'), ('item_category_id_sales_sum', 'item_category_id_sales_sum_L3'), ('item_category_id_sales_count', 'item_category_id_sales_count_L3'), ('item_cluster_sales_count', 'item_cluster_sales_count_L3')]))])), (4, OrderedDict([('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('item_id_sales_sum', 'item_id_sales_sum_L4')]))])), (5, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L5')]))])), (6, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L6')]))])), (7, OrderedDict([('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('item_id_sales_sum', 'item_id_sales_sum_L7')]))])), (8, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L8')]))]))]), 'stats_set': OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'shop_group': ['first'], 'item_category_id': ['first'], 'item_group': ['first'], 'item_cluster': ['first'], 'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_group', 'item_category_id', 'item_group', 'item_cluster', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_revenue_sum']}), ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_revenue_sum']}), ('shop_id_x_item_cluster', {'group': ['shop_id', 'item_cluster'], 'stats': {'sales': ['sum', 'median']}, 'agg_names': ['shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median']}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['count', 'sum']}, 'agg_names': ['shop_id_sales_count', 'shop_id_sales_sum']}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['item_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_revenue_sum']}), ('shop_group', {'group': ['shop_group'], 'stats': {'revenue': ['sum']}, 'agg_names': ['shop_group_revenue_sum']}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_category_id_sales_count', 'item_category_id_sales_sum', 'item_category_id_revenue_sum']}), ('item_group', {'group': ['item_group'], 'stats': {'sales': ['sum'], 'revenue': ['sum']}, 'agg_names': ['item_group_sales_sum', 'item_group_revenue_sum']}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_cluster_sales_count', 'item_cluster_sales_sum', 'item_cluster_revenue_sum']})]), 'stats_set_feature_names': ['shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', 'shop_id_sales_sum', 'shop_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', 'shop_group_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', 'item_group_sales_sum', 'item_group_revenue_sum', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum', 'shop_id_x_item_category_id_revenue_sum']}), ('all_feature_names', ['month', 'shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster', 'shop_id_x_item_id_sales_sum_L1', 'shop_id_x_item_id_sales_median_L1', 'shop_id_x_item_id_sales_count_L1', 'shop_id_x_item_id_revenue_sum_L1', 'shop_id_x_item_category_id_sales_sum_L1', 'shop_id_x_item_category_id_sales_median_L1', 'shop_id_x_item_category_id_sales_count_L1', 'shop_id_x_item_cluster_sales_sum_L1', 'shop_id_x_item_cluster_sales_median_L1', 'shop_id_sales_sum_L1', 'shop_id_sales_count_L1', 'item_id_sales_sum_L1', 'item_id_sales_median_L1', 'item_id_sales_count_L1', 'item_id_revenue_sum_L1', 'shop_group_revenue_sum_L1', 'item_category_id_sales_sum_L1', 'item_category_id_sales_count_L1', 'item_category_id_revenue_sum_L1', 'item_group_sales_sum_L1', 'item_group_revenue_sum_L1', 'item_cluster_sales_sum_L1', 'item_cluster_sales_count_L1', 'item_cluster_revenue_sum_L1', 'shop_id_x_item_id_sales_sum_L2', 'shop_id_x_item_id_sales_count_L2', 'shop_id_x_item_id_revenue_sum_L2', 'shop_id_x_item_category_id_sales_count_L2', 'shop_id_x_item_category_id_revenue_sum_L2', 'shop_id_sales_sum_L2', 'item_id_sales_sum_L2', 'item_id_sales_count_L2', 'item_id_revenue_sum_L2', 'item_category_id_sales_sum_L2', 'item_category_id_sales_count_L2', 'item_cluster_sales_sum_L2', 'item_cluster_sales_count_L2', 'item_cluster_revenue_sum_L2', 'shop_id_x_item_id_sales_sum_L3', 'shop_id_sales_sum_L3', 'item_id_sales_sum_L3', 'item_id_sales_count_L3', 'item_id_revenue_sum_L3', 'item_category_id_sales_sum_L3', 'item_category_id_sales_count_L3', 'item_cluster_sales_count_L3', 'item_id_sales_sum_L4', 'shop_id_x_item_id_sales_sum_L5', 'shop_id_x_item_id_sales_sum_L6', 'item_id_sales_sum_L7', 'shop_id_x_item_id_sales_sum_L8']), ('n_features', 58)])], 'data_sources': [[('items_enc', 'data_output/items_enc.csv'), ('shops_enc', 'data_output/shops_enc.csv'), ('date_scaling', 'data_output/date_scaling.csv'), ('stt', 'data_output/stt.csv.gz'), ('test', 'readonly/final_project_data/test.csv.gz')], [('items_enc', 'data_output/items_enc.csv'), ('shops_enc', 'data_output/shops_enc.csv'), ('date_scaling', 'data_output/date_scaling.csv'), ('stt', 'data_output/stt.csv.gz'), ('test', 'readonly/final_project_data/test.csv.gz')]], 'eda_delete_shops': [[9, 20], [9, 20]], 'eda_delete_item_cats': [[8, 10, 32, 59, 80, 81, 82], [8, 10, 32, 59, 80, 81, 82]], 'eda_scale_month': ['week_retail_weight', 'week_retail_weight'], 'feather_stt': [True, True], 'cartprod_fillna0': [True, True], 'cartprod_first_month': [13, 13], 'cartprod_test_pairs': [False, False], 'clip_train_H': [20, 20], 'clip_train_L': [0, 0], 'feather_monthly_stt': [True, True], 'feature_data_type': [<class 'numpy.int16'>, <class 'numpy.int16'>], 'minmax_scaler_range': [(0, 16000), (0, 16000)], 'robust_scaler_quantiles': [(20, 80), (20, 80)], 'use_cartprod_fill': [True, True], 'use_categorical': [True, True], 'use_minmax_scaler': [True, True], 'use_robust_scaler': [True, True], 'feather_tvt_split': [False, False], 'test_month': [34, 34], 'train_start_month': [13, 13], 'train_final_month': [29, 29], 'validate_months': [999, 999], 'boosting_type': ['gbdt', 'gbdt'], 'metric': ['rmse', 'rmse'], 'learning_rate': [0.05, 0.05], 'n_estimators': [200, 200], 'colsample_bytree': [0.4, 0.4], 'random_state': [42, 42], 'subsample_for_bin': [200000, 800000], 'num_leaves': [31, 31], 'max_depth': [-1, -1], 'min_split_gain': [0.0, 0.0], 'min_child_weight': [0.001, 0.001], 'min_child_samples': [20, 20], 'silent': [False, False], 'importance_type': ['split', 'split'], 'reg_alpha': [0.0, 0.0], 'reg_lambda': [0.0, 0.0], 'n_jobs': [-1, -1], 'subsample': [1.0, 1.0], 'subsample_freq': [0, 0], 'objective': ['regression', 'regression'], 'early_stopping_rounds': [20, 20], 'eval_metric': ['rmse', 'rmse'], 'init_score': [None, None], 'eval_init_score': [None, None], 'verbose': [True, True], 'feature_name': ['auto', 'auto'], 'categorical_feature': ['auto', 'auto'], 'callbacks': [None, None], 'clip_predict_H': [20, 20], 'clip_predict_L': [0, 0]}\n","\n","Feature Column Info:\n","Iteration 0:\n","  keep_columns: OrderedDict([('items_enc', ['item_id', 'item_category_id', 'item_group', 'item_cluster']), ('shops_enc', ['shop_id', 'shop_group']), ('date_scaling', ['month', 'week_retail_weight']), ('stt', ['month', 'sales', 'price', 'shop_id', 'item_id']), ('test', ['ID', 'shop_id', 'item_id'])])\n","  aggregate_stats: OrderedDict([('sales', ['sum', 'median', 'count']), ('revenue', ['sum'])])\n","  stt_final: ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']\n","  categorical: ['shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster']\n","  integer: ['month', 'shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster']\n","  Lag Split Info:      months_list: [1, 2, 3, 4, 5, 6, 7, 8]\n","    params: OrderedDict([(1, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum', 'median', 'count'], 'revenue': ['sum']}}), ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_revenue_sum']}), ('shop_id_x_item_cluster', {'group': ['shop_id', 'item_cluster'], 'stats': {'sales': ['sum', 'median']}, 'agg_names': ['shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median']}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['count', 'sum']}, 'agg_names': ['shop_id_sales_count', 'shop_id_sales_sum']}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['item_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_revenue_sum']}), ('shop_group', {'group': ['shop_group'], 'stats': {'revenue': ['sum']}, 'agg_names': ['shop_group_revenue_sum']}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_category_id_sales_count', 'item_category_id_sales_sum', 'item_category_id_revenue_sum']}), ('item_group', {'group': ['item_group'], 'stats': {'sales': ['sum'], 'revenue': ['sum']}, 'agg_names': ['item_group_sales_sum', 'item_group_revenue_sum']}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_cluster_sales_count', 'item_cluster_sales_sum', 'item_cluster_revenue_sum']}), ('feature_root', ['shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', 'shop_id_sales_sum', 'shop_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', 'shop_group_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', 'item_group_sales_sum', 'item_group_revenue_sum', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L1'), ('shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_median_L1'), ('shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_count_L1'), ('shop_id_x_item_id_revenue_sum', 'shop_id_x_item_id_revenue_sum_L1'), ('shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_sum_L1'), ('shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_median_L1'), ('shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_count_L1'), ('shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_sum_L1'), ('shop_id_x_item_cluster_sales_median', 'shop_id_x_item_cluster_sales_median_L1'), ('shop_id_sales_sum', 'shop_id_sales_sum_L1'), ('shop_id_sales_count', 'shop_id_sales_count_L1'), ('item_id_sales_sum', 'item_id_sales_sum_L1'), ('item_id_sales_median', 'item_id_sales_median_L1'), ('item_id_sales_count', 'item_id_sales_count_L1'), ('item_id_revenue_sum', 'item_id_revenue_sum_L1'), ('shop_group_revenue_sum', 'shop_group_revenue_sum_L1'), ('item_category_id_sales_sum', 'item_category_id_sales_sum_L1'), ('item_category_id_sales_count', 'item_category_id_sales_count_L1'), ('item_category_id_revenue_sum', 'item_category_id_revenue_sum_L1'), ('item_group_sales_sum', 'item_group_sales_sum_L1'), ('item_group_revenue_sum', 'item_group_revenue_sum_L1'), ('item_cluster_sales_sum', 'item_cluster_sales_sum_L1'), ('item_cluster_sales_count', 'item_cluster_sales_count_L1'), ('item_cluster_revenue_sum', 'item_cluster_revenue_sum_L1')]))])), (2, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count'], 'revenue': ['sum']}}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['sum']}}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['sum', 'count']}}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_revenue_sum', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L2'), ('shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_count_L2'), ('shop_id_x_item_id_revenue_sum', 'shop_id_x_item_id_revenue_sum_L2'), ('shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_count_L2'), ('shop_id_x_item_category_id_revenue_sum', 'shop_id_x_item_category_id_revenue_sum_L2'), ('shop_id_sales_sum', 'shop_id_sales_sum_L2'), ('item_id_sales_sum', 'item_id_sales_sum_L2'), ('item_id_sales_count', 'item_id_sales_count_L2'), ('item_id_revenue_sum', 'item_id_revenue_sum_L2'), ('item_category_id_sales_sum', 'item_category_id_sales_sum_L2'), ('item_category_id_sales_count', 'item_category_id_sales_count_L2'), ('item_cluster_sales_sum', 'item_cluster_sales_sum_L2'), ('item_cluster_sales_count', 'item_cluster_sales_count_L2'), ('item_cluster_revenue_sum', 'item_cluster_revenue_sum_L2')]))])), (3, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['sum']}}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['sum', 'count']}}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['count']}}), ('feature_root', ['shop_id_x_item_id_sales_sum', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_cluster_sales_count']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L3'), ('shop_id_sales_sum', 'shop_id_sales_sum_L3'), ('item_id_sales_sum', 'item_id_sales_sum_L3'), ('item_id_sales_count', 'item_id_sales_count_L3'), ('item_id_revenue_sum', 'item_id_revenue_sum_L3'), ('item_category_id_sales_sum', 'item_category_id_sales_sum_L3'), ('item_category_id_sales_count', 'item_category_id_sales_count_L3'), ('item_cluster_sales_count', 'item_cluster_sales_count_L3')]))])), (4, OrderedDict([('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('item_id_sales_sum', 'item_id_sales_sum_L4')]))])), (5, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L5')]))])), (6, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L6')]))])), (7, OrderedDict([('item_id', {'group': ['item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('item_id_sales_sum', 'item_id_sales_sum_L7')]))])), (8, OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum']}}), ('feature_root', ['shop_id_x_item_id_sales_sum']), ('lagged_feature_name', OrderedDict([('shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_sum_L8')]))]))])\n","    stats_set: OrderedDict([('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'shop_group': ['first'], 'item_category_id': ['first'], 'item_group': ['first'], 'item_cluster': ['first'], 'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_group', 'item_category_id', 'item_group', 'item_cluster', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_revenue_sum']}), ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_revenue_sum']}), ('shop_id_x_item_cluster', {'group': ['shop_id', 'item_cluster'], 'stats': {'sales': ['sum', 'median']}, 'agg_names': ['shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median']}), ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['count', 'sum']}, 'agg_names': ['shop_id_sales_count', 'shop_id_sales_sum']}), ('item_id', {'group': ['item_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['item_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_revenue_sum']}), ('shop_group', {'group': ['shop_group'], 'stats': {'revenue': ['sum']}, 'agg_names': ['shop_group_revenue_sum']}), ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_category_id_sales_count', 'item_category_id_sales_sum', 'item_category_id_revenue_sum']}), ('item_group', {'group': ['item_group'], 'stats': {'sales': ['sum'], 'revenue': ['sum']}, 'agg_names': ['item_group_sales_sum', 'item_group_revenue_sum']}), ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_cluster_sales_count', 'item_cluster_sales_sum', 'item_cluster_revenue_sum']})])\n","    stats_set_feature_names: ['shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', 'shop_id_sales_sum', 'shop_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', 'shop_group_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', 'item_group_sales_sum', 'item_group_revenue_sum', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum', 'shop_id_x_item_category_id_revenue_sum']\n","  all_feature_names: ['month', 'shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster', 'shop_id_x_item_id_sales_sum_L1', 'shop_id_x_item_id_sales_median_L1', 'shop_id_x_item_id_sales_count_L1', 'shop_id_x_item_id_revenue_sum_L1', 'shop_id_x_item_category_id_sales_sum_L1', 'shop_id_x_item_category_id_sales_median_L1', 'shop_id_x_item_category_id_sales_count_L1', 'shop_id_x_item_cluster_sales_sum_L1', 'shop_id_x_item_cluster_sales_median_L1', 'shop_id_sales_sum_L1', 'shop_id_sales_count_L1', 'item_id_sales_sum_L1', 'item_id_sales_median_L1', 'item_id_sales_count_L1', 'item_id_revenue_sum_L1', 'shop_group_revenue_sum_L1', 'item_category_id_sales_sum_L1', 'item_category_id_sales_count_L1', 'item_category_id_revenue_sum_L1', 'item_group_sales_sum_L1', 'item_group_revenue_sum_L1', 'item_cluster_sales_sum_L1', 'item_cluster_sales_count_L1', 'item_cluster_revenue_sum_L1', 'shop_id_x_item_id_sales_sum_L2', 'shop_id_x_item_id_sales_count_L2', 'shop_id_x_item_id_revenue_sum_L2', 'shop_id_x_item_category_id_sales_count_L2', 'shop_id_x_item_category_id_revenue_sum_L2', 'shop_id_sales_sum_L2', 'item_id_sales_sum_L2', 'item_id_sales_count_L2', 'item_id_revenue_sum_L2', 'item_category_id_sales_sum_L2', 'item_category_id_sales_count_L2', 'item_cluster_sales_sum_L2', 'item_cluster_sales_count_L2', 'item_cluster_revenue_sum_L2', 'shop_id_x_item_id_sales_sum_L3', 'shop_id_sales_sum_L3', 'item_id_sales_sum_L3', 'item_id_sales_count_L3', 'item_id_revenue_sum_L3', 'item_category_id_sales_sum_L3', 'item_category_id_sales_count_L3', 'item_cluster_sales_count_L3', 'item_id_sales_sum_L4', 'shop_id_x_item_id_sales_sum_L5', 'shop_id_x_item_id_sales_sum_L6', 'item_id_sales_sum_L7', 'shop_id_x_item_id_sales_sum_L8']\n","  n_features: 58\n","\n","\n","Iteration Parameters Defined:               Multiprocessing Active Children = []\n","Memory Use: | 0.15 | 0.62 | 26.77 | 27.39 | GB:  pid, vm used / available / total.  Wed 03:36:15 09/09/20.\n","\n","Run Splits/Parameters Identified: Wed 03:36:15 09/09/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MY4tx3FwtVq7"},"source":["##**Mount Google Drive for access to Google Drive local repo; Load Data**"]},{"cell_type":"code","metadata":{"id":"CUIE1PVjSAmg","cellView":"both","executionInfo":{"status":"error","timestamp":1601460703117,"user_tz":240,"elapsed":25763,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"78df0296-abd4-4e50-e044-c043ec1ed588","colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, \n","#     then paste it into the input box and hit 'enter' to complete mounting of the drive\n","\n","drive.mount('/content/drive')\n","MEMORY_STATS.append(get_memory_stats(\"Google Drive mounted\", printout=True))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-1c0650df63d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mMEMORY_STATS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_memory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Google Drive mounted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'MEMORY_STATS' is not defined"]}]},{"cell_type":"code","metadata":{"id":"ICsyW1VCfKYS","executionInfo":{"status":"ok","timestamp":1601474192201,"user_tz":240,"elapsed":493,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"36b0d2c1-2c7c-4bac-8dbf-8b968d11ece4","colab":{"base_uri":"https://localhost:8080/","height":83}},"source":["# %cd '{GDRIVE_REPO_PATH}'\n","# os.chdir(OUT_OF_REPO_PATH)\n","print(GDRIVE_REPO_PATH.absolute())\n","print(GDRIVE_REPO_PATH.resolve())\n","p=Path('/content'+'/drive')\n","print(p)\n","platform.platform(terse=0)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","/content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic'"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"DuRQMg0OiDCc","executionInfo":{"status":"ok","timestamp":1601492041873,"user_tz":240,"elapsed":381,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"bf23dfb3-8c24-416e-9c1c-7043e424eccf","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["a = globals().copy()\n","import types\n","from pkg_resources import get_distribution\n","\n","print(f'\\n#################\\n#################\\n')\n","imports = OrderedDict()\n","vers = OrderedDict()\n","for k, vv in a.items():\n","    v1 = k.split('.')[0]\n","    v = eval(v1)\n","    if isinstance(v, types.ModuleType) or callable(v):\n","        imports[k] = OrderedDict()\n","        imports[k]['k'] = ('callable', type(v), v) if callable(v) else ('isinstance', type(v), v)\n","        try:\n","            v1 = v.__module__\n","            imports[k]['m'] = (type(v1), v1, '  len: ', len(v1))\n","            if len(v1) > 1:\n","                v = eval(v1.split('.')[0])\n","            else:\n","                imports[k]['m'] = 'Null __module__'\n","        except:\n","            imports[k]['m'] = '  no __module__'\n","        try:\n","            v1 = v.__package__\n","            if len(v1) > 1:\n","                imports[k]['p'] = (type(v1), v1)\n","                v = eval(v1.split('.')[0])\n","            else:\n","                imports[k]['p'] = 'Null __package__'\n","        except:\n","            imports[k]['p'] = '  no __package__'\n","        # try:\n","        #     v = eval(v)\n","        #     imports[k]['e'] = (type(v), v)\n","        # except:\n","        #     imports[k]['e'] = 0\n","        vers[v1] = OrderedDict()\n","        try:\n","            imports[k]['v'] = vers[v1]['v'] = v.__version__\n","        except:\n","            imports[k]['v'] = vers[v1]['v'] = ''\n","        try:\n","            imports[k]['g'] = vers[v1]['g'] = get_distribution(v)\n","        except:\n","            imports[k]['g'] = vers[v1]['g'] = ''\n","\n","        # try:\n","        #     ver = v.__package__.__version__  # get the root module\n","        # except:\n","        #     print('  2. bad pkg ver v for: ', k)\n","        #     try:\n","        #         ver = eval(v.__package__).__version__\n","        #     except:\n","        #         print('  3. bad eval pkg ver for:', k)\n","        #         try:\n","        #             ver = v.__version__\n","        #         except:\n","        #             ver = (f'4. no version info: {k} : {v}')\n","        # vers.append(ver)\n","        # try:\n","        #     vers.append(v.__version__)\n","        #     #print('vvvv  ',imports[-1],':',vers[-1])\n","        # except:\n","        #     print(k,'    version issue')\n","        #     try: \n","        #         vers.append((get_distribution(v),'viss'))\n","        #     except:\n","        #         vers.append('No distribution info')\n","    # if callable(v):\n","    #     print('iscallable',k)\n","    #     try:\n","    #         v = v.__package__  # get the root module\n","    #     except:\n","    #         print('  bad pkg eval v for: ', k)\n","    #     try:\n","    #         imports.append(v.__module__)\n","    #         # imports.append(inspect.getmodule(k).__name__)\n","    #     except:\n","    #         print('  cannot find name for callable: ',k,v)\n","    #     try:\n","    #         vers.append(v.__version__)\n","    #         #print('vvvv  ',imports[-1],':',vers[-1])\n","    #     except:\n","    #         print(k,'    callable version issue')\n","    #         try: \n","    #             vers.append((get_distribution(v),'c viss'))\n","    #         except:\n","    #             vers.append('No distribution info')\n","print('\\n\\n####################################################\\n################################\\n')\n","# for i in range(len(vers)):\n","#     print(imports[i],' : ',vers[i])\n","\n","for k,v in imports.items():\n","    print(k,':')\n","    try:\n","        for i,j in v.items():\n","            print(f'  {i:<8}: {j}')\n","    except:\n","        print(f'     {v}')\n","\n","\n","print(f'\\n#################\\n#################\\n')\n","\n","for k,v in vers.items():\n","    print(k,':')\n","    try:\n","        for i,j in v.items():\n","            print(f'  {i:<8}: {j}')\n","    except:\n","        print(f'     {v}')\n","\n","# # b = re.findall('<module [\\'\"](.+?)[\\'\"]', str(a))\n","# # c = re.findall('<class [\\'\"]([^_].+?)\\..+?[\\'\"]', str(a))\n","# # d = re.findall('<function (.+?)>', str(a))\n","# # print(a)\n","# # print(f'Modules:\\n{b}')\n","# # print(f'\\nClasses:\\n{c}')\n","# # print(f'\\nFunctions:\\n{d}')\n","# # print(get_distribution('pandas'))\n","# print('\\n\\n')\n","# s = {'a':'b'}\n","# print(type(s))\n","# print(type(a['__builtin__']))\n","# import types\n","# for k, v in a.items():\n","#     if callable(v): #isinstance(v, types.ModuleType) or isinstance(v, types.FunctionType) or isinstance(v, types.ClassType):\n","#         print(k,':',v)\n","# # for k,v in a.items():\n","# #     if type(v) == False:  # type(a['__builtin__']):  # type(s):\n","# #         print(k,':.....')\n","# #         for i,j in v.items():\n","# #             print('  ',i,':',j)\n","# #     else:\n","# #         print(k,':',v)\n","# print('\\n\\n####################################################\\n################################\\n')\n","# # z = re.findall('.{20}(final_mg_v1p0\\.ipynb).{20}', str(a))\n","# # print(z)\n","# # print('\\n\\n')\n","# # modulenames = set(sys.modules) & set(globals())\n","# # allmodules = [sys.modules[name] for name in modulenames]\n","# # print('allmodules:\\n',allmodules)\n","\n","# e = Path(sys.argv[2])\n","# print(sys.argv)\n","# # print(sys.argv[0], ':', e)\n","# f = e.read_text()\n","# print(f)\n","# # import_re = re.compile('(from (.*?) import|import (.*?)[\\#\\s])')\n","# # g = re.findall(import_re, f)\n","# # for i in g[:-2]:\n","# #     h = i[1] + i[2]\n","# #     try:\n","# #         print(h, get_distribution(h))\n","# #     except:\n","# #         print('   ',h)\n","# from pkg_resources import get_distribution\n","# import inspect\n","# print(inspect.getmodule(globals()['get_distribution']))\n","print('\\n\\n####################################################\\n################################\\n')\n","print('scikit-learn',get_distribution('scikit-learn'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","#################\n","#################\n","\n","\n","\n","####################################################\n","################################\n","\n","__builtin__ :\n","  k       : ('isinstance', <class 'module'>, <module 'builtins' (built-in)>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : \n","  g       : \n","__builtins__ :\n","  k       : ('isinstance', <class 'module'>, <module 'builtins' (built-in)>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : \n","  g       : \n","_sh :\n","  k       : ('isinstance', <class 'module'>, <module 'IPython.core.shadowns' from '/usr/local/lib/python3.6/dist-packages/IPython/core/shadowns.py'>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : \n","  g       : \n","get_ipython :\n","  k       : ('callable', <class 'method'>, <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7f6085a09160>>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : \n","  g       : \n","exit :\n","  k       : ('callable', <class 'IPython.core.autocall.ZMQExitAutocall'>, <IPython.core.autocall.ZMQExitAutocall object at 0x7f6085a236a0>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : \n","  g       : \n","quit :\n","  k       : ('callable', <class 'IPython.core.autocall.ZMQExitAutocall'>, <IPython.core.autocall.ZMQExitAutocall object at 0x7f6085a236a0>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : \n","  g       : \n","drive :\n","  k       : ('isinstance', <class 'module'>, <module 'google.colab.drive' from '/usr/local/lib/python3.6/dist-packages/google/colab/drive.py'>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : \n","  g       : \n","product :\n","  k       : ('callable', <class 'type'>, <class 'itertools.product'>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : \n","  g       : \n","OrderedDict :\n","  k       : ('callable', <class 'type'>, <class 'collections.OrderedDict'>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : \n","  g       : \n","pprint :\n","  k       : ('isinstance', <class 'module'>, <module 'pprint' from '/usr/lib/python3.6/pprint.py'>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : \n","  g       : \n","re :\n","  k       : ('isinstance', <class 'module'>, <module 're' from '/usr/lib/python3.6/re.py'>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : 2.2.1\n","  g       : \n","os :\n","  k       : ('isinstance', <class 'module'>, <module 'os' from '/usr/lib/python3.6/os.py'>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : \n","  g       : \n","sys :\n","  k       : ('isinstance', <class 'module'>, <module 'sys' (built-in)>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : \n","  g       : \n","Path :\n","  k       : ('callable', <class 'type'>, <class 'pathlib.Path'>)\n","  m       : (<class 'str'>, 'pathlib', '  len: ', 7)\n","  p       : Null __package__\n","  v       : \n","  g       : \n","platform :\n","  k       : ('isinstance', <class 'module'>, <module 'platform' from '/usr/lib/python3.6/platform.py'>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : 1.0.8\n","  g       : \n","pkg_resources :\n","  k       : ('isinstance', <class 'module'>, <module 'pkg_resources' from '/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py'>)\n","  m       :   no __module__\n","  p       : (<class 'str'>, 'pkg_resources')\n","  v       : \n","  g       : \n","psutil :\n","  k       : ('isinstance', <class 'module'>, <module 'psutil' from '/usr/local/lib/python3.6/dist-packages/psutil/__init__.py'>)\n","  m       :   no __module__\n","  p       : (<class 'str'>, 'psutil')\n","  v       : 5.4.8\n","  g       : \n","gc :\n","  k       : ('isinstance', <class 'module'>, <module 'gc' (built-in)>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : \n","  g       : \n","multiprocessing :\n","  k       : ('isinstance', <class 'module'>, <module 'multiprocessing' from '/usr/lib/python3.6/multiprocessing/__init__.py'>)\n","  m       :   no __module__\n","  p       : (<class 'str'>, 'multiprocessing')\n","  v       : \n","  g       : \n","ContextDecorator :\n","  k       : ('callable', <class 'type'>, <class 'contextlib.ContextDecorator'>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : \n","  g       : \n","time :\n","  k       : ('isinstance', <class 'module'>, <module 'time' (built-in)>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : \n","  g       : \n","strftime :\n","  k       : ('callable', <class 'builtin_function_or_method'>, <built-in function strftime>)\n","  m       : (<class 'str'>, 'time', '  len: ', 4)\n","  p       : Null __package__\n","  v       : \n","  g       : \n","tzset :\n","  k       : ('callable', <class 'builtin_function_or_method'>, <built-in function tzset>)\n","  m       : (<class 'str'>, 'time', '  len: ', 4)\n","  p       : Null __package__\n","  v       : \n","  g       : \n","perf_counter :\n","  k       : ('callable', <class 'builtin_function_or_method'>, <built-in function perf_counter>)\n","  m       : (<class 'str'>, 'time', '  len: ', 4)\n","  p       : Null __package__\n","  v       : \n","  g       : \n","default_timer :\n","  k       : ('callable', <class 'builtin_function_or_method'>, <built-in function perf_counter>)\n","  m       : (<class 'str'>, 'time', '  len: ', 4)\n","  p       : Null __package__\n","  v       : \n","  g       : \n","datetime :\n","  k       : ('callable', <class 'type'>, <class 'datetime.datetime'>)\n","  m       : (<class 'str'>, 'datetime', '  len: ', 8)\n","  p       :   no __package__\n","  v       : \n","  g       : \n","pd :\n","  k       : ('isinstance', <class 'module'>, <module 'pandas' from '/usr/local/lib/python3.6/dist-packages/pandas/__init__.py'>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : 1.1.2\n","  g       : \n","plt :\n","  k       : ('isinstance', <class 'module'>, <module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : \n","  g       : \n","np :\n","  k       : ('isinstance', <class 'module'>, <module 'numpy' from '/usr/local/lib/python3.6/dist-packages/numpy/__init__.py'>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : 1.18.5\n","  g       : \n","lgb :\n","  k       : ('isinstance', <class 'module'>, <module 'lightgbm' from '/usr/local/lib/python3.6/dist-packages/lightgbm/__init__.py'>)\n","  m       :   no __module__\n","  p       :   no __package__\n","  v       : 2.2.3\n","  g       : \n","sklearn :\n","  k       : ('isinstance', <class 'module'>, <module 'sklearn' from '/usr/local/lib/python3.6/dist-packages/sklearn/__init__.py'>)\n","  m       :   no __module__\n","  p       : (<class 'str'>, 'sklearn')\n","  v       : 0.22.2.post1\n","  g       : \n","MinMaxScaler :\n","  k       : ('callable', <class 'type'>, <class 'sklearn.preprocessing._data.MinMaxScaler'>)\n","  m       : (<class 'str'>, 'sklearn.preprocessing._data', '  len: ', 27)\n","  p       : (<class 'str'>, 'sklearn')\n","  v       : 0.22.2.post1\n","  g       : \n","RobustScaler :\n","  k       : ('callable', <class 'type'>, <class 'sklearn.preprocessing._data.RobustScaler'>)\n","  m       : (<class 'str'>, 'sklearn.preprocessing._data', '  len: ', 27)\n","  p       : (<class 'str'>, 'sklearn')\n","  v       : 0.22.2.post1\n","  g       : \n","enable_hist_gradient_boosting :\n","  k       : ('isinstance', <class 'module'>, <module 'sklearn.experimental.enable_hist_gradient_boosting' from '/usr/local/lib/python3.6/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py'>)\n","  m       :   no __module__\n","  p       : (<class 'str'>, 'sklearn.experimental')\n","  v       : 0.22.2.post1\n","  g       : \n","HistGradientBoostingRegressor :\n","  k       : ('callable', <class 'abc.ABCMeta'>, <class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>)\n","  m       : (<class 'str'>, 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting', '  len: ', 58)\n","  p       : (<class 'str'>, 'sklearn')\n","  v       : 0.22.2.post1\n","  g       : \n","mean_squared_error :\n","  k       : ('callable', <class 'function'>, <function mean_squared_error at 0x7f6076463840>)\n","  m       : (<class 'str'>, 'sklearn.metrics._regression', '  len: ', 27)\n","  p       : (<class 'str'>, 'sklearn')\n","  v       : 0.22.2.post1\n","  g       : \n","r2_score :\n","  k       : ('callable', <class 'function'>, <function r2_score at 0x7f6076463a60>)\n","  m       : (<class 'str'>, 'sklearn.metrics._regression', '  len: ', 27)\n","  p       : (<class 'str'>, 'sklearn')\n","  v       : 0.22.2.post1\n","  g       : \n","_16 :\n","  k       : ('callable', <class 'function'>, <function cpu_freq at 0x7f6081426510>)\n","  m       : (<class 'str'>, 'psutil', '  len: ', 6)\n","  p       : (<class 'str'>, 'psutil')\n","  v       : 5.4.8\n","  g       : \n","_41 :\n","  k       : ('callable', <class 'type'>, <class 'platform.uname_result'>)\n","  m       : (<class 'str'>, 'platform', '  len: ', 8)\n","  p       : Null __package__\n","  v       : 1.0.8\n","  g       : \n","inspect :\n","  k       : ('isinstance', <class 'module'>, <module 'inspect' from '/usr/lib/python3.6/inspect.py'>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : \n","  g       : \n","get_distribution :\n","  k       : ('callable', <class 'function'>, <function get_distribution at 0x7f609d4cfc80>)\n","  m       : (<class 'str'>, 'pkg_resources', '  len: ', 13)\n","  p       : (<class 'str'>, 'pkg_resources')\n","  v       : \n","  g       : \n","types :\n","  k       : ('isinstance', <class 'module'>, <module 'types' from '/usr/lib/python3.6/types.py'>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : \n","  g       : \n","importlib :\n","  k       : ('isinstance', <class 'module'>, <module 'importlib' from '/usr/lib/python3.6/importlib/__init__.py'>)\n","  m       :   no __module__\n","  p       : (<class 'str'>, 'importlib')\n","  v       : \n","  g       : \n","pathlib :\n","  k       : ('isinstance', <class 'module'>, <module 'pathlib' from '/usr/lib/python3.6/pathlib.py'>)\n","  m       :   no __module__\n","  p       : Null __package__\n","  v       : \n","  g       : \n","\n","#################\n","#################\n","\n"," :\n","  v       : \n","  g       : \n","IPython.core :\n","  v       : \n","  g       : \n","IPython.core.interactiveshell :\n","  v       : \n","  g       : \n","IPython.core.autocall :\n","  v       : \n","  g       : \n","google.colab :\n","  v       : \n","  g       : \n","itertools :\n","  v       : \n","  g       : \n","collections :\n","  v       : \n","  g       : \n","pkg_resources :\n","  v       : \n","  g       : \n","psutil :\n","  v       : 5.4.8\n","  g       : \n","multiprocessing :\n","  v       : \n","  g       : \n","contextlib :\n","  v       : \n","  g       : \n","datetime :\n","  v       : \n","  g       : \n","pandas :\n","  v       : 1.1.2\n","  g       : \n","matplotlib :\n","  v       : \n","  g       : \n","numpy :\n","  v       : 1.18.5\n","  g       : \n","lightgbm :\n","  v       : 2.2.3\n","  g       : \n","sklearn :\n","  v       : 0.22.2.post1\n","  g       : \n","sklearn.experimental :\n","  v       : 0.22.2.post1\n","  g       : \n","importlib :\n","  v       : \n","  g       : \n","\n","\n","####################################################\n","################################\n","\n","scikit-learn scikit-learn 0.22.2.post1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6lhtmlb5UBt9","executionInfo":{"status":"error","timestamp":1601492341229,"user_tz":240,"elapsed":319,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"c5fad438-db3e-4bf3-c535-214691fcaeb8","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","import pathlib\n","print(get_distribution(plt.__package__))\n","print(f'1{pathlib.__package__}1')\n","for k, v in pathlib.__dict__.items():\n","    print(k,\":\",v)\n","print('\\n###########\\n')\n","i = 0\n","for k, v in plt.__dict__.items():\n","    if i<100:\n","        print(k,\":\",v)\n","    i+=1\n","print('\\n###########\\n')\n","for k, v in pkg_resources.__dict__.items():\n","    print(k,\":\",v)\n","print('\\n###########\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["matplotlib 3.2.2\n","11\n","__name__ : pathlib\n","__doc__ : None\n","__package__ : \n","__loader__ : <_frozen_importlib_external.SourceFileLoader object at 0x7f609e19f588>\n","__spec__ : ModuleSpec(name='pathlib', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f609e19f588>, origin='/usr/lib/python3.6/pathlib.py')\n","__file__ : /usr/lib/python3.6/pathlib.py\n","__cached__ : /usr/lib/python3.6/__pycache__/pathlib.cpython-36.pyc\n","__builtins__ : {'__name__': 'builtins', '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x7f6085a09048>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'Exception': <class 'Exception'>, 'TypeError': <class 'TypeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'GeneratorExit': <class 'GeneratorExit'>, 'SystemExit': <class 'SystemExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'ImportError': <class 'ImportError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'OSError': <class 'OSError'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'EOFError': <class 'EOFError'>, 'RuntimeError': <class 'RuntimeError'>, 'RecursionError': <class 'RecursionError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'NameError': <class 'NameError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'AttributeError': <class 'AttributeError'>, 'SyntaxError': <class 'SyntaxError'>, 'IndentationError': <class 'IndentationError'>, 'TabError': <class 'TabError'>, 'LookupError': <class 'LookupError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ValueError': <class 'ValueError'>, 'UnicodeError': <class 'UnicodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'AssertionError': <class 'AssertionError'>, 'ArithmeticError': <class 'ArithmeticError'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'SystemError': <class 'SystemError'>, 'ReferenceError': <class 'ReferenceError'>, 'BufferError': <class 'BufferError'>, 'MemoryError': <class 'MemoryError'>, 'Warning': <class 'Warning'>, 'UserWarning': <class 'UserWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'BytesWarning': <class 'BytesWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'ConnectionError': <class 'ConnectionError'>, 'BlockingIOError': <class 'BlockingIOError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'InterruptedError': <class 'InterruptedError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2019 Python Software Foundation.\n","All Rights Reserved.\n","\n","Copyright (c) 2000 BeOpen.com.\n","All Rights Reserved.\n","\n","Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n","All Rights Reserved.\n","\n","Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n","All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n","    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., '__IPYTHON__': True, 'display': <function display at 0x7f609e18ed90>, '__pybind11_internals_v3_gcc_libstdcpp_cxxabi1002__': <capsule object NULL at 0x7f6080fb0c30>, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7f6085a09160>>, 'dreload': <function _dreload at 0x7f60841b22f0>}\n","fnmatch : <module 'fnmatch' from '/usr/lib/python3.6/fnmatch.py'>\n","functools : <module 'functools' from '/usr/lib/python3.6/functools.py'>\n","io : <module 'io' from '/usr/lib/python3.6/io.py'>\n","ntpath : <module 'ntpath' from '/usr/lib/python3.6/ntpath.py'>\n","os : <module 'os' from '/usr/lib/python3.6/os.py'>\n","posixpath : <module 'posixpath' from '/usr/lib/python3.6/posixpath.py'>\n","re : <module 're' from '/usr/lib/python3.6/re.py'>\n","sys : <module 'sys' (built-in)>\n","Sequence : <class 'collections.abc.Sequence'>\n","contextmanager : <function contextmanager at 0x7f609fb16730>\n","EINVAL : 22\n","ENOENT : 2\n","ENOTDIR : 20\n","attrgetter : <class 'operator.attrgetter'>\n","S_ISDIR : <built-in function S_ISDIR>\n","S_ISLNK : <built-in function S_ISLNK>\n","S_ISREG : <built-in function S_ISREG>\n","S_ISSOCK : <built-in function S_ISSOCK>\n","S_ISBLK : <built-in function S_ISBLK>\n","S_ISCHR : <built-in function S_ISCHR>\n","S_ISFIFO : <built-in function S_ISFIFO>\n","urlquote_from_bytes : <function quote_from_bytes at 0x7f609e6d29d8>\n","supports_symlinks : True\n","nt : None\n","__all__ : ['PurePath', 'PurePosixPath', 'PureWindowsPath', 'Path', 'PosixPath', 'WindowsPath']\n","_is_wildcard_pattern : <function _is_wildcard_pattern at 0x7f609e1be0d0>\n","_Flavour : <class 'pathlib._Flavour'>\n","_WindowsFlavour : <class 'pathlib._WindowsFlavour'>\n","_PosixFlavour : <class 'pathlib._PosixFlavour'>\n","_windows_flavour : <pathlib._WindowsFlavour object at 0x7f609e1c5390>\n","_posix_flavour : <pathlib._PosixFlavour object at 0x7f609e1c53c8>\n","_Accessor : <class 'pathlib._Accessor'>\n","_NormalAccessor : <class 'pathlib._NormalAccessor'>\n","_normal_accessor : <pathlib._NormalAccessor object at 0x7f609e1c5eb8>\n","_make_selector : <functools._lru_cache_wrapper object at 0x7f609e1cc240>\n","_Selector : <class 'pathlib._Selector'>\n","_TerminatingSelector : <class 'pathlib._TerminatingSelector'>\n","_PreciseSelector : <class 'pathlib._PreciseSelector'>\n","_WildcardSelector : <class 'pathlib._WildcardSelector'>\n","_RecursiveWildcardSelector : <class 'pathlib._RecursiveWildcardSelector'>\n","_PathParents : <class 'pathlib._PathParents'>\n","PurePath : <class 'pathlib.PurePath'>\n","PurePosixPath : <class 'pathlib.PurePosixPath'>\n","PureWindowsPath : <class 'pathlib.PureWindowsPath'>\n","Path : <class 'pathlib.Path'>\n","PosixPath : <class 'pathlib.PosixPath'>\n","WindowsPath : <class 'pathlib.WindowsPath'>\n","\n","###########\n","\n","__name__ : matplotlib.pyplot\n","__doc__ : \n","`matplotlib.pyplot` is a state-based interface to matplotlib. It provides\n","a MATLAB-like way of plotting.\n","\n","pyplot is mainly intended for interactive plots and simple cases of\n","programmatic plot generation::\n","\n","    import numpy as np\n","    import matplotlib.pyplot as plt\n","\n","    x = np.arange(0, 5, 0.1)\n","    y = np.sin(x)\n","    plt.plot(x, y)\n","\n","The object-oriented API is recommended for more complex plots.\n","\n","__package__ : matplotlib\n","__loader__ : <_frozen_importlib_external.SourceFileLoader object at 0x7f6083f7a518>\n","__spec__ : ModuleSpec(name='matplotlib.pyplot', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f6083f7a518>, origin='/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py')\n","__file__ : /usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\n","__cached__ : /usr/local/lib/python3.6/dist-packages/matplotlib/__pycache__/pyplot.cpython-36.pyc\n","__builtins__ : {'__name__': 'builtins', '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x7f6085a09048>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'Exception': <class 'Exception'>, 'TypeError': <class 'TypeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'GeneratorExit': <class 'GeneratorExit'>, 'SystemExit': <class 'SystemExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'ImportError': <class 'ImportError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'OSError': <class 'OSError'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'EOFError': <class 'EOFError'>, 'RuntimeError': <class 'RuntimeError'>, 'RecursionError': <class 'RecursionError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'NameError': <class 'NameError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'AttributeError': <class 'AttributeError'>, 'SyntaxError': <class 'SyntaxError'>, 'IndentationError': <class 'IndentationError'>, 'TabError': <class 'TabError'>, 'LookupError': <class 'LookupError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ValueError': <class 'ValueError'>, 'UnicodeError': <class 'UnicodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'AssertionError': <class 'AssertionError'>, 'ArithmeticError': <class 'ArithmeticError'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'SystemError': <class 'SystemError'>, 'ReferenceError': <class 'ReferenceError'>, 'BufferError': <class 'BufferError'>, 'MemoryError': <class 'MemoryError'>, 'Warning': <class 'Warning'>, 'UserWarning': <class 'UserWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'BytesWarning': <class 'BytesWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'ConnectionError': <class 'ConnectionError'>, 'BlockingIOError': <class 'BlockingIOError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'InterruptedError': <class 'InterruptedError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2019 Python Software Foundation.\n","All Rights Reserved.\n","\n","Copyright (c) 2000 BeOpen.com.\n","All Rights Reserved.\n","\n","Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n","All Rights Reserved.\n","\n","Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n","All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n","    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., '__IPYTHON__': True, 'display': <function display at 0x7f609e18ed90>, '__pybind11_internals_v3_gcc_libstdcpp_cxxabi1002__': <capsule object NULL at 0x7f6080fb0c30>, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7f6085a09160>>, 'dreload': <function _dreload at 0x7f60841b22f0>}\n","functools : <module 'functools' from '/usr/lib/python3.6/functools.py'>\n","importlib : <module 'importlib' from '/usr/lib/python3.6/importlib/__init__.py'>\n","inspect : <module 'inspect' from '/usr/lib/python3.6/inspect.py'>\n","logging : <module 'logging' from '/usr/lib/python3.6/logging/__init__.py'>\n","Number : <class 'numbers.Number'>\n","re : <module 're' from '/usr/lib/python3.6/re.py'>\n","sys : <module 'sys' (built-in)>\n","time : <module 'time' (built-in)>\n","cycler : <function cycler at 0x7f6083fd9158>\n","matplotlib : <module 'matplotlib' from '/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py'>\n","rcsetup : <module 'matplotlib.rcsetup' from '/usr/local/lib/python3.6/dist-packages/matplotlib/rcsetup.py'>\n","style : <module 'matplotlib.style' from '/usr/local/lib/python3.6/dist-packages/matplotlib/style/__init__.py'>\n","_pylab_helpers : <module 'matplotlib._pylab_helpers' from '/usr/local/lib/python3.6/dist-packages/matplotlib/_pylab_helpers.py'>\n","interactive : <function interactive at 0x7f6083f7eae8>\n","cbook : <module 'matplotlib.cbook' from '/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py'>\n","dedent : <function dedent at 0x7f6084198d08>\n","deprecated : <function deprecated at 0x7f6084189598>\n","silent_list : <class 'matplotlib.cbook.silent_list'>\n","warn_deprecated : <function warn_deprecated at 0x7f6084189510>\n","docstring : <module 'matplotlib.docstring' from '/usr/local/lib/python3.6/dist-packages/matplotlib/docstring.py'>\n","FigureCanvasBase : <class 'matplotlib.backend_bases.FigureCanvasBase'>\n","Figure : <class 'matplotlib.figure.Figure'>\n","figaspect : <function figaspect at 0x7f6082e9f268>\n","GridSpec : <class 'matplotlib.gridspec.GridSpec'>\n","rcParams : _internal.classic_mode: False\n","agg.path.chunksize: 0\n","animation.avconv_args: []\n","animation.avconv_path: avconv\n","animation.bitrate: -1\n","animation.codec: h264\n","animation.convert_args: []\n","animation.convert_path: convert\n","animation.embed_limit: 20.0\n","animation.ffmpeg_args: []\n","animation.ffmpeg_path: ffmpeg\n","animation.frame_format: png\n","animation.html: none\n","animation.html_args: []\n","animation.writer: ffmpeg\n","axes.autolimit_mode: data\n","axes.axisbelow: line\n","axes.edgecolor: black\n","axes.facecolor: white\n","axes.formatter.limits: [-5, 6]\n","axes.formatter.min_exponent: 0\n","axes.formatter.offset_threshold: 4\n","axes.formatter.use_locale: False\n","axes.formatter.use_mathtext: False\n","axes.formatter.useoffset: True\n","axes.grid: False\n","axes.grid.axis: both\n","axes.grid.which: major\n","axes.labelcolor: black\n","axes.labelpad: 4.0\n","axes.labelsize: medium\n","axes.labelweight: normal\n","axes.linewidth: 0.8\n","axes.prop_cycle: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n","axes.spines.bottom: True\n","axes.spines.left: True\n","axes.spines.right: True\n","axes.spines.top: True\n","axes.titlecolor: auto\n","axes.titlelocation: center\n","axes.titlepad: 6.0\n","axes.titlesize: large\n","axes.titleweight: normal\n","axes.unicode_minus: True\n","axes.xmargin: 0.05\n","axes.ymargin: 0.05\n","axes3d.grid: True\n","backend: module://ipykernel.pylab.backend_inline\n","backend_fallback: True\n","boxplot.bootstrap: None\n","boxplot.boxprops.color: black\n","boxplot.boxprops.linestyle: -\n","boxplot.boxprops.linewidth: 1.0\n","boxplot.capprops.color: black\n","boxplot.capprops.linestyle: -\n","boxplot.capprops.linewidth: 1.0\n","boxplot.flierprops.color: black\n","boxplot.flierprops.linestyle: none\n","boxplot.flierprops.linewidth: 1.0\n","boxplot.flierprops.marker: o\n","boxplot.flierprops.markeredgecolor: black\n","boxplot.flierprops.markeredgewidth: 1.0\n","boxplot.flierprops.markerfacecolor: none\n","boxplot.flierprops.markersize: 6.0\n","boxplot.meanline: False\n","boxplot.meanprops.color: C2\n","boxplot.meanprops.linestyle: --\n","boxplot.meanprops.linewidth: 1.0\n","boxplot.meanprops.marker: ^\n","boxplot.meanprops.markeredgecolor: C2\n","boxplot.meanprops.markerfacecolor: C2\n","boxplot.meanprops.markersize: 6.0\n","boxplot.medianprops.color: C1\n","boxplot.medianprops.linestyle: -\n","boxplot.medianprops.linewidth: 1.0\n","boxplot.notch: False\n","boxplot.patchartist: False\n","boxplot.showbox: True\n","boxplot.showcaps: True\n","boxplot.showfliers: True\n","boxplot.showmeans: False\n","boxplot.vertical: True\n","boxplot.whiskerprops.color: black\n","boxplot.whiskerprops.linestyle: -\n","boxplot.whiskerprops.linewidth: 1.0\n","boxplot.whiskers: 1.5\n","contour.corner_mask: True\n","contour.negative_linestyle: dashed\n","datapath: /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data\n","date.autoformatter.day: %Y-%m-%d\n","date.autoformatter.hour: %m-%d %H\n","date.autoformatter.microsecond: %M:%S.%f\n","date.autoformatter.minute: %d %H:%M\n","date.autoformatter.month: %Y-%m\n","date.autoformatter.second: %H:%M:%S\n","date.autoformatter.year: %Y\n","docstring.hardcopy: False\n","errorbar.capsize: 0.0\n","figure.autolayout: False\n","figure.constrained_layout.h_pad: 0.04167\n","figure.constrained_layout.hspace: 0.02\n","figure.constrained_layout.use: False\n","figure.constrained_layout.w_pad: 0.04167\n","figure.constrained_layout.wspace: 0.02\n","figure.dpi: 72.0\n","figure.edgecolor: (1, 1, 1, 0)\n","figure.facecolor: (1, 1, 1, 0)\n","figure.figsize: [6.0, 4.0]\n","figure.frameon: True\n","figure.max_open_warning: 20\n","figure.subplot.bottom: 0.125\n","figure.subplot.hspace: 0.2\n","figure.subplot.left: 0.125\n","figure.subplot.right: 0.9\n","figure.subplot.top: 0.88\n","figure.subplot.wspace: 0.2\n","figure.titlesize: large\n","figure.titleweight: normal\n","font.cursive: ['Apple Chancery', 'Textile', 'Zapf Chancery', 'Sand', 'Script MT', 'Felipa', 'cursive']\n","font.family: ['sans-serif']\n","font.fantasy: ['Comic Neue', 'Comic Sans MS', 'Chicago', 'Charcoal', 'Impact', 'Western', 'Humor Sans', 'xkcd', 'fantasy']\n","font.monospace: ['DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Computer Modern Typewriter', 'Andale Mono', 'Nimbus Mono L', 'Courier New', 'Courier', 'Fixed', 'Terminal', 'monospace']\n","font.sans-serif: ['DejaVu Sans', 'Bitstream Vera Sans', 'Computer Modern Sans Serif', 'Lucida Grande', 'Verdana', 'Geneva', 'Lucid', 'Arial', 'Helvetica', 'Avant Garde', 'sans-serif']\n","font.serif: ['DejaVu Serif', 'Bitstream Vera Serif', 'Computer Modern Roman', 'New Century Schoolbook', 'Century Schoolbook L', 'Utopia', 'ITC Bookman', 'Bookman', 'Nimbus Roman No9 L', 'Times New Roman', 'Times', 'Palatino', 'Charter', 'serif']\n","font.size: 10.0\n","font.stretch: normal\n","font.style: normal\n","font.variant: normal\n","font.weight: normal\n","grid.alpha: 1.0\n","grid.color: #b0b0b0\n","grid.linestyle: -\n","grid.linewidth: 0.8\n","hatch.color: black\n","hatch.linewidth: 1.0\n","hist.bins: 10\n","image.aspect: equal\n","image.cmap: viridis\n","image.composite_image: True\n","image.interpolation: antialiased\n","image.lut: 256\n","image.origin: upper\n","image.resample: True\n","interactive: True\n","keymap.all_axes: ['a']\n","keymap.back: ['left', 'c', 'backspace', 'MouseButton.BACK']\n","keymap.copy: ['ctrl+c', 'cmd+c']\n","keymap.forward: ['right', 'v', 'MouseButton.FORWARD']\n","keymap.fullscreen: ['f', 'ctrl+f']\n","keymap.grid: ['g']\n","keymap.grid_minor: ['G']\n","keymap.help: ['f1']\n","keymap.home: ['h', 'r', 'home']\n","keymap.pan: ['p']\n","keymap.quit: ['ctrl+w', 'cmd+w', 'q']\n","keymap.quit_all: ['W', 'cmd+W', 'Q']\n","keymap.save: ['s', 'ctrl+s']\n","keymap.xscale: ['k', 'L']\n","keymap.yscale: ['l']\n","keymap.zoom: ['o']\n","legend.borderaxespad: 0.5\n","legend.borderpad: 0.4\n","legend.columnspacing: 2.0\n","legend.edgecolor: 0.8\n","legend.facecolor: inherit\n","legend.fancybox: True\n","legend.fontsize: medium\n","legend.framealpha: 0.8\n","legend.frameon: True\n","legend.handleheight: 0.7\n","legend.handlelength: 2.0\n","legend.handletextpad: 0.8\n","legend.labelspacing: 0.5\n","legend.loc: best\n","legend.markerscale: 1.0\n","legend.numpoints: 1\n","legend.scatterpoints: 1\n","legend.shadow: False\n","legend.title_fontsize: None\n","lines.antialiased: True\n","lines.color: C0\n","lines.dash_capstyle: butt\n","lines.dash_joinstyle: round\n","lines.dashdot_pattern: [6.4, 1.6, 1.0, 1.6]\n","lines.dashed_pattern: [3.7, 1.6]\n","lines.dotted_pattern: [1.0, 1.65]\n","lines.linestyle: -\n","lines.linewidth: 1.5\n","lines.marker: None\n","lines.markeredgecolor: auto\n","lines.markeredgewidth: 1.0\n","lines.markerfacecolor: auto\n","lines.markersize: 6.0\n","lines.scale_dashes: True\n","lines.solid_capstyle: projecting\n","lines.solid_joinstyle: round\n","markers.fillstyle: full\n","mathtext.bf: sans:bold\n","mathtext.cal: cursive\n","mathtext.default: it\n","mathtext.fallback_to_cm: True\n","mathtext.fontset: dejavusans\n","mathtext.it: sans:italic\n","mathtext.rm: sans\n","mathtext.sf: sans\n","mathtext.tt: monospace\n","mpl_toolkits.legacy_colorbar: True\n","patch.antialiased: True\n","patch.edgecolor: black\n","patch.facecolor: C0\n","patch.force_edgecolor: False\n","patch.linewidth: 1.0\n","path.effects: []\n","path.simplify: True\n","path.simplify_threshold: 0.1111111111111111\n","path.sketch: None\n","path.snap: True\n","pdf.compression: 6\n","pdf.fonttype: 3\n","pdf.inheritcolor: False\n","pdf.use14corefonts: False\n","pgf.preamble: \n","pgf.rcfonts: True\n","pgf.texsystem: xelatex\n","polaraxes.grid: True\n","ps.distiller.res: 6000\n","ps.fonttype: 3\n","ps.papersize: letter\n","ps.useafm: False\n","ps.usedistiller: None\n","savefig.bbox: None\n","savefig.directory: ~\n","savefig.dpi: figure\n","savefig.edgecolor: white\n","savefig.facecolor: white\n","savefig.format: png\n","savefig.frameon: True\n","savefig.jpeg_quality: 95\n","savefig.orientation: portrait\n","savefig.pad_inches: 0.1\n","savefig.transparent: False\n","scatter.edgecolors: face\n","scatter.marker: o\n","svg.fonttype: path\n","svg.hashsalt: None\n","svg.image_inline: True\n","text.antialiased: True\n","text.color: black\n","text.hinting: auto\n","text.hinting_factor: 8\n","text.kerning_factor: 0\n","text.latex.preamble: \n","text.latex.preview: False\n","text.latex.unicode: True\n","text.usetex: False\n","timezone: UTC\n","tk.window_focus: False\n","toolbar: toolbar2\n","verbose.fileo: sys.stdout\n","verbose.level: silent\n","webagg.address: 127.0.0.1\n","webagg.open_in_browser: True\n","webagg.port: 8988\n","webagg.port_retries: 50\n","xtick.alignment: center\n","xtick.bottom: True\n","xtick.color: black\n","xtick.direction: out\n","xtick.labelbottom: True\n","xtick.labelsize: medium\n","xtick.labeltop: False\n","xtick.major.bottom: True\n","xtick.major.pad: 3.5\n","xtick.major.size: 3.5\n","xtick.major.top: True\n","xtick.major.width: 0.8\n","xtick.minor.bottom: True\n","xtick.minor.pad: 3.4\n","xtick.minor.size: 2.0\n","xtick.minor.top: True\n","xtick.minor.visible: False\n","xtick.minor.width: 0.6\n","xtick.top: False\n","ytick.alignment: center_baseline\n","ytick.color: black\n","ytick.direction: out\n","ytick.labelleft: True\n","ytick.labelright: False\n","ytick.labelsize: medium\n","ytick.left: True\n","ytick.major.left: True\n","ytick.major.pad: 3.5\n","ytick.major.right: True\n","ytick.major.size: 3.5\n","ytick.major.width: 0.8\n","ytick.minor.left: True\n","ytick.minor.pad: 3.4\n","ytick.minor.right: True\n","ytick.minor.size: 2.0\n","ytick.minor.visible: False\n","ytick.minor.width: 0.6\n","ytick.right: False\n","rcParamsDefault : _internal.classic_mode: False\n","agg.path.chunksize: 0\n","animation.avconv_args: []\n","animation.avconv_path: avconv\n","animation.bitrate: -1\n","animation.codec: h264\n","animation.convert_args: []\n","animation.convert_path: convert\n","animation.embed_limit: 20.0\n","animation.ffmpeg_args: []\n","animation.ffmpeg_path: ffmpeg\n","animation.frame_format: png\n","animation.html: none\n","animation.html_args: []\n","animation.writer: ffmpeg\n","axes.autolimit_mode: data\n","axes.axisbelow: line\n","axes.edgecolor: black\n","axes.facecolor: white\n","axes.formatter.limits: [-5, 6]\n","axes.formatter.min_exponent: 0\n","axes.formatter.offset_threshold: 4\n","axes.formatter.use_locale: False\n","axes.formatter.use_mathtext: False\n","axes.formatter.useoffset: True\n","axes.grid: False\n","axes.grid.axis: both\n","axes.grid.which: major\n","axes.labelcolor: black\n","axes.labelpad: 4.0\n","axes.labelsize: medium\n","axes.labelweight: normal\n","axes.linewidth: 0.8\n","axes.prop_cycle: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n","axes.spines.bottom: True\n","axes.spines.left: True\n","axes.spines.right: True\n","axes.spines.top: True\n","axes.titlecolor: auto\n","axes.titlelocation: center\n","axes.titlepad: 6.0\n","axes.titlesize: large\n","axes.titleweight: normal\n","axes.unicode_minus: True\n","axes.xmargin: 0.05\n","axes.ymargin: 0.05\n","axes3d.grid: True\n","backend: module://ipykernel.pylab.backend_inline\n","backend_fallback: True\n","boxplot.bootstrap: None\n","boxplot.boxprops.color: black\n","boxplot.boxprops.linestyle: -\n","boxplot.boxprops.linewidth: 1.0\n","boxplot.capprops.color: black\n","boxplot.capprops.linestyle: -\n","boxplot.capprops.linewidth: 1.0\n","boxplot.flierprops.color: black\n","boxplot.flierprops.linestyle: none\n","boxplot.flierprops.linewidth: 1.0\n","boxplot.flierprops.marker: o\n","boxplot.flierprops.markeredgecolor: black\n","boxplot.flierprops.markeredgewidth: 1.0\n","boxplot.flierprops.markerfacecolor: none\n","boxplot.flierprops.markersize: 6.0\n","boxplot.meanline: False\n","boxplot.meanprops.color: C2\n","boxplot.meanprops.linestyle: --\n","boxplot.meanprops.linewidth: 1.0\n","boxplot.meanprops.marker: ^\n","boxplot.meanprops.markeredgecolor: C2\n","boxplot.meanprops.markerfacecolor: C2\n","boxplot.meanprops.markersize: 6.0\n","boxplot.medianprops.color: C1\n","boxplot.medianprops.linestyle: -\n","boxplot.medianprops.linewidth: 1.0\n","boxplot.notch: False\n","boxplot.patchartist: False\n","boxplot.showbox: True\n","boxplot.showcaps: True\n","boxplot.showfliers: True\n","boxplot.showmeans: False\n","boxplot.vertical: True\n","boxplot.whiskerprops.color: black\n","boxplot.whiskerprops.linestyle: -\n","boxplot.whiskerprops.linewidth: 1.0\n","boxplot.whiskers: 1.5\n","contour.corner_mask: True\n","contour.negative_linestyle: dashed\n","datapath: /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data\n","date.autoformatter.day: %Y-%m-%d\n","date.autoformatter.hour: %m-%d %H\n","date.autoformatter.microsecond: %M:%S.%f\n","date.autoformatter.minute: %d %H:%M\n","date.autoformatter.month: %Y-%m\n","date.autoformatter.second: %H:%M:%S\n","date.autoformatter.year: %Y\n","docstring.hardcopy: False\n","errorbar.capsize: 0.0\n","figure.autolayout: False\n","figure.constrained_layout.h_pad: 0.04167\n","figure.constrained_layout.hspace: 0.02\n","figure.constrained_layout.use: False\n","figure.constrained_layout.w_pad: 0.04167\n","figure.constrained_layout.wspace: 0.02\n","figure.dpi: 100.0\n","figure.edgecolor: white\n","figure.facecolor: white\n","figure.figsize: [6.4, 4.8]\n","figure.frameon: True\n","figure.max_open_warning: 20\n","figure.subplot.bottom: 0.11\n","figure.subplot.hspace: 0.2\n","figure.subplot.left: 0.125\n","figure.subplot.right: 0.9\n","figure.subplot.top: 0.88\n","figure.subplot.wspace: 0.2\n","figure.titlesize: large\n","figure.titleweight: normal\n","font.cursive: ['Apple Chancery', 'Textile', 'Zapf Chancery', 'Sand', 'Script MT', 'Felipa', 'cursive']\n","font.family: ['sans-serif']\n","font.fantasy: ['Comic Neue', 'Comic Sans MS', 'Chicago', 'Charcoal', 'Impact', 'Western', 'Humor Sans', 'xkcd', 'fantasy']\n","font.monospace: ['DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Computer Modern Typewriter', 'Andale Mono', 'Nimbus Mono L', 'Courier New', 'Courier', 'Fixed', 'Terminal', 'monospace']\n","font.sans-serif: ['DejaVu Sans', 'Bitstream Vera Sans', 'Computer Modern Sans Serif', 'Lucida Grande', 'Verdana', 'Geneva', 'Lucid', 'Arial', 'Helvetica', 'Avant Garde', 'sans-serif']\n","font.serif: ['DejaVu Serif', 'Bitstream Vera Serif', 'Computer Modern Roman', 'New Century Schoolbook', 'Century Schoolbook L', 'Utopia', 'ITC Bookman', 'Bookman', 'Nimbus Roman No9 L', 'Times New Roman', 'Times', 'Palatino', 'Charter', 'serif']\n","font.size: 10.0\n","font.stretch: normal\n","font.style: normal\n","font.variant: normal\n","font.weight: normal\n","grid.alpha: 1.0\n","grid.color: #b0b0b0\n","grid.linestyle: -\n","grid.linewidth: 0.8\n","hatch.color: black\n","hatch.linewidth: 1.0\n","hist.bins: 10\n","image.aspect: equal\n","image.cmap: viridis\n","image.composite_image: True\n","image.interpolation: antialiased\n","image.lut: 256\n","image.origin: upper\n","image.resample: True\n","interactive: False\n","keymap.all_axes: ['a']\n","keymap.back: ['left', 'c', 'backspace', 'MouseButton.BACK']\n","keymap.copy: ['ctrl+c', 'cmd+c']\n","keymap.forward: ['right', 'v', 'MouseButton.FORWARD']\n","keymap.fullscreen: ['f', 'ctrl+f']\n","keymap.grid: ['g']\n","keymap.grid_minor: ['G']\n","keymap.help: ['f1']\n","keymap.home: ['h', 'r', 'home']\n","keymap.pan: ['p']\n","keymap.quit: ['ctrl+w', 'cmd+w', 'q']\n","keymap.quit_all: ['W', 'cmd+W', 'Q']\n","keymap.save: ['s', 'ctrl+s']\n","keymap.xscale: ['k', 'L']\n","keymap.yscale: ['l']\n","keymap.zoom: ['o']\n","legend.borderaxespad: 0.5\n","legend.borderpad: 0.4\n","legend.columnspacing: 2.0\n","legend.edgecolor: 0.8\n","legend.facecolor: inherit\n","legend.fancybox: True\n","legend.fontsize: medium\n","legend.framealpha: 0.8\n","legend.frameon: True\n","legend.handleheight: 0.7\n","legend.handlelength: 2.0\n","legend.handletextpad: 0.8\n","legend.labelspacing: 0.5\n","legend.loc: best\n","legend.markerscale: 1.0\n","legend.numpoints: 1\n","legend.scatterpoints: 1\n","legend.shadow: False\n","legend.title_fontsize: None\n","lines.antialiased: True\n","lines.color: C0\n","lines.dash_capstyle: butt\n","lines.dash_joinstyle: round\n","lines.dashdot_pattern: [6.4, 1.6, 1.0, 1.6]\n","lines.dashed_pattern: [3.7, 1.6]\n","lines.dotted_pattern: [1.0, 1.65]\n","lines.linestyle: -\n","lines.linewidth: 1.5\n","lines.marker: None\n","lines.markeredgecolor: auto\n","lines.markeredgewidth: 1.0\n","lines.markerfacecolor: auto\n","lines.markersize: 6.0\n","lines.scale_dashes: True\n","lines.solid_capstyle: projecting\n","lines.solid_joinstyle: round\n","markers.fillstyle: full\n","mathtext.bf: sans:bold\n","mathtext.cal: cursive\n","mathtext.default: it\n","mathtext.fallback_to_cm: True\n","mathtext.fontset: dejavusans\n","mathtext.it: sans:italic\n","mathtext.rm: sans\n","mathtext.sf: sans\n","mathtext.tt: monospace\n","mpl_toolkits.legacy_colorbar: True\n","patch.antialiased: True\n","patch.edgecolor: black\n","patch.facecolor: C0\n","patch.force_edgecolor: False\n","patch.linewidth: 1.0\n","path.effects: []\n","path.simplify: True\n","path.simplify_threshold: 0.1111111111111111\n","path.sketch: None\n","path.snap: True\n","pdf.compression: 6\n","pdf.fonttype: 3\n","pdf.inheritcolor: False\n","pdf.use14corefonts: False\n","pgf.preamble: \n","pgf.rcfonts: True\n","pgf.texsystem: xelatex\n","polaraxes.grid: True\n","ps.distiller.res: 6000\n","ps.fonttype: 3\n","ps.papersize: letter\n","ps.useafm: False\n","ps.usedistiller: None\n","savefig.bbox: None\n","savefig.directory: ~\n","savefig.dpi: figure\n","savefig.edgecolor: white\n","savefig.facecolor: white\n","savefig.format: png\n","savefig.frameon: True\n","savefig.jpeg_quality: 95\n","savefig.orientation: portrait\n","savefig.pad_inches: 0.1\n","savefig.transparent: False\n","scatter.edgecolors: face\n","scatter.marker: o\n","svg.fonttype: path\n","svg.hashsalt: None\n","svg.image_inline: True\n","text.antialiased: True\n","text.color: black\n","text.hinting: auto\n","text.hinting_factor: 8\n","text.kerning_factor: 0\n","text.latex.preamble: \n","text.latex.preview: False\n","text.latex.unicode: True\n","text.usetex: False\n","timezone: UTC\n","tk.window_focus: False\n","toolbar: toolbar2\n","verbose.fileo: sys.stdout\n","verbose.level: silent\n","webagg.address: 127.0.0.1\n","webagg.open_in_browser: True\n","webagg.port: 8988\n","webagg.port_retries: 50\n","xtick.alignment: center\n","xtick.bottom: True\n","xtick.color: black\n","xtick.direction: out\n","xtick.labelbottom: True\n","xtick.labelsize: medium\n","xtick.labeltop: False\n","xtick.major.bottom: True\n","xtick.major.pad: 3.5\n","xtick.major.size: 3.5\n","xtick.major.top: True\n","xtick.major.width: 0.8\n","xtick.minor.bottom: True\n","xtick.minor.pad: 3.4\n","xtick.minor.size: 2.0\n","xtick.minor.top: True\n","xtick.minor.visible: False\n","xtick.minor.width: 0.6\n","xtick.top: False\n","ytick.alignment: center_baseline\n","ytick.color: black\n","ytick.direction: out\n","ytick.labelleft: True\n","ytick.labelright: False\n","ytick.labelsize: medium\n","ytick.left: True\n","ytick.major.left: True\n","ytick.major.pad: 3.5\n","ytick.major.right: True\n","ytick.major.size: 3.5\n","ytick.major.width: 0.8\n","ytick.minor.left: True\n","ytick.minor.pad: 3.4\n","ytick.minor.right: True\n","ytick.minor.size: 2.0\n","ytick.minor.visible: False\n","ytick.minor.width: 0.6\n","ytick.right: False\n","get_backend : <function get_backend at 0x7f6083f7ea60>\n","rcParamsOrig : Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-146-e87fd719ea36>\", line 11, in <module>\n","    print(k,\":\",v)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 846, in __str__\n","    return '\\n'.join(map('{0[0]}: {0[1]}'.format, sorted(self.items())))\n","  File \"/usr/lib/python3.6/_collections_abc.py\", line 744, in __iter__\n","    yield (key, self._mapping[key])\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 832, in __getitem__\n","    plt.switch_backend(rcsetup._auto_backend_sentinel)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 204, in switch_backend\n","    switch_backend(candidate)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 221, in switch_backend\n","    backend_mod = importlib.import_module(backend_name)\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n","  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_qt5agg.py\", line 11, in <module>\n","    from .backend_qt5 import (\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_qt5.py\", line 15, in <module>\n","    import matplotlib.backends.qt_editor.figureoptions as figureoptions\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/qt_editor/figureoptions.py\", line 12, in <module>\n","    from matplotlib.backends.qt_compat import QtGui\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/qt_compat.py\", line 163, in <module>\n","    _setup()\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/qt_compat.py\", line 70, in _setup_pyqt5\n","    from PyQt5 import QtCore, QtGui, QtWidgets\n","  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 951, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 894, in _find_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1157, in find_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1126, in _get_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1090, in _path_importer_cache\n","OSError: [Errno 107] Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.6/posixpath.py\", line 383, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"bNdTBeAzTras","executionInfo":{"status":"error","timestamp":1601491123759,"user_tz":240,"elapsed":527,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"7ff75ad0-c671-4726-8454-61312cb8bbc5","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for i in inspect.getmembers(plt): \n","      \n","    # to remove private and protected \n","    # functions \n","    if not i[0].startswith('_'): \n","          \n","        # To remove other methods that \n","        # doesnot start with a underscore \n","        if not inspect.ismethod(i[1]):  \n","            print(i) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["('Annotation', <class 'matplotlib.text.Annotation'>)\n","('Arrow', <class 'matplotlib.patches.Arrow'>)\n","('Artist', <class 'matplotlib.artist.Artist'>)\n","('AutoLocator', <class 'matplotlib.ticker.AutoLocator'>)\n","('Axes', <class 'matplotlib.axes._axes.Axes'>)\n","('Button', <class 'matplotlib.widgets.Button'>)\n","('Circle', <class 'matplotlib.patches.Circle'>)\n","('Figure', <class 'matplotlib.figure.Figure'>)\n","('FigureCanvasBase', <class 'matplotlib.backend_bases.FigureCanvasBase'>)\n","('FixedFormatter', <class 'matplotlib.ticker.FixedFormatter'>)\n","('FixedLocator', <class 'matplotlib.ticker.FixedLocator'>)\n","('FormatStrFormatter', <class 'matplotlib.ticker.FormatStrFormatter'>)\n","('Formatter', <class 'matplotlib.ticker.Formatter'>)\n","('FuncFormatter', <class 'matplotlib.ticker.FuncFormatter'>)\n","('GridSpec', <class 'matplotlib.gridspec.GridSpec'>)\n","('IndexLocator', <class 'matplotlib.ticker.IndexLocator'>)\n","('Line2D', <class 'matplotlib.lines.Line2D'>)\n","('LinearLocator', <class 'matplotlib.ticker.LinearLocator'>)\n","('Locator', <class 'matplotlib.ticker.Locator'>)\n","('LogFormatter', <class 'matplotlib.ticker.LogFormatter'>)\n","('LogFormatterExponent', <class 'matplotlib.ticker.LogFormatterExponent'>)\n","('LogFormatterMathtext', <class 'matplotlib.ticker.LogFormatterMathtext'>)\n","('LogLocator', <class 'matplotlib.ticker.LogLocator'>)\n","('MaxNLocator', <class 'matplotlib.ticker.MaxNLocator'>)\n","('MultipleLocator', <class 'matplotlib.ticker.MultipleLocator'>)\n","('Normalize', <class 'matplotlib.colors.Normalize'>)\n","('NullFormatter', <class 'matplotlib.ticker.NullFormatter'>)\n","('NullLocator', <class 'matplotlib.ticker.NullLocator'>)\n","('Number', <class 'numbers.Number'>)\n","('PolarAxes', <class 'matplotlib.projections.polar.PolarAxes'>)\n","('Polygon', <class 'matplotlib.patches.Polygon'>)\n","('Rectangle', <class 'matplotlib.patches.Rectangle'>)\n","('ScalarFormatter', <class 'matplotlib.ticker.ScalarFormatter'>)\n","('Slider', <class 'matplotlib.widgets.Slider'>)\n","('Subplot', <class 'matplotlib.axes._subplots.AxesSubplot'>)\n","('SubplotTool', <class 'matplotlib.widgets.SubplotTool'>)\n","('Text', <class 'matplotlib.text.Text'>)\n","('TickHelper', <class 'matplotlib.ticker.TickHelper'>)\n","('Widget', <class 'matplotlib.widgets.Widget'>)\n","('acorr', <function acorr at 0x7f6081d9f268>)\n","('angle_spectrum', <function angle_spectrum at 0x7f6081d9f2f0>)\n","('annotate', <function annotate at 0x7f6081d9f378>)\n","('arrow', <function arrow at 0x7f6081d9f400>)\n","('autoscale', <function autoscale at 0x7f6081d9f488>)\n","('autumn', <function autumn at 0x7f6081d9cea0>)\n","('axes', <function axes at 0x7f6082eab598>)\n","('axhline', <function axhline at 0x7f6081d9f510>)\n","('axhspan', <function axhspan at 0x7f6081d9f598>)\n","('axis', <function axis at 0x7f6081d9f620>)\n","('axvline', <function axvline at 0x7f6081d9f6a8>)\n","('axvspan', <function axvspan at 0x7f6081d9f730>)\n","('bar', <function bar at 0x7f6081d9f7b8>)\n","('barbs', <function barbs at 0x7f6081d9f840>)\n","('barh', <function barh at 0x7f6081d9f8c8>)\n","('bone', <function bone at 0x7f6081da4ae8>)\n","('box', <function box at 0x7f6082eabbf8>)\n","('boxplot', <function boxplot at 0x7f6081d9f950>)\n","('broken_barh', <function broken_barh at 0x7f6081d9f9d8>)\n","('cbook', <module 'matplotlib.cbook' from '/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py'>)\n","('cla', <function cla at 0x7f6081d9fa60>)\n","('clabel', <function clabel at 0x7f6081d9fae8>)\n","('clf', <function clf at 0x7f6082eab378>)\n","('clim', <function clim at 0x7f6082ea92f0>)\n","('close', <function close at 0x7f6082eab1e0>)\n","('cm', <module 'matplotlib.cm' from '/usr/local/lib/python3.6/dist-packages/matplotlib/cm.py'>)\n","('cohere', <function cohere at 0x7f6081d9fb70>)\n","('colorbar', <function colorbar at 0x7f6082ea9268>)\n","('colormaps', <function colormaps at 0x7f6082ea9158>)\n","('connect', <function connect at 0x7f6082eab268>)\n","('contour', <function contour at 0x7f6081d9fbf8>)\n","('contourf', <function contourf at 0x7f6081d9fc80>)\n","('cool', <function cool at 0x7f6081da4b70>)\n","('copper', <function copper at 0x7f6081da4bf8>)\n","('csd', <function csd at 0x7f6081d9fd08>)\n","('cycler', <function cycler at 0x7f6083fd9158>)\n","('dedent', <function dedent at 0x7f6084198d08>)\n","('delaxes', <function delaxes at 0x7f6082eab620>)\n","('deprecated', <function deprecated at 0x7f6084189598>)\n","('disconnect', <function disconnect at 0x7f6082eab2f0>)\n","('docstring', <module 'matplotlib.docstring' from '/usr/local/lib/python3.6/dist-packages/matplotlib/docstring.py'>)\n","('draw', <function draw at 0x7f6082eab400>)\n","('draw_if_interactive', <function flag_calls.<locals>.wrapper at 0x7f60710fae18>)\n","('errorbar', <function errorbar at 0x7f6081d9fd90>)\n","('eventplot', <function eventplot at 0x7f6081d9fe18>)\n","('figaspect', <function figaspect at 0x7f6082e9f268>)\n","('figimage', <function figimage at 0x7f6081d9cf28>)\n","('figlegend', <function figlegend at 0x7f6082eab488>)\n","('fignum_exists', <function fignum_exists at 0x7f6082ea5f28>)\n","('figtext', <function figtext at 0x7f6081d9f048>)\n","('figure', <function figure at 0x7f6082ea5d90>)\n","('fill', <function fill at 0x7f6081d9fea0>)\n","('fill_between', <function fill_between at 0x7f6081d9ff28>)\n","('fill_betweenx', <function fill_betweenx at 0x7f6081da3048>)\n","('findobj', <function findobj at 0x7f6082ea5730>)\n","('flag', <function flag at 0x7f6081da4c80>)\n","('functools', <module 'functools' from '/usr/lib/python3.6/functools.py'>)\n","('gca', <function gca at 0x7f6082eab730>)\n","('gcf', <function gcf at 0x7f6082ea5ea0>)\n","('gci', <function gci at 0x7f6082ea5a60>)\n","('get', <function getp at 0x7f60837d4a60>)\n","('get_backend', <function get_backend at 0x7f6083f7ea60>)\n","('get_cmap', <function get_cmap at 0x7f608373f158>)\n","('get_current_fig_manager', <function get_current_fig_manager at 0x7f6082eab158>)\n","('get_figlabels', <function get_figlabels at 0x7f6082eab0d0>)\n","('get_fignums', <function get_fignums at 0x7f6082eab048>)\n","('get_plot_commands', <function get_plot_commands at 0x7f6082ea90d0>)\n","('get_scale_docs', <function get_scale_docs at 0x7f6082fd0378>)\n","('get_scale_names', <function get_scale_names at 0x7f6082fb2488>)\n","('getp', <function getp at 0x7f60837d4a60>)\n","('ginput', <function ginput at 0x7f6081d9f0d0>)\n","('gray', <function gray at 0x7f6081da4d08>)\n","('grid', <function grid at 0x7f6081da30d0>)\n","('hexbin', <function hexbin at 0x7f6081da3158>)\n","('hist', <function hist at 0x7f6081da31e0>)\n","('hist2d', <function hist2d at 0x7f6081da3268>)\n","('hlines', <function hlines at 0x7f6081da32f0>)\n","('hot', <function hot at 0x7f6081da4d90>)\n","('hsv', <function hsv at 0x7f6081da4e18>)\n","('importlib', <module 'importlib' from '/usr/lib/python3.6/importlib/__init__.py'>)\n","('imread', <function imread at 0x7f6082ea9488>)\n","('imsave', <function imsave at 0x7f6082ea9510>)\n","('imshow', <function imshow at 0x7f6081da3378>)\n","('inferno', <function inferno at 0x7f6081da62f0>)\n","('inspect', <module 'inspect' from '/usr/lib/python3.6/inspect.py'>)\n","('install_repl_displayhook', <function install_repl_displayhook at 0x7f6083f8d268>)\n","('interactive', <function interactive at 0x7f6083f7eae8>)\n","('ioff', <function ioff at 0x7f6082ea58c8>)\n","('ion', <function ion at 0x7f6082ea5950>)\n","('isinteractive', <function isinteractive at 0x7f6082ea5840>)\n","('jet', <function jet at 0x7f6081da4ea0>)\n","('legend', <function legend at 0x7f6081da3400>)\n","('locator_params', <function locator_params at 0x7f6081da3488>)\n","('logging', <module 'logging' from '/usr/lib/python3.6/logging/__init__.py'>)\n","('loglog', <function loglog at 0x7f6081da3510>)\n","('magma', <function magma at 0x7f6081da6268>)\n","('magnitude_spectrum', <function magnitude_spectrum at 0x7f6081da3598>)\n","('margins', <function margins at 0x7f6081da3620>)\n","('matplotlib', <module 'matplotlib' from '/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py'>)\n","('matshow', <function matshow at 0x7f6082ea9400>)\n","('minorticks_off', <function minorticks_off at 0x7f6081da36a8>)\n","('minorticks_on', <function minorticks_on at 0x7f6081da3730>)\n","('mlab', <module 'matplotlib.mlab' from '/usr/local/lib/python3.6/dist-packages/matplotlib/mlab.py'>)\n","('nipy_spectral', <function nipy_spectral at 0x7f6081da6488>)\n","('np', <module 'numpy' from '/usr/local/lib/python3.6/dist-packages/numpy/__init__.py'>)\n","('pause', <function pause at 0x7f6082ea59d8>)\n","('pcolor', <function pcolor at 0x7f6081da37b8>)\n","('pcolormesh', <function pcolormesh at 0x7f6081da3840>)\n","('phase_spectrum', <function phase_spectrum at 0x7f6081da38c8>)\n","('pie', <function pie at 0x7f6081da3950>)\n","('pink', <function pink at 0x7f6081da4f28>)\n","('plasma', <function plasma at 0x7f6081da6378>)\n","('plot', <function plot at 0x7f6081da39d8>)\n","('plot_date', <function plot_date at 0x7f6081da3a60>)\n","('plotfile', <function plotfile at 0x7f6082ea9840>)\n","('plotting', <function plotting at 0x7f6082ea9048>)\n","('polar', <function polar at 0x7f6082ea9598>)\n","('prism', <function prism at 0x7f6081da6048>)\n","('psd', <function psd at 0x7f6081da3ae8>)\n","('quiver', <function quiver at 0x7f6081da3b70>)\n","('quiverkey', <function quiverkey at 0x7f6081da3bf8>)\n","('rc', <function rc at 0x7f6082ea5ae8>)\n","('rcParams', RcParams({'_internal.classic_mode': False,\n","          'agg.path.chunksize': 0,\n","          'animation.avconv_args': [],\n","          'animation.avconv_path': 'avconv',\n","          'animation.bitrate': -1,\n","          'animation.codec': 'h264',\n","          'animation.convert_args': [],\n","          'animation.convert_path': 'convert',\n","          'animation.embed_limit': 20.0,\n","          'animation.ffmpeg_args': [],\n","          'animation.ffmpeg_path': 'ffmpeg',\n","          'animation.frame_format': 'png',\n","          'animation.html': 'none',\n","          'animation.html_args': [],\n","          'animation.writer': 'ffmpeg',\n","          'axes.autolimit_mode': 'data',\n","          'axes.axisbelow': 'line',\n","          'axes.edgecolor': 'black',\n","          'axes.facecolor': 'white',\n","          'axes.formatter.limits': [-5, 6],\n","          'axes.formatter.min_exponent': 0,\n","          'axes.formatter.offset_threshold': 4,\n","          'axes.formatter.use_locale': False,\n","          'axes.formatter.use_mathtext': False,\n","          'axes.formatter.useoffset': True,\n","          'axes.grid': False,\n","          'axes.grid.axis': 'both',\n","          'axes.grid.which': 'major',\n","          'axes.labelcolor': 'black',\n","          'axes.labelpad': 4.0,\n","          'axes.labelsize': 'medium',\n","          'axes.labelweight': 'normal',\n","          'axes.linewidth': 0.8,\n","          'axes.prop_cycle': cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']),\n","          'axes.spines.bottom': True,\n","          'axes.spines.left': True,\n","          'axes.spines.right': True,\n","          'axes.spines.top': True,\n","          'axes.titlecolor': 'auto',\n","          'axes.titlelocation': 'center',\n","          'axes.titlepad': 6.0,\n","          'axes.titlesize': 'large',\n","          'axes.titleweight': 'normal',\n","          'axes.unicode_minus': True,\n","          'axes.xmargin': 0.05,\n","          'axes.ymargin': 0.05,\n","          'axes3d.grid': True,\n","          'backend': 'module://ipykernel.pylab.backend_inline',\n","          'backend_fallback': True,\n","          'boxplot.bootstrap': None,\n","          'boxplot.boxprops.color': 'black',\n","          'boxplot.boxprops.linestyle': '-',\n","          'boxplot.boxprops.linewidth': 1.0,\n","          'boxplot.capprops.color': 'black',\n","          'boxplot.capprops.linestyle': '-',\n","          'boxplot.capprops.linewidth': 1.0,\n","          'boxplot.flierprops.color': 'black',\n","          'boxplot.flierprops.linestyle': 'none',\n","          'boxplot.flierprops.linewidth': 1.0,\n","          'boxplot.flierprops.marker': 'o',\n","          'boxplot.flierprops.markeredgecolor': 'black',\n","          'boxplot.flierprops.markeredgewidth': 1.0,\n","          'boxplot.flierprops.markerfacecolor': 'none',\n","          'boxplot.flierprops.markersize': 6.0,\n","          'boxplot.meanline': False,\n","          'boxplot.meanprops.color': 'C2',\n","          'boxplot.meanprops.linestyle': '--',\n","          'boxplot.meanprops.linewidth': 1.0,\n","          'boxplot.meanprops.marker': '^',\n","          'boxplot.meanprops.markeredgecolor': 'C2',\n","          'boxplot.meanprops.markerfacecolor': 'C2',\n","          'boxplot.meanprops.markersize': 6.0,\n","          'boxplot.medianprops.color': 'C1',\n","          'boxplot.medianprops.linestyle': '-',\n","          'boxplot.medianprops.linewidth': 1.0,\n","          'boxplot.notch': False,\n","          'boxplot.patchartist': False,\n","          'boxplot.showbox': True,\n","          'boxplot.showcaps': True,\n","          'boxplot.showfliers': True,\n","          'boxplot.showmeans': False,\n","          'boxplot.vertical': True,\n","          'boxplot.whiskerprops.color': 'black',\n","          'boxplot.whiskerprops.linestyle': '-',\n","          'boxplot.whiskerprops.linewidth': 1.0,\n","          'boxplot.whiskers': 1.5,\n","          'contour.corner_mask': True,\n","          'contour.negative_linestyle': 'dashed',\n","          'datapath': '/usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data',\n","          'date.autoformatter.day': '%Y-%m-%d',\n","          'date.autoformatter.hour': '%m-%d %H',\n","          'date.autoformatter.microsecond': '%M:%S.%f',\n","          'date.autoformatter.minute': '%d %H:%M',\n","          'date.autoformatter.month': '%Y-%m',\n","          'date.autoformatter.second': '%H:%M:%S',\n","          'date.autoformatter.year': '%Y',\n","          'docstring.hardcopy': False,\n","          'errorbar.capsize': 0.0,\n","          'figure.autolayout': False,\n","          'figure.constrained_layout.h_pad': 0.04167,\n","          'figure.constrained_layout.hspace': 0.02,\n","          'figure.constrained_layout.use': False,\n","          'figure.constrained_layout.w_pad': 0.04167,\n","          'figure.constrained_layout.wspace': 0.02,\n","          'figure.dpi': 72.0,\n","          'figure.edgecolor': (1, 1, 1, 0),\n","          'figure.facecolor': (1, 1, 1, 0),\n","          'figure.figsize': [6.0, 4.0],\n","          'figure.frameon': True,\n","          'figure.max_open_warning': 20,\n","          'figure.subplot.bottom': 0.125,\n","          'figure.subplot.hspace': 0.2,\n","          'figure.subplot.left': 0.125,\n","          'figure.subplot.right': 0.9,\n","          'figure.subplot.top': 0.88,\n","          'figure.subplot.wspace': 0.2,\n","          'figure.titlesize': 'large',\n","          'figure.titleweight': 'normal',\n","          'font.cursive': ['Apple Chancery',\n","                           'Textile',\n","                           'Zapf Chancery',\n","                           'Sand',\n","                           'Script MT',\n","                           'Felipa',\n","                           'cursive'],\n","          'font.family': ['sans-serif'],\n","          'font.fantasy': ['Comic Neue',\n","                           'Comic Sans MS',\n","                           'Chicago',\n","                           'Charcoal',\n","                           'Impact',\n","                           'Western',\n","                           'Humor Sans',\n","                           'xkcd',\n","                           'fantasy'],\n","          'font.monospace': ['DejaVu Sans Mono',\n","                             'Bitstream Vera Sans Mono',\n","                             'Computer Modern Typewriter',\n","                             'Andale Mono',\n","                             'Nimbus Mono L',\n","                             'Courier New',\n","                             'Courier',\n","                             'Fixed',\n","                             'Terminal',\n","                             'monospace'],\n","          'font.sans-serif': ['DejaVu Sans',\n","                              'Bitstream Vera Sans',\n","                              'Computer Modern Sans Serif',\n","                              'Lucida Grande',\n","                              'Verdana',\n","                              'Geneva',\n","                              'Lucid',\n","                              'Arial',\n","                              'Helvetica',\n","                              'Avant Garde',\n","                              'sans-serif'],\n","          'font.serif': ['DejaVu Serif',\n","                         'Bitstream Vera Serif',\n","                         'Computer Modern Roman',\n","                         'New Century Schoolbook',\n","                         'Century Schoolbook L',\n","                         'Utopia',\n","                         'ITC Bookman',\n","                         'Bookman',\n","                         'Nimbus Roman No9 L',\n","                         'Times New Roman',\n","                         'Times',\n","                         'Palatino',\n","                         'Charter',\n","                         'serif'],\n","          'font.size': 10.0,\n","          'font.stretch': 'normal',\n","          'font.style': 'normal',\n","          'font.variant': 'normal',\n","          'font.weight': 'normal',\n","          'grid.alpha': 1.0,\n","          'grid.color': '#b0b0b0',\n","          'grid.linestyle': '-',\n","          'grid.linewidth': 0.8,\n","          'hatch.color': 'black',\n","          'hatch.linewidth': 1.0,\n","          'hist.bins': 10,\n","          'image.aspect': 'equal',\n","          'image.cmap': 'viridis',\n","          'image.composite_image': True,\n","          'image.interpolation': 'antialiased',\n","          'image.lut': 256,\n","          'image.origin': 'upper',\n","          'image.resample': True,\n","          'interactive': True,\n","          'keymap.all_axes': ['a'],\n","          'keymap.back': ['left', 'c', 'backspace', 'MouseButton.BACK'],\n","          'keymap.copy': ['ctrl+c', 'cmd+c'],\n","          'keymap.forward': ['right', 'v', 'MouseButton.FORWARD'],\n","          'keymap.fullscreen': ['f', 'ctrl+f'],\n","          'keymap.grid': ['g'],\n","          'keymap.grid_minor': ['G'],\n","          'keymap.help': ['f1'],\n","          'keymap.home': ['h', 'r', 'home'],\n","          'keymap.pan': ['p'],\n","          'keymap.quit': ['ctrl+w', 'cmd+w', 'q'],\n","          'keymap.quit_all': ['W', 'cmd+W', 'Q'],\n","          'keymap.save': ['s', 'ctrl+s'],\n","          'keymap.xscale': ['k', 'L'],\n","          'keymap.yscale': ['l'],\n","          'keymap.zoom': ['o'],\n","          'legend.borderaxespad': 0.5,\n","          'legend.borderpad': 0.4,\n","          'legend.columnspacing': 2.0,\n","          'legend.edgecolor': '0.8',\n","          'legend.facecolor': 'inherit',\n","          'legend.fancybox': True,\n","          'legend.fontsize': 'medium',\n","          'legend.framealpha': 0.8,\n","          'legend.frameon': True,\n","          'legend.handleheight': 0.7,\n","          'legend.handlelength': 2.0,\n","          'legend.handletextpad': 0.8,\n","          'legend.labelspacing': 0.5,\n","          'legend.loc': 'best',\n","          'legend.markerscale': 1.0,\n","          'legend.numpoints': 1,\n","          'legend.scatterpoints': 1,\n","          'legend.shadow': False,\n","          'legend.title_fontsize': None,\n","          'lines.antialiased': True,\n","          'lines.color': 'C0',\n","          'lines.dash_capstyle': 'butt',\n","          'lines.dash_joinstyle': 'round',\n","          'lines.dashdot_pattern': [6.4, 1.6, 1.0, 1.6],\n","          'lines.dashed_pattern': [3.7, 1.6],\n","          'lines.dotted_pattern': [1.0, 1.65],\n","          'lines.linestyle': '-',\n","          'lines.linewidth': 1.5,\n","          'lines.marker': 'None',\n","          'lines.markeredgecolor': 'auto',\n","          'lines.markeredgewidth': 1.0,\n","          'lines.markerfacecolor': 'auto',\n","          'lines.markersize': 6.0,\n","          'lines.scale_dashes': True,\n","          'lines.solid_capstyle': 'projecting',\n","          'lines.solid_joinstyle': 'round',\n","          'markers.fillstyle': 'full',\n","          'mathtext.bf': 'sans:bold',\n","          'mathtext.cal': 'cursive',\n","          'mathtext.default': 'it',\n","          'mathtext.fallback_to_cm': True,\n","          'mathtext.fontset': 'dejavusans',\n","          'mathtext.it': 'sans:italic',\n","          'mathtext.rm': 'sans',\n","          'mathtext.sf': 'sans',\n","          'mathtext.tt': 'monospace',\n","          'mpl_toolkits.legacy_colorbar': True,\n","          'patch.antialiased': True,\n","          'patch.edgecolor': 'black',\n","          'patch.facecolor': 'C0',\n","          'patch.force_edgecolor': False,\n","          'patch.linewidth': 1.0,\n","          'path.effects': [],\n","          'path.simplify': True,\n","          'path.simplify_threshold': 0.1111111111111111,\n","          'path.sketch': None,\n","          'path.snap': True,\n","          'pdf.compression': 6,\n","          'pdf.fonttype': 3,\n","          'pdf.inheritcolor': False,\n","          'pdf.use14corefonts': False,\n","          'pgf.preamble': '',\n","          'pgf.rcfonts': True,\n","          'pgf.texsystem': 'xelatex',\n","          'polaraxes.grid': True,\n","          'ps.distiller.res': 6000,\n","          'ps.fonttype': 3,\n","          'ps.papersize': 'letter',\n","          'ps.useafm': False,\n","          'ps.usedistiller': None,\n","          'savefig.bbox': None,\n","          'savefig.directory': '~',\n","          'savefig.dpi': 'figure',\n","          'savefig.edgecolor': 'white',\n","          'savefig.facecolor': 'white',\n","          'savefig.format': 'png',\n","          'savefig.frameon': True,\n","          'savefig.jpeg_quality': 95,\n","          'savefig.orientation': 'portrait',\n","          'savefig.pad_inches': 0.1,\n","          'savefig.transparent': False,\n","          'scatter.edgecolors': 'face',\n","          'scatter.marker': 'o',\n","          'svg.fonttype': 'path',\n","          'svg.hashsalt': None,\n","          'svg.image_inline': True,\n","          'text.antialiased': True,\n","          'text.color': 'black',\n","          'text.hinting': 'auto',\n","          'text.hinting_factor': 8,\n","          'text.kerning_factor': 0,\n","          'text.latex.preamble': '',\n","          'text.latex.preview': False,\n","          'text.latex.unicode': True,\n","          'text.usetex': False,\n","          'timezone': 'UTC',\n","          'tk.window_focus': False,\n","          'toolbar': 'toolbar2',\n","          'verbose.fileo': 'sys.stdout',\n","          'verbose.level': 'silent',\n","          'webagg.address': '127.0.0.1',\n","          'webagg.open_in_browser': True,\n","          'webagg.port': 8988,\n","          'webagg.port_retries': 50,\n","          'xtick.alignment': 'center',\n","          'xtick.bottom': True,\n","          'xtick.color': 'black',\n","          'xtick.direction': 'out',\n","          'xtick.labelbottom': True,\n","          'xtick.labelsize': 'medium',\n","          'xtick.labeltop': False,\n","          'xtick.major.bottom': True,\n","          'xtick.major.pad': 3.5,\n","          'xtick.major.size': 3.5,\n","          'xtick.major.top': True,\n","          'xtick.major.width': 0.8,\n","          'xtick.minor.bottom': True,\n","          'xtick.minor.pad': 3.4,\n","          'xtick.minor.size': 2.0,\n","          'xtick.minor.top': True,\n","          'xtick.minor.visible': False,\n","          'xtick.minor.width': 0.6,\n","          'xtick.top': False,\n","          'ytick.alignment': 'center_baseline',\n","          'ytick.color': 'black',\n","          'ytick.direction': 'out',\n","          'ytick.labelleft': True,\n","          'ytick.labelright': False,\n","          'ytick.labelsize': 'medium',\n","          'ytick.left': True,\n","          'ytick.major.left': True,\n","          'ytick.major.pad': 3.5,\n","          'ytick.major.right': True,\n","          'ytick.major.size': 3.5,\n","          'ytick.major.width': 0.8,\n","          'ytick.minor.left': True,\n","          'ytick.minor.pad': 3.4,\n","          'ytick.minor.right': True,\n","          'ytick.minor.size': 2.0,\n","          'ytick.minor.visible': False,\n","          'ytick.minor.width': 0.6,\n","          'ytick.right': False}))\n","('rcParamsDefault', RcParams({'_internal.classic_mode': False,\n","          'agg.path.chunksize': 0,\n","          'animation.avconv_args': [],\n","          'animation.avconv_path': 'avconv',\n","          'animation.bitrate': -1,\n","          'animation.codec': 'h264',\n","          'animation.convert_args': [],\n","          'animation.convert_path': 'convert',\n","          'animation.embed_limit': 20.0,\n","          'animation.ffmpeg_args': [],\n","          'animation.ffmpeg_path': 'ffmpeg',\n","          'animation.frame_format': 'png',\n","          'animation.html': 'none',\n","          'animation.html_args': [],\n","          'animation.writer': 'ffmpeg',\n","          'axes.autolimit_mode': 'data',\n","          'axes.axisbelow': 'line',\n","          'axes.edgecolor': 'black',\n","          'axes.facecolor': 'white',\n","          'axes.formatter.limits': [-5, 6],\n","          'axes.formatter.min_exponent': 0,\n","          'axes.formatter.offset_threshold': 4,\n","          'axes.formatter.use_locale': False,\n","          'axes.formatter.use_mathtext': False,\n","          'axes.formatter.useoffset': True,\n","          'axes.grid': False,\n","          'axes.grid.axis': 'both',\n","          'axes.grid.which': 'major',\n","          'axes.labelcolor': 'black',\n","          'axes.labelpad': 4.0,\n","          'axes.labelsize': 'medium',\n","          'axes.labelweight': 'normal',\n","          'axes.linewidth': 0.8,\n","          'axes.prop_cycle': cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']),\n","          'axes.spines.bottom': True,\n","          'axes.spines.left': True,\n","          'axes.spines.right': True,\n","          'axes.spines.top': True,\n","          'axes.titlecolor': 'auto',\n","          'axes.titlelocation': 'center',\n","          'axes.titlepad': 6.0,\n","          'axes.titlesize': 'large',\n","          'axes.titleweight': 'normal',\n","          'axes.unicode_minus': True,\n","          'axes.xmargin': 0.05,\n","          'axes.ymargin': 0.05,\n","          'axes3d.grid': True,\n","          'backend': 'module://ipykernel.pylab.backend_inline',\n","          'backend_fallback': True,\n","          'boxplot.bootstrap': None,\n","          'boxplot.boxprops.color': 'black',\n","          'boxplot.boxprops.linestyle': '-',\n","          'boxplot.boxprops.linewidth': 1.0,\n","          'boxplot.capprops.color': 'black',\n","          'boxplot.capprops.linestyle': '-',\n","          'boxplot.capprops.linewidth': 1.0,\n","          'boxplot.flierprops.color': 'black',\n","          'boxplot.flierprops.linestyle': 'none',\n","          'boxplot.flierprops.linewidth': 1.0,\n","          'boxplot.flierprops.marker': 'o',\n","          'boxplot.flierprops.markeredgecolor': 'black',\n","          'boxplot.flierprops.markeredgewidth': 1.0,\n","          'boxplot.flierprops.markerfacecolor': 'none',\n","          'boxplot.flierprops.markersize': 6.0,\n","          'boxplot.meanline': False,\n","          'boxplot.meanprops.color': 'C2',\n","          'boxplot.meanprops.linestyle': '--',\n","          'boxplot.meanprops.linewidth': 1.0,\n","          'boxplot.meanprops.marker': '^',\n","          'boxplot.meanprops.markeredgecolor': 'C2',\n","          'boxplot.meanprops.markerfacecolor': 'C2',\n","          'boxplot.meanprops.markersize': 6.0,\n","          'boxplot.medianprops.color': 'C1',\n","          'boxplot.medianprops.linestyle': '-',\n","          'boxplot.medianprops.linewidth': 1.0,\n","          'boxplot.notch': False,\n","          'boxplot.patchartist': False,\n","          'boxplot.showbox': True,\n","          'boxplot.showcaps': True,\n","          'boxplot.showfliers': True,\n","          'boxplot.showmeans': False,\n","          'boxplot.vertical': True,\n","          'boxplot.whiskerprops.color': 'black',\n","          'boxplot.whiskerprops.linestyle': '-',\n","          'boxplot.whiskerprops.linewidth': 1.0,\n","          'boxplot.whiskers': 1.5,\n","          'contour.corner_mask': True,\n","          'contour.negative_linestyle': 'dashed',\n","          'datapath': '/usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data',\n","          'date.autoformatter.day': '%Y-%m-%d',\n","          'date.autoformatter.hour': '%m-%d %H',\n","          'date.autoformatter.microsecond': '%M:%S.%f',\n","          'date.autoformatter.minute': '%d %H:%M',\n","          'date.autoformatter.month': '%Y-%m',\n","          'date.autoformatter.second': '%H:%M:%S',\n","          'date.autoformatter.year': '%Y',\n","          'docstring.hardcopy': False,\n","          'errorbar.capsize': 0.0,\n","          'figure.autolayout': False,\n","          'figure.constrained_layout.h_pad': 0.04167,\n","          'figure.constrained_layout.hspace': 0.02,\n","          'figure.constrained_layout.use': False,\n","          'figure.constrained_layout.w_pad': 0.04167,\n","          'figure.constrained_layout.wspace': 0.02,\n","          'figure.dpi': 100.0,\n","          'figure.edgecolor': 'white',\n","          'figure.facecolor': 'white',\n","          'figure.figsize': [6.4, 4.8],\n","          'figure.frameon': True,\n","          'figure.max_open_warning': 20,\n","          'figure.subplot.bottom': 0.11,\n","          'figure.subplot.hspace': 0.2,\n","          'figure.subplot.left': 0.125,\n","          'figure.subplot.right': 0.9,\n","          'figure.subplot.top': 0.88,\n","          'figure.subplot.wspace': 0.2,\n","          'figure.titlesize': 'large',\n","          'figure.titleweight': 'normal',\n","          'font.cursive': ['Apple Chancery',\n","                           'Textile',\n","                           'Zapf Chancery',\n","                           'Sand',\n","                           'Script MT',\n","                           'Felipa',\n","                           'cursive'],\n","          'font.family': ['sans-serif'],\n","          'font.fantasy': ['Comic Neue',\n","                           'Comic Sans MS',\n","                           'Chicago',\n","                           'Charcoal',\n","                           'Impact',\n","                           'Western',\n","                           'Humor Sans',\n","                           'xkcd',\n","                           'fantasy'],\n","          'font.monospace': ['DejaVu Sans Mono',\n","                             'Bitstream Vera Sans Mono',\n","                             'Computer Modern Typewriter',\n","                             'Andale Mono',\n","                             'Nimbus Mono L',\n","                             'Courier New',\n","                             'Courier',\n","                             'Fixed',\n","                             'Terminal',\n","                             'monospace'],\n","          'font.sans-serif': ['DejaVu Sans',\n","                              'Bitstream Vera Sans',\n","                              'Computer Modern Sans Serif',\n","                              'Lucida Grande',\n","                              'Verdana',\n","                              'Geneva',\n","                              'Lucid',\n","                              'Arial',\n","                              'Helvetica',\n","                              'Avant Garde',\n","                              'sans-serif'],\n","          'font.serif': ['DejaVu Serif',\n","                         'Bitstream Vera Serif',\n","                         'Computer Modern Roman',\n","                         'New Century Schoolbook',\n","                         'Century Schoolbook L',\n","                         'Utopia',\n","                         'ITC Bookman',\n","                         'Bookman',\n","                         'Nimbus Roman No9 L',\n","                         'Times New Roman',\n","                         'Times',\n","                         'Palatino',\n","                         'Charter',\n","                         'serif'],\n","          'font.size': 10.0,\n","          'font.stretch': 'normal',\n","          'font.style': 'normal',\n","          'font.variant': 'normal',\n","          'font.weight': 'normal',\n","          'grid.alpha': 1.0,\n","          'grid.color': '#b0b0b0',\n","          'grid.linestyle': '-',\n","          'grid.linewidth': 0.8,\n","          'hatch.color': 'black',\n","          'hatch.linewidth': 1.0,\n","          'hist.bins': 10,\n","          'image.aspect': 'equal',\n","          'image.cmap': 'viridis',\n","          'image.composite_image': True,\n","          'image.interpolation': 'antialiased',\n","          'image.lut': 256,\n","          'image.origin': 'upper',\n","          'image.resample': True,\n","          'interactive': False,\n","          'keymap.all_axes': ['a'],\n","          'keymap.back': ['left', 'c', 'backspace', 'MouseButton.BACK'],\n","          'keymap.copy': ['ctrl+c', 'cmd+c'],\n","          'keymap.forward': ['right', 'v', 'MouseButton.FORWARD'],\n","          'keymap.fullscreen': ['f', 'ctrl+f'],\n","          'keymap.grid': ['g'],\n","          'keymap.grid_minor': ['G'],\n","          'keymap.help': ['f1'],\n","          'keymap.home': ['h', 'r', 'home'],\n","          'keymap.pan': ['p'],\n","          'keymap.quit': ['ctrl+w', 'cmd+w', 'q'],\n","          'keymap.quit_all': ['W', 'cmd+W', 'Q'],\n","          'keymap.save': ['s', 'ctrl+s'],\n","          'keymap.xscale': ['k', 'L'],\n","          'keymap.yscale': ['l'],\n","          'keymap.zoom': ['o'],\n","          'legend.borderaxespad': 0.5,\n","          'legend.borderpad': 0.4,\n","          'legend.columnspacing': 2.0,\n","          'legend.edgecolor': '0.8',\n","          'legend.facecolor': 'inherit',\n","          'legend.fancybox': True,\n","          'legend.fontsize': 'medium',\n","          'legend.framealpha': 0.8,\n","          'legend.frameon': True,\n","          'legend.handleheight': 0.7,\n","          'legend.handlelength': 2.0,\n","          'legend.handletextpad': 0.8,\n","          'legend.labelspacing': 0.5,\n","          'legend.loc': 'best',\n","          'legend.markerscale': 1.0,\n","          'legend.numpoints': 1,\n","          'legend.scatterpoints': 1,\n","          'legend.shadow': False,\n","          'legend.title_fontsize': None,\n","          'lines.antialiased': True,\n","          'lines.color': 'C0',\n","          'lines.dash_capstyle': 'butt',\n","          'lines.dash_joinstyle': 'round',\n","          'lines.dashdot_pattern': [6.4, 1.6, 1.0, 1.6],\n","          'lines.dashed_pattern': [3.7, 1.6],\n","          'lines.dotted_pattern': [1.0, 1.65],\n","          'lines.linestyle': '-',\n","          'lines.linewidth': 1.5,\n","          'lines.marker': 'None',\n","          'lines.markeredgecolor': 'auto',\n","          'lines.markeredgewidth': 1.0,\n","          'lines.markerfacecolor': 'auto',\n","          'lines.markersize': 6.0,\n","          'lines.scale_dashes': True,\n","          'lines.solid_capstyle': 'projecting',\n","          'lines.solid_joinstyle': 'round',\n","          'markers.fillstyle': 'full',\n","          'mathtext.bf': 'sans:bold',\n","          'mathtext.cal': 'cursive',\n","          'mathtext.default': 'it',\n","          'mathtext.fallback_to_cm': True,\n","          'mathtext.fontset': 'dejavusans',\n","          'mathtext.it': 'sans:italic',\n","          'mathtext.rm': 'sans',\n","          'mathtext.sf': 'sans',\n","          'mathtext.tt': 'monospace',\n","          'mpl_toolkits.legacy_colorbar': True,\n","          'patch.antialiased': True,\n","          'patch.edgecolor': 'black',\n","          'patch.facecolor': 'C0',\n","          'patch.force_edgecolor': False,\n","          'patch.linewidth': 1.0,\n","          'path.effects': [],\n","          'path.simplify': True,\n","          'path.simplify_threshold': 0.1111111111111111,\n","          'path.sketch': None,\n","          'path.snap': True,\n","          'pdf.compression': 6,\n","          'pdf.fonttype': 3,\n","          'pdf.inheritcolor': False,\n","          'pdf.use14corefonts': False,\n","          'pgf.preamble': '',\n","          'pgf.rcfonts': True,\n","          'pgf.texsystem': 'xelatex',\n","          'polaraxes.grid': True,\n","          'ps.distiller.res': 6000,\n","          'ps.fonttype': 3,\n","          'ps.papersize': 'letter',\n","          'ps.useafm': False,\n","          'ps.usedistiller': None,\n","          'savefig.bbox': None,\n","          'savefig.directory': '~',\n","          'savefig.dpi': 'figure',\n","          'savefig.edgecolor': 'white',\n","          'savefig.facecolor': 'white',\n","          'savefig.format': 'png',\n","          'savefig.frameon': True,\n","          'savefig.jpeg_quality': 95,\n","          'savefig.orientation': 'portrait',\n","          'savefig.pad_inches': 0.1,\n","          'savefig.transparent': False,\n","          'scatter.edgecolors': 'face',\n","          'scatter.marker': 'o',\n","          'svg.fonttype': 'path',\n","          'svg.hashsalt': None,\n","          'svg.image_inline': True,\n","          'text.antialiased': True,\n","          'text.color': 'black',\n","          'text.hinting': 'auto',\n","          'text.hinting_factor': 8,\n","          'text.kerning_factor': 0,\n","          'text.latex.preamble': '',\n","          'text.latex.preview': False,\n","          'text.latex.unicode': True,\n","          'text.usetex': False,\n","          'timezone': 'UTC',\n","          'tk.window_focus': False,\n","          'toolbar': 'toolbar2',\n","          'verbose.fileo': 'sys.stdout',\n","          'verbose.level': 'silent',\n","          'webagg.address': '127.0.0.1',\n","          'webagg.open_in_browser': True,\n","          'webagg.port': 8988,\n","          'webagg.port_retries': 50,\n","          'xtick.alignment': 'center',\n","          'xtick.bottom': True,\n","          'xtick.color': 'black',\n","          'xtick.direction': 'out',\n","          'xtick.labelbottom': True,\n","          'xtick.labelsize': 'medium',\n","          'xtick.labeltop': False,\n","          'xtick.major.bottom': True,\n","          'xtick.major.pad': 3.5,\n","          'xtick.major.size': 3.5,\n","          'xtick.major.top': True,\n","          'xtick.major.width': 0.8,\n","          'xtick.minor.bottom': True,\n","          'xtick.minor.pad': 3.4,\n","          'xtick.minor.size': 2.0,\n","          'xtick.minor.top': True,\n","          'xtick.minor.visible': False,\n","          'xtick.minor.width': 0.6,\n","          'xtick.top': False,\n","          'ytick.alignment': 'center_baseline',\n","          'ytick.color': 'black',\n","          'ytick.direction': 'out',\n","          'ytick.labelleft': True,\n","          'ytick.labelright': False,\n","          'ytick.labelsize': 'medium',\n","          'ytick.left': True,\n","          'ytick.major.left': True,\n","          'ytick.major.pad': 3.5,\n","          'ytick.major.right': True,\n","          'ytick.major.size': 3.5,\n","          'ytick.major.width': 0.8,\n","          'ytick.minor.left': True,\n","          'ytick.minor.pad': 3.4,\n","          'ytick.minor.right': True,\n","          'ytick.minor.size': 2.0,\n","          'ytick.minor.visible': False,\n","          'ytick.minor.width': 0.6,\n","          'ytick.right': False}))\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-135-3cf3dddcb8e4>\", line 10, in <module>\n","    print(i)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 840, in __repr__\n","    repr_split = pprint.pformat(dict(self), indent=1,\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 832, in __getitem__\n","    plt.switch_backend(rcsetup._auto_backend_sentinel)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 204, in switch_backend\n","    switch_backend(candidate)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 221, in switch_backend\n","    backend_mod = importlib.import_module(backend_name)\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n","  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_qt5agg.py\", line 11, in <module>\n","    from .backend_qt5 import (\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_qt5.py\", line 15, in <module>\n","    import matplotlib.backends.qt_editor.figureoptions as figureoptions\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/qt_editor/figureoptions.py\", line 12, in <module>\n","    from matplotlib.backends.qt_compat import QtGui\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/qt_compat.py\", line 163, in <module>\n","    _setup()\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/qt_compat.py\", line 70, in _setup_pyqt5\n","    from PyQt5 import QtCore, QtGui, QtWidgets\n","  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 951, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 894, in _find_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1157, in find_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1126, in _get_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1090, in _path_importer_cache\n","OSError: [Errno 107] Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.6/posixpath.py\", line 383, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"kFzNeIQ86E4j","executionInfo":{"status":"error","timestamp":1601489999223,"user_tz":240,"elapsed":442,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"31764236-9b6c-48c3-b8f5-e6b5545adf81","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import importlib\n","for k, v in sklearn.__dict__.items():\n","    print(k,\":\",v)\n","print('\\n###########\\n')\n","for k, v in plt.__dict__.items():\n","    print(k,\":\",v)\n","print('\\n###########\\n')\n","for k, v in lgb.__dict__.items():\n","    print(k,\":\",v)\n","print('\\n###########\\n')\n","print(sklearn.__version__)\n","print('\\n###########\\n')\n","print(importlib.find_loader('sklearn'))\n","print(importlib.util.find_spec('sklearn').loader)\n","print('\\n###########\\n')\n","for k, v in sklearn.preprocessing._data.__dict__.items():\n","    print(k,\":\",v)\n","print('\\n###########\\n')\n","for k, v in eval(sklearn.preprocessing._data.__name__.split('.')[0]).__dict__.items():\n","    print(k,\":\",v)\n","print('\\n###########\\n')\n","print(sklearn.show_versions())\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["__name__ : pathlib\n","__doc__ : None\n","__package__ : \n","__loader__ : <_frozen_importlib_external.SourceFileLoader object at 0x7f609e19f588>\n","__spec__ : ModuleSpec(name='pathlib', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f609e19f588>, origin='/usr/lib/python3.6/pathlib.py')\n","__file__ : /usr/lib/python3.6/pathlib.py\n","__cached__ : /usr/lib/python3.6/__pycache__/pathlib.cpython-36.pyc\n","__builtins__ : {'__name__': 'builtins', '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x7f6085a09048>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'Exception': <class 'Exception'>, 'TypeError': <class 'TypeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'GeneratorExit': <class 'GeneratorExit'>, 'SystemExit': <class 'SystemExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'ImportError': <class 'ImportError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'OSError': <class 'OSError'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'EOFError': <class 'EOFError'>, 'RuntimeError': <class 'RuntimeError'>, 'RecursionError': <class 'RecursionError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'NameError': <class 'NameError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'AttributeError': <class 'AttributeError'>, 'SyntaxError': <class 'SyntaxError'>, 'IndentationError': <class 'IndentationError'>, 'TabError': <class 'TabError'>, 'LookupError': <class 'LookupError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ValueError': <class 'ValueError'>, 'UnicodeError': <class 'UnicodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'AssertionError': <class 'AssertionError'>, 'ArithmeticError': <class 'ArithmeticError'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'SystemError': <class 'SystemError'>, 'ReferenceError': <class 'ReferenceError'>, 'BufferError': <class 'BufferError'>, 'MemoryError': <class 'MemoryError'>, 'Warning': <class 'Warning'>, 'UserWarning': <class 'UserWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'BytesWarning': <class 'BytesWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'ConnectionError': <class 'ConnectionError'>, 'BlockingIOError': <class 'BlockingIOError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'InterruptedError': <class 'InterruptedError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2019 Python Software Foundation.\n","All Rights Reserved.\n","\n","Copyright (c) 2000 BeOpen.com.\n","All Rights Reserved.\n","\n","Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n","All Rights Reserved.\n","\n","Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n","All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n","    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., '__IPYTHON__': True, 'display': <function display at 0x7f609e18ed90>, '__pybind11_internals_v3_gcc_libstdcpp_cxxabi1002__': <capsule object NULL at 0x7f6080fb0c30>, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7f6085a09160>>, 'dreload': <function _dreload at 0x7f60841b22f0>}\n","fnmatch : <module 'fnmatch' from '/usr/lib/python3.6/fnmatch.py'>\n","functools : <module 'functools' from '/usr/lib/python3.6/functools.py'>\n","io : <module 'io' from '/usr/lib/python3.6/io.py'>\n","ntpath : <module 'ntpath' from '/usr/lib/python3.6/ntpath.py'>\n","os : <module 'os' from '/usr/lib/python3.6/os.py'>\n","posixpath : <module 'posixpath' from '/usr/lib/python3.6/posixpath.py'>\n","re : <module 're' from '/usr/lib/python3.6/re.py'>\n","sys : <module 'sys' (built-in)>\n","Sequence : <class 'collections.abc.Sequence'>\n","contextmanager : <function contextmanager at 0x7f609fb16730>\n","EINVAL : 22\n","ENOENT : 2\n","ENOTDIR : 20\n","attrgetter : <class 'operator.attrgetter'>\n","S_ISDIR : <built-in function S_ISDIR>\n","S_ISLNK : <built-in function S_ISLNK>\n","S_ISREG : <built-in function S_ISREG>\n","S_ISSOCK : <built-in function S_ISSOCK>\n","S_ISBLK : <built-in function S_ISBLK>\n","S_ISCHR : <built-in function S_ISCHR>\n","S_ISFIFO : <built-in function S_ISFIFO>\n","urlquote_from_bytes : <function quote_from_bytes at 0x7f609e6d29d8>\n","supports_symlinks : True\n","nt : None\n","__all__ : ['PurePath', 'PurePosixPath', 'PureWindowsPath', 'Path', 'PosixPath', 'WindowsPath']\n","_is_wildcard_pattern : <function _is_wildcard_pattern at 0x7f609e1be0d0>\n","_Flavour : <class 'pathlib._Flavour'>\n","_WindowsFlavour : <class 'pathlib._WindowsFlavour'>\n","_PosixFlavour : <class 'pathlib._PosixFlavour'>\n","_windows_flavour : <pathlib._WindowsFlavour object at 0x7f609e1c5390>\n","_posix_flavour : <pathlib._PosixFlavour object at 0x7f609e1c53c8>\n","_Accessor : <class 'pathlib._Accessor'>\n","_NormalAccessor : <class 'pathlib._NormalAccessor'>\n","_normal_accessor : <pathlib._NormalAccessor object at 0x7f609e1c5eb8>\n","_make_selector : <functools._lru_cache_wrapper object at 0x7f609e1cc240>\n","_Selector : <class 'pathlib._Selector'>\n","_TerminatingSelector : <class 'pathlib._TerminatingSelector'>\n","_PreciseSelector : <class 'pathlib._PreciseSelector'>\n","_WildcardSelector : <class 'pathlib._WildcardSelector'>\n","_RecursiveWildcardSelector : <class 'pathlib._RecursiveWildcardSelector'>\n","_PathParents : <class 'pathlib._PathParents'>\n","PurePath : <class 'pathlib.PurePath'>\n","PurePosixPath : <class 'pathlib.PurePosixPath'>\n","PureWindowsPath : <class 'pathlib.PureWindowsPath'>\n","Path : <class 'pathlib.Path'>\n","PosixPath : <class 'pathlib.PosixPath'>\n","WindowsPath : <class 'pathlib.WindowsPath'>\n","\n","###########\n","\n","__name__ : sklearn\n","__doc__ : \n","Machine learning module for Python\n","==================================\n","\n","sklearn is a Python module integrating classical machine\n","learning algorithms in the tightly-knit world of scientific Python\n","packages (numpy, scipy, matplotlib).\n","\n","It aims to provide simple and efficient solutions to learning problems\n","that are accessible to everybody and reusable in various contexts:\n","machine-learning as a versatile tool for science and engineering.\n","\n","See http://scikit-learn.org for complete documentation.\n","\n","__package__ : sklearn\n","__loader__ : <_frozen_importlib_external.SourceFileLoader object at 0x7f6080012048>\n","__spec__ : ModuleSpec(name='sklearn', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f6080012048>, origin='/usr/local/lib/python3.6/dist-packages/sklearn/__init__.py', submodule_search_locations=['/usr/local/lib/python3.6/dist-packages/sklearn'])\n","__path__ : ['/usr/local/lib/python3.6/dist-packages/sklearn']\n","__file__ : /usr/local/lib/python3.6/dist-packages/sklearn/__init__.py\n","__cached__ : /usr/local/lib/python3.6/dist-packages/sklearn/__pycache__/__init__.cpython-36.pyc\n","__builtins__ : {'__name__': 'builtins', '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x7f6085a09048>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'Exception': <class 'Exception'>, 'TypeError': <class 'TypeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'GeneratorExit': <class 'GeneratorExit'>, 'SystemExit': <class 'SystemExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'ImportError': <class 'ImportError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'OSError': <class 'OSError'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'EOFError': <class 'EOFError'>, 'RuntimeError': <class 'RuntimeError'>, 'RecursionError': <class 'RecursionError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'NameError': <class 'NameError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'AttributeError': <class 'AttributeError'>, 'SyntaxError': <class 'SyntaxError'>, 'IndentationError': <class 'IndentationError'>, 'TabError': <class 'TabError'>, 'LookupError': <class 'LookupError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ValueError': <class 'ValueError'>, 'UnicodeError': <class 'UnicodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'AssertionError': <class 'AssertionError'>, 'ArithmeticError': <class 'ArithmeticError'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'SystemError': <class 'SystemError'>, 'ReferenceError': <class 'ReferenceError'>, 'BufferError': <class 'BufferError'>, 'MemoryError': <class 'MemoryError'>, 'Warning': <class 'Warning'>, 'UserWarning': <class 'UserWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'BytesWarning': <class 'BytesWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'ConnectionError': <class 'ConnectionError'>, 'BlockingIOError': <class 'BlockingIOError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'InterruptedError': <class 'InterruptedError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2019 Python Software Foundation.\n","All Rights Reserved.\n","\n","Copyright (c) 2000 BeOpen.com.\n","All Rights Reserved.\n","\n","Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n","All Rights Reserved.\n","\n","Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n","All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n","    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., '__IPYTHON__': True, 'display': <function display at 0x7f609e18ed90>, '__pybind11_internals_v3_gcc_libstdcpp_cxxabi1002__': <capsule object NULL at 0x7f6080fb0c30>, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7f6085a09160>>, 'dreload': <function _dreload at 0x7f60841b22f0>}\n","sys : <module 'sys' (built-in)>\n","re : <module 're' from '/usr/lib/python3.6/re.py'>\n","logging : <module 'logging' from '/usr/lib/python3.6/logging/__init__.py'>\n","os : <module 'os' from '/usr/lib/python3.6/os.py'>\n","_config : <module 'sklearn._config' from '/usr/local/lib/python3.6/dist-packages/sklearn/_config.py'>\n","get_config : <function get_config at 0x7f6080021ea0>\n","set_config : <function set_config at 0x7f6080025048>\n","config_context : <function config_context at 0x7f6080025158>\n","logger : <Logger sklearn (INFO)>\n","__version__ : 0.22.2.post1\n","__SKLEARN_SETUP__ : False\n","_distributor_init : <module 'sklearn._distributor_init' from '/usr/local/lib/python3.6/dist-packages/sklearn/_distributor_init.py'>\n","__check_build : <module 'sklearn.__check_build' from '/usr/local/lib/python3.6/dist-packages/sklearn/__check_build/__init__.py'>\n","exceptions : <module 'sklearn.exceptions' from '/usr/local/lib/python3.6/dist-packages/sklearn/exceptions.py'>\n","externals : <module 'sklearn.externals' from '/usr/local/lib/python3.6/dist-packages/sklearn/externals/__init__.py'>\n","utils : <module 'sklearn.utils' from '/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py'>\n","base : <module 'sklearn.base' from '/usr/local/lib/python3.6/dist-packages/sklearn/base.py'>\n","clone : <function clone at 0x7f6080025378>\n","show_versions : <function show_versions at 0x7f607705e8c8>\n","__all__ : ['calibration', 'cluster', 'covariance', 'cross_decomposition', 'datasets', 'decomposition', 'dummy', 'ensemble', 'exceptions', 'experimental', 'externals', 'feature_extraction', 'feature_selection', 'gaussian_process', 'inspection', 'isotonic', 'kernel_approximation', 'kernel_ridge', 'linear_model', 'manifold', 'metrics', 'mixture', 'model_selection', 'multiclass', 'multioutput', 'naive_bayes', 'neighbors', 'neural_network', 'pipeline', 'preprocessing', 'random_projection', 'semi_supervised', 'svm', 'tree', 'discriminant_analysis', 'impute', 'compose', 'clone', 'get_config', 'set_config', 'config_context', 'show_versions']\n","setup_module : <function setup_module at 0x7f60800251e0>\n","preprocessing : <module 'sklearn.preprocessing' from '/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/__init__.py'>\n","metrics : <module 'sklearn.metrics' from '/usr/local/lib/python3.6/dist-packages/sklearn/metrics/__init__.py'>\n","model_selection : <module 'sklearn.model_selection' from '/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/__init__.py'>\n","experimental : <module 'sklearn.experimental' from '/usr/local/lib/python3.6/dist-packages/sklearn/experimental/__init__.py'>\n","svm : <module 'sklearn.svm' from '/usr/local/lib/python3.6/dist-packages/sklearn/svm/__init__.py'>\n","linear_model : <module 'sklearn.linear_model' from '/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/__init__.py'>\n","decomposition : <module 'sklearn.decomposition' from '/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/__init__.py'>\n","neighbors : <module 'sklearn.neighbors' from '/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/__init__.py'>\n","tree : <module 'sklearn.tree' from '/usr/local/lib/python3.6/dist-packages/sklearn/tree/__init__.py'>\n","dummy : <module 'sklearn.dummy' from '/usr/local/lib/python3.6/dist-packages/sklearn/dummy.py'>\n","ensemble : <module 'sklearn.ensemble' from '/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/__init__.py'>\n","\n","###########\n","\n","__name__ : matplotlib.pyplot\n","__doc__ : \n","`matplotlib.pyplot` is a state-based interface to matplotlib. It provides\n","a MATLAB-like way of plotting.\n","\n","pyplot is mainly intended for interactive plots and simple cases of\n","programmatic plot generation::\n","\n","    import numpy as np\n","    import matplotlib.pyplot as plt\n","\n","    x = np.arange(0, 5, 0.1)\n","    y = np.sin(x)\n","    plt.plot(x, y)\n","\n","The object-oriented API is recommended for more complex plots.\n","\n","__package__ : matplotlib\n","__loader__ : <_frozen_importlib_external.SourceFileLoader object at 0x7f6083f7a518>\n","__spec__ : ModuleSpec(name='matplotlib.pyplot', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f6083f7a518>, origin='/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py')\n","__file__ : /usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\n","__cached__ : /usr/local/lib/python3.6/dist-packages/matplotlib/__pycache__/pyplot.cpython-36.pyc\n","__builtins__ : {'__name__': 'builtins', '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x7f6085a09048>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'Exception': <class 'Exception'>, 'TypeError': <class 'TypeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'GeneratorExit': <class 'GeneratorExit'>, 'SystemExit': <class 'SystemExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'ImportError': <class 'ImportError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'OSError': <class 'OSError'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'EOFError': <class 'EOFError'>, 'RuntimeError': <class 'RuntimeError'>, 'RecursionError': <class 'RecursionError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'NameError': <class 'NameError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'AttributeError': <class 'AttributeError'>, 'SyntaxError': <class 'SyntaxError'>, 'IndentationError': <class 'IndentationError'>, 'TabError': <class 'TabError'>, 'LookupError': <class 'LookupError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ValueError': <class 'ValueError'>, 'UnicodeError': <class 'UnicodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'AssertionError': <class 'AssertionError'>, 'ArithmeticError': <class 'ArithmeticError'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'SystemError': <class 'SystemError'>, 'ReferenceError': <class 'ReferenceError'>, 'BufferError': <class 'BufferError'>, 'MemoryError': <class 'MemoryError'>, 'Warning': <class 'Warning'>, 'UserWarning': <class 'UserWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'BytesWarning': <class 'BytesWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'ConnectionError': <class 'ConnectionError'>, 'BlockingIOError': <class 'BlockingIOError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'InterruptedError': <class 'InterruptedError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2019 Python Software Foundation.\n","All Rights Reserved.\n","\n","Copyright (c) 2000 BeOpen.com.\n","All Rights Reserved.\n","\n","Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n","All Rights Reserved.\n","\n","Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n","All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n","    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., '__IPYTHON__': True, 'display': <function display at 0x7f609e18ed90>, '__pybind11_internals_v3_gcc_libstdcpp_cxxabi1002__': <capsule object NULL at 0x7f6080fb0c30>, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7f6085a09160>>, 'dreload': <function _dreload at 0x7f60841b22f0>}\n","functools : <module 'functools' from '/usr/lib/python3.6/functools.py'>\n","importlib : <module 'importlib' from '/usr/lib/python3.6/importlib/__init__.py'>\n","inspect : <module 'inspect' from '/usr/lib/python3.6/inspect.py'>\n","logging : <module 'logging' from '/usr/lib/python3.6/logging/__init__.py'>\n","Number : <class 'numbers.Number'>\n","re : <module 're' from '/usr/lib/python3.6/re.py'>\n","sys : <module 'sys' (built-in)>\n","time : <module 'time' (built-in)>\n","cycler : <function cycler at 0x7f6083fd9158>\n","matplotlib : <module 'matplotlib' from '/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py'>\n","rcsetup : <module 'matplotlib.rcsetup' from '/usr/local/lib/python3.6/dist-packages/matplotlib/rcsetup.py'>\n","style : <module 'matplotlib.style' from '/usr/local/lib/python3.6/dist-packages/matplotlib/style/__init__.py'>\n","_pylab_helpers : <module 'matplotlib._pylab_helpers' from '/usr/local/lib/python3.6/dist-packages/matplotlib/_pylab_helpers.py'>\n","interactive : <function interactive at 0x7f6083f7eae8>\n","cbook : <module 'matplotlib.cbook' from '/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py'>\n","dedent : <function dedent at 0x7f6084198d08>\n","deprecated : <function deprecated at 0x7f6084189598>\n","silent_list : <class 'matplotlib.cbook.silent_list'>\n","warn_deprecated : <function warn_deprecated at 0x7f6084189510>\n","docstring : <module 'matplotlib.docstring' from '/usr/local/lib/python3.6/dist-packages/matplotlib/docstring.py'>\n","FigureCanvasBase : <class 'matplotlib.backend_bases.FigureCanvasBase'>\n","Figure : <class 'matplotlib.figure.Figure'>\n","figaspect : <function figaspect at 0x7f6082e9f268>\n","GridSpec : <class 'matplotlib.gridspec.GridSpec'>\n","rcParams : _internal.classic_mode: False\n","agg.path.chunksize: 0\n","animation.avconv_args: []\n","animation.avconv_path: avconv\n","animation.bitrate: -1\n","animation.codec: h264\n","animation.convert_args: []\n","animation.convert_path: convert\n","animation.embed_limit: 20.0\n","animation.ffmpeg_args: []\n","animation.ffmpeg_path: ffmpeg\n","animation.frame_format: png\n","animation.html: none\n","animation.html_args: []\n","animation.writer: ffmpeg\n","axes.autolimit_mode: data\n","axes.axisbelow: line\n","axes.edgecolor: black\n","axes.facecolor: white\n","axes.formatter.limits: [-5, 6]\n","axes.formatter.min_exponent: 0\n","axes.formatter.offset_threshold: 4\n","axes.formatter.use_locale: False\n","axes.formatter.use_mathtext: False\n","axes.formatter.useoffset: True\n","axes.grid: False\n","axes.grid.axis: both\n","axes.grid.which: major\n","axes.labelcolor: black\n","axes.labelpad: 4.0\n","axes.labelsize: medium\n","axes.labelweight: normal\n","axes.linewidth: 0.8\n","axes.prop_cycle: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n","axes.spines.bottom: True\n","axes.spines.left: True\n","axes.spines.right: True\n","axes.spines.top: True\n","axes.titlecolor: auto\n","axes.titlelocation: center\n","axes.titlepad: 6.0\n","axes.titlesize: large\n","axes.titleweight: normal\n","axes.unicode_minus: True\n","axes.xmargin: 0.05\n","axes.ymargin: 0.05\n","axes3d.grid: True\n","backend: module://ipykernel.pylab.backend_inline\n","backend_fallback: True\n","boxplot.bootstrap: None\n","boxplot.boxprops.color: black\n","boxplot.boxprops.linestyle: -\n","boxplot.boxprops.linewidth: 1.0\n","boxplot.capprops.color: black\n","boxplot.capprops.linestyle: -\n","boxplot.capprops.linewidth: 1.0\n","boxplot.flierprops.color: black\n","boxplot.flierprops.linestyle: none\n","boxplot.flierprops.linewidth: 1.0\n","boxplot.flierprops.marker: o\n","boxplot.flierprops.markeredgecolor: black\n","boxplot.flierprops.markeredgewidth: 1.0\n","boxplot.flierprops.markerfacecolor: none\n","boxplot.flierprops.markersize: 6.0\n","boxplot.meanline: False\n","boxplot.meanprops.color: C2\n","boxplot.meanprops.linestyle: --\n","boxplot.meanprops.linewidth: 1.0\n","boxplot.meanprops.marker: ^\n","boxplot.meanprops.markeredgecolor: C2\n","boxplot.meanprops.markerfacecolor: C2\n","boxplot.meanprops.markersize: 6.0\n","boxplot.medianprops.color: C1\n","boxplot.medianprops.linestyle: -\n","boxplot.medianprops.linewidth: 1.0\n","boxplot.notch: False\n","boxplot.patchartist: False\n","boxplot.showbox: True\n","boxplot.showcaps: True\n","boxplot.showfliers: True\n","boxplot.showmeans: False\n","boxplot.vertical: True\n","boxplot.whiskerprops.color: black\n","boxplot.whiskerprops.linestyle: -\n","boxplot.whiskerprops.linewidth: 1.0\n","boxplot.whiskers: 1.5\n","contour.corner_mask: True\n","contour.negative_linestyle: dashed\n","datapath: /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data\n","date.autoformatter.day: %Y-%m-%d\n","date.autoformatter.hour: %m-%d %H\n","date.autoformatter.microsecond: %M:%S.%f\n","date.autoformatter.minute: %d %H:%M\n","date.autoformatter.month: %Y-%m\n","date.autoformatter.second: %H:%M:%S\n","date.autoformatter.year: %Y\n","docstring.hardcopy: False\n","errorbar.capsize: 0.0\n","figure.autolayout: False\n","figure.constrained_layout.h_pad: 0.04167\n","figure.constrained_layout.hspace: 0.02\n","figure.constrained_layout.use: False\n","figure.constrained_layout.w_pad: 0.04167\n","figure.constrained_layout.wspace: 0.02\n","figure.dpi: 72.0\n","figure.edgecolor: (1, 1, 1, 0)\n","figure.facecolor: (1, 1, 1, 0)\n","figure.figsize: [6.0, 4.0]\n","figure.frameon: True\n","figure.max_open_warning: 20\n","figure.subplot.bottom: 0.125\n","figure.subplot.hspace: 0.2\n","figure.subplot.left: 0.125\n","figure.subplot.right: 0.9\n","figure.subplot.top: 0.88\n","figure.subplot.wspace: 0.2\n","figure.titlesize: large\n","figure.titleweight: normal\n","font.cursive: ['Apple Chancery', 'Textile', 'Zapf Chancery', 'Sand', 'Script MT', 'Felipa', 'cursive']\n","font.family: ['sans-serif']\n","font.fantasy: ['Comic Neue', 'Comic Sans MS', 'Chicago', 'Charcoal', 'Impact', 'Western', 'Humor Sans', 'xkcd', 'fantasy']\n","font.monospace: ['DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Computer Modern Typewriter', 'Andale Mono', 'Nimbus Mono L', 'Courier New', 'Courier', 'Fixed', 'Terminal', 'monospace']\n","font.sans-serif: ['DejaVu Sans', 'Bitstream Vera Sans', 'Computer Modern Sans Serif', 'Lucida Grande', 'Verdana', 'Geneva', 'Lucid', 'Arial', 'Helvetica', 'Avant Garde', 'sans-serif']\n","font.serif: ['DejaVu Serif', 'Bitstream Vera Serif', 'Computer Modern Roman', 'New Century Schoolbook', 'Century Schoolbook L', 'Utopia', 'ITC Bookman', 'Bookman', 'Nimbus Roman No9 L', 'Times New Roman', 'Times', 'Palatino', 'Charter', 'serif']\n","font.size: 10.0\n","font.stretch: normal\n","font.style: normal\n","font.variant: normal\n","font.weight: normal\n","grid.alpha: 1.0\n","grid.color: #b0b0b0\n","grid.linestyle: -\n","grid.linewidth: 0.8\n","hatch.color: black\n","hatch.linewidth: 1.0\n","hist.bins: 10\n","image.aspect: equal\n","image.cmap: viridis\n","image.composite_image: True\n","image.interpolation: antialiased\n","image.lut: 256\n","image.origin: upper\n","image.resample: True\n","interactive: True\n","keymap.all_axes: ['a']\n","keymap.back: ['left', 'c', 'backspace', 'MouseButton.BACK']\n","keymap.copy: ['ctrl+c', 'cmd+c']\n","keymap.forward: ['right', 'v', 'MouseButton.FORWARD']\n","keymap.fullscreen: ['f', 'ctrl+f']\n","keymap.grid: ['g']\n","keymap.grid_minor: ['G']\n","keymap.help: ['f1']\n","keymap.home: ['h', 'r', 'home']\n","keymap.pan: ['p']\n","keymap.quit: ['ctrl+w', 'cmd+w', 'q']\n","keymap.quit_all: ['W', 'cmd+W', 'Q']\n","keymap.save: ['s', 'ctrl+s']\n","keymap.xscale: ['k', 'L']\n","keymap.yscale: ['l']\n","keymap.zoom: ['o']\n","legend.borderaxespad: 0.5\n","legend.borderpad: 0.4\n","legend.columnspacing: 2.0\n","legend.edgecolor: 0.8\n","legend.facecolor: inherit\n","legend.fancybox: True\n","legend.fontsize: medium\n","legend.framealpha: 0.8\n","legend.frameon: True\n","legend.handleheight: 0.7\n","legend.handlelength: 2.0\n","legend.handletextpad: 0.8\n","legend.labelspacing: 0.5\n","legend.loc: best\n","legend.markerscale: 1.0\n","legend.numpoints: 1\n","legend.scatterpoints: 1\n","legend.shadow: False\n","legend.title_fontsize: None\n","lines.antialiased: True\n","lines.color: C0\n","lines.dash_capstyle: butt\n","lines.dash_joinstyle: round\n","lines.dashdot_pattern: [6.4, 1.6, 1.0, 1.6]\n","lines.dashed_pattern: [3.7, 1.6]\n","lines.dotted_pattern: [1.0, 1.65]\n","lines.linestyle: -\n","lines.linewidth: 1.5\n","lines.marker: None\n","lines.markeredgecolor: auto\n","lines.markeredgewidth: 1.0\n","lines.markerfacecolor: auto\n","lines.markersize: 6.0\n","lines.scale_dashes: True\n","lines.solid_capstyle: projecting\n","lines.solid_joinstyle: round\n","markers.fillstyle: full\n","mathtext.bf: sans:bold\n","mathtext.cal: cursive\n","mathtext.default: it\n","mathtext.fallback_to_cm: True\n","mathtext.fontset: dejavusans\n","mathtext.it: sans:italic\n","mathtext.rm: sans\n","mathtext.sf: sans\n","mathtext.tt: monospace\n","mpl_toolkits.legacy_colorbar: True\n","patch.antialiased: True\n","patch.edgecolor: black\n","patch.facecolor: C0\n","patch.force_edgecolor: False\n","patch.linewidth: 1.0\n","path.effects: []\n","path.simplify: True\n","path.simplify_threshold: 0.1111111111111111\n","path.sketch: None\n","path.snap: True\n","pdf.compression: 6\n","pdf.fonttype: 3\n","pdf.inheritcolor: False\n","pdf.use14corefonts: False\n","pgf.preamble: \n","pgf.rcfonts: True\n","pgf.texsystem: xelatex\n","polaraxes.grid: True\n","ps.distiller.res: 6000\n","ps.fonttype: 3\n","ps.papersize: letter\n","ps.useafm: False\n","ps.usedistiller: None\n","savefig.bbox: None\n","savefig.directory: ~\n","savefig.dpi: figure\n","savefig.edgecolor: white\n","savefig.facecolor: white\n","savefig.format: png\n","savefig.frameon: True\n","savefig.jpeg_quality: 95\n","savefig.orientation: portrait\n","savefig.pad_inches: 0.1\n","savefig.transparent: False\n","scatter.edgecolors: face\n","scatter.marker: o\n","svg.fonttype: path\n","svg.hashsalt: None\n","svg.image_inline: True\n","text.antialiased: True\n","text.color: black\n","text.hinting: auto\n","text.hinting_factor: 8\n","text.kerning_factor: 0\n","text.latex.preamble: \n","text.latex.preview: False\n","text.latex.unicode: True\n","text.usetex: False\n","timezone: UTC\n","tk.window_focus: False\n","toolbar: toolbar2\n","verbose.fileo: sys.stdout\n","verbose.level: silent\n","webagg.address: 127.0.0.1\n","webagg.open_in_browser: True\n","webagg.port: 8988\n","webagg.port_retries: 50\n","xtick.alignment: center\n","xtick.bottom: True\n","xtick.color: black\n","xtick.direction: out\n","xtick.labelbottom: True\n","xtick.labelsize: medium\n","xtick.labeltop: False\n","xtick.major.bottom: True\n","xtick.major.pad: 3.5\n","xtick.major.size: 3.5\n","xtick.major.top: True\n","xtick.major.width: 0.8\n","xtick.minor.bottom: True\n","xtick.minor.pad: 3.4\n","xtick.minor.size: 2.0\n","xtick.minor.top: True\n","xtick.minor.visible: False\n","xtick.minor.width: 0.6\n","xtick.top: False\n","ytick.alignment: center_baseline\n","ytick.color: black\n","ytick.direction: out\n","ytick.labelleft: True\n","ytick.labelright: False\n","ytick.labelsize: medium\n","ytick.left: True\n","ytick.major.left: True\n","ytick.major.pad: 3.5\n","ytick.major.right: True\n","ytick.major.size: 3.5\n","ytick.major.width: 0.8\n","ytick.minor.left: True\n","ytick.minor.pad: 3.4\n","ytick.minor.right: True\n","ytick.minor.size: 2.0\n","ytick.minor.visible: False\n","ytick.minor.width: 0.6\n","ytick.right: False\n","rcParamsDefault : _internal.classic_mode: False\n","agg.path.chunksize: 0\n","animation.avconv_args: []\n","animation.avconv_path: avconv\n","animation.bitrate: -1\n","animation.codec: h264\n","animation.convert_args: []\n","animation.convert_path: convert\n","animation.embed_limit: 20.0\n","animation.ffmpeg_args: []\n","animation.ffmpeg_path: ffmpeg\n","animation.frame_format: png\n","animation.html: none\n","animation.html_args: []\n","animation.writer: ffmpeg\n","axes.autolimit_mode: data\n","axes.axisbelow: line\n","axes.edgecolor: black\n","axes.facecolor: white\n","axes.formatter.limits: [-5, 6]\n","axes.formatter.min_exponent: 0\n","axes.formatter.offset_threshold: 4\n","axes.formatter.use_locale: False\n","axes.formatter.use_mathtext: False\n","axes.formatter.useoffset: True\n","axes.grid: False\n","axes.grid.axis: both\n","axes.grid.which: major\n","axes.labelcolor: black\n","axes.labelpad: 4.0\n","axes.labelsize: medium\n","axes.labelweight: normal\n","axes.linewidth: 0.8\n","axes.prop_cycle: cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n","axes.spines.bottom: True\n","axes.spines.left: True\n","axes.spines.right: True\n","axes.spines.top: True\n","axes.titlecolor: auto\n","axes.titlelocation: center\n","axes.titlepad: 6.0\n","axes.titlesize: large\n","axes.titleweight: normal\n","axes.unicode_minus: True\n","axes.xmargin: 0.05\n","axes.ymargin: 0.05\n","axes3d.grid: True\n","backend: module://ipykernel.pylab.backend_inline\n","backend_fallback: True\n","boxplot.bootstrap: None\n","boxplot.boxprops.color: black\n","boxplot.boxprops.linestyle: -\n","boxplot.boxprops.linewidth: 1.0\n","boxplot.capprops.color: black\n","boxplot.capprops.linestyle: -\n","boxplot.capprops.linewidth: 1.0\n","boxplot.flierprops.color: black\n","boxplot.flierprops.linestyle: none\n","boxplot.flierprops.linewidth: 1.0\n","boxplot.flierprops.marker: o\n","boxplot.flierprops.markeredgecolor: black\n","boxplot.flierprops.markeredgewidth: 1.0\n","boxplot.flierprops.markerfacecolor: none\n","boxplot.flierprops.markersize: 6.0\n","boxplot.meanline: False\n","boxplot.meanprops.color: C2\n","boxplot.meanprops.linestyle: --\n","boxplot.meanprops.linewidth: 1.0\n","boxplot.meanprops.marker: ^\n","boxplot.meanprops.markeredgecolor: C2\n","boxplot.meanprops.markerfacecolor: C2\n","boxplot.meanprops.markersize: 6.0\n","boxplot.medianprops.color: C1\n","boxplot.medianprops.linestyle: -\n","boxplot.medianprops.linewidth: 1.0\n","boxplot.notch: False\n","boxplot.patchartist: False\n","boxplot.showbox: True\n","boxplot.showcaps: True\n","boxplot.showfliers: True\n","boxplot.showmeans: False\n","boxplot.vertical: True\n","boxplot.whiskerprops.color: black\n","boxplot.whiskerprops.linestyle: -\n","boxplot.whiskerprops.linewidth: 1.0\n","boxplot.whiskers: 1.5\n","contour.corner_mask: True\n","contour.negative_linestyle: dashed\n","datapath: /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data\n","date.autoformatter.day: %Y-%m-%d\n","date.autoformatter.hour: %m-%d %H\n","date.autoformatter.microsecond: %M:%S.%f\n","date.autoformatter.minute: %d %H:%M\n","date.autoformatter.month: %Y-%m\n","date.autoformatter.second: %H:%M:%S\n","date.autoformatter.year: %Y\n","docstring.hardcopy: False\n","errorbar.capsize: 0.0\n","figure.autolayout: False\n","figure.constrained_layout.h_pad: 0.04167\n","figure.constrained_layout.hspace: 0.02\n","figure.constrained_layout.use: False\n","figure.constrained_layout.w_pad: 0.04167\n","figure.constrained_layout.wspace: 0.02\n","figure.dpi: 100.0\n","figure.edgecolor: white\n","figure.facecolor: white\n","figure.figsize: [6.4, 4.8]\n","figure.frameon: True\n","figure.max_open_warning: 20\n","figure.subplot.bottom: 0.11\n","figure.subplot.hspace: 0.2\n","figure.subplot.left: 0.125\n","figure.subplot.right: 0.9\n","figure.subplot.top: 0.88\n","figure.subplot.wspace: 0.2\n","figure.titlesize: large\n","figure.titleweight: normal\n","font.cursive: ['Apple Chancery', 'Textile', 'Zapf Chancery', 'Sand', 'Script MT', 'Felipa', 'cursive']\n","font.family: ['sans-serif']\n","font.fantasy: ['Comic Neue', 'Comic Sans MS', 'Chicago', 'Charcoal', 'Impact', 'Western', 'Humor Sans', 'xkcd', 'fantasy']\n","font.monospace: ['DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Computer Modern Typewriter', 'Andale Mono', 'Nimbus Mono L', 'Courier New', 'Courier', 'Fixed', 'Terminal', 'monospace']\n","font.sans-serif: ['DejaVu Sans', 'Bitstream Vera Sans', 'Computer Modern Sans Serif', 'Lucida Grande', 'Verdana', 'Geneva', 'Lucid', 'Arial', 'Helvetica', 'Avant Garde', 'sans-serif']\n","font.serif: ['DejaVu Serif', 'Bitstream Vera Serif', 'Computer Modern Roman', 'New Century Schoolbook', 'Century Schoolbook L', 'Utopia', 'ITC Bookman', 'Bookman', 'Nimbus Roman No9 L', 'Times New Roman', 'Times', 'Palatino', 'Charter', 'serif']\n","font.size: 10.0\n","font.stretch: normal\n","font.style: normal\n","font.variant: normal\n","font.weight: normal\n","grid.alpha: 1.0\n","grid.color: #b0b0b0\n","grid.linestyle: -\n","grid.linewidth: 0.8\n","hatch.color: black\n","hatch.linewidth: 1.0\n","hist.bins: 10\n","image.aspect: equal\n","image.cmap: viridis\n","image.composite_image: True\n","image.interpolation: antialiased\n","image.lut: 256\n","image.origin: upper\n","image.resample: True\n","interactive: False\n","keymap.all_axes: ['a']\n","keymap.back: ['left', 'c', 'backspace', 'MouseButton.BACK']\n","keymap.copy: ['ctrl+c', 'cmd+c']\n","keymap.forward: ['right', 'v', 'MouseButton.FORWARD']\n","keymap.fullscreen: ['f', 'ctrl+f']\n","keymap.grid: ['g']\n","keymap.grid_minor: ['G']\n","keymap.help: ['f1']\n","keymap.home: ['h', 'r', 'home']\n","keymap.pan: ['p']\n","keymap.quit: ['ctrl+w', 'cmd+w', 'q']\n","keymap.quit_all: ['W', 'cmd+W', 'Q']\n","keymap.save: ['s', 'ctrl+s']\n","keymap.xscale: ['k', 'L']\n","keymap.yscale: ['l']\n","keymap.zoom: ['o']\n","legend.borderaxespad: 0.5\n","legend.borderpad: 0.4\n","legend.columnspacing: 2.0\n","legend.edgecolor: 0.8\n","legend.facecolor: inherit\n","legend.fancybox: True\n","legend.fontsize: medium\n","legend.framealpha: 0.8\n","legend.frameon: True\n","legend.handleheight: 0.7\n","legend.handlelength: 2.0\n","legend.handletextpad: 0.8\n","legend.labelspacing: 0.5\n","legend.loc: best\n","legend.markerscale: 1.0\n","legend.numpoints: 1\n","legend.scatterpoints: 1\n","legend.shadow: False\n","legend.title_fontsize: None\n","lines.antialiased: True\n","lines.color: C0\n","lines.dash_capstyle: butt\n","lines.dash_joinstyle: round\n","lines.dashdot_pattern: [6.4, 1.6, 1.0, 1.6]\n","lines.dashed_pattern: [3.7, 1.6]\n","lines.dotted_pattern: [1.0, 1.65]\n","lines.linestyle: -\n","lines.linewidth: 1.5\n","lines.marker: None\n","lines.markeredgecolor: auto\n","lines.markeredgewidth: 1.0\n","lines.markerfacecolor: auto\n","lines.markersize: 6.0\n","lines.scale_dashes: True\n","lines.solid_capstyle: projecting\n","lines.solid_joinstyle: round\n","markers.fillstyle: full\n","mathtext.bf: sans:bold\n","mathtext.cal: cursive\n","mathtext.default: it\n","mathtext.fallback_to_cm: True\n","mathtext.fontset: dejavusans\n","mathtext.it: sans:italic\n","mathtext.rm: sans\n","mathtext.sf: sans\n","mathtext.tt: monospace\n","mpl_toolkits.legacy_colorbar: True\n","patch.antialiased: True\n","patch.edgecolor: black\n","patch.facecolor: C0\n","patch.force_edgecolor: False\n","patch.linewidth: 1.0\n","path.effects: []\n","path.simplify: True\n","path.simplify_threshold: 0.1111111111111111\n","path.sketch: None\n","path.snap: True\n","pdf.compression: 6\n","pdf.fonttype: 3\n","pdf.inheritcolor: False\n","pdf.use14corefonts: False\n","pgf.preamble: \n","pgf.rcfonts: True\n","pgf.texsystem: xelatex\n","polaraxes.grid: True\n","ps.distiller.res: 6000\n","ps.fonttype: 3\n","ps.papersize: letter\n","ps.useafm: False\n","ps.usedistiller: None\n","savefig.bbox: None\n","savefig.directory: ~\n","savefig.dpi: figure\n","savefig.edgecolor: white\n","savefig.facecolor: white\n","savefig.format: png\n","savefig.frameon: True\n","savefig.jpeg_quality: 95\n","savefig.orientation: portrait\n","savefig.pad_inches: 0.1\n","savefig.transparent: False\n","scatter.edgecolors: face\n","scatter.marker: o\n","svg.fonttype: path\n","svg.hashsalt: None\n","svg.image_inline: True\n","text.antialiased: True\n","text.color: black\n","text.hinting: auto\n","text.hinting_factor: 8\n","text.kerning_factor: 0\n","text.latex.preamble: \n","text.latex.preview: False\n","text.latex.unicode: True\n","text.usetex: False\n","timezone: UTC\n","tk.window_focus: False\n","toolbar: toolbar2\n","verbose.fileo: sys.stdout\n","verbose.level: silent\n","webagg.address: 127.0.0.1\n","webagg.open_in_browser: True\n","webagg.port: 8988\n","webagg.port_retries: 50\n","xtick.alignment: center\n","xtick.bottom: True\n","xtick.color: black\n","xtick.direction: out\n","xtick.labelbottom: True\n","xtick.labelsize: medium\n","xtick.labeltop: False\n","xtick.major.bottom: True\n","xtick.major.pad: 3.5\n","xtick.major.size: 3.5\n","xtick.major.top: True\n","xtick.major.width: 0.8\n","xtick.minor.bottom: True\n","xtick.minor.pad: 3.4\n","xtick.minor.size: 2.0\n","xtick.minor.top: True\n","xtick.minor.visible: False\n","xtick.minor.width: 0.6\n","xtick.top: False\n","ytick.alignment: center_baseline\n","ytick.color: black\n","ytick.direction: out\n","ytick.labelleft: True\n","ytick.labelright: False\n","ytick.labelsize: medium\n","ytick.left: True\n","ytick.major.left: True\n","ytick.major.pad: 3.5\n","ytick.major.right: True\n","ytick.major.size: 3.5\n","ytick.major.width: 0.8\n","ytick.minor.left: True\n","ytick.minor.pad: 3.4\n","ytick.minor.right: True\n","ytick.minor.size: 2.0\n","ytick.minor.visible: False\n","ytick.minor.width: 0.6\n","ytick.right: False\n","get_backend : <function get_backend at 0x7f6083f7ea60>\n","rcParamsOrig : Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-131-cb6d388c8b1b>\", line 10, in <module>\n","    print(k,\":\",v)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 846, in __str__\n","    return '\\n'.join(map('{0[0]}: {0[1]}'.format, sorted(self.items())))\n","  File \"/usr/lib/python3.6/_collections_abc.py\", line 744, in __iter__\n","    yield (key, self._mapping[key])\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 832, in __getitem__\n","    plt.switch_backend(rcsetup._auto_backend_sentinel)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 204, in switch_backend\n","    switch_backend(candidate)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 221, in switch_backend\n","    backend_mod = importlib.import_module(backend_name)\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n","  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_qt5agg.py\", line 11, in <module>\n","    from .backend_qt5 import (\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_qt5.py\", line 15, in <module>\n","    import matplotlib.backends.qt_editor.figureoptions as figureoptions\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/qt_editor/figureoptions.py\", line 12, in <module>\n","    from matplotlib.backends.qt_compat import QtGui\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/qt_compat.py\", line 163, in <module>\n","    _setup()\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/qt_compat.py\", line 70, in _setup_pyqt5\n","    from PyQt5 import QtCore, QtGui, QtWidgets\n","  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 951, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 894, in _find_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1157, in find_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1126, in _get_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1090, in _path_importer_cache\n","OSError: [Errno 107] Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.6/posixpath.py\", line 383, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"iww85stFma4N"},"source":["# import psutil\n","# print(psutil.cpu_count(logical=False))\n","# print(psutil.cpu_freq(percpu=False))\n","# for k,v in os.environ.items():\n","#     print(k, v)\n","# import sys\n","# print(sys.platform)\n","\n","# Output:\n","# 2\n","# None\n","# ENV /root/.bashrc\n","# GCS_READ_CACHE_BLOCK_SIZE_MB 16\n","# CLOUDSDK_CONFIG /content/.config\n","# CUDA_VERSION 10.1.243\n","# PATH /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin\n","# HOME /root\n","# LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","# LANG en_US.UTF-8\n","# SHELL /bin/bash\n","# LIBRARY_PATH /usr/local/cuda/lib64/stubs\n","# CUDA_PKG_VERSION 10-1=10.1.243-1\n","# SHLVL 1\n","# GCE_METADATA_TIMEOUT 0\n","# NCCL_VERSION 2.7.8\n","# NVIDIA_VISIBLE_DEVICES all\n","# TF_FORCE_GPU_ALLOW_GROWTH true\n","# DEBIAN_FRONTEND noninteractive\n","# CUDNN_VERSION 7.6.5.32\n","# LAST_FORCED_REBUILD 20200910\n","# JPY_PARENT_PID 24\n","# PYTHONPATH /env/python\n","# DATALAB_SETTINGS_OVERRIDES {\"kernelManagerProxyPort\":6000,\"kernelManagerProxyHost\":\"172.28.0.3\",\"jupyterArgs\":[\"--ip=\\\"172.28.0.2\\\"\"]}\n","# NO_GCE_CHECK True\n","# GLIBCXX_FORCE_NEW 1\n","# NVIDIA_DRIVER_CAPABILITIES compute,utility\n","# _ /tools/node/bin/node\n","# LD_PRELOAD /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4\n","# NVIDIA_REQUIRE_CUDA cuda>=10.1 brand=tesla,driver>=396,driver<397 brand=tesla,driver>=410,driver<411 brand=tesla,driver>=418,driver<419\n","# OLDPWD /\n","# HOSTNAME 931aab700078\n","# COLAB_GPU 0\n","# PWD /\n","# CLOUDSDK_PYTHON python3\n","# GLIBCPP_FORCE_NEW 1\n","# PYTHONWARNINGS ignore:::pip._internal.cli.base_command\n","# TBE_CREDS_ADDR 172.28.0.1:8008\n","# TERM xterm-color\n","# CLICOLOR 1\n","# PAGER cat\n","# GIT_PAGER cat\n","# MPLBACKEND module://ipykernel.pylab.backend_inline\n","# TZ EST+05EDT,M4.1.0,M10.5.0\n","# KMP_DUPLICATE_LIB_OK True\n","# KMP_INIT_AT_FORK FALSE\n","# linux"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9sjtsvzM-dqV","executionInfo":{"status":"ok","timestamp":1601485562741,"user_tz":240,"elapsed":249,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"8ccbc25e-2b03-4ba6-cb78-53e0de33dd5d","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["ilp = Path(\"/usr/lib/python3.6/importlib/__init__.py\")\n","print(ilp.read_text())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"\"\"A pure Python implementation of import.\"\"\"\n","__all__ = ['__import__', 'import_module', 'invalidate_caches', 'reload']\n","\n","# Bootstrap help #####################################################\n","\n","# Until bootstrapping is complete, DO NOT import any modules that attempt\n","# to import importlib._bootstrap (directly or indirectly). Since this\n","# partially initialised package would be present in sys.modules, those\n","# modules would get an uninitialised copy of the source version, instead\n","# of a fully initialised version (either the frozen one or the one\n","# initialised below if the frozen one is not available).\n","import _imp  # Just the builtin component, NOT the full Python module\n","import sys\n","\n","try:\n","    import _frozen_importlib as _bootstrap\n","except ImportError:\n","    from . import _bootstrap\n","    _bootstrap._setup(sys, _imp)\n","else:\n","    # importlib._bootstrap is the built-in import, ensure we don't create\n","    # a second copy of the module.\n","    _bootstrap.__name__ = 'importlib._bootstrap'\n","    _bootstrap.__package__ = 'importlib'\n","    try:\n","        _bootstrap.__file__ = __file__.replace('__init__.py', '_bootstrap.py')\n","    except NameError:\n","        # __file__ is not guaranteed to be defined, e.g. if this code gets\n","        # frozen by a tool like cx_Freeze.\n","        pass\n","    sys.modules['importlib._bootstrap'] = _bootstrap\n","\n","try:\n","    import _frozen_importlib_external as _bootstrap_external\n","except ImportError:\n","    from . import _bootstrap_external\n","    _bootstrap_external._setup(_bootstrap)\n","    _bootstrap._bootstrap_external = _bootstrap_external\n","else:\n","    _bootstrap_external.__name__ = 'importlib._bootstrap_external'\n","    _bootstrap_external.__package__ = 'importlib'\n","    try:\n","        _bootstrap_external.__file__ = __file__.replace('__init__.py', '_bootstrap_external.py')\n","    except NameError:\n","        # __file__ is not guaranteed to be defined, e.g. if this code gets\n","        # frozen by a tool like cx_Freeze.\n","        pass\n","    sys.modules['importlib._bootstrap_external'] = _bootstrap_external\n","\n","# To simplify imports in test code\n","_w_long = _bootstrap_external._w_long\n","_r_long = _bootstrap_external._r_long\n","\n","# Fully bootstrapped at this point, import whatever you like, circular\n","# dependencies and startup overhead minimisation permitting :)\n","\n","import types\n","import warnings\n","\n","\n","# Public API #########################################################\n","\n","from ._bootstrap import __import__\n","\n","\n","def invalidate_caches():\n","    \"\"\"Call the invalidate_caches() method on all meta path finders stored in\n","    sys.meta_path (where implemented).\"\"\"\n","    for finder in sys.meta_path:\n","        if hasattr(finder, 'invalidate_caches'):\n","            finder.invalidate_caches()\n","\n","\n","def find_loader(name, path=None):\n","    \"\"\"Return the loader for the specified module.\n","\n","    This is a backward-compatible wrapper around find_spec().\n","\n","    This function is deprecated in favor of importlib.util.find_spec().\n","\n","    \"\"\"\n","    warnings.warn('Use importlib.util.find_spec() instead.',\n","                  DeprecationWarning, stacklevel=2)\n","    try:\n","        loader = sys.modules[name].__loader__\n","        if loader is None:\n","            raise ValueError('{}.__loader__ is None'.format(name))\n","        else:\n","            return loader\n","    except KeyError:\n","        pass\n","    except AttributeError:\n","        raise ValueError('{}.__loader__ is not set'.format(name)) from None\n","\n","    spec = _bootstrap._find_spec(name, path)\n","    # We won't worry about malformed specs (missing attributes).\n","    if spec is None:\n","        return None\n","    if spec.loader is None:\n","        if spec.submodule_search_locations is None:\n","            raise ImportError('spec for {} missing loader'.format(name),\n","                              name=name)\n","        raise ImportError('namespace packages do not have loaders',\n","                          name=name)\n","    return spec.loader\n","\n","\n","def import_module(name, package=None):\n","    \"\"\"Import a module.\n","\n","    The 'package' argument is required when performing a relative import. It\n","    specifies the package to use as the anchor point from which to resolve the\n","    relative import to an absolute import.\n","\n","    \"\"\"\n","    level = 0\n","    if name.startswith('.'):\n","        if not package:\n","            msg = (\"the 'package' argument is required to perform a relative \"\n","                   \"import for {!r}\")\n","            raise TypeError(msg.format(name))\n","        for character in name:\n","            if character != '.':\n","                break\n","            level += 1\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","\n","\n","_RELOADING = {}\n","\n","\n","def reload(module):\n","    \"\"\"Reload the module and return it.\n","\n","    The module must have been successfully imported before.\n","\n","    \"\"\"\n","    if not module or not isinstance(module, types.ModuleType):\n","        raise TypeError(\"reload() argument must be a module\")\n","    try:\n","        name = module.__spec__.name\n","    except AttributeError:\n","        name = module.__name__\n","\n","    if sys.modules.get(name) is not module:\n","        msg = \"module {} not in sys.modules\"\n","        raise ImportError(msg.format(name), name=name)\n","    if name in _RELOADING:\n","        return _RELOADING[name]\n","    _RELOADING[name] = module\n","    try:\n","        parent_name = name.rpartition('.')[0]\n","        if parent_name:\n","            try:\n","                parent = sys.modules[parent_name]\n","            except KeyError:\n","                msg = \"parent {!r} not in sys.modules\"\n","                raise ImportError(msg.format(parent_name),\n","                                  name=parent_name) from None\n","            else:\n","                pkgpath = parent.__path__\n","        else:\n","            pkgpath = None\n","        target = module\n","        spec = module.__spec__ = _bootstrap._find_spec(name, pkgpath, target)\n","        _bootstrap._exec(spec, module)\n","        # The module may have replaced itself in sys.modules!\n","        return sys.modules[name]\n","    finally:\n","        try:\n","            del _RELOADING[name]\n","        except KeyError:\n","            pass\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RHbcGFx1sp-g"},"source":["##**Load Data Files into Similarly-Named pd DataFrames**"]},{"cell_type":"code","metadata":{"id":"WaoqJmt42Yo4","cellView":"both"},"source":["# Issue: freeing up memory used by pandas dataframes that are no longer required by the program (del + gc.collect() does not reliably do this, \n","#   nor does re-defining the df as empty pd.DataFrame())\n","#   to keep from overloading Colab memory limits, we utilize multiprocessing function calls, which use the OS level to discard unwanted dataframes\n","\n","###############################################################################\n","def files_to_dfs(datasources_keepcols_tuple):   \n","    \"\"\"\n","    get raw data from disk using filenames defined above; [ datasources_keepcols_tuple = ((pd_df_name_str, csv_filepath_str), keep_cols_list)]\n","    Returns pandas DataFrame for each of the data files...\n","        these get ordered into a list by the multiprocessing map function that calls this function\n","    \"\"\"\n","    data_frame_name = datasources_keepcols_tuple[0][0]\n","    exec(data_frame_name + ' = pd.read_csv(datasources_keepcols_tuple[0][1])')\n","    # remove unnecessary columns from dataframes, to help with speed and memory savings\n","    exec(data_frame_name + \" = eval(data_frame_name)[datasources_keepcols_tuple[1]]\")  #.rename(columns = SHOPS_COLUMN_RENAME)\n","    return eval(data_frame_name)\n","\n","###############################################################################\n","###############################################################################\n","###############################################################################\n","@time_eda    # decorator [x.get_elapsed_time() or x.function_total_time -HH:MM:SS] [x.function_init_time-y,m,d,h,min,sec]\n","def eda_cleanup(eda_delete_shops, eda_delete_item_cats, eda_scale_month, feather_stt, model_filename_base, model_type, \n","                feature_params, data_sources):\n","#########################################\n","    \"\"\"\n","    1) load datafiles (stt sales_train_test,items,shops,date,test) created, feature-augmented (some manually done), and/or cleaned with:\n","        ['calculate_days_per_month.ipynb', 'EDA_sales_by_day_of_week_mg.ipynb'] --> date_scaling.csv\n","        [['time_item_category_shop_correlations_v10_mg.ipynb'] --> shops_enc.csv, 'nlp_clustering_item_names_v1_june2020_mg.ipynb'] --> items_enc.csv\n","        ['data_cleaning_and_eda_feature_merging_v2_mg.ipynb'] --> stt.csv.gz         [no modifications] ==>> test.csv.gz\n","    2) modify dataframes using splits defined above (drop some features, merge into stt, scale according to date, set datatypes)\n","    x) use 'multiprocessing' python configuration to help ensure pandas dataframes release RAM back to the VM when they are no longer needed\n","        (simple 'del' command is unreliable; only workaround I have found to work is to multiprocess to call a new process, which releases all\n","        memory *at the OS level* when process is complete)... Also provide option to save quick-loading \"feather\" files and eliminate need to\n","        keep all dataframes in RAM at all times.  This is particularly important when you are RAM-limited, as sometimes happens in Google Colab\n","    \"\"\"\n","    time_eda.restart_time()\n","    print(f'Start EDA Module: {strftime(\"%a %X %x\")}')\n","    MEMORY_STATS.append(get_memory_stats('At Top of EDA Function', printout=False))\n","    %cd \"{GDRIVE_REPO_PATH}\"\n","    print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","\n","    ''' ###################  start multiproc  ################### '''\n","    with multiprocessing.Pool(None) as load_dfs_pool:  #None = use all processors, could use Pool(1, maxtasksperchild = 1)\n","        load_dfs = load_dfs_pool.map(files_to_dfs, list(zip(data_sources,list(feature_params['keep_columns'].values()))))\n","    load_dfs_pool.close()  # pool.terminate()\n","    load_dfs_pool.join()\n","    ''' ###################   end  multiproc  ################### '''\n","\n","    MEMORY_STATS.append(get_memory_stats('load_dfs_pool Closed, Joined', printout=False))\n","\n","    dfidx = {}\n","    for i in range(len(load_dfs)):\n","        name = data_sources[i][0]\n","        dfidx[name] = i\n","        print(f'---------- {name} ----------') \n","        print(f'DataFrame shape: {load_dfs[i].shape}     DataFrame total memory usage: {(load_dfs[i].memory_usage(deep=True) /1e6).sum():.0f} MB')\n","        print(f'DataFrame Column Names: {load_dfs[i].columns.to_list()}')\n","        # print_col_info(load_dfs[i])\n","        print(f'^^^^^\\n{load_dfs[i].head(2)}\\n\\n')\n","\n","    #########################################  Merge shops and items into stt  #########################################\n","    load_dfs[dfidx['stt']] = load_dfs[dfidx['stt']].merge(load_dfs[dfidx['shops_enc']], on='shop_id', how='left')\n","    load_dfs[dfidx['stt']] = load_dfs[dfidx['stt']].merge(load_dfs[dfidx['items_enc']], on='item_id', how='left')\n","\n","    MEMORY_STATS.append(get_memory_stats('stt Merged with shops and items', printout=False))\n","\n","    print(f'---------- merged stt ----------') \n","    print_col_info(load_dfs[dfidx['stt']])\n","    print(f\"^^^^^\\n{load_dfs[dfidx['stt']].head(2)}\\n\\n\")\n","\n","    #########################################  Del unwanted shops, item cats; Scale sales for days in month, etc. ######\n","    print('----------\\n')\n","    if eda_delete_shops:  # drop undesirable shops\n","        load_dfs[dfidx['stt']] = load_dfs[dfidx['stt']].query('shop_id != @eda_delete_shops')\n","        print(f\"Shape of stt after deleting shops {eda_delete_shops}: {load_dfs[dfidx['stt']].shape}\")\n","    if eda_delete_item_cats:   # drop undesirable item categories\n","        load_dfs[dfidx['stt']] = load_dfs[dfidx['stt']].query('item_category_id != @eda_delete_item_cats')\n","        print(f\"Shape of stt after deleting item categories {eda_delete_item_cats}: {load_dfs[dfidx['stt']].shape}\\n\")\n","    if eda_scale_month:    # scale by date_scaling as desired\n","        load_dfs[dfidx['stt']] = load_dfs[dfidx['stt']].merge(load_dfs[dfidx['date_scaling']][['month',eda_scale_month]], on='month', how='left')\n","        load_dfs[dfidx['stt']].sales = load_dfs[dfidx['stt']].sales * load_dfs[dfidx['stt']][eda_scale_month]\n","        load_dfs[dfidx['stt']].drop(eda_scale_month, axis=1, inplace=True) \n","\n","    #########################################  Insert revenue feature; adjust data types; drop unnecessary cols; set desired col order\n","    load_dfs[dfidx['stt']]['revenue'] = load_dfs[dfidx['stt']].sales * load_dfs[dfidx['stt']].price / 1000\n","    load_dfs[dfidx['stt']][['sales','price','revenue']].astype(np.float32)  # float so date_adj weight is accurate; can use price\n","    load_dfs[dfidx['stt']][feature_params['integer']] = load_dfs[dfidx['stt']][feature_params['integer']].astype('int16')\n","    load_dfs[dfidx['stt']] = load_dfs[dfidx['stt']][feature_params['stt_final']]  # drop, re-order\n","    load_dfs[dfidx['stt']] = load_dfs[dfidx['stt']].reset_index(drop=True)  #reset index saves 25MB \n","    print(f'---------- final stt ----------') \n","    print_col_info(load_dfs[dfidx['stt']])\n","    print(f\"^^^^^\\n{load_dfs[dfidx['stt']].head(2)}\\n\\n\")\n","\n","    MEMORY_STATS.append(get_memory_stats('stt Transforms Completed', printout=False))\n","\n","    #########################################  Save ftr files or return dataframes #########################################\n","    if feather_stt:\n","        # optional save file as feather type (big file; don't store inside repo) ... does not support lists, tuples in dataframe cells\n","        # this allows reclaiming the RAM used by stt file, if this function was called by multiprocess pool\n","        block_time.restart_time()\n","        %cd \"{OUT_OF_REPO_PATH}\"\n","        ftr_names = []\n","        print('load_dfs feather files stored on google drive in \"final\" directory, outside repo:')\n","        for i in range(len(load_dfs)):\n","            name = data_sources[i][0]\n","            ftr_names.append(f'{name}_temp.ftr')\n","            load_dfs[i].to_feather(ftr_names[i])\n","            print(f'{ftr_names[i]}, ',end='') \n","        load_dfs = ftr_names\n","        print(f'Elapsed time writing EDA feather files: {block_time.get_elapsed_time()}\\n')\n","\n","        MEMORY_STATS.append(get_memory_stats('After EDA Func .ftr Write', printout=False))\n","\n","    OUTPUTS_df.at[RUN_n,\"time_eda\"] = time_eda.get_elapsed_time()\n","    print(f'EDA Module elapsed time: {OUTPUTS_df.at[RUN_n,\"time_eda\"]}')\n","    display_all_memory_stats(MEMORY_STATS)\n","    print(f'Done EDA Module: {strftime(\"%a %X %x\")}')\n","    return load_dfs, dfidx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1GXED3-jyQC7"},"source":["##**Data Preparation: Feature Merging and Feature Generation**\n","###**1) Compute and Merge Statistics-Based Features on Grouped-by-Month training data**\n","* Note that features based on price are nonsensical if we add cartesian product fill.  However, item sales and item revenues are OK to use.\n","\n","###**2) Add Cartesian Product rows to the training data:**\n","* Idea is to help the model by informing it that we explicitly have no information about certain relevant shop_item pairs in certain months.\n","* Each month in train data will have additional rows such that the Cartesian Product of all shops and items ALREADY PRESENT IN THAT MONTH will be included.* * When we merge lagged features below, we will only forward-shift the shop-item pairs that are present in the later month. *(Might revisit later, if memory requirements are not too big, can forward-shift all shop-item pairs.)*\n","* **If not adding Cartesian Product, or if fillna(0) is used, can round features to integers, saving memory (pandas integers cannot store np.NaN; need float32)**\n","\n","###**3) Add Lagged Statistics columns to the training data:**"]},{"cell_type":"code","metadata":{"id":"AghbBFHC_ljc","cellView":"both"},"source":["from concurrent.futures import ThreadPoolExecutor  # for nested threaded multiprocessing\n","# https://stackoverflow.com/questions/49947935/nested-parallelism-in-python-multiprocessing\n","\n","# Create MONTHLY_STT DataFrame = stt grouped by month, with statistics\n","# Compute values in \"real time,\" then in a later code cell we will compute shifted (lagged) versions\n","\n","###############################################################################\n","def compute_stats(stats_arg):\n","    \"\"\"\n","    function for computing statistics-based features; flexible if we wish to add in extra statistics or extra group-by categories\n","    \"\"\"\n","    # stt = stats_arg[0]\n","    # stats_set_dict = stats_arg[1]\n","    group = stats_arg[1]['group']\n","    group = ['month'] + group if 'month' not in group else group\n","    grouped_df = stats_arg[0].groupby(group).agg(stats_arg[1]['stats'])\n","    grouped_df.columns = stats_arg[1]['agg_names']\n","    grouped_df.reset_index(inplace=True)\n","    #gpr = [grouped_df, group]\n","    return grouped_df #gpr\n","\n","###############################################################################\n","def create_and_merge_groups(stats_arg):\n","    # stt = stats_arg[0]\n","    # stats_set_dict = stats_arg[1]\n","    # monthly_stt = stats_arg[2]\n","\n","    print('in create and merge groups')\n","\n","    # ''' ###################  start multiproc  ################### '''\n","    # with multiprocessing.Pool(None) as stats_pool:  \n","    #     grouped_df_list = stats_pool.apply(compute_stats, stats_set_dict)\n","    # # with multiprocessing.Pool(None, maxtasksperchild = 1) as stats_pool:  \n","    # #     grouped_df_list = stats_pool.map(compute_stats, [stats_set_dict])\n","    # stats_pool.close()  # pool.terminate()\n","    # stats_pool.join()\n","    # ''' ###################   end  multiproc  ################### '''\n","    # MEMORY_STATS.append(get_memory_stats('stats_pool Closed and Joined',printout=False))\n","    grouped_df = compute_stats(stats_arg)\n","\n","\n","    # fix so monthly stt is properly initialized even though multiproc map fn doesn't guarantee process order at this point?\n","\n","    # monthly_stt = grouped_df_list.pop(0)  # initialize grouping by month with stats based on monthly transactions of shop-item pairs\n","\n","    monthly_stt = stats_arg[2].merge(grouped_df, on = stats_arg[1]['group'], how = 'left')\n","\n","    return monthly_stt\n","\n","\n","\n","\n","def calc_stats(stats_arg):\n","    \"\"\"\n","    function for computing statistics-based features; flexible if we wish to add in extra statistics or extra group-by categories\n","    \"\"\"\n","    # stt = stats_arg[0]\n","    # stats_set_dict = stats_arg[1]\n","    group = stats_arg[1]['group']\n","    group = ['month'] + group if 'month' not in group else group\n","    grouped_df = stats_arg[0].groupby(group).agg(stats_arg[1]['stats'])\n","    grouped_df.columns = stats_arg[1]['agg_names']\n","    grouped_df.reset_index(inplace=True)\n","\n","    monthly_stt_ = monthly_stt_merge_pool.submit(merge_monthly_stt, monthly_stt_)\n","\n","\n","    return \n","\n","\n","\n","def merge_monthly_stt(stt, monthly_stt, stats_dict):\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","###############################################################################\n","def numpy_cartesian_product(query_string):\n","    cartprod_rows = monthly_stt[['month','shop_id','item_id']].query(query_string)\n","    return np.array(list(product([i], cartprod_rows.shop_id.unique(), cartprod_rows.item_id.unique())), dtype=np.int16)\n","\n","###############################################################################\n","###############################################################################\n","###############################################################################\n","@time_data_manip\n","def data_conditioning(  stt, shops_enc, items_enc,\n","                        cartprod_fillna0, cartprod_first_month, cartprod_test_pairs, clip_train_H, clip_train_L, feather_monthly_stt, \n","                        feature_data_type, minmax_scaler_range, robust_scaler_quantiles, use_cartprod_fill, use_categorical, use_minmax_scaler, \n","                        use_robust_scaler, eda_delete_shops, eda_delete_item_cats, eda_scale_month, feather_stt, \n","                        model_filename_base, model_type, feature_params, data_sources):\n","    \n","    print('In data cond fn')\n","    # stt = 'stt_temp.ftr'\n","    # shops_enc = 'shops_enc_temp.ftr'\n","    # items_enc = 'items_enc_temp.ftr'\n","\n","\n","    \"\"\"\n","    1) group training/val data by months, compute statistics while aggregating\n","    2) scale the feature columns for better use of full range of available datatype values (np.int16, np.int8, np.uint16,...) unless float\n","    3) add cartesian product fill to each month, as cartesian product of shop_id and item_id from that single month\n","    4) merge time-lagged statistics, discarding those that don't have a match with an existing shop_id,item_id pair in destination month\n","    5) adjust feature column datatypes as desired, ideally reducing VM RAM usage\n","    inputs:     *stt (sales train test) dataframe, *shops_enc, *items_enc, and the various *parameters to guide the above actions\n","    outputs:    *monthly_stt dataframe (grouped by month, cartesian product rows added, lagged statistics added, datatypes set)\n","                *robust_scalers and *minmax_scalers for each column (to inverse transform our model prediction before submission)\n","    \"\"\"\n","    time_data_manip.restart_time()\n","    print(f'Start Data Module: {strftime(\"%a %X %x\")}')\n","    MEMORY_STATS.append(get_memory_stats('At Start of Data Function',printout=False))\n","\n","\n","    print('At start of feather_stt load')\n","\n","    if feather_stt:  # load dataframes from disk\n","        block_time.restart_time()\n","        print(f'Feather File Source Directory: ', end='')\n","        %cd \"{OUT_OF_REPO_PATH}\"\n","        stt = pd.read_feather(stt, columns=None, use_threads=True)\n","        shops_enc = pd.read_feather(shops_enc, columns=None, use_threads=True)\n","        items_enc = pd.read_feather(items_enc, columns=None, use_threads=True)\n","        print(\"Loaded DataFrames 'stt', 'shops_enc', 'items_enc' from Google Drive feather files into Colab...\")\n","        print(f'Elapsed time loading feather file DataFrames: {block_time.get_elapsed_time()}\\n')\n","        MEMORY_STATS.append(get_memory_stats('After Data Func .ftr Load',printout=False))\n","\n","    ################### Aggregate Monthly Stats ############################################################################\n","    monthly_stt = pd.DataFrame()\n","    # create iterable list of stats calculations, so we can use multiprocess map function\n","    ''' ###################  start multiproc  ################### '''\n","\n","    # print(f\"listfeatparams: {list(feature_params['lag_splits']['stats_set'].values())}\")\n","    # # listfeatparams: [{'group': ['shop_id', 'item_id'], \n","    # #                 'stats': {'shop_group': ['first'], 'item_category_id': ['first'], 'item_group': ['first'], \n","    # #                             'item_cluster': ['first'], 'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, \n","    # #                 'agg_names': ['shop_group', 'item_category_id', 'item_group', 'item_cluster', 'shop_id_x_item_id_sales_count', \n","    # #                             'shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_revenue_sum']}, \n","    # #\n","    # #                 {'group': ['shop_id', 'item_category_id'], \n","    # #                 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, \n","    # #                 'agg_names': ['shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_sum', \n","    # #                             'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_revenue_sum']}, \n","    # #\n","    # #                 {'group': ['shop_id', 'item_cluster'], \n","    # #                 'stats': {'sales': ['sum', 'median']}, \n","    # #                 'agg_names': ['shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median']}, \n","    # # \n","    # #                 {'group': ['shop_id'], 'stats': {'sales': ['count', 'sum']}, 'agg_names': ['shop_id_sales_count', 'shop_id_sales_sum']}, {'group': ['item_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['item_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_revenue_sum']}, {'group': ['shop_group'], 'stats': {'revenue': ['sum']}, 'agg_names': ['shop_group_revenue_sum']}, {'group': ['item_category_id'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_category_id_sales_count', 'item_category_id_sales_sum', 'item_category_id_revenue_sum']}, {'group': ['item_group'], 'stats': {'sales': ['sum'], 'revenue': ['sum']}, 'agg_names': ['item_group_sales_sum', 'item_group_revenue_sum']}, {'group': ['item_cluster'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_cluster_sales_count', 'item_cluster_sales_sum', 'item_cluster_revenue_sum']}]\n","    # for i in list(feature_params['lag_splits']['stats_set'].values()):\n","    #     for k,v in i.items():\n","    #         print(k,v)\n","    #     print('\\n')\n","\n","\n","\n","\n","    stats_arg = [[stt, fp, monthly_stt] for fp in feature_params['lag_splits']['stats_set'].values()]\n","    MEMORY_STATS.append(get_memory_stats('stats_arg List Created',printout=False))\n","    # for i in stats_arg:\n","    #     print(i)\n","    monthly_stt = compute_stats(stats_arg.pop(0))   # initialize monthly_stt to capture standard features (stats='first')\n","\n","    MEMORY_STATS.append(get_memory_stats('monthly_stt Initialized with 1st Stats',printout=False))\n","    with multiprocessing.Pool(None) as monthly_stt_pool:  \n","        # monthly_stt = monthly_stt_pool.map(create_and_merge_groups, list(feature_params['lag_splits']['stats_set'].values()))\n","        grouped_df_list = monthly_stt_pool.map(create_and_merge_groups, stats_arg) #compute_stats, stats_arg) #list(feature_params['lag_splits']['stats_set'].values()))\n","    monthly_stt_pool.close()  # pool.terminate()\n","    monthly_stt_pool.join()\n","    ''' ###################   end  multiproc  ################### '''\n","    MEMORY_STATS.append(get_memory_stats('monthly_stt_pool Closed, Joined', printout=False))\n","    print('grouped df list',grouped_df_list)\n","\n","    monthly_stt = grouped_df_list.pop(0)  # initialize grouping by month with stats based on monthly transactions of shop-item pairs\n","    for group_tuple in grouped_df_list:\n","        monthly_stt = monthly_stt.merge(group_tuple[0], on = group_tuple[1], how = 'left')\n","\n","\n","\n","\n","\n","\n","    # monthly_stt_merge_pool = ThreadPoolExecutor(max_workers=4)\n","    # stats_calcs_pool = ThreadPoolExecutor(max_workers=4)\n","\n","    monthly_stt_pool = multiprocessing.Pool(4)\n","\n","    for stats_dict in list(feature_params['lag_splits']['stats_set'].values()):\n","        monthly_stt = monthly_stt_pool.submit(merge_monthly_stt,)\n","\n","# header: referrer= google or imfp or something\n","# python param library to standardize above input dict\n","# qgrid = python dataframe viewer for jupyter nb\n","#google-auth-oauthlib\n","# tempora stopwatch\n","# use webappify 0.3 for Regex app on desktop? or is there regex in windows store?\n","#tqdm or progressbar to show progress\n","# use arrow instead of datetime, etc.\n","#  SEE ALSO package pandas-profiling   (HTML report that gives mega info on a df )\n","\n","# pyperformance can also track memory\n","# pympler does memory usage well   #utf-8 with ftfy  #memory_profiler   #multidict  #pkginfo #progressbar2  #imagesize from jpeg header\n","#py-cpuinfo  # arrow  #curl  #httpx   #asyncio #imageio  #infinity\n","#pygithub   #unidecode  #cachecontrol\n","#tzlocal\n","# perhaps an alternative to the \"eval\" statements when doing file imports, etc??:\n","# istr\n","# CIMultiDict accepts str as key argument for dict lookups but uses case-folded (lower-cased) strings for the comparison internally.\n","#          also see orderedmultidict\n","# For more effective processing it should know if the key is already case-folded to skip the lower() call.\n","\n","# The performant code may create case-folded string keys explicitly hand, e.g:\n","\n","# >>> key = istr('Key')\n","# >>> key\n","# 'Key'\n","# >>> mdict = CIMultiDict(key='value')\n","# >>> key in mdict\n","# True\n","# >>> mdict[key]\n","# 'value'\n","# For performance istr strings should be created once and stored somewhere for the later usage, see aiohttp.hdrs for example.\n","\n","see https://www.toptal.com/python/python-class-attributes-an-overly-thorough-guide\n","item 3 - tracking instances\n","\n","\n","#   - lightgbm -> python[version='>=3.5,<3.6.0a0']\n","#   - memory_profiler -> python[version='>=2.7,<2.8.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.5,<3.6.0a0']\n","#   - py-cpuinfo -> python[version='>=2.7,<2.8.0a0|>=3.5,<3.6.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0']\n","#   - pympler -> python[version='>=2.7,<2.8.0a0|>=3.6,<3.7.0a0|>=3.5,<3.6.0a0|>=3.7,<3.8.0a0']\n","#   - scandir -> python[version='>=2.7,<2.8.0a0']\n","#   - tqdm -> python[version='>=2.7,<2.8.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.5,<3.6.0a0']\n","#   - unidecode -> python[version='>=2.7,<2.8.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.5,<3.6.0a0']\n","#   - whoosh -> python[version='>=2.7,<2.8.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.5,<3.6.0a0']\n","\n","# Your python: python==3.8.0\n","\n","\n","        #  https://nnn.com/ajax/actions.php?action=like&i=&g=47068000\n","#py3.8.5 not with whoosh, pympler, \n","#nbsmoke, numexpr, orderedmultidict, pandas-profiling, param, pep8naming, pkginfo, pylint, pycpuinfo, pyperformance, pyquery, qgrid, scandir, tempora, tzlocal, unidecode\n","\n","    MEMORY_STATS.append(get_memory_stats('monthly_stt Created',printout=False))\n","\n","\n","\n","\n","    ################### Clean, Scale, Datatype Monthly DataFrame ###########################################################\n","    #  monthly_stt = monthly_stt.rename(columns={'shop_id_x_item_id_sales_sum':'y_sales'})  # rename for convenience\n","    monthly_stt = monthly_stt[feature_params['integer'] + feature_params['lag_splits']['stats_set_feature_names']] # order, keep desired features\n","    monthly_stt.sort_values(['month','shop_id','item_id'], inplace=True)\n","    # clip to adhere to kaggle contest instructions\n","    monthly_stt.shop_id_x_item_id_sales_sum = monthly_stt.shop_id_x_item_id_sales_sum.clip(clip_train_L, clip_train_H)\n","    # always do minmax scaling after robust scaling; and do inverse scaling with minmax first, then robust\n","    robust_scalers = {} \n","    if use_robust_scaler:  # squeeze outliers into central distribution\n","        for aggcol in feature_params['lag_splits']['stats_set_feature_names']:\n","            robust_scalers[aggcol] = RobustScaler(with_centering=False, quantile_range=robust_scaler_quantiles)\n","            monthly_stt[aggcol] = robust_scalers[aggcol].fit_transform(monthly_stt[aggcol].to_numpy().reshape(-1, 1))\n","    minmax_scalers = {} \n","    if use_minmax_scaler:  # apply min-max scaler to make best use of np.int16 and memory usage\n","        for aggcol in feature_params['lag_splits']['stats_set_feature_names']:\n","            minmax_scalers[aggcol] = MinMaxScaler(feature_range=minmax_scaler_range)\n","            monthly_stt[aggcol] = minmax_scalers[aggcol].fit_transform(monthly_stt[aggcol].to_numpy().reshape(-1, 1))\n","    if feature_data_type in [np.int16, np.uint16]:\n","        monthly_stt = monthly_stt.fillna(0).round()\n","    monthly_stt = monthly_stt.astype(feature_data_type).reset_index(drop=True)\n","    print(f'\\nmonthly_stt fully grouped and merged and scaled: shape = {monthly_stt.shape}')\n","    print_col_info(monthly_stt,8)\n","    MEMORY_STATS.append(get_memory_stats('monthly_stt Scaled and Downcast', printout=False))\n","\n","    ################### Cartesian Product Insertion ########################################################################\n","    if use_cartprod_fill:\n","        # Create cartesian product so model has info to look at for every relevant shop-item-month combination in the months desired\n","        # add enough months of cart prod that after time-LAGS, we end up with cart products in months cartprod_first_month through 33\n","        first_month = max (cartprod_first_month - max(feature_params['lag_splits']['months_list']), 0)\n","        query_string_list = []\n","        for i in range(first_month,34):\n","            query_string_list.append('(month == @i)|(month == 34)' if cartprod_test_pairs else '(month == @i)')\n","            \n","        ''' ###################  start multiproc  ################### '''\n","        with multiprocessing.Pool(None, maxtasksperchild = 1) as cart_prod_pool:  \n","            numpy_cartprod_array_list = cart_prod_pool.map(numpy_cartesian_product, query_string_list)\n","        cart_prod_pool.close()  # pool.terminate()\n","        cart_prod_pool.join()\n","        ''' ###################   end  multiproc  ################### '''\n","        MEMORY_STATS.append(get_memory_stats('cart_prod_pool Closed, Joined',printout=False))\n","\n","        monthly_stt = monthly_stt.merge( pd.DataFrame(np.vstack(matrix), columns=['month','shop_id','item_id']),\n","                                         how = 'outer', on = 'month')\n","        monthly_stt = monthly_stt.merge( shops_enc, how = 'left', on = 'shop_id')\n","        monthly_stt = monthly_stt.merge( items_enc, how = 'left', on = 'item_id')\n","        monthly_stt = monthly_stt.sort_values(['month','shop_id','item_id']).reset_index(drop=True)\n","        if cartprod_fillna0:\n","            monthly_stt = monthly_stt.fillna(0)  # store as integers to save memory, but 'price=0' values = nonsense; use revenue instead\n","\n","        print(f'Column Data Types: \\n{df.dtypes}\\ndf memory usage: {df.memory_usage(deep=True).sum()/1e6:.0f} MBytes')\n","        print(f'Number of months: {df.month.nunique():,d}\\nNumber of shops: {df.shop_id.nunique():,d}')\n","        print(f'Number of items: {df.item_id.nunique():,d}\\nDataFrame length: {len(df):,d}\\n')\n","\n","    monthly_stt = monthly_stt.astype(feature_data_type).reset_index(drop=True) #np.int16 #.apply(pd.to_numeric, downcast= np.float32)\n","    print_col_info(monthly_stt,8)\n","    print(f'\\nmonthly_stt.head:\\n{monthly_stt.head(2)}')\n","    print(f'\\nmonthly_stt.tail:\\n{monthly_stt.tail(2)}\\n')  # display(monthly_stt.describe())\n","    MEMORY_STATS.append(get_memory_stats('Cartesian Product Rows Added', printout=False))\n","\n","    ################### Merge Time-Lag Features ############################################################################\n","    #   drop any rows in month (m minus lag) that don't have matching shop-item pair at month m\n","    print(f'Unlagged DataFrame length: {len(df):,d}\\n')\n","    monthly_stt['y_target'] = monthly_stt.shop_id_x_item_id_sales_sum.copy(deep=True)  # unlagged shop_item sales/month for our predict target\n","    if cartprod_fillna0 or (feature_data_type in [np.int16, np.uint16]):\n","        monthly_stt.y_target = monthly_stt.y_target.fillna(0)     #.clip(INTEGER_MULTIPLIER*clip_train_L, INTEGER_MULTIPLIER*clip_train_H)\n","    lag_merge_on_cols = ['month','shop_id','item_id']\n","    for lag in feature_params['lag_splits']['months_list']:\n","        cols_to_shift = lag_merge_on_cols + feature_params['lag_splits']['params'][lag]['feature_root']\n","        lag_df = monthly_stt[cols_to_shift].copy(deep=True).rename(columns = feature_params['lag_splits']['params'][lag]['lagged_feature_name'])\n","        lag_df.eval('month = month + @lag', inplace=True).astype(feature_data_type)\n","        monthly_stt = monthly_stt.merge(lag_df, on = lag_merge_on_cols, how = 'left')\n","        if cartprod_fillna0 or (feature_data_type in [np.int16, np.uint16]):\n","            monthly_stt = monthly_stt.fillna(0)  \n","        monthly_stt = monthly_stt.astype(feature_data_type).reset_index(drop=True)\n","    if use_categorical:\n","        for cat_col in feature_params['categorical']:\n","            monthly_stt[cat_col] = monthly_stt[cat_col].astype('category')\n","    print('lagged features monthly_stt:')\n","    print(f'monthly_stt lagged dataframe shape: {monthly_stt.shape}\\n')\n","    print_col_info(monthly_stt,8)\n","    print(f'\\nlagged monthly_stt.head():\\n{monthly_stt.head()}\\n')\n","    MEMORY_STATS.append(get_memory_stats('Time Lag Features Added',printout=False))\n","\n","    #########################################  Save ftr files or return dataframes #########################################\n","    if feather_monthly_stt:\n","        # optional save file as feather type (big file; don't store inside repo) ... does not support lists, tuples in dataframe cells\n","        # this allows reclaiming the RAM used by monthly_stt file, if this function was called by multiprocess pool\n","        block_time.restart_time()\n","        %cd \"{OUT_OF_REPO_PATH}\"\n","        monthly_stt.to_feather('monthly_stt_temp.ftr')\n","        monthly_stt = 'monthly_stt_temp.ftr'\n","        print(\"'monthly_stt_temp.ftr' feather file stored on google drive in 'final' directory, outside repo.\")\n","        print(f'Elapsed time writing monthly_stt feather file: {block_time.get_elapsed_time()}\\n')\n","        MEMORY_STATS.append(get_memory_stats('After Data Func .ftr Write',printout=False))\n","    \n","    OUTPUTS_df.at[RUN_n,\"time_data_manip\"] = time_data_manip.get_elapsed_time()\n","    print(f'Data Module elapsed time : {OUTPUTS_df.at[RUN_n,\"time_data_manip\"]}')\n","    display_all_memory_stats(MEMORY_STATS)\n","    print(f'End Data Module: {strftime(\"%a %X %x\")}')\n","    return monthly_stt, robust_scalers, minmax_scalers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9TJk9bzeCjqF"},"source":["##**Train/Test split**"]},{"cell_type":"code","metadata":{"id":"9YCzNW7-oHsF","cellView":"both"},"source":["\n","###############################################################################\n","###############################################################################\n","###############################################################################\n","@time_dataset_splits\n","def tvt_split_function(DataSets={}, feather_tvt_split=False, test_month=34, train_start_month=13, train_final_month=29, validate_months=999, \n","                       feather_monthly_stt=True, feature_data_type=np.int16, \n","                       use_categorical=True, categorical_features=[\"shop_id,item_id\"], model_type='LGBM'):\n","    \"\"\"\n","    DataSets is a dictionary containing all split data (train_X, train_y, val_X, val_y, test_X) with the split data names as keys, and\n","    the values = pandas DataFrames corresponding to those keys.  For flexibility and possible memory savings, we are able to read in the\n","    original (pre-split) data from a feather file on disk, and can save the 5 DataSet values dataframes as feather files as well.\n","    \"\"\"\n","    time_dataset_splits.restart_time()\n","    print(f'Start Train-Val-Test Split Module: {strftime(\"%a %X %x\")}')\n","    MEMORY_STATS.append(get_memory_stats('At Top of TVT_SPLIT Function',printout=False))\n","    if feather_monthly_stt:  # load monthly_stt dataframe from disk into \"train_X\" dataframe in VM\n","        print(f'{DataSets[\"train_X\"]} source directory: ', end='')\n","        block_time.restart_time()\n","        %cd \"{OUT_OF_REPO_PATH}\"\n","        DataSets[\"train_X\"] = pd.read_feather(DataSets[\"train_X\"], columns=None, use_threads=True)\n","        print(\"Loaded monthly_stt from Google Drive feather file into Colab as train_X ...\")\n","        print(f'Elapsed time loading monthly_stt feather file: {block_time.get_elapsed_time()}\\n')\n","        MEMORY_STATS.append(get_memory_stats('After TVT_SPLIT Fn ftr Load',printout=False))\n","\n","    if model_type == 'LGBM':\n","        # This train/val/test split is for time-ordered data; may not want same sort of algorithm with other source data or model types\n","        DataSets['train_X'] = DataSets['train_X'].query('month >= @train_start_month')  # remove early months that don't participate in model training\n","        DataSets['train_X'] = DataSets['train_X'].astype(feature_data_type).reset_index(drop=True)\n","\n","        DataSets['test_X'] = DataSets['train_X'].query('month == @test_month').drop('y_target',axis=1).reset_index(drop=True)\n","        \n","        if validate_months == 999:  # include all months from end of training up to the start of test month; at a minimum include month 33\n","            DataSets['val_X'] = data.query('((month > (@train_final_month)) & (month < @test_month)) | (month == (@test_month-1))')\n","        else:  # include only n (validate_months = 1,2,3,...) months after training; but no later than start of test\n","            DataSets['val_X'] = data.query('(month > (@train_final_month)) & (month <= (@train_final_month + @validate_months)) & (month < @test_month)')\n","        DataSets['val_y'] = DataSets['val_X'].pop('y_target')\n","\n","        DataSets['train_X'].query('month <= @train_final_month')\n","        DataSets['train_y'] = DataSets['train_X'].pop('y_target')\n","\n","        feature_names = X_train.columns\n","\n","        print('train_X:')\n","        print(DataSets['train_X'].head(2))\n","        print_col_info(DataSets['train_X'],8)\n","        print('\\ntrain_y:')\n","        print(DataSets['train_y'].head(2))\n","        print_col_info(DataSets['train_y'],8)\n","\n","        # Make sure all data sets are properly categorized and typed\n","        for ds in DataSets.keys():\n","            datatype = feature_data_type if ds[-1] == 'X' else np.float32  # 'target y' value can be high accuracy; 'features' can be int or float\n","            DataSets[ds] = DataSets[ds].astype(datatype).reset_index(drop=True)\n","            if use_categorical:\n","                DataSets[ds][categorical_features] = DataSets[ds][categorical_features].astype('category')\n","        MEMORY_STATS.append(get_memory_stats('At End of TVT_SPLIT Function',printout=False))\n","\n","    if feather_tvt_split:\n","        # optional save file as feather type (big file; don't store inside repo) ... does not support lists, tuples in dataframe cells\n","        # this allows reclaiming the RAM used by train/val/test files, if this function was called by multiprocess pool\n","        block_time.restart_time()\n","        %cd \"{OUT_OF_REPO_PATH}\"\n","        ftr_names = {}\n","        print('Train-Val-Test feather files stored on google drive in \"final\" directory, outside repo:')\n","        for dfname in DataSets.keys():\n","            ftr_names[dfname] = f'{dfname}_temp.ftr'\n","            DataSets[dfname].to_feather(ftr_names[dfname])\n","            print(ftr_names[dfname],end='') \n","        print(f'Elapsed time writing Train-Val-Test feather files: {block_time.get_elapsed_time()}\\n')\n","        DataSets = ftr_names\n","        MEMORY_STATS.append(get_memory_stats('After TVT_SPLIT Fn ftr Write',printout=False))\n","\n","    OUTPUTS_df.at[RUN_n, \"time_dataset_splits\"] = time_dataset_splits.get_elapsed_time()\n","    print(f'Data Module elapsed time : {OUTPUTS_df.at[RUN_n, \"time_dataset_splits\"]}')\n","    display_all_memory_stats(MEMORY_STATS)\n","    print(f'End Train-Val-Test Splits Module: {strftime(\"%a %X %x\")}')\n","    return DataSets, feature_names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HprRItOZV8o6"},"source":["##**LightGBM - Lightweight Gradient-Boosted Decision Tree**"]},{"cell_type":"code","metadata":{"id":"CyAnC6q2qG6w","cellView":"both"},"source":["\n","###############################################################################\n","def unscale(scaler,target):\n","    return scaler.inverse_transform(target.reshape(-1, 1)).squeeze()\n","\n","###############################################################################\n","###############################################################################\n","###############################################################################\n","@time_model_fit\n","@time_model_predict\n","def gbdt_model(filename_submission, DataSets, gbdt_setup_params, gbdt_fit_params,\n","               clip_predict_H, clip_predict_L, feather_tvt_split, test, model_type, robust_scalers, minmax_scalers):\n","    \"\"\"\n","    DataSets dictionary includes train_X, train_y, val_X, val_y, test_X (keys = string version of values), or filenames (if stored on disk)\n","    RUN_n is the nth model being trained/fit/predicted/ensembled\n","    lgbm model = LightGBM is a particular case of a gradient-boosted decision tree model, so it is a subroutine in this GBDT function\n","    \"\"\"\n","    print(f'Start Gradient-Boosted Decision Tree Modeling Module: {strftime(\"%a %X %x\")}')\n","    MEMORY_STATS.append(get_memory_stats('At Top of Modeling Function',printout=False))\n","    if feather_tvt_split:  # load train_X, train_y, val_X, val_y, test_X dataframes from disk\n","        print(f'DataSet Feather File Source Directory: ', end='')\n","        block_time.restart_time()\n","        %cd \"{OUT_OF_REPO_PATH}\"\n","        for dfname, df_filename in DataSets.items():\n","            DataSets[dfname] = pd.read_feather(DataSets[df_filename], columns=None, use_threads=True)\n","        print(\"Loaded DataFrames train_X, train_y, val_X, val_y, test_X from Google Drive feather files into Colab...\")\n","        print(f'Elapsed time loading feather files: {block_time.get_elapsed_time()}\\n')\n","        MEMORY_STATS.append(get_memory_stats('After Modeling Fn ftr Load',printout=False))\n","\n","    ########################################################################################################\n","    ''' ###################  multiproc?  ################### '''\n","    #########################################\n","    if model_type == 'LGBM':\n","        print('Starting training...')\n","        time_model_fit.restart_time()\n","        model_lgbm = lgb.LGBMRegressor(**gbdt_setup_params)\n","        model_lgbm.fit(\n","            DataSets['train_X'],                       # Input feature matrix or df 'train_X' (array-like or sparse of shape = [n_samples, n_features])\n","            DataSets['train_y'],                       # The target values 'train_y' (real numbers in regression) (array-like of shape = [n_samples])\n","            [(DataSets['val_X'], DataSets['val_y'])],  # 'eval_set' list [(val_X, val_y)] can have multiple tuples of validation data inside this list\n","            None,                                      # eval_names = Names of eval_set (list of strings or None, optional (default=None))\n","            **gbdt_fit_params)\n","        OUTPUTS_df.at[RUN_n,\"time_model_fit\"]           = time_model_fit.get_elapsed_time()  # HH:MM:SS\n","        OUTPUTS_df.at[RUN_n,\"best_iteration_\"]          = model_lgbm.best_iteration_\n","        OUTPUTS_df.at[RUN_n,\"best_score_\"]              = model_lgbm.best_score_['valid_0']['rmse']\n","        OUTPUTS_df.at[RUN_n,\"feature_name_\"]            = model_lgbm.feature_name_    # The names of features, in an array of shape [n_features]\n","        OUTPUTS_df.at[RUN_n,\"feature_importances_\"]     = model_lgbm.feature_importances_\n","        OUTPUTS_df.at[RUN_n,\"model_params\"]             = model_lgbm.get_params()\n","        print(f'Done fitting; Model LGBM fit time: {OUTPUTS_df.at[RUN_n, \"time_model_fit\"]}')\n","\n","    ########################################################################################################\n","    ''' ###################  multiproc?  ################### '''\n","    #########################################\n","    print(\"Starting predictions...\")\n","    time_model_predict.restart_time()\n","    ''' ###################  multiproc?  ################### '''\n","    y_pred_train =  model_lgbm.predict( DataSets['train_X'], num_iteration=model_lgbm.best_iteration_ )\n","    y_pred_val =    model_lgbm.predict( DataSets['val_X'],   num_iteration=model_lgbm.best_iteration_ )\n","    y_pred_test =   model_lgbm.predict( DataSets['test_X'],  num_iteration=model_lgbm.best_iteration_ )\n","    y_train =       DataSets['train_y'].to_numpy()\n","    y_val =         DataSets['val_y'].to_numpy()\n","    # always do minmax scaling after robust scaling; and do inverse scaling with minmax first, then robust (like here)\n","    if any(minmax_scalers):\n","        ''' ###################  multiproc?  ################### '''\n","        y_pred_train =  unscale(minmax_scalers['shop_id_x_item_id_sales_sum'],  y_pred_train)\n","        y_pred_val =    unscale(minmax_scalers['shop_id_x_item_id_sales_sum'],  y_pred_val)\n","        y_pred_test =   unscale(minmax_scalers['shop_id_x_item_id_sales_sum'],  y_pred_test)\n","        y_train =       unscale(minmax_scalers['shop_id_x_item_id_sales_sum'],  y_train)\n","        y_val =         unscale(minmax_scalers['shop_id_x_item_id_sales_sum'],  y_val)\n","    if any(robust_scalers):\n","        ''' ###################  multiproc?  ################### '''\n","        y_pred_train =  unscale(robust_scalers['shop_id_x_item_id_sales_sum'],  y_pred_train)\n","        y_pred_val =    unscale(robust_scalers['shop_id_x_item_id_sales_sum'],  y_pred_val)\n","        y_pred_test =   unscale(robust_scalers['shop_id_x_item_id_sales_sum'],  y_pred_test)\n","        y_train =       unscale(robust_scalers['shop_id_x_item_id_sales_sum'],  y_train)\n","        y_val =         unscale(robust_scalers['shop_id_x_item_id_sales_sum'],  y_val)\n","    ''' ###################  multiproc?  ################### '''\n","    y_pred_train =  y_pred_train.clip(clip_predict_L,clip_predict_H)\n","    y_pred_val =    y_pred_val.clip(clip_predict_L,clip_predict_H)\n","    y_pred_test =   y_pred_test.clip(clip_predict_L,clip_predict_H)\n","    \n","    ''' ###################  multiproc?  ################### '''\n","    OUTPUTS_df.at[RUN_n, \"time_model_predict\"]  = time_model_predict.get_elapsed_time()  # HH:MM:SS\n","    OUTPUTS_df.at[RUN_n, 'tr_R2']               = sklearn.metrics.r2_score(y_train, y_pred_train)   \n","    OUTPUTS_df.at[RUN_n, 'val_R2']              = sklearn.metrics.r2_score(y_val, y_pred_val)\n","    OUTPUTS_df.at[RUN_n, 'tr_rmse']             = np.sqrt(sklearn.metrics.mean_squared_error(y_train, y_pred_train)) \n","    OUTPUTS_df.at[RUN_n, 'val_rmse']            = np.sqrt(sklearn.metrics.mean_squared_error(y_val, y_pred_val))\n","    \n","    print(f'Model LGBM fit time: {OUTPUTS_df.at[RUN_n, \"time_model_fit\"]}')\n","    print(f'Transform and Predict train/val/test time: {OUTPUTS_df.at[RUN_n, \"time_model_predict\"]}')\n","    print(f'R^2 train  = {OUTPUTS_df.at[RUN_n,\"tr_R2\"]:.4f}    R^2 val  = {OUTPUTS_df.at[RUN_n,\"val_R2\"]:.4f}')\n","    print(f'RMSE train = {OUTPUTS_df.at[RUN_n,\"tr_rmse\"]:.4f}    RMSE val = {OUTPUTS_df.at[RUN_n,\"val_rmse\"]:.4f}\\n')\n","\n","    a = sklearn.metrics.r2_score(y_train, y_pred_train)   \n","    b = sklearn.metrics.r2_score(y_val, y_pred_val)\n","    c = np.sqrt(sklearn.metrics.mean_squared_error(y_train, y_pred_train)) \n","    d = np.sqrt(sklearn.metrics.mean_squared_error(y_val, y_pred_val))\n","    print(f'a R^2 train  = {a:.4f}    b R^2 val  = {b:.4f}')\n","    print(f'c RMSE train = {c:.4f}    d RMSE val = {d:.4f}\\n')\n","\n","    # re-format feature importances? dict with key=featurename val=importance?\n","    # save model?\n","\n","    # Merge the test predictions with IDs from the original test dataset, and keep only columns \"ID\" and \"item_cnt_month\"\n","    y_submission = pd.DataFrame.from_dict({'item_cnt_month':y_pred_test,'shop_id':DataSets['X_test'].shop_id,'item_id':DataSets['X_test'].item_id})\n","    y_submission = test.merge(y_submission, on=['shop_id','item_id'], how= 'left').reset_index(drop=True).drop(['shop_id','item_id'],axis=1)\n","    # save prediction for every one of the run iterations; can ensemble them later if desired\n","    %cd \"{GDRIVE_REPO_PATH}\"\n","    y_submission.to_csv(\"./models_and_predictions/\" + filename_submission, index=False)\n","    MEMORY_STATS.append(get_memory_stats('End of Modeling and Predictions',printout=True))\n","    # print(f'Modeling and Predictions Done: {strftime(\"%a %X %x\")}')\n","\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mlt0pnbYxmPI"},"source":["##**Main Control Loop**"]},{"cell_type":"code","metadata":{"id":"T9XG5rwRB57i","executionInfo":{"status":"error","timestamp":1599641230170,"user_tz":240,"elapsed":18069,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"200579db-18be-4bf9-957e-612acb2865b6","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if __name__ == '__main__':  # possibly needed for multiprocessing to work smoothly??\n","    # Main Control Loop\n","    # model_cols          = ['model_filename_base','model_type','feature_params','data_sources']\n","    # eda_cols            = ['eda_delete_shops','eda_delete_item_cats','eda_scale_month','feather_stt']\n","    # data_cols           = ['cartprod_fillna0','cartprod_first_month','cartprod_test_pairs','clip_train_H','clip_train_L','feather_monthly_stt',\n","    #                         'feature_data_type','minmax_scaler_range','robust_scaler_quantiles','use_cartprod_fill','use_categorical',\n","    #                         'use_minmax_scaler','use_robust_scaler']\n","    # tvt_split_cols            = ['feather_tvt_split','test_month','train_start_month','train_final_month','validate_months']\n","    # lgbm_setup_cols     = ['boosting_type','metric','learning_rate','n_estimators','colsample_bytree','random_state','subsample_for_bin','num_leaves',\n","    #                         'max_depth','min_split_gain','min_child_weight','min_child_samples','silent','importance_type','reg_alpha','reg_lambda',\n","    #                         'n_jobs','subsample','subsample_freq','objective']\n","    # lgbm_fit_cols       = ['eval_metric','early_stopping_rounds','init_score','eval_init_score','verbose','feature_name','categorical_feature','callbacks']\n","    # processing_cols     = ['clip_predict_H','clip_predict_L']\n","    # OUTPUT_cols         = ['model_filename','best_iteration_','best_score_','feature_importances_','feature_name_','model_params','time_cumulative',\n","    #                         'time_data_manip','time_dataset_splits','time_eda','time_full_iteration','time_model_fit','time_model_predict',\n","    #                         'tr_R2','tr_rmse','val_R2','val_rmse','runtime_type','n_cpus','ram_gb','package_versions']\n","\n","    # \"exploded DataFrames\" --> OrderedDicts for straightforward looping/enumeration of splits\n","    model_params_dict = SPLIT_PARAMS_module_dfs['MODEL'].to_dict('index', into=OrderedDict)\n","    eda_params_dict = SPLIT_PARAMS_module_dfs['EDA'].to_dict('index', into=OrderedDict)\n","    data_params_dict = SPLIT_PARAMS_module_dfs['DATA'].to_dict('index', into=OrderedDict)\n","    tvt_split_params_dict = SPLIT_PARAMS_module_dfs['TVT_SPLIT'].to_dict('index', into=OrderedDict)\n","    lgbm_setup_params_dict = SPLIT_PARAMS_module_dfs['LGBM_SETUP'].to_dict('index', into=OrderedDict)\n","    lgbm_fit_params_dict = SPLIT_PARAMS_module_dfs['LGBM_FIT'].to_dict('index', into=OrderedDict)\n","    processing_params_dict = SPLIT_PARAMS_module_dfs['PROCESSING'].to_dict('index', into=OrderedDict)\n","\n","    RUN_n = 0\n","    time_full_iteration.restart_time()\n","    time_cumulative.restart_time()\n","    for model_iter_num, model_iter_params in model_params_dict.items(): #########################################\n","        if not model_iter_params['model_filename_base']:\n","            model_iter_params['model_filename_base'] = input(\"Enter the Base Model Filename Substring for Output (like: 'v4mg_01' )\")\n","        model_done = (model_iter_num+1 == len(model_params_dict)) # True if we have finished all splits in model_params_dict\n","\n","        for eda_iter_num, eda_iter_params in eda_params_dict.items():   #########################################\n","            # load datafiles, adjust stt for monthly stats grouping, output = stt, items_enc, shops_enc, test \n","            eda_iter_params.update(model_iter_params)  # add to eda dict because feature_params, data files, names are needed for eda module\n","            ''' ###################  multiproc??  ################### '''\n","            print(eda_iter_params.keys())\n","            #########################################\n","            load_dfs, dfidx = eda_cleanup(**eda_iter_params)    # load_dfs dict holds stt, shops_enc, items_enc, test dfs (or filenames if feathered)\n","            #########################################\n","            test = load_dfs[dfidx['test']]\n","            if type(test) == str:\n","                %cd \"{OUT_OF_REPO_PATH}\"\n","                test = pd.read_feather(test, columns=None, use_threads=True)    # save for later to configure Coursera submission\n","            eda_done = (model_done and (eda_iter_num+1 == len(eda_params_dict)))    # True if finished all splits in model_params_dict and eda_params_dict\n","\n","            # print(f'test:\\n{test.head()}')\n","            # print(f'dfidx: {dfidx}\\n')\n","            # print(f'load_dfs: {load_dfs}\\n')\n","            # for i in ['stt','shops_enc','items_enc']:\n","            #     print(f'{i}:\\n{load_dfs[dfidx[i]]}\\n')\n","\n","\n","            for data_iter_num, data_iter_params in data_params_dict.items():  #########################################\n","                # compute grouped-by-month agg stats features, add cartesian product rows, add lagged feature columns\n","                data_iter_params.update(eda_iter_params)  # add to data dict because feature_params, feather_stt needed for data conditioning module\n","                ''' ###################  multiproc??  ################### '''\n","\n","\n","                # print(f'dataiterparams= {data_iter_params}')\n","                # print(f'dataiterparamskeys = {list(data_iter_params.keys())}')\n","                # for k,v in data_iter_params.items():\n","                #     print(k,v)\n","\n","\n","                #########################################\n","                monthly_stt, robust_scalers, minmax_scalers = data_conditioning(\n","                                    load_dfs[dfidx['stt']],load_dfs[dfidx['shops_enc']],load_dfs[dfidx['items_enc']], **data_iter_params)\n","                #########################################\n","                data_cond_done = (eda_done and (data_iter_num+1 == len(data_params_dict)))\n","                if data_cond_done: \n","                    try:  # (no future splits need these dfs)\n","                        del [load_dfs[dfidx['stt']],load_dfs[dfidx['shops_enc']],load_dfs[dfidx['items_enc']],load_dfs[dfidx['date_scaling']]]\n","                    except:\n","                        print(\"Couldn't delete load_dfs: stt, shops, items\")\n","                        \n","                for tvt_iter_num, tvt_split_iter_params in tvt_split_params_dict.items():  #########################################\n","                    # split data into train/val/test and assign desired datatypes where needed\n","                    tvt_split_iter_params['feather_monthly_stt'] = data_iter_params['feather_monthly_stt']\n","                    tvt_split_iter_params['feature_data_type'] = data_iter_params['feature_data_type']\n","                    tvt_split_iter_params['use_categorical'] = data_iter_params['use_categorical']\n","                    tvt_split_iter_params['categorical_features'] = model_iter_params['feature_params']['categorical']\n","                    tvt_split_iter_params['model_type'] = model_iter_params['model_type']\n","                    tvt_xy_datasets = {'train_X':monthly_stt}\n","                    ''' ###################  multiproc??  ################### '''\n","                    #########################################\n","                    DataSets, feature_names = tvt_split_function(tvt_xy_datasets, **tvt_split_iter_params)\n","                    #########################################\n","\n","                    #OUTPUTS_df.at[RUN_n, 'feature_name_'] = feature_names #??? get this from lgbm model fit routine?\n","\n","                    for lgbm_setup_iter_num, lgbm_setup_iter_params in lgbm_setup_params_dict.items():  #########################################\n","                        for lgbm_fit_iter_num, lgbm_fit_iter_params in lgbm_fit_params_dict.items():    #########################################\n","\n","                            for processing_iter_num, processing_iter_params in processing_params_dict.items():  ############################\n","                                processing_iter_params['feather_tvt_split'] = tvt_split_iter_params['feather_tvt_split']\n","                                processing_iter_params['test'] = test\n","                                processing_iter_params['model_type'] = model_iter_params['model_type']\n","                                #processing_iter_params['feature_name_'] = feature_names\n","                                processing_iter_params['robust_scalers'] = robust_scalers # if any(robust_scalers): then do inverse scaler xform\n","                                processing_iter_params['minmax_scalers'] = minmax_scalers # if any(minmax_scalers): then do inverse scaler xform\n","                                base_filename = f'{model_iter_params[\"model_type\"]}_{model_iter_params[\"model_filename_base\"]}'\n","                                OUTPUTS_df.at[RUN_n,\"model_filename\"] = f'{base_filename}_{RUN_n:02d}'\n","                                filename_results =    f'{base_filename}_results.csv'\n","                                filename_submission = f'{OUTPUTS_df.at[RUN_n,\"model_filename\"]}_submission.csv'\n","                            \n","                                ''' ###################  multiproc??  ################### '''\n","                                #########################################\n","                                gbdt_model(filename_submission, DataSets, lgbm_setup_iter_params, lgbm_fit_iter_params, **processing_iter_params)\n","                                #########################################\n","\n","                                OUTPUTS_df.at[RUN_n,'time_full_iteration'] = time_full_iteration.get_elapsed_time()\n","                                OUTPUTS_df.at[RUN_n,'time_cumulative'] = time_cumulative.get_elapsed_time()\n","                                %cd \"{GDRIVE_REPO_PATH}\"\n","                                OUTPUTS_df.to_csv(\"./models_and_predictions/\" + filename_results, index=False) # save intermediate/ final params+outputs \n","\n","                                RUN_n += 1\n","                                if RUN_n < ALL_exploded_shape[0]:\n","                                    time_full_iteration.restart_time()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dict_keys(['eda_delete_shops', 'eda_delete_item_cats', 'eda_scale_month', 'feather_stt', 'model_filename_base', 'model_type', 'feature_params', 'data_sources'])\n","Start EDA Module: Wed 04:46:51 09/09/20\n","/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","Loading Files from Google Drive repo into Colab...\n","\n","---------- items_enc ----------\n","DataFrame shape: (22170, 4)     DataFrame total memory usage: 1 MB\n","DataFrame Column Names: ['item_id', 'item_category_id', 'item_group', 'item_cluster']\n","^^^^^\n","   item_id  item_category_id  item_group  item_cluster\n","0        0                40           6           100\n","1        1                76           6           105\n","\n","\n","---------- shops_enc ----------\n","DataFrame shape: (60, 2)     DataFrame total memory usage: 0 MB\n","DataFrame Column Names: ['shop_id', 'shop_group']\n","^^^^^\n","   shop_id  shop_group\n","0        0           7\n","1        1           7\n","\n","\n","---------- date_scaling ----------\n","DataFrame shape: (35, 2)     DataFrame total memory usage: 0 MB\n","DataFrame Column Names: ['month', 'week_retail_weight']\n","^^^^^\n","   month  week_retail_weight\n","0      0               1.030\n","1      1               1.146\n","\n","\n","---------- stt ----------\n","DataFrame shape: (3150043, 5)     DataFrame total memory usage: 126 MB\n","DataFrame Column Names: ['month', 'sales', 'price', 'shop_id', 'item_id']\n","^^^^^\n","   month  sales  price  shop_id  item_id\n","0      0      1     99        2      991\n","1      0      1   2599        2     1472\n","\n","\n","---------- test ----------\n","DataFrame shape: (214200, 3)     DataFrame total memory usage: 5 MB\n","DataFrame Column Names: ['ID', 'shop_id', 'item_id']\n","^^^^^\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","\n","\n","---------- merged stt ----------\n","DataFrame shape: (3150043, 9)\n","DataFrame total memory usage: 252 MB\n","DataFrame Column Names: ['month', 'sales', 'price', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']\n","   Column Name      DType    MBytes               Column Name      DType    MBytes       \n","         Index      int64      25.2                   item_id      int64      25.2       \n","         month      int64      25.2                shop_group      int64      25.2       \n","         sales    float64      25.2          item_category_id      int64      25.2       \n","         price    float64      25.2                item_group      int64      25.2       \n","^^^^^\n","   month  sales  price  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0      1     99        2      991           9                67           5           463\n","1      0      1   2599        2     1472           9                23           7           585\n","\n","\n","----------\n","\n","Shape of stt after deleting shops [9, 20]: (3144500, 9)\n","Shape of stt after deleting item categories [8, 10, 32, 59, 80, 81, 82]: (3124617, 9)\n","\n","---------- final stt ----------\n","DataFrame shape: (3124617, 9)\n","DataFrame total memory usage: 94 MB\n","DataFrame Column Names: ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']\n","   Column Name      DType    MBytes               Column Name    DType    MBytes       \n","         Index      int64       0.0                   item_id    int16       6.2       \n","         month      int16       6.2                shop_group    int16       6.2       \n","         sales    float64      25.0          item_category_id    int16       6.2       \n","       revenue    float64      25.0                item_group    int16       6.2       \n","^^^^^\n","   month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0  1.030    0.102        2      991           9                67           5           463\n","1      0  1.030    2.677        2     1472           9                23           7           585\n","\n","\n","/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final\n","load_dfs feather files stored on google drive in \"final\" directory, outside repo:\n","items_enc_temp.ftr, shops_enc_temp.ftr, date_scaling_temp.ftr, stt_temp.ftr, test_temp.ftr, Elapsed time writing EDA feather files: 00:00:00\n","\n","EDA Module elapsed time: 00:00:08\n","                                                        |  pid   |               vm                             |\n","  Time and Date       |   Measurement Point             | pid-GB | used-GB | avail-GB | total-GB | Active Procs |\n","Wed 03:36:14 09/09/20 | At Memory Stats Func Def        |   0.14 |    0.62 |    26.77 |    27.39 | []\n","Wed 03:36:15 09/09/20 | Iteration Parameters Defined    |   0.15 |    0.62 |    26.77 |    27.39 | []\n","Wed 03:36:42 09/09/20 | Google Drive mounted            |   0.15 |    0.64 |    26.75 |    27.39 | []\n","Wed 03:36:43 09/09/20 | At Top of EDA Function          |   0.15 |    0.65 |    26.74 |    27.39 | []\n","Wed 03:36:48 09/09/20 | load_dfs_pool Closed, Joined    |   0.39 |    0.93 |    26.47 |    27.39 | []\n","Wed 04:46:51 09/09/20 | At Top of EDA Function          |   1.17 |    1.76 |    25.63 |    27.39 | []\n","Wed 04:46:55 09/09/20 | load_dfs_pool Closed, Joined    |   1.17 |    1.88 |    25.52 |    27.39 | []\n","Wed 04:46:56 09/09/20 | stt Merged with shops and items |   1.17 |    3.13 |    24.26 |    27.39 | []\n","Wed 04:46:58 09/09/20 | stt Transforms Completed        |   1.17 |    5.50 |    21.89 |    27.39 | []\n","Wed 04:46:59 09/09/20 | After EDA Func .ftr Write       |   1.17 |    6.34 |    21.05 |    27.39 | []\n","Done EDA Module: Wed 04:46:59 09/09/20\n","/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final\n","In data cond fn\n","Start Data Module: Wed 04:46:59 09/09/20\n","At start of feather_stt load\n","Feather File Source Directory: /content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final\n","Loaded DataFrames 'stt', 'shops_enc', 'items_enc' from Google Drive feather files into Colab...\n","Elapsed time loading feather file DataFrames: 00:00:00\n","\n","group ['shop_id', 'item_id']\n","stats {'shop_group': ['first'], 'item_category_id': ['first'], 'item_group': ['first'], 'item_cluster': ['first'], 'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}\n","agg_names ['shop_group', 'item_category_id', 'item_group', 'item_cluster', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_revenue_sum']\n","\n","\n","group ['shop_id', 'item_category_id']\n","stats {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}\n","agg_names ['shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_revenue_sum']\n","\n","\n","group ['shop_id', 'item_cluster']\n","stats {'sales': ['sum', 'median']}\n","agg_names ['shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median']\n","\n","\n","group ['shop_id']\n","stats {'sales': ['count', 'sum']}\n","agg_names ['shop_id_sales_count', 'shop_id_sales_sum']\n","\n","\n","group ['item_id']\n","stats {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}\n","agg_names ['item_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_revenue_sum']\n","\n","\n","group ['shop_group']\n","stats {'revenue': ['sum']}\n","agg_names ['shop_group_revenue_sum']\n","\n","\n","group ['item_category_id']\n","stats {'sales': ['count', 'sum'], 'revenue': ['sum']}\n","agg_names ['item_category_id_sales_count', 'item_category_id_sales_sum', 'item_category_id_revenue_sum']\n","\n","\n","group ['item_group']\n","stats {'sales': ['sum'], 'revenue': ['sum']}\n","agg_names ['item_group_sales_sum', 'item_group_revenue_sum']\n","\n","\n","group ['item_cluster']\n","stats {'sales': ['count', 'sum'], 'revenue': ['sum']}\n","agg_names ['item_cluster_sales_count', 'item_cluster_sales_sum', 'item_cluster_revenue_sum']\n","\n","\n","[         month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0            0  1.030    0.102        2      991           9                67           5           463\n","1            0  1.030    2.677        2     1472           9                23           7           585\n","2            0  1.030    0.256        2     1905           9                30           6           265\n","3            0  2.060    1.234        2     2920           9                21           2           515\n","4            0  1.030    2.059        2     3320           9                19           5           313\n","...        ...    ...      ...      ...      ...         ...               ...         ...           ...\n","3124612     34      0        0       59    22162           6                40           6           737\n","3124613     34      0        0       59    22163           6                40           6           737\n","3124614     34      0        0       59    22164           6                37          18           737\n","3124615     34      0        0       59    22166           6                54           6           215\n","3124616     34      0        0       59    22167           6                49          19           218\n","\n","[3124617 rows x 9 columns], {'group': ['shop_id', 'item_id'], 'stats': {'shop_group': ['first'], 'item_category_id': ['first'], 'item_group': ['first'], 'item_cluster': ['first'], 'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_group', 'item_category_id', 'item_group', 'item_cluster', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_sales_sum', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_revenue_sum']}]\n","[         month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0            0  1.030    0.102        2      991           9                67           5           463\n","1            0  1.030    2.677        2     1472           9                23           7           585\n","2            0  1.030    0.256        2     1905           9                30           6           265\n","3            0  2.060    1.234        2     2920           9                21           2           515\n","4            0  1.030    2.059        2     3320           9                19           5           313\n","...        ...    ...      ...      ...      ...         ...               ...         ...           ...\n","3124612     34      0        0       59    22162           6                40           6           737\n","3124613     34      0        0       59    22163           6                40           6           737\n","3124614     34      0        0       59    22164           6                37          18           737\n","3124615     34      0        0       59    22166           6                54           6           215\n","3124616     34      0        0       59    22167           6                49          19           218\n","\n","[3124617 rows x 9 columns], {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_revenue_sum']}]\n","[         month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0            0  1.030    0.102        2      991           9                67           5           463\n","1            0  1.030    2.677        2     1472           9                23           7           585\n","2            0  1.030    0.256        2     1905           9                30           6           265\n","3            0  2.060    1.234        2     2920           9                21           2           515\n","4            0  1.030    2.059        2     3320           9                19           5           313\n","...        ...    ...      ...      ...      ...         ...               ...         ...           ...\n","3124612     34      0        0       59    22162           6                40           6           737\n","3124613     34      0        0       59    22163           6                40           6           737\n","3124614     34      0        0       59    22164           6                37          18           737\n","3124615     34      0        0       59    22166           6                54           6           215\n","3124616     34      0        0       59    22167           6                49          19           218\n","\n","[3124617 rows x 9 columns], {'group': ['shop_id', 'item_cluster'], 'stats': {'sales': ['sum', 'median']}, 'agg_names': ['shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median']}]\n","[         month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0            0  1.030    0.102        2      991           9                67           5           463\n","1            0  1.030    2.677        2     1472           9                23           7           585\n","2            0  1.030    0.256        2     1905           9                30           6           265\n","3            0  2.060    1.234        2     2920           9                21           2           515\n","4            0  1.030    2.059        2     3320           9                19           5           313\n","...        ...    ...      ...      ...      ...         ...               ...         ...           ...\n","3124612     34      0        0       59    22162           6                40           6           737\n","3124613     34      0        0       59    22163           6                40           6           737\n","3124614     34      0        0       59    22164           6                37          18           737\n","3124615     34      0        0       59    22166           6                54           6           215\n","3124616     34      0        0       59    22167           6                49          19           218\n","\n","[3124617 rows x 9 columns], {'group': ['shop_id'], 'stats': {'sales': ['count', 'sum']}, 'agg_names': ['shop_id_sales_count', 'shop_id_sales_sum']}]\n","[         month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0            0  1.030    0.102        2      991           9                67           5           463\n","1            0  1.030    2.677        2     1472           9                23           7           585\n","2            0  1.030    0.256        2     1905           9                30           6           265\n","3            0  2.060    1.234        2     2920           9                21           2           515\n","4            0  1.030    2.059        2     3320           9                19           5           313\n","...        ...    ...      ...      ...      ...         ...               ...         ...           ...\n","3124612     34      0        0       59    22162           6                40           6           737\n","3124613     34      0        0       59    22163           6                40           6           737\n","3124614     34      0        0       59    22164           6                37          18           737\n","3124615     34      0        0       59    22166           6                54           6           215\n","3124616     34      0        0       59    22167           6                49          19           218\n","\n","[3124617 rows x 9 columns], {'group': ['item_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}, 'agg_names': ['item_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_revenue_sum']}]\n","[         month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0            0  1.030    0.102        2      991           9                67           5           463\n","1            0  1.030    2.677        2     1472           9                23           7           585\n","2            0  1.030    0.256        2     1905           9                30           6           265\n","3            0  2.060    1.234        2     2920           9                21           2           515\n","4            0  1.030    2.059        2     3320           9                19           5           313\n","...        ...    ...      ...      ...      ...         ...               ...         ...           ...\n","3124612     34      0        0       59    22162           6                40           6           737\n","3124613     34      0        0       59    22163           6                40           6           737\n","3124614     34      0        0       59    22164           6                37          18           737\n","3124615     34      0        0       59    22166           6                54           6           215\n","3124616     34      0        0       59    22167           6                49          19           218\n","\n","[3124617 rows x 9 columns], {'group': ['shop_group'], 'stats': {'revenue': ['sum']}, 'agg_names': ['shop_group_revenue_sum']}]\n","[         month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0            0  1.030    0.102        2      991           9                67           5           463\n","1            0  1.030    2.677        2     1472           9                23           7           585\n","2            0  1.030    0.256        2     1905           9                30           6           265\n","3            0  2.060    1.234        2     2920           9                21           2           515\n","4            0  1.030    2.059        2     3320           9                19           5           313\n","...        ...    ...      ...      ...      ...         ...               ...         ...           ...\n","3124612     34      0        0       59    22162           6                40           6           737\n","3124613     34      0        0       59    22163           6                40           6           737\n","3124614     34      0        0       59    22164           6                37          18           737\n","3124615     34      0        0       59    22166           6                54           6           215\n","3124616     34      0        0       59    22167           6                49          19           218\n","\n","[3124617 rows x 9 columns], {'group': ['item_category_id'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_category_id_sales_count', 'item_category_id_sales_sum', 'item_category_id_revenue_sum']}]\n","[         month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0            0  1.030    0.102        2      991           9                67           5           463\n","1            0  1.030    2.677        2     1472           9                23           7           585\n","2            0  1.030    0.256        2     1905           9                30           6           265\n","3            0  2.060    1.234        2     2920           9                21           2           515\n","4            0  1.030    2.059        2     3320           9                19           5           313\n","...        ...    ...      ...      ...      ...         ...               ...         ...           ...\n","3124612     34      0        0       59    22162           6                40           6           737\n","3124613     34      0        0       59    22163           6                40           6           737\n","3124614     34      0        0       59    22164           6                37          18           737\n","3124615     34      0        0       59    22166           6                54           6           215\n","3124616     34      0        0       59    22167           6                49          19           218\n","\n","[3124617 rows x 9 columns], {'group': ['item_group'], 'stats': {'sales': ['sum'], 'revenue': ['sum']}, 'agg_names': ['item_group_sales_sum', 'item_group_revenue_sum']}]\n","[         month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0            0  1.030    0.102        2      991           9                67           5           463\n","1            0  1.030    2.677        2     1472           9                23           7           585\n","2            0  1.030    0.256        2     1905           9                30           6           265\n","3            0  2.060    1.234        2     2920           9                21           2           515\n","4            0  1.030    2.059        2     3320           9                19           5           313\n","...        ...    ...      ...      ...      ...         ...               ...         ...           ...\n","3124612     34      0        0       59    22162           6                40           6           737\n","3124613     34      0        0       59    22163           6                40           6           737\n","3124614     34      0        0       59    22164           6                37          18           737\n","3124615     34      0        0       59    22166           6                54           6           215\n","3124616     34      0        0       59    22167           6                49          19           218\n","\n","[3124617 rows x 9 columns], {'group': ['item_cluster'], 'stats': {'sales': ['count', 'sum'], 'revenue': ['sum']}, 'agg_names': ['item_cluster_sales_count', 'item_cluster_sales_sum', 'item_cluster_revenue_sum']}]\n","in compute stats\n","in compute stats1 ['shop_id', 'item_id']\n","in compute stats2 ['month', 'shop_id', 'item_id'] {'shop_group': ['first'], 'item_category_id': ['first'], 'item_group': ['first'], 'item_cluster': ['first'], 'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}\n","stt coming up\n","stt_head    month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0  1.030    0.102        2      991           9                67           5           463\n","1      0  1.030    2.677        2     1472           9                23           7           585\n","in compute stats\n","in compute stats1 ['shop_id', 'item_category_id']\n","in compute stats2 ['month', 'shop_id', 'item_category_id'] {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}\n","stt coming up\n","stt_head    month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0  1.030    0.102        2      991           9                67           5           463\n","1      0  1.030    2.677        2     1472           9                23           7           585\n","in compute stats\n","in compute stats1 ['shop_id']\n","in compute stats\n","in compute stats2 ['month', 'shop_id'] {'sales': ['count', 'sum']}\n","stt coming up\n","in compute stats1 ['shop_id', 'item_cluster']\n","in compute stats2 ['month', 'shop_id', 'item_cluster'] {'sales': ['sum', 'median']}\n","stt coming up\n","stt_head    month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0  1.030    0.102        2      991           9                67           5           463\n","1      0  1.030    2.677        2     1472           9                23           7           585stt_head    month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0  1.030    0.102        2      991           9                67           5           463\n","1      0  1.030    2.677        2     1472           9                23           7           585\n","\n","in compute stats3\n","in compute stats4\n","in compute stats5\n","gpr in compute stats6:  [       month  shop_id  item_category_id  shop_id_x_item_category_id_sales_count  shop_id_x_item_category_id_sales_sum  shop_id_x_item_category_id_sales_median  shop_id_x_item_category_id_revenue_sum\n","0          0        2                 2                                      25                                25.748                                    1.030                                  50.559\n","1          0        2                 3                                       6                                 9.269                                    1.030                                   4.625\n","2          0        2                 4                                       8                                10.299                                    1.030                                   9.409\n","3          0        2                 5                                       9                                10.299                                    1.030                                  11.468\n","4          0        2                 6                                      11                                12.359                                    1.030                                  28.142\n","...      ...      ...               ...                                     ...                                   ...                                      ...                                     ...\n","66037     34       59                76                                      61                                     0                                        0                                       0\n","66038     34       59                77                                      16                                     0                                        0                                       0\n","66039     34       59                78                                      49                                     0                                        0                                       0\n","66040     34       59                79                                       1                                     0                                        0                                       0\n","66041     34       59                83                                       4                                     0                                        0                                       0\n","\n","[66042 rows x 7 columns], ['month', 'shop_id', 'item_category_id']]\n","in compute stats\n","in compute stats1 ['item_id']\n","in compute stats2 ['month', 'item_id'] {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}\n","stt coming up\n","stt_head    month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0  1.030    0.102        2      991           9                67           5           463\n","1      0  1.030    2.677        2     1472           9                23           7           585\n","in compute stats3\n","in compute stats4\n","in compute stats5\n","gpr in compute stats6:  [      month  shop_id  shop_id_sales_count  shop_id_sales_sum\n","0         0        2                 1056          1,154.527\n","1         0        3                  749            789.939\n","2         0        4                 1815          2,040.248\n","3         0        6                 3185          3,690.160\n","4         0        7                 2276          2,567.561\n","...     ...      ...                  ...                ...\n","1616     34       55                 5100                  0\n","1617     34       56                 5100                  0\n","1618     34       57                 5100                  0\n","1619     34       58                 5100                  0\n","1620     34       59                 5100                  0\n","\n","[1621 rows x 4 columns], ['month', 'shop_id']]\n","in compute stats\n","in compute stats1 ['shop_group']\n","in compute stats2 ['month', 'shop_group'] {'revenue': ['sum']}\n","stt coming up\n","stt_head    month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0  1.030    0.102        2      991           9                67           5           463\n","1      0  1.030    2.677        2     1472           9                23           7           585\n","in compute stats3\n","in compute stats4\n","in compute stats5\n","gpr in compute stats6:  [     month  shop_group  shop_group_revenue_sum\n","0        0           0               4,610.451\n","1        0           1               5,595.285\n","2        0           2               4,079.176\n","3        0           3               3,234.554\n","4        0           4              15,083.264\n","..     ...         ...                     ...\n","378     34           9                       0\n","379     34          10                       0\n","380     34          11                       0\n","381     34          12                       0\n","382     34          13                       0\n","\n","[383 rows x 3 columns], ['month', 'shop_group']]\n","in compute stats\n","in compute stats1 ['item_category_id']\n","in compute stats2 ['month', 'item_category_id'] {'sales': ['count', 'sum'], 'revenue': ['sum']}\n","stt coming up\n","stt_head    month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0  1.030    0.102        2      991           9                67           5           463\n","1      0  1.030    2.677        2     1472           9                23           7           585\n","in compute stats3\n","in compute stats4\n","in compute stats5\n","gpr in compute stats6:  [        month  shop_id  item_cluster  shop_id_x_item_cluster_sales_sum  shop_id_x_item_cluster_sales_median\n","0           0        2            37                             5.150                                1.030\n","1           0        2            40                            38.107                                1.030\n","2           0        2            41                             1.030                                1.030\n","3           0        2            73                             2.060                                1.030\n","4           0        2            77                             2.060                                1.030\n","...       ...      ...           ...                               ...                                  ...\n","578812     34       59          1481                                 0                                    0\n","578813     34       59          1482                                 0                                    0\n","578814     34       59          1485                                 0                                    0\n","578815     34       59          1486                                 0                                    0\n","578816     34       59          1494                                 0                                    0\n","\n","[578817 rows x 5 columns], ['month', 'shop_id', 'item_cluster']]\n","in compute stats3\n","in compute stats4\n","in compute stats5\n","gpr in compute stats6:  [        month  item_id  item_id_sales_count  item_id_sales_sum  item_id_sales_median  item_id_revenue_sum\n","0           0       19                    1              1.030                 1.030                0.029\n","1           0       27                    7              7.209                 1.030               16.762\n","2           0       28                    8              8.239                 1.030                4.523\n","3           0       29                    6              4.120                 1.030                9.668\n","4           0       32                  225            307.942                 1.030              104.552\n","...       ...      ...                  ...                ...                   ...                  ...\n","236111     34    22162                   42                  0                     0                    0\n","236112     34    22163                   42                  0                     0                    0\n","236113     34    22164                   42                  0                     0                    0\n","236114     34    22166                   42                  0                     0                    0\n","236115     34    22167                   42                  0                     0                    0\n","\n","[236116 rows x 6 columns], ['month', 'item_id']]\n","in compute stats\n","in compute stats1 ['item_group']\n","in compute stats3\n","in compute stats4\n","in compute stats2 ['month', 'item_group'] {'sales': ['sum'], 'revenue': ['sum']}\n","in compute stats5\n","stt coming up\n","gpr in compute stats6:  [      month  item_category_id  item_category_id_sales_count  item_category_id_sales_sum  item_category_id_revenue_sum\n","0         0                 0                             1                       1.030                         0.152\n","1         0                 1                             1                       1.030                         0.152\n","2         0                 2                          1263                   1,431.572                     2,933.749\n","3         0                 3                           334                     453.160                       215.416\n","4         0                 4                           238                     258.507                       239.041\n","...     ...               ...                           ...                         ...                           ...\n","2037     34                76                          2562                           0                             0\n","2038     34                77                           672                           0                             0\n","2039     34                78                          2058                           0                             0\n","2040     34                79                            42                           0                             0\n","2041     34                83                           168                           0                             0\n","\n","[2042 rows x 5 columns], ['month', 'item_category_id']]stt_head    month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0  1.030    0.102        2      991           9                67           5           463\n","1      0  1.030    2.677        2     1472           9                23           7           585\n","\n","in compute stats\n","in compute stats1 ['item_cluster']\n","in compute stats2 ['month', 'item_cluster'] {'sales': ['count', 'sum'], 'revenue': ['sum']}\n","stt coming up\n","stt_head    month  sales  revenue  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster\n","0      0  1.030    0.102        2      991           9                67           5           463\n","1      0  1.030    2.677        2     1472           9                23           7           585\n","in compute stats3\n","in compute stats4\n","in compute stats5\n","gpr in compute stats6:  [     month  item_group  item_group_sales_sum  item_group_revenue_sum\n","0        0           0             3,207.134               2,262.938\n","1        0           1                 1.030                   0.152\n","2        0           2             3,941.458               2,840.458\n","3        0           3             6,481.211               4,932.088\n","4        0           4             5,114.523               3,047.422\n","..     ...         ...                   ...                     ...\n","777     34          21                     0                       0\n","778     34          23                     0                       0\n","779     34          24                     0                       0\n","780     34          25                     0                       0\n","781     34          26                     0                       0\n","\n","[782 rows x 4 columns], ['month', 'item_group']]\n","in compute stats3\n","in compute stats4\n","in compute stats5\n","gpr in compute stats6:  [       month  item_cluster  item_cluster_sales_count  item_cluster_sales_sum  item_cluster_revenue_sum\n","0          0            37                      1014               1,062.865                   344.646\n","1          0            40                      6016               6,656.295                 1,663.518\n","2          0            41                       405                 425.352                   181.701\n","3          0            73                       142                 158.606                   319.721\n","4          0            77                        39                  44.286                    14.957\n","...      ...           ...                       ...                     ...                       ...\n","33568     34          1481                        42                       0                         0\n","33569     34          1482                        42                       0                         0\n","33570     34          1485                        42                       0                         0\n","33571     34          1486                        42                       0                         0\n","33572     34          1494                        42                       0                         0\n","\n","[33573 rows x 5 columns], ['month', 'item_cluster']]\n","in compute stats3\n","in compute stats4\n","in compute stats5\n","gpr in compute stats6:  [         month  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster  shop_id_x_item_id_sales_count  shop_id_x_item_id_sales_sum  shop_id_x_item_id_sales_median  shop_id_x_item_id_revenue_sum\n","0            0        2       27           9                19           5           158                              1                        1.030                           1.030                          2.574\n","1            0        2       33           9                37          18           167                              1                        1.030                           1.030                          0.514\n","2            0        2      317           9                45           6           114                              1                        1.030                           1.030                          0.308\n","3            0        2      438           9                45           6           114                              1                        1.030                           1.030                          0.308\n","4            0        2      471           9                49          19           318                              2                        2.060                           1.030                          0.822\n","...        ...      ...      ...         ...               ...         ...           ...                            ...                          ...                             ...                            ...\n","1811467     34       59    22162           6                40           6           737                              1                            0                               0                              0\n","1811468     34       59    22163           6                40           6           737                              1                            0                               0                              0\n","1811469     34       59    22164           6                37          18           737                              1                            0                               0                              0\n","1811470     34       59    22166           6                54           6           215                              1                            0                               0                              0\n","1811471     34       59    22167           6                49          19           218                              1                            0                               0                              0\n","\n","[1811472 rows x 11 columns], ['month', 'shop_id', 'item_id']]\n","grouped df list [[         month  shop_id  item_id  shop_group  item_category_id  item_group  item_cluster  shop_id_x_item_id_sales_count  shop_id_x_item_id_sales_sum  shop_id_x_item_id_sales_median  shop_id_x_item_id_revenue_sum\n","0            0        2       27           9                19           5           158                              1                        1.030                           1.030                          2.574\n","1            0        2       33           9                37          18           167                              1                        1.030                           1.030                          0.514\n","2            0        2      317           9                45           6           114                              1                        1.030                           1.030                          0.308\n","3            0        2      438           9                45           6           114                              1                        1.030                           1.030                          0.308\n","4            0        2      471           9                49          19           318                              2                        2.060                           1.030                          0.822\n","...        ...      ...      ...         ...               ...         ...           ...                            ...                          ...                             ...                            ...\n","1811467     34       59    22162           6                40           6           737                              1                            0                               0                              0\n","1811468     34       59    22163           6                40           6           737                              1                            0                               0                              0\n","1811469     34       59    22164           6                37          18           737                              1                            0                               0                              0\n","1811470     34       59    22166           6                54           6           215                              1                            0                               0                              0\n","1811471     34       59    22167           6                49          19           218                              1                            0                               0                              0\n","\n","[1811472 rows x 11 columns], ['month', 'shop_id', 'item_id']], [       month  shop_id  item_category_id  shop_id_x_item_category_id_sales_count  shop_id_x_item_category_id_sales_sum  shop_id_x_item_category_id_sales_median  shop_id_x_item_category_id_revenue_sum\n","0          0        2                 2                                      25                                25.748                                    1.030                                  50.559\n","1          0        2                 3                                       6                                 9.269                                    1.030                                   4.625\n","2          0        2                 4                                       8                                10.299                                    1.030                                   9.409\n","3          0        2                 5                                       9                                10.299                                    1.030                                  11.468\n","4          0        2                 6                                      11                                12.359                                    1.030                                  28.142\n","...      ...      ...               ...                                     ...                                   ...                                      ...                                     ...\n","66037     34       59                76                                      61                                     0                                        0                                       0\n","66038     34       59                77                                      16                                     0                                        0                                       0\n","66039     34       59                78                                      49                                     0                                        0                                       0\n","66040     34       59                79                                       1                                     0                                        0                                       0\n","66041     34       59                83                                       4                                     0                                        0                                       0\n","\n","[66042 rows x 7 columns], ['month', 'shop_id', 'item_category_id']], [        month  shop_id  item_cluster  shop_id_x_item_cluster_sales_sum  shop_id_x_item_cluster_sales_median\n","0           0        2            37                             5.150                                1.030\n","1           0        2            40                            38.107                                1.030\n","2           0        2            41                             1.030                                1.030\n","3           0        2            73                             2.060                                1.030\n","4           0        2            77                             2.060                                1.030\n","...       ...      ...           ...                               ...                                  ...\n","578812     34       59          1481                                 0                                    0\n","578813     34       59          1482                                 0                                    0\n","578814     34       59          1485                                 0                                    0\n","578815     34       59          1486                                 0                                    0\n","578816     34       59          1494                                 0                                    0\n","\n","[578817 rows x 5 columns], ['month', 'shop_id', 'item_cluster']], [      month  shop_id  shop_id_sales_count  shop_id_sales_sum\n","0         0        2                 1056          1,154.527\n","1         0        3                  749            789.939\n","2         0        4                 1815          2,040.248\n","3         0        6                 3185          3,690.160\n","4         0        7                 2276          2,567.561\n","...     ...      ...                  ...                ...\n","1616     34       55                 5100                  0\n","1617     34       56                 5100                  0\n","1618     34       57                 5100                  0\n","1619     34       58                 5100                  0\n","1620     34       59                 5100                  0\n","\n","[1621 rows x 4 columns], ['month', 'shop_id']], [        month  item_id  item_id_sales_count  item_id_sales_sum  item_id_sales_median  item_id_revenue_sum\n","0           0       19                    1              1.030                 1.030                0.029\n","1           0       27                    7              7.209                 1.030               16.762\n","2           0       28                    8              8.239                 1.030                4.523\n","3           0       29                    6              4.120                 1.030                9.668\n","4           0       32                  225            307.942                 1.030              104.552\n","...       ...      ...                  ...                ...                   ...                  ...\n","236111     34    22162                   42                  0                     0                    0\n","236112     34    22163                   42                  0                     0                    0\n","236113     34    22164                   42                  0                     0                    0\n","236114     34    22166                   42                  0                     0                    0\n","236115     34    22167                   42                  0                     0                    0\n","\n","[236116 rows x 6 columns], ['month', 'item_id']], [     month  shop_group  shop_group_revenue_sum\n","0        0           0               4,610.451\n","1        0           1               5,595.285\n","2        0           2               4,079.176\n","3        0           3               3,234.554\n","4        0           4              15,083.264\n","..     ...         ...                     ...\n","378     34           9                       0\n","379     34          10                       0\n","380     34          11                       0\n","381     34          12                       0\n","382     34          13                       0\n","\n","[383 rows x 3 columns], ['month', 'shop_group']], [      month  item_category_id  item_category_id_sales_count  item_category_id_sales_sum  item_category_id_revenue_sum\n","0         0                 0                             1                       1.030                         0.152\n","1         0                 1                             1                       1.030                         0.152\n","2         0                 2                          1263                   1,431.572                     2,933.749\n","3         0                 3                           334                     453.160                       215.416\n","4         0                 4                           238                     258.507                       239.041\n","...     ...               ...                           ...                         ...                           ...\n","2037     34                76                          2562                           0                             0\n","2038     34                77                           672                           0                             0\n","2039     34                78                          2058                           0                             0\n","2040     34                79                            42                           0                             0\n","2041     34                83                           168                           0                             0\n","\n","[2042 rows x 5 columns], ['month', 'item_category_id']], [     month  item_group  item_group_sales_sum  item_group_revenue_sum\n","0        0           0             3,207.134               2,262.938\n","1        0           1                 1.030                   0.152\n","2        0           2             3,941.458               2,840.458\n","3        0           3             6,481.211               4,932.088\n","4        0           4             5,114.523               3,047.422\n","..     ...         ...                   ...                     ...\n","777     34          21                     0                       0\n","778     34          23                     0                       0\n","779     34          24                     0                       0\n","780     34          25                     0                       0\n","781     34          26                     0                       0\n","\n","[782 rows x 4 columns], ['month', 'item_group']], [       month  item_cluster  item_cluster_sales_count  item_cluster_sales_sum  item_cluster_revenue_sum\n","0          0            37                      1014               1,062.865                   344.646\n","1          0            40                      6016               6,656.295                 1,663.518\n","2          0            41                       405                 425.352                   181.701\n","3          0            73                       142                 158.606                   319.721\n","4          0            77                        39                  44.286                    14.957\n","...      ...           ...                       ...                     ...                       ...\n","33568     34          1481                        42                       0                         0\n","33569     34          1482                        42                       0                         0\n","33570     34          1485                        42                       0                         0\n","33571     34          1486                        42                       0                         0\n","33572     34          1494                        42                       0                         0\n","\n","[33573 rows x 5 columns], ['month', 'item_cluster']]]\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-f6aee7801209>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m#########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 monthly_stt, robust_scalers, minmax_scalers = data_conditioning(\n\u001b[0;32m---> 70\u001b[0;31m                                     load_dfs[dfidx['stt']],load_dfs[dfidx['shops_enc']],load_dfs[dfidx['items_enc']], **data_iter_params)\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0;31m#########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mdata_cond_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meda_done\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_iter_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_params_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"]}]},{"cell_type":"code","metadata":{"id":"Kr5WvevpX2Uw"},"source":["# domino tiles:  https://www.fileformat.info/info/unicode/block/domino_tiles/utf8test.htm\n","print('\\u2227'*5,'\\u2228'*5,'\\u2303'*5,'\\u2304'*5,'^^^','\\u02c5','\\u02c4','\\u02c6'*5,'\\u02ec'*5,'\\u22c0'*5,'\\u22c1'*5,'\\u2306'*5,'\\u2305'*5,'\\u23f7'*5)\n","print('\\u25b2'*5,'\\u25bc'*5,'\\u25c6'*5,'\\u25d2'*5,'\\u25d3'*5,'\\u25b4'*5,'\\u25be'*5,'\\u2635'*5,'\\u269c'*5,'\\u26d6'*5,'\\u2b81'*5,'\\u2b7f'*5,'\\u26dd'*5)\n","print('\\u26ba'*5,'\\u26bb'*5,'\\u2622'*5,'\\u262f'*5,'\\u2934'*5,'\\u2935'*5,'\\u2b71'*5,'\\u2b73'*5,'\\u2b9d'*5,'\\u2b9f'*5,'\\u2bc5'*5,'\\u2bc6'*5)\n","print('\\u2ba4'*5,'\\u2ba5'*5,'\\u2182'*5,'\\u2180'*5,'\\u2b12'*5,'\\u2b13'*5,'\\u2b18'*5,'\\u2b19'*5,'\\u273d'*5,'\\u2720'*5,'\\u1cf2'*5,'\\u1cf6'*5,'\\u205a'*5,'\\u2021'*5)\n","print('\\u224b'*5,'\\u224d'*5,'\\u2259'*5,'\\u225a'*5,'\\u2263'*5,'\\u2251'*5,'\\u2253'*5,'\\u22ce'*5,'\\u22cf'*5,'\\u2339'*5,'\\u2797'*5,'\\u27d7'*5,'\\u267b'*5)\n","print('\\u29d6'*5,'\\u29d7'*5,'\\u2bc1'*5,'\\u2b27'*5,'\\u2a77'*5,'\\u2a8b'*5,'\\u2ad8'*5,'\\u2e44'*5,'\\u2e0e'*5,'\\u2e1e'*5,'\\u2e1f'*5,'\\u3013'*5)\n","print('\\U0001f503'*5,'\\U0001f501'*5,'\\U0001f3ac','\\U0001f5aa','\\U0001f5ab','\\U0001f536'*5,'\\U0001f06d'*5,'\\U0001f6d1'*5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GF6qWsBIgBUX"},"source":["##**Stop Execution of code below by invoking error**"]},{"cell_type":"code","metadata":{"id":"Hma9PfJGRCHk","cellView":"both"},"source":["# Dummy cell to stop the execution so we don't run any of the random code below (if we select \"Run All\", e.g.)\n","stop_running_code_at_this_cell = yes\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_w0uA8VfBPV7"},"source":["a1 = 'a'\n","l1 = ['m','n','o']\n","[[a1,x] for x in l1]\n","MEMORY_STATS = MEMORY_STATS[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0e0MOvMExQoT"},"source":["##**To Do List:**"]},{"cell_type":"markdown","metadata":{"id":"rxqHPvEuB-En"},"source":["###**Loop over things to compute statistics, column scaling, adding cartesian product rows, and adding lagged features**\n","* multiprocess --> pool(merge,[months list]) do months in parallel? (maybe split monthly_stt by months, then do the merges in parallel, then concatenate all the months back together)\n","* multiprocess --> pool(merge,[lags list]) do lags in parallel?; check for proper column order / reset if necessary  (can I just add the shifted columns, and delete any N/A things where I don't have a shop-item match?, or make a big df with all the lags and then just one single merge (how=\"left\") )\n","\n","###**Loop over model fitting parameter splits**\n","* multiprocess.Pool (if not too overwhelming, can do several (or all) model fitting iterations in parallel)\n","* (?replicate \"test\" with simple code?) so we don't need to load and carry \"test\" dataframe in memory throughout.  Or, load from disk when loading ftr files in prediction module.\n","* del all dataframes containing data, after all loops over model params are done\n","\n","###**Additional routines**\n","* Plot feature importance ... each iteration, and ensemble averages (e.g., if rmse is < xxx).  Make df of feature importances (names=columns) vs. run iteration number (rows) and compute mean, stddev, range, quantiles\n","* Compute ensemble averages: straight average, weighted by rmse, etc.     \n","```\n","Simple ensemble averaging ensemble_y_pred_test = []\n","? ensemble_y_pred_test.append(y_pred_test)\n","? y_test_pred_avg = np.mean(ensemble_y_pred_test, axis=0)\n","compute feature importances averaged over ensemble\n","```\n","* Look at other locations for multiprocessing\n","* Look at other locations for timing blocks (and maybe save in MEMORY_STATS, as in have a MEMORY_STATS append at the end of every timed block)\n","* Look at other locations for MEMORY_STATS\n","</br>\n","\n","* Categorical features with LGBM: double-check it is working?\n","```\n","categorical_feature 🔗︎, default = \"\", type = multi-int or string, aliases: cat_feature, categorical_column, cat_column\n","        used to specify categorical features\n","        use number for index, e.g. categorical_feature=0,1,2 means columns 0, 1 and 2 are categorical features\n","        add a prefix name: for column name, e.g. categorical_feature=name:c1,c2,c3 means c1, c2 and c3\n","        Index starts from 0 and it doesn’t count the label column when passing type is int\n","        All values should be less than Int32.MaxValue (2147483647)\n","        Using large values could be memory consuming. Tree decision rule works best when categorical features are presented by consecutive integers starting from zero\n","        All negative values will be treated as missing values\n","```\n","Scikit-learn API: If ‘auto’ and data is pandas DataFrame, pandas unordered categorical columns are used. *????Note: (WHAT API?) only supports categorical int type (**not applicable** for data represented as **pandas DataFrame** in Python-package)*... double-check that we are using a working API\n","</br>\n","\n","* Special code needed for **GPU** enabled-LGBM modeling??\n","</br>\n","\n","* Possible setup for continued training, especially if we find runtimes are cut off by Colab.  (will probably need to save lgbm.model to disk at each step)...  init_model (string, Booster, LGBMModel or None, optional (default=None)) – \n","Filename of LightGBM model, Booster instance or LGBMModel instance used for **continue training**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LF91nOlf3Km7"},"source":["#**Potentially Useful Code Snippets**"]},{"cell_type":"markdown","metadata":{"id":"8CCw_Tu1xITH"},"source":["###**Ensembling and Trend/Feature Importance**"]},{"cell_type":"code","metadata":{"id":"eRDLNnGzvIIB"},"source":["# ENSEMBLING and FEATURE IMPORTANCE / TRENDS ############################################\n","\n","# ensembling_fn = True # ensembling_fn(output_file_names):\n","#     # can pull the submission files off the disk using OUTPUTS_df[model filename], after optionally setting a threshold for inclusion in the\n","#     #    ensemble, such as OUTPUTS_df[val_rmse] must be in the lowest quantile or something similar\n","#     # average, weighted-average, other method, to combine anything already saved to disk (default = straight avg of all runs in above loop)\n","\n","# compute_trends_fn = True # compute_trends_fn(output_results):\n","#     # create a df of features & feature importances for each run (or \"explode\" sideways the OUTPUTS_df features & importances)... use pd.info to\n","#     #    compute quantiles, mean, stddev for each of the features, and determine if anything looks interesting\n","#     # look at feature importances all together, and see if anything obvious good or bad\n","#     '''\n","#     Might want to look at the predict_contrib parameter for LGBM:  https://lightgbm.readthedocs.io/en/latest/Parameters.html\n","#     '''\n","#     # look at splits and see if any parameters obviously good or bad (correlation matrix of parameters with output results?)\n","#     #feat_imp = pd.DataFrame.from_dict(OUTPUTS_df[\"feature_importances_\"])\n","#     #OUTPUTS_df.at[RUN_n,\"feature_name_\"]\n","#     make empty df with columns from list at = outputs.at[0,featname]\n","#     append rows with elements = list elements in feature_importances_ for each feature name\n","#     df now has as many rows as N_TRAIN_iterations\n","#     compute df.info stats or quantiles/mean/std and store somewhere; make some plots; make some automated recommendations?\n","nocode=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zPM4RLKcn3RE"},"source":["###**Averaging Several Stored Predictions/Submissions from Disk**"]},{"cell_type":"code","metadata":{"id":"dzzImBPS3CsO","cellView":"both"},"source":["# average several submission files to get ensemble average\n","%cd \"{GDRIVE_REPO_PATH}\"\n","# source_dir = Path('models_and_predictions/bagging_LGBM')\n","# prediction_files = source_dir.iterdir()\n","source_dir = 'models_and_predictions/bagging_LGBM'\n","prediction_files = os.listdir(source_dir)\n","print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","\n","# filename to save ensemble average predictions for submission\n","ensemble_name = 'LGBMv6v7_bag06'\n","\n","print(f'filename {ensemble_name}')\n","# Loop to load the data files into appropriately-named pandas DataFrames, and save in np array for easy calc of ensemble average\n","preds = []\n","for f_name in prediction_files:\n","    filename = f_name.rsplit(\"/\")[-1]\n","    data_frame_name = filename.split(\".\")[0][:-11]\n","    path_name = os.path.join('models_and_predictions/bagging_LGBM/'+ filename)\n","    exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","    print(f'Data Frame: {data_frame_name}; n_rows = {len(eval(data_frame_name))}, n_cols = ')\n","    preds.append(eval(data_frame_name).item_cnt_month.to_numpy())\n","\n","# Simple ensemble averaging\n","pred_ens_avg = np.mean(preds, axis=0)\n","ensemble_submission = LGBMv6mg_17_.copy(deep=True)\n","ensemble_submission.item_cnt_month = pred_ens_avg\n","\n","ensemble_submission.to_csv(\"./models_and_predictions/\" + ensemble_name + '_submission.csv', index=False)\n","\n","display(ensemble_submission.head(8))\n","print(f'filename {ensemble_name} saved: {strftime(\"%a %X %x\")}')\n","print('Coursera:  ')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lKGrJUG7f50F"},"source":["###**Feature Importances**"]},{"cell_type":"code","metadata":{"id":"LbszXTq0vhZ-","cellView":"both"},"source":["# Plot feature importance - Results Visualization\n","itercount=0\n","if ITERS.at[itercount,'_model_type'] == 'LGBM':\n","    print_threshold = 25\n","    feature_importances_ = ITERS.at[itercount,'feature_importances_']\n","    feature_name_        = ITERS.at[itercount,'feature_name_']\n","    fi = pd.DataFrame(zip(feature_name_,feature_importances_),columns=['feature','value'])\n","    fi = fi.sort_values('value',ascending=False,ignore_index=True)\n","    fi['norm_value'] = round(100*fi.value / fi.value.max(),2)\n","    fi['lag'] = fi.feature.apply(lambda x: (x.split('L')[-1]) if len(x.split('L'))> 1 else 0)\n","    fi['feature_base'] = fi.feature.apply(lambda x: x.split('_L')[0])\n","    print(fi.iloc[list(range(0,8))+list(range(-7,0)),:]) #[[1,3,5,7,-7,-5]][:])\n","    # model_filename_fi = ITERS.at[itercount,'_model_type']+ITERS.at[itercount,'_model_filename'] + \"_feature_importance.csv\"\n","    # fi.to_csv(\"./models_and_predictions/\" + model_filename_fi, index=False)\n","    # printout to assist with removing low-importance features for following runs\n","    if fi.norm_value.min() < print_threshold:\n","        fi_low = fi[fi.norm_value < print_threshold]\n","        fi_low = fi_low.sort_values(['lag','norm_value'])\n","        fi_low.norm_value = fi_low.norm_value.apply(lambda x: f'{round(x):d}')\n","        fi_low['lag_feature_importance'] = fi_low.apply(lambda x: f\"{f'L{x.lag} fi{x.norm_value}':{len(x.feature_base)}s}\",axis=1)\n","        print(fi_low.lag_feature_importance.to_list())\n","        print(fi_low.feature_base.to_list())\n","    # make importances relative to max importance\n","    feature_importances_ = 100.0 * (feature_importances_ / feature_importances_.max())\n","    sorted_idx = np.arange(feature_importances_.shape[0])\n","    pos = np.arange(sorted_idx.shape[0]) + .5\n","    plt.figure(figsize=(24,12)) \n","    plt.bar(pos, feature_importances_[sorted_idx], align='center')\n","    plt.xticks(pos, feature_name_[sorted_idx])\n","    plt.ylabel('Relative Importance')\n","    plt.title('Variable Importance')\n","    plt.tick_params(axis='x', which='major', labelsize = 13, labelrotation=90)\n","    plt.grid(True,which='major',axis='y')\n","    plt.tick_params(axis='y', which='major', grid_color='black',grid_alpha=0.7)\n","    # plt.savefig('LGBM_feature_importance_v1.4_mg.png')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KDSgvfrtukP1"},"source":["###**Using GPUs with LGBM**"]},{"cell_type":"code","metadata":{"id":"PNBWs59yutWO"},"source":["# GPU use with LGBM modeling:\n","'''\n","May want to see if we can better inform LGBM routine when we are using a GPU\n","https://lightgbm.readthedocs.io/en/latest/GPU-Targets.html#query-opencl-devices-in-your-system\n","Your system might have multiple GPUs from different vendors (“platforms”) installed. Setting up LightGBM GPU device requires two parameters: \n","OpenCL Platform ID (gpu_platform_id) and OpenCL Device ID (gpu_device_id). Generally speaking, each vendor provides an OpenCL platform, \n","and devices from the same vendor have different device IDs under that platform. For example, if your system has an Intel integrated GPU and \n","two discrete GPUs from AMD, you will have two OpenCL platforms (with gpu_platform_id=0 and gpu_platform_id=1). If the platform 0 is Intel, \n","it has one device (gpu_device_id=0) representing the Intel GPU; if the platform 1 is AMD, it has two devices (gpu_device_id=0, gpu_device_id=1) \n","representing the two AMD GPUs. If you have a discrete GPU by AMD/NVIDIA and an integrated GPU by Intel, make sure to select the correct gpu_platform_id \n","to use the discrete GPU as it usually provides better performance.\n","\n","On Windows, OpenCL devices can be queried using GPUCapsViewer, under the OpenCL tab. http://www.ozone3d.net/gpu_caps_viewer/ \n","Note that the platform and device IDs reported by this utility start from 1. So you should minus the reported IDs by 1.\n","\n","On Linux, OpenCL devices can be listed using the clinfo command. On Ubuntu, you can install clinfo by executing sudo apt-get install clinfo.\n","\n","Make sure you list the OpenCL devices in your system and set gpu_platform_id and gpu_device_id correctly. \n","In the following examples, our system has 1 GPU platform (gpu_platform_id = 0) from AMD APP SDK. \n","The first device gpu_device_id = 0 is a GPU device (AMD Oland), and the second device gpu_device_id = 1 is the x86 CPU backend.\n","\n","R Example of using GPU (gpu_platform_id = 0 and gpu_device_id = 0 in our system):\n","> params <- list(objective = \"regression\",\n","+                metric = \"rmse\",\n","+                device = \"gpu\",\n","+                gpu_platform_id = 0,\n","+                gpu_device_id = 0,\n","+                nthread = 1,\n","+                boost_from_average = FALSE,\n","+                num_tree_per_iteration = 10,\n","+                max_bin = 32)\n","> model <- lgb.train(params,\n","+                    dtrain,\n","+                    2,\n","+                    valids,\n","+                    min_data = 1,\n","+                    learning_rate = 1,\n","+                    early_stopping_rounds = 10)\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 232\n","[LightGBM] [Info] Number of data: 6513, number of used features: 116\n","[LightGBM] [Info] Using GPU Device: Oland, Vendor: Advanced Micro Devices, Inc.\n","[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 12\n","[LightGBM] [Info] 40 dense feature groups (0.12 MB) transferred to GPU in 0.004211 secs. 76 sparse feature groups.\n","[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Trained a tree with leaves=16 and max_depth=8\n","[1]:    test's rmse:1.10643e-17\n","[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Trained a tree with leaves=7 and max_depth=5\n","[2]:    test's rmse:0\n","\n","Running on OpenCL CPU backend devices is in generally slow, and we observe crashes on some Windows and macOS systems. \n","Make sure you check the Using GPU Device line in the log and it is not using a CPU. The above log shows that we are using Oland GPU from AMD and not CPU.\n","\n","Example of using CPU (gpu_platform_id = 0, gpu_device_id = 1). The GPU device reported is Intel(R) Core(TM) i7-4600U CPU, \n","so it is using the CPU backend rather than a real GPU.\n","\n","> params <- list(objective = \"regression\",\n","+                metric = \"rmse\",\n","+                device = \"gpu\",\n","+                gpu_platform_id = 0,\n","+                gpu_device_id = 1,\n","+                nthread = 1,\n","+                boost_from_average = FALSE,\n","+                num_tree_per_iteration = 10,\n","+                max_bin = 32)\n","> model <- lgb.train(params,\n","+                    dtrain,\n","+                    2,\n","+                    valids,\n","+                    min_data = 1,\n","+                    learning_rate = 1,\n","+                    early_stopping_rounds = 10)\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 232\n","[LightGBM] [Info] Number of data: 6513, number of used features: 116\n","[LightGBM] [Info] Using requested OpenCL platform 0 device 1\n","[LightGBM] [Info] Using GPU Device: Intel(R) Core(TM) i7-4600U CPU @ 2.10GHz, Vendor: GenuineIntel\n","[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 12\n","[LightGBM] [Info] 40 dense feature groups (0.12 MB) transferred to GPU in 0.004540 secs. 76 sparse feature groups.\n","[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Trained a tree with leaves=16 and max_depth=8\n","[1]:    test's rmse:1.10643e-17\n","[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Trained a tree with leaves=7 and max_depth=5\n","[2]:    test's rmse:0\n","\n","Known issues:\n","Using a bad combination of gpu_platform_id and gpu_device_id can potentially lead to a crash due to OpenCL driver issues on some machines \n","(you will lose your entire session content). Beware of it.\n","****\n","**** some systems have integrated graphics card (Intel HD Graphics) and a dedicated graphics card (AMD, NVIDIA), the dedicated graphics card may \n","automatically override the integrated graphics card. The workaround is to disable your dedicated graphics card to use your integrated graphics card.\n","'''\n","nocode=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x4YcaYylQ0d8"},"source":["###**Multiprocessing to Reduce pandas Memory Usage**"]},{"cell_type":"code","metadata":{"id":"fVXBcQQXQR9T"},"source":["\n","# 16.6.2.9. Process Pools   https://docs.python.org/2/library/multiprocessing.html\n","# One can create a pool of processes which will carry out tasks submitted to it with the Pool class.\n","# class multiprocessing.Pool([processes[, initializer[, initargs[, maxtasksperchild]]]])\n","# processes is the number of worker processes to use. If processes is None then the number returned by cpu_count() is used. If initializer is not None then each worker process will call initializer(*initargs) when it starts.\n","# Note that the methods of the pool object should only be called by the process which created the pool.\n","# New in version 2.7: maxtasksperchild is the number of tasks a worker process can complete before it will exit and be replaced with a fresh worker process, to enable unused resources to be freed. \n","# The default maxtasksperchild is None, which means worker processes will live as long as the pool.\n","\n","# https://stackoverflow.com/questions/39100971/how-do-i-release-memory-used-by-a-pandas-dataframe/39101287#39101287\n","# There is one thing that always works, however, because it is done at the OS, not language, level.\n","# Suppose you have a function that creates an intermediate huge DataFrame, and returns a smaller result (which might also be a DataFrame):\n","# >     def huge_intermediate_calc(something):\n","# >         ...\n","# >         huge_df = pd.DataFrame(...)\n","# >         ...\n","# >         return some_aggregate\n","# Then if you do something like:\n","# >     import multiprocessing\n","# >     result = multiprocessing.Pool(1).map(huge_intermediate_calc, [something_])[0]\n","# Then the function is executed at a different process. When that process completes, the OS retakes all the resources it used. \n","# Maybe it help someone, when creating the Pool try to use maxtasksperchild = 1\n","# In an iPython environment (like jupyter notebook), you need to .close() and .join() or .terminate() the pool to get rid of the spawned process. \n","# The easiest way of doing that since Python 3.3 is to use the context management protocol: \n","# >     with multiprocessing.Pool(1) as pool: \n","# >         result = pool.map(huge_intermediate_calc, [something])\n","# Another option is for the subprocess to write the dataframe to disk using something like parquet. \n","#    It may be faster than moving a big pickled dataframe back to the parent. It will be in the disk cache so it is fast. \n","# If you stick to numeric numpy arrays, those are freed, but boxed object types are not.\n","# When modifying your dataframe, prefer inplace=True, so you don't create copies.\n","\n","#  Use PIPE to concatenate functions in a multiprocess pool, keeping workers / dataframes all within the process so it kills them when the process terminates\n","# The Pipe() function returns a pair of connection objects connected by a pipe which by default is duplex (two-way). For example:\n","# from multiprocessing import Process, Pipe\n","# def f(conn):\n","#     conn.send([42, None, 'hello'])\n","#     conn.close()\n","# if __name__ == '__main__':\n","#     parent_conn, child_conn = Pipe()\n","#     p = Process(target=f, args=(child_conn,))\n","#     p.start()\n","#     print parent_conn.recv()   # prints \"[42, None, 'hello']\"\n","#     p.join()\n","# parent,child = multiprocessing.Pipe()\n","\n","# with multiprocessing.Pool(None,maxtasksperchild = 1) as pool:  # None = use all available processors, could use Pool(1, maxtasksperchild = 1)\n","#     result = pool.map(load_dfs, [args])[0]\n","# pool.close()  # pool.terminate()\n","# pool.join()   \n","\n","# gc.collect()\n","#print(parent.recv())\n","#child.close()\n","#print(f'Result = {result[:5]}') #.head(2))\n","#print(f'Result = {result[:5]}') #.head(2))\n","#print(result.head(2))\n","#result = 1.0\n","#print(f'Result = {result}') #.head(2))\n","# del result\n","# gc.collect()\n","# result = pd.DataFrame()  # doesn't seem to help immediately, nor does \"del df\" or put df in list and \"del list\" or \"del list[0]\" or \"del list[0] --> del list\", all followed by gc.collect()\n","# pool.close(); pool.join();  mu,vu,vt,va,t = get_memory_stats(\"4) After checking active children: \")  # pool.terminate()\n","# gc.collect(); mu,vu,vt,va,t = get_memory_stats(\"5) After gc.collect(): \")\n","# %reset Out   # flush the output cache (no obvious improvement with IPynb)\n","# gc.collect(); mu,vu,vt,va,t = get_memory_stats(\"6) After Reset Cache and gc.collect(): \")\n","\n","                # \"readonly/final_project_data/test.csv.gz\",\n","                # \"data_output/stt.csv.gz\"]\n","\n","# dummy test program to verify OK to run with multiprocessing (yes; it releases memory when parent process is done)\n","# def print_tail(df=pd.DataFrame()):\n","#     print(df.tail(2))\n","#     return df\n","#             exec(data_frame_name + ' = print_tail(' + data_frame_name + ')')\n","#         df = print_tail(eval(data_frame_name)) ;  arg_dict['q'].put(df)  #(df.tail(1))\n","#         arg_dict['q'].put(eval(data_frame_name)) # very slow to try passing full dfs by queue\n","# q = multiprocessing.Queue(len(data_files))  # optional argument: length of queue\n","# proc = multiprocessing.Process(target=load_dfs, args=(args_dict,))\n","# proc.start()\n","# proc.join()  # waits for queue to be filled/flushed\n","# mu,vu,vt,va,t = get_memory_stats(\"C) After Close and Join Pool: \")\n","# print(q.get().head(1))\n","# print(items_enc.head(2))\n","# print(q.get().head(1))\n","# print(shops_enc.head(2))\n","# print(q.get().head(1))\n","# print(date_scaling.head(2))\n","# print(q.get().head(1))\n","# print(test.head(2))\n","# print(q.get().head(1))\n","# print(stt.head(2))\n","\n","\n","# args_dict = {'repo_path':GDRIVE_REPO_PATH,'data_file_list':data_files}\n","# print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","# #%cd \"{paths['repo_path']}\"\n","# %cd \"{GDRIVE_REPO_PATH}\"\n","# with multiprocessing.Pool(None,maxtasksperchild = 1) as pool:  # None = use all available processors, could use Pool(1, maxtasksperchild = 1)\n","#     # dfs = pool.map(load_dfs, data_files)#[0] #[args_dict])[0]\n","#     #items_enc,shops_enc,date_scaling,stt,test = pool.map(load_data_files_to_dfs, data_files)\n","#     dfs = pool.map(load_data_files_to_dfs, data_files)  # dfs is a list of ordered elements, each being the function \"load_data_files_to_dfs\" operating on an element in \"data_files\" list/iterable\n","    \n","\n","\n","\n","# Multiprocessing with 0 returned from function\n","# Ran this 2x, and memory use stabilizes at low level as expected, with dataframes inside function being discarded when process is close-joined\n","        # # List of the *data* files (path relative to GitHub branch), to be loaded into pandas DataFrames\n","        # data_files = [  \"data_output/items_enc.csv\",\n","        #                 \"data_output/shops_enc.csv\",\n","        #                 \"data_output/date_scaling.csv\",\n","        #                 \"data_output/stt.csv.gz\",\n","        #                 \"readonly/final_project_data/test.csv.gz\"]\n","\n","        # data_df_names = []\n","        # for path_name in data_files:\n","        #     filename = path_name.rsplit(\"/\")[-1]\n","        #     data_df_names.append(filename.split(\".\")[0])\n","\n","        # def load_data_files_to_dfs(datafile):\n","        #     filename = datafile.rsplit(\"/\")[-1]\n","        #     data_frame_name = filename.split(\".\")[0]\n","        #     exec(data_frame_name + ' = pd.read_csv(datafile)')\n","        #     return 0#eval(data_frame_name)\n","\n","        # # Issue: freeing up memory used by pandas dataframes that are no longer required by the program (del + gc.collect() does not reliably do this, nor does redefining the df as pd.DataFrame())\n","        # MEMORY_STATS.append(get_memory_stats('Data File Paths Defined',printout=False))\n","\n","        # %cd \"{GDRIVE_REPO_PATH}\"\n","        # print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","        # with multiprocessing.Pool(None,maxtasksperchild = 1) as pool:  # None = use all available processors, could use Pool(1, maxtasksperchild = 1)\n","        #     dfs = pool.map(load_data_files_to_dfs, data_files)  # dfs is a list of ordered elements, each being the function \"load_data_files_to_dfs\" operating on an element in \"data_files\" list/iterable\n","            \n","        # MEMORY_STATS.append(get_memory_stats('Pool Mapped Onto Load Data fn: ',printout=False))\n","        # pool.close()  # pool.terminate()\n","        # pool.join()\n","        # MEMORY_STATS.append(get_memory_stats('Pool Closed and Joined',printout=False))\n","\n","        # # data_dfs = OrderedDict(zip(data_df_names, dfs))\n","        # # for name,df in data_dfs.items():\n","        # #     print(f'Data Frame: {name}; n_rows = {len(df)}, n_cols = ',end=\"\")\n","        # #     print(f'{len(df.columns)}') #\\nData Types: {df.dtypes}\\n')\n","        # #     print(f'Column Names: {df.columns.to_list()}')\n","        # #     print(df.head(2),'\\n')\n","        # print(\"dfs = \", dfs)\n","        # dfs = []; print(\"dfs = \", dfs)\n","        # data_dfs = {}; print(\"data_dfs = \", data_dfs,'\\n')\n","        # print(f'Multiprocessing Active Children: {multiprocessing.active_children()}')  # should be zero (empty list) if all processes are terminated and memory is returned to system\n","        # MEMORY_STATS.append(get_memory_stats('dfs List Set to Empty',printout=False))\n","        # display_all_memory_stats(MEMORY_STATS)\n","        # ------------------------------------------------------------------------------------------ Output:\n","        # /content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","        # Loading Files from Google Drive repo into Colab...\n","\n","        # dfs =  [0, 0, 0, 0, 0]\n","        # dfs =  []\n","        # data_dfs =  {} \n","\n","        # Multiprocessing Active Children: []\n","\n","        #                                                         |  pid   |              vm               |\n","        #   Time and Date       |   Memory Usage Measure Point    | pid-GB | used-GB | avail-GB | total-GB |\n","        # Wed 09:08:08 07/29/20 | Environment Setup Done          |   0.38 |    0.81 |    26.59 |    27.39 |\n","        # Wed 09:08:09 07/29/20 | Defined Iteration Parameters    |   0.39 |    0.81 |    26.59 |    27.39 |\n","        # Wed 09:08:09 07/29/20 | Mounted Google Drive            |   0.39 |    0.81 |    26.59 |    27.39 |\n","        # Wed 09:08:09 07/29/20 | Data File Paths Defined         |   0.39 |    0.81 |    26.59 |    27.39 |\n","        # Wed 09:08:11 07/29/20 | Pool Mapped Onto Load Data fn:  |   0.39 |    0.81 |    26.58 |    27.39 |\n","        # Wed 09:08:11 07/29/20 | Pool Closed and Joined          |   0.39 |    0.81 |    26.58 |    27.39 |\n","        # Wed 09:08:11 07/29/20 | dfs List Set to Empty           |   0.39 |    0.81 |    26.59 |    27.39 |\n","        # Wed 09:08:20 07/29/20 | Data File Paths Defined         |   0.39 |    0.81 |    26.58 |    27.39 |\n","        # Wed 09:08:22 07/29/20 | Pool Mapped Onto Load Data fn:  |   0.39 |    0.81 |    26.58 |    27.39 |\n","        # Wed 09:08:22 07/29/20 | Pool Closed and Joined          |   0.39 |    0.81 |    26.58 |    27.39 |\n","        # Wed 09:08:22 07/29/20 | dfs List Set to Empty           |   0.39 |    0.81 |    26.58 |    27.39 |\n","\n","\n","# Multiprocessing with dataframe returned from function\n","# Ran this 4x, and memory use stabilizes\n","    # # List of the *data* files (path relative to GitHub branch), to be loaded into pandas DataFrames\n","    # data_files = [  \"data_output/items_enc.csv\",\n","    #                 \"data_output/shops_enc.csv\",\n","    #                 \"data_output/date_scaling.csv\",\n","    #                 \"data_output/stt.csv.gz\",\n","    #                 \"readonly/final_project_data/test.csv.gz\"]\n","\n","    # data_df_names = []\n","    # for path_name in data_files:\n","    #     filename = path_name.rsplit(\"/\")[-1]\n","    #     data_df_names.append(filename.split(\".\")[0])\n","\n","    # def load_data_files_to_dfs(datafile):\n","    #     filename = datafile.rsplit(\"/\")[-1]\n","    #     data_frame_name = filename.split(\".\")[0]\n","    #     exec(data_frame_name + ' = pd.read_csv(datafile)')\n","    #     return eval(data_frame_name)\n","\n","    # # Issue: freeing up memory used by pandas dataframes that are no longer required by the program (del + gc.collect() does not reliably do this, nor does redefining the df as pd.DataFrame())\n","    # MEMORY_STATS.append(get_memory_stats('Data File Paths Defined',printout=False))\n","\n","    # %cd \"{GDRIVE_REPO_PATH}\"\n","    # print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","    # with multiprocessing.Pool(None,maxtasksperchild = 1) as pool:  # None = use all available processors, could use Pool(1, maxtasksperchild = 1)\n","    #     dfs = pool.map(load_data_files_to_dfs, data_files)  # dfs is a list of ordered elements, each being the function \"load_data_files_to_dfs\" operating on an element in \"data_files\" list/iterable\n","        \n","    # MEMORY_STATS.append(get_memory_stats('Pool Mapped Onto Load Data fn: ',printout=False))\n","    # pool.close()  # pool.terminate()\n","    # pool.join()\n","    # MEMORY_STATS.append(get_memory_stats('Pool Closed and Joined',printout=False))\n","\n","    # data_dfs = OrderedDict(zip(data_df_names, dfs))\n","    # for name,df in data_dfs.items():\n","    #     print(f'Data Frame: {name}; n_rows = {len(df)}, n_cols = ',end=\"\")\n","    #     print(f'{len(df.columns)}') #\\nData Types: {df.dtypes}\\n')\n","    #     print(f'Column Names: {df.columns.to_list()}')\n","    #     print(df.head(2),'\\n')\n","    # dfs = []; print(\"dfs = \", dfs)\n","    # data_dfs = {}; print(\"data_dfs = \", data_dfs,'\\n')\n","    # print(f'Multiprocessing Active Children: {multiprocessing.active_children()}')  # should be zero (empty list) if all processes are terminated and memory is returned to system\n","    # MEMORY_STATS.append(get_memory_stats('dfs List Set to Empty',printout=False))\n","    # display_all_memory_stats(MEMORY_STATS)\n","\n","    #         # ------------------------------------------------------------------------------------------ Output:\n","\n","    # /content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","    # Loading Files from Google Drive repo into Colab...\n","\n","    # Data Frame: items_enc; n_rows = 22170, n_cols = 10\n","    # Column Names: ['item_id', 'item_tested', 'item_cluster', 'item_category_id', 'item_cat_tested', 'item_group', 'item_category1', 'item_category2', 'item_category3', 'item_category4']\n","    #    item_id  item_tested  item_cluster  item_category_id  item_cat_tested  item_group  item_category1  item_category2  item_category3  item_category4\n","    # 0        0            0           100                40                1           6               8               3               7               3\n","    # 1        1            0           105                76                1           6              11               6              10               5 \n","\n","    # Data Frame: shops_enc; n_rows = 60, n_cols = 9\n","    # Column Names: ['shop_id', 'shop_tested', 'shop_group', 'shop_type', 's_type_broad', 'shop_federal_district', 'fd_popdens', 'fd_gdp', 'shop_city']\n","    #    shop_id  shop_tested  shop_group  shop_type  s_type_broad  shop_federal_district  fd_popdens  fd_gdp  shop_city\n","    # 0        0            0           7          5             2                      1           3       1         26\n","    # 1        1            0           7          1             0                      1           3       1         26 \n","\n","    # Data Frame: date_scaling; n_rows = 35, n_cols = 8\n","    # Column Names: ['month', 'year', 'season', 'MoY', 'days_in_M', 'weekday_weight', 'retail_sales', 'week_retail_weight']\n","    #    month  year  season  MoY  days_in_M  weekday_weight  retail_sales  week_retail_weight\n","    # 0      0  2013       2    1         31           0.979         1.052               1.030\n","    # 1      1  2013       3    2         28           1.069         1.072               1.146 \n","\n","    # Data Frame: stt; n_rows = 3150043, n_cols = 9\n","    # Column Names: ['day', 'week', 'qtr', 'season', 'month', 'price', 'sales', 'shop_id', 'item_id']\n","    #    day  week  qtr  season  month  price  sales  shop_id  item_id\n","    # 0    0     0    0       2      0     99      1        2      991\n","    # 1    0     0    0       2      0   2599      1        2     1472 \n","\n","    # Data Frame: test; n_rows = 214200, n_cols = 3\n","    # Column Names: ['ID', 'shop_id', 'item_id']\n","    #    ID  shop_id  item_id\n","    # 0   0        5     5037\n","    # 1   1        5     5320 \n","\n","    # dfs =  []\n","    # data_dfs =  {} \n","\n","    # Multiprocessing Active Children: []\n","\n","    #                                                         |  pid   |              vm               |\n","    #   Time and Date       |   Memory Usage Measure Point    | pid-GB | used-GB | avail-GB | total-GB |\n","    # Wed 09:11:01 07/29/20 | Environment Setup Done          |   0.38 |    0.81 |    26.59 |    27.39 |\n","    # Wed 09:11:01 07/29/20 | Defined Iteration Parameters    |   0.39 |    0.81 |    26.59 |    27.39 |\n","    # Wed 09:11:01 07/29/20 | Mounted Google Drive            |   0.39 |    0.81 |    26.59 |    27.39 |\n","    # Wed 09:11:01 07/29/20 | Data File Paths Defined         |   0.39 |    0.81 |    26.59 |    27.39 |\n","    # Wed 09:11:05 07/29/20 | Pool Mapped Onto Load Data fn:  |   0.82 |    1.27 |    26.12 |    27.39 |\n","    # Wed 09:11:05 07/29/20 | Pool Closed and Joined          |   0.82 |    1.27 |    26.12 |    27.39 |\n","    # Wed 09:11:05 07/29/20 | dfs List Set to Empty           |   0.82 |    1.27 |    26.12 |    27.39 |\n","    # Wed 09:11:14 07/29/20 | Data File Paths Defined         |   0.82 |    1.27 |    26.12 |    27.39 |\n","    # Wed 09:11:18 07/29/20 | Pool Mapped Onto Load Data fn:  |   0.82 |    1.28 |    26.11 |    27.39 |\n","    # Wed 09:11:18 07/29/20 | Pool Closed and Joined          |   0.82 |    1.28 |    26.11 |    27.39 |\n","    # Wed 09:11:18 07/29/20 | dfs List Set to Empty           |   0.82 |    1.28 |    26.12 |    27.39 |\n","    # Wed 09:11:24 07/29/20 | Data File Paths Defined         |   0.82 |    1.28 |    26.12 |    27.39 |\n","    # Wed 09:11:28 07/29/20 | Pool Mapped Onto Load Data fn:  |   0.82 |    1.28 |    26.11 |    27.39 |\n","    # Wed 09:11:28 07/29/20 | Pool Closed and Joined          |   0.82 |    1.28 |    26.12 |    27.39 |\n","    # Wed 09:11:28 07/29/20 | dfs List Set to Empty           |   0.82 |    1.28 |    26.12 |    27.39 |\n","    # Wed 09:11:33 07/29/20 | Data File Paths Defined         |   0.82 |    1.28 |    26.12 |    27.39 |\n","    # Wed 09:11:38 07/29/20 | Pool Mapped Onto Load Data fn:  |   0.82 |    1.28 |    26.12 |    27.39 |\n","    # Wed 09:11:38 07/29/20 | Pool Closed and Joined          |   0.82 |    1.28 |    26.12 |    27.39 |\n","    # Wed 09:11:38 07/29/20 | dfs List Set to Empty           |   0.82 |    1.28 |    26.12 |    27.39 |\n","\n","nocode=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h4RFEOavsvYK"},"source":["###**Timer created as Context Manager, allowing function decoration**"]},{"cell_type":"code","metadata":{"id":"m_cNgasXycw7"},"source":["# some functions for calculating elapsed time, allowing timer blocks inside other timer blocks\n","#    (as opposed to just using time.perf_counter() )\n","\n","# class elapsed_timer(ContextDecorator): # base class enables a contextmanager to also be used as a decorator\n","#     def __init__(self):   \n","#         self.function_init_time   = f'{strftime(\"%a %X %x\")}'\n","#         self.start_time           = default_timer()\n","#         self.function_total_time  = datetime.utcfromtimestamp(default_timer() - self.start_time).strftime('%H:%M:%S')\n","#     def __enter__(self):  \n","#         self.start_time = default_timer()\n","#         return datetime.utcfromtimestamp(default_timer() - self.start_time).strftime('%H:%M:%S')\n","#     def __exit__(self, exc_type, exc_value, exc_traceback):\n","#         self.function_total_time = datetime.utcfromtimestamp(default_timer() - self.start_time).strftime('%H:%M:%S')\n","#         return True # ignores errors in while loop\n","#     def get_elapsed_time(self):\n","#         return datetime.utcfromtimestamp(default_timer() - self.start_time).strftime('%H:%M:%S')\n","\n","# example usage of context manager with context decorator:\n","# xt = elapsed_timer()\n","# yt = elapsed_timer()\n","# @xt\n","# def printme():\n","#     for i in range(1500000): a=np.arctan(np.sqrt(i/3.367))**2; c = np.sqrt(a**a)\n","#     print(f\"after first calc in function:     x={xt.get_elapsed_time()}\")\n","#     with yt:\n","#         for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b*b**b)\n","#         print(f\"inside inner block at end:     x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","# print(f'xt function_total_time = {xt.function_total_time};   initial time: {xt.function_init_time}')\n","# yt.restart_time()\n","\n","# xt = elapsed_timer()\n","# yt = elapsed_timer()\n","# @xt\n","# def printsometimes():\n","#     print(\"Hello\")\n","#     print(f\"at start of outer block:             x={xt.get_elapsed_time()}\")\n","#     for i in range(1500000): a=np.arctan(np.sqrt(i/3.367))**2; c = np.sqrt(a**a)\n","#     print(f\"after first calc in outer block:     x={xt.get_elapsed_time()}\")\n","#     for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b*b**b)\n","#     print(f\"after second calc in outer block:    x={xt.get_elapsed_time()}\")\n","#     with yt:\n","#         print(\"Goodbye\")\n","#         print(f\"at start of inner block:             x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","#         for i in range(1500000): a=np.arctan(np.sqrt(i/3.367))**2; c = np.sqrt(a**a)\n","#         print(f\"at middle of inner block:            x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","#         for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b*b**b)\n","#         print(f\"inside inner block at end:           x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","#     print(f'yt function_elapsed_time1 = {yt.function_total_time};   initial time: {yt.function_init_time}')\n","#     print(f\"just outside of inner block:         x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","#     for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b**b)    \n","#     print(f\"after calc outside inner block:      x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","#     for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b*b**b)\n","#     print(f\"inside outer block at end            x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","#     print(f'yt function_elapsed_time2 = {yt.function_total_time};   initial time: {yt.function_init_time}')\n","\n","# print(f\"before calculation before fn call:   x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","# for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b**b)\n","# print(f\"after calculation before fn call:    x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","# print(f'yt function_elapsed_time3 = {yt.function_total_time};   initial time: {yt.function_init_time}')\n","\n","# printsometimes()\n","\n","# print(f\"just outside of function call:       x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","# for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b**b)\n","# print(f\"after calc after function call:      x={xt.get_elapsed_time()}    y={yt.get_elapsed_time()}\")\n","# print(f'yt function_elapsed_time4 = {yt.function_total_time};   initial time: {yt.function_init_time}')\n","\n","# before calculation before fn call:   x=00:00:00    y=00:00:00\n","# after calculation before fn call:    x=00:00:04    y=00:00:04\n","# yt function_elapsed_time3 = 00:00:00;   initial time: Mon 08:33:22 08/10/20\n","# Hello\n","# at start of outer block:             x=00:00:00\n","# after first calc in outer block:     x=00:00:06\n","# after second calc in outer block:    x=00:00:10\n","# Goodbye\n","# at start of inner block:             x=00:00:10    y=00:00:00\n","# at middle of inner block:            x=00:00:17    y=00:00:06\n","# inside inner block at end:           x=00:00:22    y=00:00:11\n","# yt function_elapsed_time1 = 00:00:11;   initial time: Mon 08:33:22 08/10/20\n","# just outside of inner block:         x=00:00:22    y=00:00:11\n","# after calc outside inner block:      x=00:00:26    y=00:00:15\n","# inside outer block at end            x=00:00:30    y=00:00:19\n","# yt function_elapsed_time2 = 00:00:11;   initial time: Mon 08:33:22 08/10/20\n","# just outside of function call:       x=00:00:30    y=00:00:19\n","# after calc after function call:      x=00:00:35    y=00:00:24\n","# yt function_elapsed_time4 = 00:00:11;   initial time: Mon 08:33:22 08/10/20\n","\n","\n","############################################################################################\n","# @contextmanager\n","# def elapsed_timer():\n","#     \"\"\"\n","#     use like:  \"with elapsed_timer() as x_time: ->->print(x_time())... blah print(x_time())... blah <-<-print(f'totaltime={x_time()}')\"\n","#     x_time() will continue increasing when you are inside the \"with\" block, but freezes as \"x_time()\" when you exit the \"with\"\n","#     \"\"\"\n","#     start = default_timer()\n","#     elapser = lambda: datetime.utcfromtimestamp(default_timer() - start).strftime('%H:%M:%S')\n","#     yield lambda: elapser()\n","#     end = default_timer()\n","#     elapser = lambda: datetime.utcfromtimestamp(end - start).strftime('%H:%M:%S')\n","\n","# with elapsed_timer() as x_time:\n","#     print(\"Hello\")\n","#     print(f\"at start of outer block:             x={x_time()}\")\n","#     for i in range(1500000): a=np.arctan(np.sqrt(i/3.367))**2; c = np.sqrt(a**a)\n","#     print(f\"after first calc in outer block:     x={x_time()}\")\n","#     for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b*b**b)\n","#     print(f\"after second calc in outer block:    x={x_time()}\")\n","#     with elapsed_timer() as y_time:\n","#         print(\"Goodbye\")\n","#         print(f\"at start of inner block:             x={x_time()}    y={y_time()}\")\n","#         for i in range(1500000): a=np.arctan(np.sqrt(i/3.367))**2; c = np.sqrt(a**a)\n","#         print(f\"at middle of inner block:            x={x_time()}    y={y_time()}\")\n","#         for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b*b**b)\n","#         print(f\"inside inner block at end:           x={x_time()}    y={y_time()}\")\n","#     print(f\"just outside of inner block:         x={x_time()}    y={y_time()}\")\n","#     for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b**b)    \n","#     print(f\"after calc outside inner block:      x={x_time()}    y={y_time()}\")\n","#     for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b*b**b)\n","#     print(f\"inside outer block at end            x={x_time()}    y={y_time()}\")\n","\n","# print(f\"just outside of outer block:         x={x_time()}    y={y_time()}\")\n","# for j in range(1000000): b=np.arctan(np.sqrt(j/3.367))**2; d = np.sqrt(b**b)\n","# print(f\"after calc out of outer block (x,y): x={x_time()}    y={y_time()}\")\n","\n","# Hello\n","# at start of outer block:             x=00:00:00\n","# after first calc in outer block:     x=00:00:06\n","# after second calc in outer block:    x=00:00:11\n","# Goodbye\n","# at start of inner block:             x=00:00:11    y=00:00:00\n","# at middle of inner block:            x=00:00:18    y=00:00:06\n","# inside inner block at end:           x=00:00:23    y=00:00:11\n","# just outside of inner block:         x=00:00:23    y=00:00:11\n","# after calc outside inner block:      x=00:00:27    y=00:00:11\n","# inside outer block at end            x=00:00:32    y=00:00:11\n","# just outside of outer block:         x=00:00:32    y=00:00:11\n","# after calc out of outer block (x,y): x=00:00:32    y=00:00:11\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qG665uxzQ5J5"},"source":["###**Old code: LightGBM - Lightweight Gradient-Boosted Decision Tree**\n","###**Old code: SK_HGBR - SKLearn Histogram Gradient Boosting Regressor**"]},{"cell_type":"code","metadata":{"id":"_qUFDLNRQ_TC","cellView":"both"},"source":["# model_gbdt = lgb.LGBMRegressor(\n","#     objective='regression',\n","#     boosting_type='gbdt',           # gbdt= Gradient Boosting Decision Tree; dart= Dropouts meet Multiple Additive Regression Trees; goss= Gradient-based One-Side Sampling; rf= Random Forest\n","#     learning_rate=params[\"lr\"],     # You can use callbacks parameter of fit method to shrink/adapt learning rate in training using reset_parameter callback\n","#     n_estimators=params[\"maxit\"],   # Number of boosted trees to fit = max_iterations\n","#     metric='rmse',\n","#     subsample_for_bin=200000,       # Number of samples for constructing bins\n","#     num_leaves=31,                  # Maximum tree leaves for base learners\n","#     max_depth=-1,                   # Maximum tree depth for base learners, <=0 means no limit\n","#     min_split_gain=0.0,             # Minimum loss reduction required to make a further partition on a leaf node of the tree\n","#     min_child_weight=0.001,         # Minimum sum of instance weight (hessian) needed in a child (leaf)\n","#     min_child_samples=20,           # Minimum number of data needed in a child (leaf)\n","#     colsample_bytree=params[\"reg\"], # dropout fraction of columns during fitting (max=1 = no dropout)\n","#     random_state=params[\"seed\"],    # seed value\n","#     silent=False,                   # whether to print info during fitting\n","#     importance_type='split',        # feature importance type: 'split'= N times feature is used in model; 'gain'= total gains of splits which use the feature\n","#     reg_alpha=0.0,                  # L1 regularization\n","#     reg_lambda=0.0,                 # L2 regularization\n","#     n_jobs=- 1,                     # N parallel threads to use on computer\n","#     subsample=1.0,                  # row fraction used for training: keep at 1 for time series data\n","#     subsample_freq=0                # keep at 0 for time series\n","#     )\n","\n","\n","# model_gbdt.fit( \n","#     data['X_train'],                        # Input feature matrix (array-like or sparse matrix of shape = [n_samples, n_features])\n","#     data['y_train'],                        # The target values (class labels in classification, real numbers in regression) (array-like of shape = [n_samples])\n","#     eval_set=[(data['X_val'], data['y_val'])],              # can have multiple tuples of validation data inside this list\n","#     eval_names=None,                        # Names of eval_set (list of strings or None, optional (default=None))\n","#     eval_metric='rmse',                     # Default: 'l2' (= mean squared error, 'mse') for LGBMRegressor; options include 'l2_root'='root_mean_squared_error'='rmse' and 'l1'='mean_absolute_error'='mae' + more\n","#     early_stopping_rounds=params[\"estop\"],  # Activates early stopping. The model will train until the validation score stops improving. Validation score needs to improve at least every early_stopping_rounds \n","#                                             #     to continue training. Requires at least one validation data and one metric. If there’s more than one, will check all of them. But the training data is ignored anyway. \n","#                                             #     To check only the first metric, set the first_metric_only parameter to True in additional parameters **kwargs of the model constructor.\n","#     init_score=None,                        # Init score of train data\n","#     eval_init_score=None,                   # Init score of eval data (list of arrays or None, optional (default=None))\n","#     verbose=CONSTANTS[\"VERBOSITY\"] ,        # If True, metric on the eval set is printed at each boosting stage. If n=int, the metric on the eval set is printed at every nth boosting stage. Best and final also print.\n","#     feature_name='auto',                    # Feature names. If 'auto' and data is pandas DataFrame, data columns names are used. (list of strings or 'auto', optional (default='auto'))\n","#     categorical_feature='auto',             # Categorical features (list of strings or int, or 'auto', optional (default='auto')) If list of int, interpreted as indices. \n","#                                             # If list of strings, interpreted as feature names (need to specify feature_name as well). \n","#                                             # If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used. All values in categorical features should be less than int32 max value (2147483647). \n","#                                             # Large values could be memory-consuming. Consider using consecutive integers starting from zero. All negative values in categorical features are treated as missing values.\n","#     callbacks=None                          # List of callback functions that are applied at each iteration (list of callback functions or None, optional (default=None)) See Callbacks in Python API for more information.\n","#     )\n","\n","    # if mod_type == 'HGBR':\n","    #     # TTSplit should use TRAIN_FINAL = 33 (train on all data), and it will return also val=month33 for calculation at end (only)\n","    #     model_gbdt = HistGradientBoostingRegressor(\n","    #         learning_rate=LR, \n","    #         max_iter=maxiter, \n","    #         l2_regularization = reg,\n","    #         early_stopping=False, \n","    #         verbosity = verb,\n","    #         random_state=seed_val)\n","    \n","    #     tic = perf_counter()\n","    #     model_gbdt.fit(X_train_np, y_train)\n","    #     toc = perf_counter()\n","    #     model_fit_time = datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","    #     print(f\"model HGBR fit time: {model_fit_time}\")\n","    #     best_iter = maxiter\n","    #     best_val_rmse = 0\n","# model_params = {\n","        #         'objective':param_df.at[iternum,'objective'],\n","        #         'boosting_type':param_df.at[iternum,'boosting_type'],\n","        #         'learning_rate':param_df.at[iternum,'learning_rate'],\n","        #         'n_estimators':param_df.at[iternum,'n_estimators'],\n","        #         'metric':param_df.at[iternum,'metric'],\n","        #         'subsample_for_bin':param_df.at[iternum,'subsample_for_bin'],\n","        #         'num_leaves':param_df.at[iternum,'num_leaves'],\n","        #         'max_depth':param_df.at[iternum,'max_depth'],\n","        #         'min_split_gain':param_df.at[iternum,'min_split_gain'],\n","        #         'min_child_weight':param_df.at[iternum,'min_child_weight'],\n","        #         'min_child_samples':param_df.at[iternum,'min_child_samples'],\n","        #         'colsample_bytree':param_df.at[iternum,'colsample_bytree'],\n","        #         'random_state':param_df.at[iternum,'random_state'],\n","        #         'silent':param_df.at[iternum,'silent'],\n","        #         'importance_type':param_df.at[iternum,'importance_type'],\n","        #         'reg_alpha':param_df.at[iternum,'reg_alpha'],\n","        #         'reg_lambda':param_df.at[iternum,'reg_lambda'],\n","        #         'n_jobs':param_df.at[iternum,'n_jobs'],\n","        #         'subsample':param_df.at[iternum,'subsample'],\n","        #         'subsample_freq':param_df.at[iternum,'subsample_freq']\n","                # }\n","\n","        # fit_params = {\n","        #         'eval_metric':param_df.at[iternum,'eval_metric'],\n","        #         'early_stopping_rounds':param_df.at[iternum,'early_stopping_rounds'],\n","        #         'init_score':param_df.at[iternum,'init_score'],\n","        #         'eval_init_score':param_df.at[iternum,'eval_init_score'],\n","        #         'verbose':param_df.at[iternum,'verbose'],\n","        #         'feature_name':param_df.at[iternum,'feature_name'],\n","        #         'categorical_feature':param_df.at[iternum,'categorical_feature'],\n","        #         'callbacks':param_df.at[iternum,'callbacks']\n","        #         }\n","\n","        # param_df.at[iternum,\"feature_name_\"]            = model_gbdt.feature_name_\n","\n","\n","\n","\n","# # Parameters Dictionary stores everything for dumping to file later\n","# SPEC = OrderedDict()\n","# FEATURES[\"_MODEL_NAME\"] = 'LGBMv13_15ens'   # 'LGBMv10_11ens'  # Name of file model substring to save data submission to (= False if user to input it below)\n","# FEATURES[\"_MODEL_TYPE\"] = 'LGBM'  # 'HGBR'\n","# FEATURES[\"_TEST_MONTH\"] = 34\n","\n","# # Optional operations to delete irrelevant shops or item categories, and to scale sales by month length, etc.;  set to FALSE if no operation desired\n","# FEATURES[\"_EDA_DELETE_SHOPS\"]     = [9,20] #[0,1,8,9,11,13,17,20,23,27,29,30,32,33,40,43,51,54] #[8, 9, 13, 20, 23, 32, 33, 40] # [9,20] #  # False # these are online shops, untested shops, and early-termination + online shops\n","# FEATURES[\"_EDA_DELETE_ITEM_CATS\"] = [8, 10, 32, 59, 80, 81, 82]  #[1,4,8,10,13,14,17,18,32,39,46,48,50,51,52,53,59,66,68,80,81,82] #  #[8, 80, 81, 82]  # False # hokey categories, untested categories, really hokey categories\n","# FEATURES[\"_EDA_SCALE_MONTH\"]         = 'week_retail_weight'  # False # scale sales by days in month, number of each weekday, and Russian recession retail sales index\n","\n","# # columns to keep for this round of modeling (dropping some of the less important features to save memory):\n","# FEATURES[\"COLS_KEEP_ITEMS\"]             = ['item_id', 'item_group', 'item_cluster', 'item_category_id']  #, 'item_category4']\n","# FEATURES[\"COLS_KEEP_SHOPS\"]             = ['shop_id','shop_group']\n","# FEATURES[\"COLS_KEEP_DATE_SCALING\"]      = ['month', 'days_in_M', 'weekday_weight', 'retail_sales', 'week_retail_weight']\n","# FEATURES[\"COLS_KEEP_BASE_TRAIN_TEST\"]   = ['month', 'price', 'sales', 'shop_id', 'item_id']\n","\n","# # re-order columns for organized readability, for the (to be created) combined sales-train-test (stt) dataset\n","# FEATURES[\"COLS_ORDER_STT\"]        = ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster'] #,   'revenue','item_category4','shop_group'\n","# FEATURES[\"PROVIDED_INTEGER_FEATURES\"]        = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in {'sales','price','revenue'}]  \n","# FEATURES[\"FEATURES_MONTHLY_STT_START\"]       = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in {'month','sales','price','revenue','shop_id','item_id'}]  # these are categorical features that need to be merged onto test data set\n","# FEATURES[\"PROVIDED_CATEGORICAL_FEATURES\"]    = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in {'month','sales','price','revenue'}]\n","# FEATURES[\"_USE_CATEGORICAL\"]         = True  # pd dataframe columns \"PROVIDED_CATEGORICAL_FEATURES\" are changed to categorical dtype just before model fitting/creation\n","\n","# FEATURES[\"AGG_STATS\"] = OrderedDict()\n","# FEATURES[\"AGG_STATS\"][\"sales\"]     = ['sum', 'median', 'count']\n","# FEATURES[\"AGG_STATS\"][\"revenue\"]   = ['sum']  # revenue can handle fillna(0) cartesian product; price doesn't make sense with fillna(0), so don't use that at this time\n","# #FEATURES[\"AGG_STATS\"][\"price\"]     = ['median','std']\n","\n","# # aggregate statistics columns (initial computation shall be 'sales per month' prediction target for shop_id-item_id pair grouping)\n","# FEATURES[\"STATS_FEATURES\"] = [['shop_id', 'item_id'], ['shop_id', 'item_category_id'], ['shop_id', 'item_cluster']] + FEATURES[\"PROVIDED_CATEGORICAL_FEATURES\"]\n","\n","# FEATURES[\"LAGS_MONTHS\"] = [1,2,3,4,5,6,7,8]  # month lags to include in model \n","# FEATURES[\"LAG_FEATURES\"] = {}\n","# for i in FEATURES[\"LAGS_MONTHS\"]:\n","#     FEATURES[\"LAG_FEATURES\"][i] = ['y_sales', 'shop_id_x_item_category_id_sales_sum', 'item_id_sales_sum', 'item_cluster_sales_sum'] \n","# FEATURES[\"LAG_FEATURES\"][1] = ['y_sales', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', \n","#                      'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', \n","#                      'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', \n","#                      'shop_id_sales_sum', 'shop_id_sales_count', \n","#                      'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', \n","#                      'shop_group_revenue_sum', \n","#                      'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', \n","#                      'item_group_sales_sum', 'item_group_revenue_sum', \n","#                      'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","\n","# FEATURES[\"LAG_FEATURES\"][2] = ['y_sales', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', \n","#                      'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_revenue_sum', \n","#                      'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_count', \n","#                      'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', \n","#                      'item_category_id_sales_sum', 'item_category_id_sales_count', \n","#                      'item_group_sales_sum', \n","#                      'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","\n","# FEATURES[\"LAG_FEATURES\"][3] = ['y_sales', 'shop_id_x_item_id_sales_count', \n","#                      'shop_id_x_item_category_id_sales_sum', \n","#                      'shop_id_sales_sum', \n","#                      'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', \n","#                      'item_category_id_sales_sum', 'item_category_id_sales_count', \n","#                      'item_cluster_sales_sum', 'item_cluster_sales_count']\n","\n","# # keep at least the highest importance feature for each lag, but remove all others with < 20% importance (month 13-32 training)\n","# FEATURES[\"LAG_FEATURES\"][2] = [e for e in FEATURES[\"LAG_FEATURES\"][2] if e not in {'item_group_sales_sum','shop_id_x_item_category_id_sales_sum','shop_id_x_item_cluster_sales_sum','shop_id_x_item_cluster_sales_count'}]\n","# FEATURES[\"LAG_FEATURES\"][3] = [e for e in FEATURES[\"LAG_FEATURES\"][3] if e not in {'item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum','shop_id_x_item_id_sales_count'}]\n","# FEATURES[\"LAG_FEATURES\"][4] = [e for e in FEATURES[\"LAG_FEATURES\"][4] if e not in {'shop_id_x_item_category_id_sales_sum','y_sales','item_cluster_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][5] = [e for e in FEATURES[\"LAG_FEATURES\"][5] if e not in {'item_cluster_sales_sum','item_id_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][6] = [e for e in FEATURES[\"LAG_FEATURES\"][6] if e not in {'item_id_sales_sum','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][7] = [e for e in FEATURES[\"LAG_FEATURES\"][7] if e not in {'y_sales','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][8] = [e for e in FEATURES[\"LAG_FEATURES\"][8] if e not in {'item_id_sales_sum','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","\n","# # LAG_STATS_SET is SET of all aggregate statistics columns for all lags (allows us to shed the other stats, keeping memory requirements low)\n","# LAG_STATS_SET = FEATURES[\"LAG_FEATURES\"][1]\n","# for l in FEATURES[\"LAGS_MONTHS\"][1:]:\n","#     LAG_STATS_SET = LAG_STATS_SET + [x for x in FEATURES[\"LAG_FEATURES\"][l] if x not in LAG_STATS_SET]\n","# FEATURES[\"STT_MONTHLY_COLS\"] = FEATURES[\"PROVIDED_INTEGER_FEATURES\"] + LAG_STATS_SET\n","\n","# # Define various constants that drive the attributes of the various features\n","# FEATURES[\"_CLIP_TRAIN_H\"]   = 20          # this clips sales after doing monthly groupings (monthly_stt dataframe) will also clip item_cnt_month predictions to 20 after the model runs\n","# FEATURES[\"_CLIP_TRAIN_L\"]   = 0                   \n","# FEATURES[\"_CLIP_PREDICT_H\"] = 20          # this clips the final result before submission to coursera\n","# FEATURES[\"_CLIP_PREDICT_L\"] = 0    \n","\n","# FEATURES[\"_USE_ROBUST_SCALER\"]         = True        # scale features to reduce influence of outliers\n","# FEATURES[\"_ROBUST_SCALER_QUANTILES\"]   = (20,80)\n","# FEATURES[\"_USE_MINMAX_SCALER\"]         = True        # scale features to use large range of np.int16\n","# FEATURES[\"_MINMAX_SCALER_RANGE\"]       = (0,16000)   # int16 = (0,32700); uint16 = (0,65500)  --> keep this range positive for best results with LGBM; keep range smaller for faster LGBM fitting\n","# FEATURES[\"_FEATURE_DATA_TYPE\"]         = np.int16    # np.float32 #np.int16   np.uint16          # if fill n/a = 0, can adjust feature values to be integer values and save memory (not finding that int can store np.NAN)\n","# FEATURES[\"_USE_CARTPROD_FILL\"]         = True        # use cartesian fill, or not\n","# FEATURES[\"_CARTPROD_TEST_PAIRS\"]  = False       # include all shop-item pairings from test month as well as the in-month pairings\n","# FEATURES[\"_CARTPROD_FILLNA0\"]    = True        # fill n/a cartesian additions with zeros (not good for price-based stats, however)\n","# FEATURES[\"_CARTPROD_FIRST_MONTH\"] = 13          # month number + max lag to start adding Cartesian product rows (i.e., maxlag=6mo and CARTPROD_FILL_MONTH_BEGIN=10 will cartesian fill from 4 to 33)\n","# FEATURES[\"TRAIN_MONTH_START\"]         = [13]        # == 24 ==> less than a year of data, but avoids December 'outlier' of 2014\n","# FEATURES[\"TRAIN_MONTH_END\"]           = [29]        # [29,32] #,30,32]\n","# FEATURES[\"N_VAL_MONTHS\"]              = [False]     #1 # ; if false, val is all months after training, up to and including 33; otherwise val is this many months after train_month_end\n","\n","# # Define hyperparameters for modeling\n","# FEATURES[\"LEARNING_RATE\"]       = [0.05]  # default = 0.1\n","# FEATURES[\"MAX_ITERATIONS\"]      = [200] # default = 100\n","# FEATURES[\"EARLY_STOPPING\"]      = [20]\n","# FEATURES[\"REGULARIZATION\"]      = [0.4] # default = 1 for LGBM, 0 for HGBR (these models use inverse forms of regularization)\n","# FEATURES[\"VERBOSITY\"]           = True #4 four is to print every 4th iteration; True is every iteration; False is no print except best and last\n","# FEATURES[\"SEED_VALUES\"]         = [42]\n","\n","# FEATURES[\"ALL_exploded_shape[0]\"] = (len(FEATURES[\"SEED_VALUES\"])*len(FEATURES[\"N_VAL_MONTHS\"])*len(FEATURES[\"TRAIN_MONTH_END\"])*len(FEATURES[\"TRAIN_MONTH_START\"])*\n","#                          len(FEATURES[\"EARLY_STOPPING\"])*len(FEATURES[\"MAX_ITERATIONS\"])*len(FEATURES[\"REGULARIZATION\"])*len(FEATURES[\"LEARNING_RATE\"]) )\n","\n","\n","# print(f'Done: {strftime(\"%a %X %x\")}')\n","\n","\n","\n","\n","\n","\n","\n","\n","# def unscale(scaler,target):\n","#     return scaler.inverse_transform(target.reshape(-1, 1)).squeeze()\n","\n","# def GBDT_model(data=df, CONSTANTS=SPEC, params=OrderedDict()):\n","#     \"\"\"\n","#     data is entire dataframe with train, validation, and test rows, and all columns including target prediction at \"y_target\"\n","#     constants is dictionary of setup constants\n","#     params is dictionary of this particular model train/val split and model fitting/prediction parameters\n","#     \"\"\"\n","#     results = OrderedDict()\n","#     if CONSTANTS[\"_MODEL_TYPE\"] == 'LGBM':\n","        \n","#         train_start = params[\"train_start_mo\"]\n","#         train_end   = params[\"train_final_mo\"]\n","#         val_months  = params[\"val_mo\"]\n","#         test_month  = CONSTANTS[\"TEST_MONTH\"]\n","\n","#         train   = data.query('(month >= @train_start) & (month <= @train_end)')\n","#         y_train = train['y_target'].astype(np.float32)\n","#         y_train = y_train.reset_index(drop=True)\n","#         X_train = train.drop(['y_target'], axis=1)\n","#         X_train = X_train.reset_index(drop=True)\n","#         feature_names = X_train.columns\n","\n","#         if val_months:\n","#             val = data.query('(month > (@train_end)) & (month <= (@train_end + @val_months)) & (month < @test_month)')\n","#         else:\n","#             val = data.query('((month > (@train_end)) & (month < @test_month)) | (month == (@test_month-1))')\n","#         y_val = val['y_target'].astype(np.float32)\n","#         y_val = y_val.reset_index(drop=True)\n","#         X_val = val.drop(['y_target'], axis=1)\n","#         X_val = X_val.reset_index(drop=True)\n","\n","#         X_test = data.query('month == @test_month').drop(['y_target'], axis=1)\n","#         X_test = X_test.reset_index(drop=True)\n","\n","#         print('X_train:')\n","#         print_col_info(X_train,8)\n","#         print(f'\\n{X_train.head(2)}\\n\\n')\n","#         print('X_val:')\n","#         print_col_info(X_val,8)\n","#         print(f'\\n{X_val.head(2)}\\n\\n')\n","#         print('X_test:')\n","#         print_col_info(X_test,8)\n","#         print(f'\\n{X_test.head(2)}\\n\\n')\n","#         data_types = X_train.dtypes\n","\n","#         del [[data, train, val]]\n","\n","#         print('Starting training...')\n","#         model_gbdt = lgb.LGBMRegressor(\n","#             objective='regression',\n","#             boosting_type='gbdt',           # gbdt= Gradient Boosting Decision Tree; dart= Dropouts meet Multiple Additive Regression Trees; goss= Gradient-based One-Side Sampling; rf= Random Forest\n","#             learning_rate=params[\"lr\"],     # You can use callbacks parameter of fit method to shrink/adapt learning rate in training using reset_parameter callback\n","#             n_estimators=params[\"maxit\"],   # Number of boosted trees to fit = max_iterations\n","#             metric='rmse',\n","#             subsample_for_bin=200000,       # Number of samples for constructing bins\n","#             num_leaves=31,                  # Maximum tree leaves for base learners\n","#             max_depth=-1,                   # Maximum tree depth for base learners, <=0 means no limit\n","#             min_split_gain=0.0,             # Minimum loss reduction required to make a further partition on a leaf node of the tree\n","#             min_child_weight=0.001,         # Minimum sum of instance weight (hessian) needed in a child (leaf)\n","#             min_child_samples=20,           # Minimum number of data needed in a child (leaf)\n","#             colsample_bytree=params[\"reg\"], # dropout fraction of columns during fitting (max=1 = no dropout)\n","#             random_state=params[\"seed\"],    # seed value\n","#             silent=False,                   # whether to print info during fitting\n","#             importance_type='split',        # feature importance type: 'split'= N times feature is used in model; 'gain'= total gains of splits which use the feature\n","#             reg_alpha=0.0,                  # L1 regularization\n","#             reg_lambda=0.0,                 # L2 regularization\n","#             n_jobs=- 1,                     # N parallel threads to use on computer\n","#             subsample=1.0,                  # row fraction used for training: keep at 1 for time series data\n","#             subsample_freq=0,               # keep at 0 for time series\n","#             )\n","\n","#         tic = perf_counter()\n","#         model_gbdt.fit( \n","#             X_train,                                # Input feature matrix (array-like or sparse matrix of shape = [n_samples, n_features])\n","#             y_train,                                # The target values (class labels in classification, real numbers in regression) (array-like of shape = [n_samples])\n","#             eval_set=[(X_val, y_val)],              # can have multiple tuples of validation data inside this list\n","#             eval_names=None,                        # Names of eval_set (list of strings or None, optional (default=None))\n","#             eval_metric='rmse',                     # Default: 'l2' (= mean squared error, 'mse') for LGBMRegressor; options include 'l2_root'='root_mean_squared_error'='rmse' and 'l1'='mean_absolute_error'='mae' + more\n","#             early_stopping_rounds=params[\"estop\"],  # Activates early stopping. The model will train until the validation score stops improving. Validation score needs to improve at least every early_stopping_rounds \n","#                                                     #     to continue training. Requires at least one validation data and one metric. If there’s more than one, will check all of them. But the training data is ignored anyway. \n","#                                                     #     To check only the first metric, set the first_metric_only parameter to True in additional parameters **kwargs of the model constructor.\n","#             init_score=None,                        # Init score of train data\n","#             eval_init_score=None,                   # Init score of eval data (list of arrays or None, optional (default=None))\n","#             verbose=CONSTANTS[\"VERBOSITY\"] ,        # If True, metric on the eval set is printed at each boosting stage. If n=int, the metric on the eval set is printed at every nth boosting stage. Best and final also print.\n","#             feature_name='auto',                    # Feature names. If 'auto' and data is pandas DataFrame, data columns names are used. (list of strings or 'auto', optional (default='auto'))\n","#             categorical_feature='auto',             # Categorical features (list of strings or int, or 'auto', optional (default='auto')) If list of int, interpreted as indices. \n","#                                                     # If list of strings, interpreted as feature names (need to specify feature_name as well). \n","#                                                     # If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used. All values in categorical features should be less than int32 max value (2147483647). \n","#                                                     # Large values could be memory-consuming. Consider using consecutive integers starting from zero. All negative values in categorical features are treated as missing values.\n","#             callbacks=None                          # List of callback functions that are applied at each iteration (list of callback functions or None, optional (default=None)) See Callbacks in Python API for more information.\n","#             )\n","\n","#         toc = perf_counter()\n","#         results[\"model_fit_time\"] = datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","#         print(f'Model LGBM fit time: {results[\"model_fit_time\"]}')\n","#         results[\"best_iter\"] = model_gbdt.best_iteration_\n","#         results[\"best_val_rmse\"] = 0 #best_score\n","\n","\n","#     # if mod_type == 'HGBR':\n","#     #     # TTSplit should use TRAIN_FINAL = 33 (train on all data), and it will return also val=month33 for calculation at end (only)\n","#     #     model_gbdt = HistGradientBoostingRegressor(\n","#     #         learning_rate=LR, \n","#     #         max_iter=maxiter, \n","#     #         l2_regularization = reg,\n","#     #         early_stopping=False, \n","#     #         verbosity = verb,\n","#     #         random_state=seed_val)\n","    \n","#     #     tic = perf_counter()\n","#     #     model_gbdt.fit(X_train_np, y_train)\n","#     #     toc = perf_counter()\n","#     #     model_fit_time = datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","#     #     print(f\"model HGBR fit time: {model_fit_time}\")\n","#     #     best_iter = maxiter\n","#     #     best_val_rmse = 0\n","        \n","#     print(\"Starting predictions...\")\n","#     tic = perf_counter()\n","#     y_pred_train =  model_gbdt.predict( X_train, num_iteration=model_gbdt.best_iteration_ )\n","#     y_pred_val =    model_gbdt.predict( X_val,   num_iteration=model_gbdt.best_iteration_ )\n","#     y_pred_test =   model_gbdt.predict( X_test,  num_iteration=model_gbdt.best_iteration_ )\n","#     y_train =       y_train.to_numpy()\n","#     y_val =         y_val.to_numpy()\n","#     # always do minmax scaling after robust scaling; and do inverse scaling with minmax first, then robust\n","#     if CONSTANTS[\"_USE_MINMAX_SCALER\"]:\n","#         y_pred_train =  unscale(minmax_scalers['y_sales'],  y_pred_train)\n","#         y_pred_val =    unscale(minmax_scalers['y_sales'],  y_pred_val)\n","#         y_pred_test =   unscale(minmax_scalers['y_sales'],  y_pred_test)\n","#         y_train =       unscale(minmax_scalers['y_sales'],  y_train)\n","#         y_val =         unscale(minmax_scalers['y_sales'],  y_val)\n","#     if CONSTANTS[\"_USE_ROBUST_SCALER\"]:\n","#         y_pred_train =  unscale(robust_scalers['y_sales'],  y_pred_train)\n","#         y_pred_val =    unscale(robust_scalers['y_sales'],  y_pred_val)\n","#         y_pred_test =   unscale(robust_scalers['y_sales'],  y_pred_test)\n","#         y_train =       unscale(robust_scalers['y_sales'],  y_train)\n","#         y_val =         unscale(robust_scalers['y_sales'],  y_val)\n","#     y_pred_train =  y_pred_train.clip(CONSTANTS[\"_CLIP_PREDICT_L\"], CONSTANTS[\"_CLIP_PREDICT_H\"])\n","#     y_pred_val =    y_pred_val.clip(  CONSTANTS[\"_CLIP_PREDICT_L\"], CONSTANTS[\"_CLIP_PREDICT_H\"])\n","#     y_pred_test =   y_pred_test.clip( CONSTANTS[\"_CLIP_PREDICT_L\"], CONSTANTS[\"_CLIP_PREDICT_H\"]) \n","#     toc = perf_counter()\n","#     results[\"predict_time\"] = datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","#     print(f'Transform and Predict train/val/test time: {results[\"predict_time\"]}')\n","\n","#     results[\"train_r2\"],   results[\"val_r2\"]    = sk_r2(y_train, y_pred_train),            sk_r2(y_val, y_pred_val)\n","#     results[\"train_rmse\"], results[\"val_rmse\"]  = np.sqrt(sk_mse(y_train, y_pred_train)),  np.sqrt(sk_mse(y_val, y_pred_val))\n","#     print(f'R^2 train  = {results[\"train_r2\"]:.4f}      R^2 val  = {results[\"val_r2\"]:.4f}')\n","#     print(f'RMSE train = {results[\"train_rmse\"]:.4f}    RMSE val = {results[\"val_rmse\"]:.4f}\\n')\n","\n","#     return model_gbdt, model_gbdt.get_params(), X_test, y_pred_test, feature_names, data_types, results\n","\n","# print(f'Done: {strftime(\"%a %X %x\")}')\n","\n","\n","\n","\n","\n","# ensemble_feature_names = []\n","# ensemble_y_pred_test = []\n","# ensemble_df_columns = ['lr', 'reg', 'max_iter', 'estop', 'start', 'end', 'n_val_mo', 'seed', 'trR2', 'valR2', 'tr_rmse', 'val_rmse', 'best_iter', 'best_val_rmse', 'model_time','predict_time','total_time']\n","# ensemble_df_rows = []\n","# model_params = OrderedDict()\n","# itercount = 0\n","# for lr in FEATURES[\"LEARNING_RATE\"]:\n","#     for reg in FEATURES[\"REGULARIZATION\"]:\n","#         for maxit in FEATURES[\"MAX_ITERATIONS\"]:\n","#             for estop in FEATURES[\"EARLY_STOPPING\"]:\n","#                 for train_start_mo in FEATURES[\"TRAIN_MONTH_START\"]:\n","#                     for train_final_mo in FEATURES[\"TRAIN_MONTH_END\"]:\n","#                         for val_mo in FEATURES[\"N_VAL_MONTHS\"]:\n","#                             for seed in FEATURES[\"SEED_VALUES\"]:\n","#                                 itercount += 1\n","#                                 print(f'\\n\\nBelow: Model {itercount} of {FEATURES[\"ALL_exploded_shape[0]\"]}: LR = {lr}; LFF = {reg}, train_start = {train_start_mo}; train_end = {train_final_mo}; seed = {seed}\\n')\n","#                                 time0 = time.time()\n","#                                 model_params[\"lr\"] = lr\n","#                                 model_params['reg'] = reg\n","#                                 model_params['maxit'] = maxit\n","#                                 model_params['estop'] = estop\n","#                                 model_params['train_start_mo'] = train_start_mo\n","#                                 model_params['train_final_mo'] = train_final_mo\n","#                                 model_params['val_mo'] = val_mo\n","#                                 model_params['seed'] = seed\n","#                                 ##model_fit, y_pred_test, train_r2, val_r2, train_rmse, val_rmse, best_iter, best_val_rmse, model_fit_time, predict_time = \n","#                                 model_fit, model_params, X_test, y_pred_test, feature_names, data_types, results = GBDT_model(df, SPEC, model_params)\n","#                                 time2 = time.time(); model_time = datetime.utcfromtimestamp(time2 - time0).strftime('%H:%M:%S')\n","\n","#                                 ensemble_feature_names.append(feature_names)\n","#                                 ensemble_y_pred_test.append(y_pred_test)\n","#                                 ##ensemble_df_rows.append([lr,reg,maxit,estop,train_start_mo,train_final_mo,val_mo,seed,train_r2,val_r2,train_rmse,val_rmse,best_iter,best_val_rmse,model_fit_time,predict_time,model_time])\n","\n","#                                 # intermediate save after each model fit set of parameters, in case of crash or disconnect from Colab\n","#                                 # Simple ensemble averaging\n","#                                 y_test_pred_avg = np.mean(ensemble_y_pred_test, axis=0)\n","#                                 # Merge the test predictions with IDs from the original test dataset, and keep only columns \"ID\" and \"item_cnt_month\"\n","#                                 y_submission = pd.DataFrame.from_dict({'item_cnt_month':y_test_pred_avg,'shop_id':X_test.shop_id,'item_id':X_test.item_id})\n","#                                 y_submission = test.merge(y_submission, on=['shop_id','item_id'], how= 'left').reset_index(drop=True).drop(['shop_id','item_id'],axis=1)\n","#                                 y_submission.to_csv(\"./models_and_predictions/\" + FEATURES[\"_MODEL_NAME\"] + '_submission.csv', index=False)\n","#                                 ##ensemble_scores = pd.DataFrame(ensemble_df_rows, columns = ensemble_df_columns)\n","#                                 ##ensemble_scores.to_csv(\"./models_and_predictions/\" + model_filename_ens, index=False)\n","#                                 time3 = time.time(); iteration_time = datetime.utcfromtimestamp(time3 - time0).strftime('%H:%M:%S')\n","#                                 #print(f'TTSplit Execution Time = {ttsplit_time};  \n","#                                 print(f'Model fit/predict Execution Time = {model_time};  Total Iteration Execution Time = {iteration_time}')\n","#                                 print(f'Below: Model {itercount} of {FEATURES[\"ALL_exploded_shape[0]\"]}: LR = {lr}; LFF = {reg}, train_start = {train_start_mo}; train_end = {train_final_mo}; seed = {seed}\\n')\n","# print(model_params)\n","# print(feature_names)\n","# print(data_types)\n","# print(results)\n","# #display(ensemble_scores)\n","\n","# print(f'\\nDone: {strftime(\"%a %X %x\")}\\n')\n","\n","nocode=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HuOKMYlern3c"},"source":["##**Random Stuff**"]},{"cell_type":"markdown","metadata":{"id":"8aPOnrqM9y6r"},"source":["###**Parameters**"]},{"cell_type":"code","metadata":{"id":"_qXTF3EFe7eD","cellView":"both"},"source":["\n","def display_params():\n","    df_mem = df.memory_usage(deep=True)\n","    print(f'{strftime(\"%a %X %x\")};  df_size = {df_mem.sum()/(10**6):0.1f} MB, df_shape = {df.shape}; N train models: {ALL_exploded_shape[0]}; N features: {N_FEATURES}')\n","    \n","    print(\"------\\n------------\")\n","    pprint.pprint(dict(FEATURES),width=200,compact=True)\n","    print(\"\\n\")\n","    pprint.pprint(ITERS,width=200,compact=True)\n","    print(\"\\n\")\n","\n","\n","    print(f'COLS_ORDER_STT = {FEATURES[\"COLS_ORDER_STT\"]}')\n","    print(f'LAGGED_FEATURE_GROUPS = {FEATURES[\"LAGGED_FEATURE_GROUPS\"]}')\n","    print(f'AGG_STATS = {FEATURES[\"AGG_STATS\"]};  _CLIP_TRAIN_L = {FEATURES[\"_CLIP_TRAIN_L\"]}, _CLIP_PREDICT_L = {FEATURES[\"_CLIP_PREDICT_L\"]}')\n","    print(f'EDA_DELETE_SHOPS = {FEATURES[\"_EDA_DELETE_SHOPS\"]}; EDA_DELETE_ITEM_CATS = {FEATURES[\"_EDA_DELETE_ITEM_CATS\"]}; EDA_SCALE_MONTH = {FEATURES[\"_EDA_SCALE_MONTH\"]}')\n","    print(f'LAGS = {FEATURES[\"LAGS_MONTHS\"]} (months)')\n","    for lag in FEATURES[\"LAGS_MONTHS\"]:\n","        print(f'COLUMNS_TO_LAG[{lag}] = {FEATURES[\"LAG_FEATURES\"][lag]}')\n","    print(f'USE_CARTPROD_FILL = {FEATURES[\"_USE_CARTPROD_FILL\"]}, CARTPROD_WITH_TEST_PAIRS = {FEATURES[\"_CARTPROD_TEST_PAIRS\"]}, CARTPROD_FILLNA_WITH_0 = {FEATURES[\"_CARTPROD_FILLNA0\"]}')\n","    print(f'USE_ROBUST_SCALER = {FEATURES[\"_USE_ROBUST_SCALER\"]}, ROBUST_SCALER_QUANTILES = {FEATURES[\"_ROBUST_SCALER_QUANTILES\"]}, USE_MINMAX_SCALER = {FEATURES[\"_USE_MINMAX_SCALER\"]}, ',end=\"\")\n","    print(f'MINMAX_SCALER_RANGE = {FEATURES[\"_MINMAX_SCALER_RANGE\"]}, FEATURE_DATA_TYPE = {FEATURES[\"_FEATURE_DATA_TYPE\"]}')\n","    print(f'CARTPROD_FILL_MONTH_BEGIN = {FEATURES[\"_CARTPROD_FIRST_MONTH\"]}, ',end='') \n","    print(f'TRAIN_START_MONTH = {FEATURES[\"_TRAIN_START_MONTH\"]},  TRAIN_FINAL_MONTH = {FEATURES[\"_TRAIN_FINAL_MONTH\"]}, validate_months = {FEATURES[\"_validate_months\"]}')\n","    print(f'LEARNING_RATE = {ITERS[\"learning_rate\"].unique()}, MAX_ITERATIONS = {ITERS[\"n_estimators\"].unique()}, EARLY_STOPPING = {ITERS[\"early_stopping_rounds\"].unique()}, ',end='')\n","    print(f'REGULARIZATION = {ITERS[\"colsample_bytree\"].unique()}, SEED_VALUES = {ITERS[\"random_state\"].unique()}')\n","    print(f'SUBSAMPLE_FOR_BIN = {ITERS[\"subsample_for_bin\"].unique()}')\n","    return\n","\n","print(f'{FEATURES[\"_MODEL_NAME\"]}  Model Type: {FEATURES[\"_MODEL_TYPE\"]}')\n","display_params()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPzNWMRAyx6M"},"source":["###**K-Fold Training Splits; Ensemble Average; Save Intermediate Results**"]},{"cell_type":"code","metadata":{"id":"D2tNdYiiNa7x","cellView":"both"},"source":["\n","%cd \"{GDRIVE_REPO_PATH}\"\n","\n","ensemble_y_pred_test = []\n","ensemble_df_columns = ['tr_rmse','val_rmse','trR2','valR2','lr','reg','max_iter','estop','bin_sample','start','end','val_key','seed','best_iter','best_val_rmse','model_t','predict_t','total_t']\n","ensemble_df_rows = []\n","\n","\n","    if not ITERS.at[itercount,\"_model_filename\"]:\n","        ITERS.at[itercount,\"_model_filename\"] = input(\"Enter the Base Model Name Substring for Output File Naming (like: 'v4mg_01' )\")\n","    filename_parameters = ITERS.at[itercount,\"_model_type\"] + ITERS.at[itercount,\"_model_filename\"] + \"_params.csv\"\n","    filename_submission = ITERS.at[itercount,\"_model_type\"] + ITERS.at[itercount,\"_model_filename\"] + '_submission.csv'\n","\n","    print(f'\\n\\nBelow: Model {itercount+1} of {len(ITERS)}: lr= {ITERS.at[itercount,\"learning_rate\"]}; Reg= {ITERS.at[itercount,\"colsample_bytree\"]}, ',end='')\n","    print(f'train_start = {ITERS.at[itercount,\"_train_start_month\"]}; train_end = {ITERS.at[itercount,\"_train_final_month\"]}; seed = {ITERS.at[itercount,\"random_state\"]}\\n')\n","    \n","    time0 = time.time()\n","    # CHANGE --> only redo this inside the loop if the months change\n","    DataSets, ITERS.at[itercount,\"feature_name_\"] = TTSplit(data=df, params=ITERS, iternum=itercount)\n","\n","    y_pred_test, results_dict, parameters_of_model = GBDT_model(DataSets, model_params_dict, fit_params_dict) #ITERS, itercount)\n","    time1 = time.time()\n","    \n","\n","    ITERS.at[itercount,\"time_predict_end\"] = datetime.utcfromtimestamp(time1 - time0).strftime('%H:%M:%S')\n","    print(f'Total Iteration Execution Time = {ITERS.at[itercount,\"time_predict_end\"]}')\n","\n","    # intermediate save after each model fit set of parameters, in case of crash or disconnect from Colab\n","    # Simple ensemble averaging\n","    ensemble_y_pred_test.append(y_pred_test)\n","    y_test_pred_avg = np.mean(ensemble_y_pred_test, axis=0)\n","    # Merge the test predictions with IDs from the original test dataset, and keep only columns \"ID\" and \"item_cnt_month\"\n","    y_submission = pd.DataFrame.from_dict({'item_cnt_month':y_test_pred_avg,'shop_id':DataSets['X_test'].shop_id,'item_id':DataSets['X_test'].item_id})\n","    y_submission = test.merge(y_submission, on=['shop_id','item_id'], how= 'left').reset_index(drop=True).drop(['shop_id','item_id'],axis=1)\n","    y_submission.to_csv(\"./models_and_predictions/\" + filename_submission, index=False)\n","\n","    ITERS.to_csv(\"./models_and_predictions/\" + filename_parameters, index=False)\n","\n","    ensemble_df_rows.append(ITERS[['tr_rmse','val_rmse','tr_R2','val_R2','learning_rate','colsample_bytree','n_estimators','early_stopping_rounds','subsample_for_bin','_train_start_month','_train_final_month',\n","                                          '_validate_months','random_state','best_iteration_','best_score_','time_model_fit','time_model_predict','time_predict_end']].iloc[itercount].to_list())\n","    ensemble_scores = pd.DataFrame(ensemble_df_rows, columns = ensemble_df_columns)\n","\n","    print(f'\\nModel {itercount+1} of {len(ITERS)}: lr= {ITERS.at[itercount,\"learning_rate\"]}; Reg= {ITERS.at[itercount,\"colsample_bytree\"]}, ',end='')\n","    print(f'train_start = {ITERS.at[itercount,\"_train_start_month\"]}; train_end = {ITERS.at[itercount,\"_train_final_month\"]}; seed = {ITERS.at[itercount,\"random_state\"]}\\n')\n","    display(ensemble_scores)\n","    \n","    itercount += 1\n","\n","print(f'\\nDone: {strftime(\"%a %X %x\")}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4U1U6nZy8wd"},"source":["###**Document Results**"]},{"cell_type":"code","metadata":{"id":"RhUA6AyHvjf3","cellView":"both"},"source":["# Printout for copy-paste version control\n","\n","print('\\n------------------------------------------\\n------------------------------------------')\n","print(f'{FEATURES[\"_MODEL_NAME\"]}  Model Type: {FEATURES[\"_MODEL_TYPE\"]}\\nCoursera: \\n------------------------------------------')\n","display_params()\n","print('------')\n","print(ensemble_scores)\n","print('------')\n","print(ensemble_scores.describe(percentiles=[], include=np.number))\n","print(f'------\\nHighest and Lowest Feature Importance for Final Model:\\n{fi.iloc[list(range(0,8))+list(range(-7,0)),:]}\\n------')\n","print(y_submission.head(8))\n","print('------------------------------------------\\n\\n')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c98tKblpRWya"},"source":["###**Record Results**"]},{"cell_type":"code","metadata":{"id":"mIERXSQWz95V","cellView":"both"},"source":["# Results\n","'''\n","\n","Best Coursera score so far: 8/10 public and private LB scores are: 0.985186 and 0.979359 on 5/12 with Andreas' numbers\n","Best with this model: v7_ens21 8/10 public and private LB scores are: 0.974590 and 0.971219\n","\n","LGBMv10_15ens 8/10 public and private LB scores are: 0.984054 and 0.979126\n","* LGBMv10_13ens 8/10 public and private LB scores are: 0.976077 and 0.973442\n","* LGBMv10_11ens 8/10 public and private LB scores are: 0.977330 and 0.974200  same as v10_10ens, but removed all features with importance below 20%\n","*** LGBMv10_10ens 8/10 public and private LB scores are: 0.975422 and 0.971682\n","LGBMv10_09ens 8/10 public and private LB scores are: 0.984677 and 0.984238\n","LGBMv10_08ens 8/10 public and private LB scores are: 0.984238 and 0.982864\n","LGBMv10_07ens 8/10 public and private LB scores are: 0.985275 and 0.985093\n","LGBMv10_06ens 8/10 public and private LB scores are: 0.984912 and 0.983360\n","LGBMv10_v9_18noscaler 8/10 8/10 public and private LB scores are: 0.984643 and 0.985256\n","LGBMv10_v9_18 8/10 public and private LB scores are: 0.982740 and 0.983633  robust scaler used\n","LGBMv9_18ens 8/10 public and private LB scores are: 0.984137 and 0.984686\n","LGBMv9_09clip 8/10 public and private LB scores are: 0.984877 and 0.985790\n","LGBMv9_08clip 8/10 public and private LB scores are: 0.985158 and 0.986282\n","LGBMv9_04ens (less memory) 8/10 public and private LB scores are: 0.981707 and 0.985473\n","* LGBMv9_03ens 8/10 public and private LB scores are: 0.975438 and 0.973606\n","LGBMv8_v7_21B_ens 8/10 public and private LB scores are: 0.976147 and 0.972920\n","*** LGBMv6v7_bag06 8/10 public and private LB scores are: 0.974873 and 0.971385\n","* LGBMv6v7_bag05 8/10 public and private LB scores are: 0.975973 and 0.972537\n","**** v7_ens21 8/10 public and private LB scores are: 0.974590 and 0.971219\n","** v7_ens20 8/10 public and private LB scores are: 0.975499 and 0.971916\n","* v6_ens32 8/10 public and private LB scores are: 0.975826 and 0.972352\n","v6_10 8/10 public and private LB scores are: 0.984495 and 0.978631\n","v6_ens01 (avg v6 #17 through #31): 8/10 public and private LB scores are: 0.984457 and 0.978061\n","v6_ens33 8/10 public and private LB scores are: 0.980232 and 0.975554\n","v7_03 8/10 public and private LB scores are: 0.980832 and 0.975157\n","v7_ens07 8/10 public and private LB scores are: 0.980749 and 0.978082\n","\n","------------------------------------------\n","------------------------------------------\n","LGBMv10_15ens  Model Type: LGBM\n","------------------------------------------\n","Coursera: 8/10 public and private LB scores are: 0.984054 and 0.979126\n","Wed 19:10:11 07/15/20;  Size of df = 734.8 MB, Shape = (6226880, 59);  Size of X_train_np = 595.7 MB;  N Training Runs for this Model: 2\n","EDA_DELETE_SHOPS = [9, 20]; EDA_DELETE_ITEM_CATS = [8, 10, 32, 59, 80, 81, 82]; EDA_SCALE_MONTH = week_retail_weight; Lags(months) = [1, 2, 3, 4, 5, 6, 7, 8]\n","KEEP_STT_COLUMN_ORDER = ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']\n","STATS_FEATURES = [['shop_id', 'item_id'], ['shop_id', 'item_category_id'], ['shop_id', 'item_cluster'], 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']\n","AGG_STATS = OrderedDict([('sales', ['sum', 'median', 'count']), ('revenue', ['sum'])]); _CLIP_TRAIN_L = 0; _CLIP_TRAIN_H = 20; _CLIP_PREDICT_L = 0; _CLIP_PREDICT_H = 20\n","COLUMNS_TO_LAG[1] = ['y_sales', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', 'shop_id_sales_sum', 'shop_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', 'shop_group_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', 'item_group_sales_sum', 'item_group_revenue_sum', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","COLUMNS_TO_LAG[2] = ['y_sales', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_revenue_sum', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","COLUMNS_TO_LAG[3] = ['y_sales', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_cluster_sales_count']\n","COLUMNS_TO_LAG[4] = ['item_id_sales_sum']\n","COLUMNS_TO_LAG[5] = ['y_sales']\n","COLUMNS_TO_LAG[6] = ['y_sales']\n","COLUMNS_TO_LAG[7] = ['item_id_sales_sum']\n","COLUMNS_TO_LAG[8] = ['y_sales']\n","CARTPROD_FILL_MONTH_BEGIN = 13; TRAIN_MONTH_START = [13]; TRAIN_FINAL_MONTH = [29]; N_VAL_MONTHS = [False]; USE_CARTESIAN_FILL = True; CART_PROD_INCLUDES_TEST = False; CARTPROD_FILLNA_WITH_0 = True\n","USE_ROBUST_SCALER = True; ROBUST_SCALER_QUANTILES = (20, 80); USE_MINMAX_SCALER = True; MINMAX_RANGE = (0, 32000); DATA_TYPE = <class 'numpy.int16'>\n","LEARNING_RATE = [0.02, 0.005]; MAX_ITERATIONS = [8000]; EARLY_STOPPING = [200]; REGULARIZATION = [0.4]; SEED_VALUES = [42]\n","------\n","     lr   reg  max_iter  estop  start  end  n_val_mo  seed  trR2  valR2  tr_rmse  val_rmse  best_iter  best_val_rmse model_time predict_time total_time\n","0 0.020 0.400      8000    200     13   29     False    42 0.572  0.427    0.743     0.783          0      1,253.473   00:35:36     00:13:33   00:49:09\n","1 0.005 0.400      8000    200     13   29     False    42 0.549  0.427    0.763     0.784          0      1,254.303   01:24:53     00:35:09   02:00:02\n","------\n","         lr   reg  max_iter  estop  start  end  seed  trR2  valR2  tr_rmse  val_rmse  best_iter  best_val_rmse\n","count     2     2         2      2      2    2     2     2      2        2         2          2              2\n","mean  0.013 0.400      8000    200     13   29    42 0.561  0.427    0.753     0.784          0      1,253.888\n","std   0.011     0         0      0      0    0     0 0.017  0.001    0.014     0.000          0          0.587\n","min   0.005 0.400      8000    200     13   29    42 0.549  0.427    0.743     0.783          0      1,253.473\n","50%   0.013 0.400      8000    200     13   29    42 0.561  0.427    0.753     0.784          0      1,253.888\n","max   0.020 0.400      8000    200     13   29    42 0.572  0.427    0.763     0.784          0      1,254.303\n","------\n","Highest and Lowest Feature Importance for Final Model:\n","                             feature  value  norm_value lag                   feature_base\n","0             item_id_sales_count_L1   9378         100   1            item_id_sales_count\n","1               item_id_sales_sum_L1   9199      98.090   1              item_id_sales_sum\n","2                       item_cluster   8256      88.040   0                   item_cluster\n","3                            item_id   7862      83.830   0                        item_id\n","4                   item_category_id   7540      80.400   0               item_category_id\n","5             item_id_revenue_sum_L1   7261      77.430   1            item_id_revenue_sum\n","6                            shop_id   6958      74.190   0                        shop_id\n","7                         y_sales_L1   6754      72.020   1                        y_sales\n","51       item_cluster_sales_count_L2   2536      27.040   2       item_cluster_sales_count\n","52              item_id_sales_sum_L3   2535      27.030   3              item_id_sales_sum\n","53  shop_id_x_item_id_sales_count_L2   2293      24.450   2  shop_id_x_item_id_sales_count\n","54   item_category_id_sales_count_L3   2170      23.140   3   item_category_id_sales_count\n","55       item_cluster_sales_count_L3   2067      22.040   3       item_cluster_sales_count\n","56   item_category_id_sales_count_L2   1987      21.190   2   item_category_id_sales_count\n","57     item_category_id_sales_sum_L3   1879      20.040   3     item_category_id_sales_sum\n","------\n","   ID  item_cnt_month\n","0   0           0.781\n","1   1           0.079\n","2   2           1.174\n","3   3           0.247\n","4   4           0.875\n","5   5           0.697\n","6   6           0.514\n","7   7           0.095\n","------------------------------------------\n","------------------------------------------\n","------------------------------------------\n","\n","'''\n","nocode=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wgka2JBitW2F"},"source":["###**Older method of control loop**"]},{"cell_type":"code","metadata":{"id":"D8Py43PxRi6o"},"source":["# SPLIT_PARAMS_module_dfs = OrderedDict()\n","# SPLIT_PARAMS_module_dfs['MODEL']               =   [['model_type','feature_params','provided_data_files','provided_df_names']].copy(deep=True)\n","# SPLIT_PARAMS_module_dfs['EDA']                 =   [['eda_delete_shops','eda_delete_item_cats','eda_scale_month','feather_stt']].copy(deep=True)\n","# SPLIT_PARAMS_module_dfs['DATA_CONDITIONING']   =   [['cartprod_fillna0','cartprod_first_month','cartprod_test_pairs','clip_train_H','clip_train_L',\n","#                                         'feather_monthly_stt','feature_data_type','minmax_scaler_range','robust_scaler_quantiles',\n","#                                         'use_cartprod_fill','use_categorical','use_minmax_scaler','use_robust_scaler']].copy(deep=True)\n","# SPLIT_PARAMS_module_dfs['TRAIN_VAL_SPLIT']     =   [['feather_tvt_split','test_month','train_start_month','train_final_month','validate_months']].copy(deep=True)\n","# SPLIT_PARAMS_module_dfs['LGBM_SETUP']          =   [['boosting_type','metric','learning_rate','n_estimators','colsample_bytree','random_state',\n","#                                         'subsample_for_bin','num_leaves','max_depth','min_split_gain','min_child_weight','min_child_samples','silent',\n","#                                         'importance_type','reg_alpha','reg_lambda','n_jobs','subsample','subsample_freq','objective']].copy(deep=True)\n","# SPLIT_PARAMS_module_dfs['LGBM_FIT']            =   [['eval_metric','early_stopping_rounds','init_score','eval_init_score','verbose',\n","#                                         'feature_name','categorical_feature','callbacks']].copy(deep=True)\n","# SPLIT_PARAMS_module_dfs['OUTPUT_PROCESSING']   =   clip_predict_H,clip_predict_L,feather_tvt_split,test,model_type,feature_names,robust_scalers,minmax_scalers\n","#                       \n","# OUTPUTS_df                       =   [['best_iteration_','best_score_','feature_importances_','feature_name_','model_params',\n","#                                        'time_cumulative','time_data_manip','time_dataset_splits','time_eda','time_full_iteration',\n","#                                        'time_model_fit','time_model_predict','tr_R2','tr_rmse','val_R2','val_rmse']].copy(deep=True)\n","\n","# # Explode the dataframes so each row is one iteration of modeling parameters\n","# #   DataFrames each contain columns grouped by level of looping iteration (outermost loop loads data; \n","#               innermost loop saves results, for more compact assignment of looping variables)\n","# for df_name,par_df in SPLIT_PARAMS_module_dfs.items():\n","#     for col in par_df.columns:\n","#         par_df = par_df.explode(col)\n","#     par_df = par_df.reset_index(drop=True)\n","#     SPLIT_PARAMS_module_dfs[df_name] = par_df\n","# # also explode a dataframe with all parameters so we get an idea of how big our run will be\n","# SPLIT_PARAMS_module_dfs['ALL'] = ALL_PARAMS.copy(deep=True)\n","# for c in SPLIT_PARAMS_module_dfs['ALL'].columns:\n","#     SPLIT_PARAMS_module_dfs['ALL'] = SPLIT_PARAMS_module_dfs['ALL'].explode(c)\n","# SPLIT_PARAMS_module_dfs['ALL'] = SPLIT_PARAMS_module_dfs['ALL'].reset_index(drop=True)\n","\n","# df = pd.DataFrame({'shop': [1, 2, 10],\n","#                    'item': [0.5, 0.75, 0.25],\n","#                    'cate': [22, 33,11]}) #,index=['row1', 'row2'])\n","# print(df.to_dict('records'))\n","# print(df.to_dict('index'))\n","# print(df.to_dict('index', into=OrderedDict))\n","\n","# # >>  [{'shop': 1, 'item': 0.5, 'cate': 22}, {'shop': 2, 'item': 0.75, 'cate': 33}, {'shop': 10, 'item': 0.25, 'cate': 11}]\n","# # >>  {0: {'shop': 1, 'item': 0.5, 'cate': 22}, 1: {'shop': 2, 'item': 0.75, 'cate': 33}, 2: {'shop': 10, 'item': 0.25, 'cate': 11}}\n","# # >>  OrderedDict([(0, {'shop': 1, 'item': 0.5, 'cate': 22}), (1, {'shop': 2, 'item': 0.75, 'cate': 33}), (2, {'shop': 10, 'item': 0.25, 'cate': 11})])\n","\n","# model_params_dict = SPLIT_PARAMS_module_dfs['MODEL'].to_dict('index', into=OrderedDict)\n","# eda_params_dict = SPLIT_PARAMS_module_dfs['EDA'].to_dict('index', into=OrderedDict)\n","# data_conditioning_params_dict = SPLIT_PARAMS_module_dfs['DATA_CONDITIONING'].to_dict('index', into=OrderedDict)\n","# train_val_split_params_dict = SPLIT_PARAMS_module_dfs['TRAIN_VAL_SPLIT'].to_dict('index', into=OrderedDict)\n","# lgbm_setup_params_dict = SPLIT_PARAMS_module_dfs['LGBM_SETUP'].to_dict('index', into=OrderedDict)\n","# lgbm_fit_params_dict = SPLIT_PARAMS_module_dfs['LGBM_FIT'].to_dict('index', into=OrderedDict)\n","# output_processing_params_dict = SPLIT_PARAMS_module_dfs['OUTPUT_PROCESSING'].to_dict('index', into=OrderedDict)\n","# RUN_n = 0\n","# for model_iter, model_params in model_params_dict.items():\n","#     # ['model_type','feature_params','provided_data_files','provided_df_names'] \n","#     model_done = (model_iter+1 == len(model_params_dict))\n","\n","#     for eda_iter, eda_params in eda_params_dict.items():\n","#         # ['eda_delete_shops','eda_delete_item_cats','eda_scale_month','feather_stt'] \n","#         # + ['model_type','feature_params','provided_data_files','provided_df_names']\n","#         eda_params.update(model_params)  # add to eda dict because feature_params, data files, names are needed for eda module\n","#         # load datafiles, adjust stt for monthly stats grouping, output = stt, items_enc, shops_enc, test (?replicate \"test\" with simple code?)\n","        \n","#     multiproc\n","#         load_dfs = eda_function(**eda_params) # stt, shops_enc, items_enc, test dataframes in dictionary (or their filenames if feathered to disk)\n","#         output_processing_params_dict[RUN_n]['test'] = load_dfs['test']  # dataframe, or feather file name\n","#         eda_done = (model_done and (eda_iter+1 == len(eda_params_dict)))\n","\n","#         for data_iter, data_params in data_conditioning_params_dict.items():\n","#             # ['cartprod_fillna0','cartprod_first_month','cartprod_test_pairs','clip_train_H','clip_train_L','feather_monthly_stt','feature_data_type',\n","#             #  'minmax_scaler_range','robust_scaler_quantiles','use_cartprod_fill','use_categorical','use_minmax_scaler','use_robust_scaler']\n","#             #  + ['eda_delete_shops','eda_delete_item_cats','eda_scale_month','feather_stt']\n","#             #  + ['model_type','feature_params','provided_data_files','provided_df_names']\n","#             data_params.update(eda_params)  # add to data dict because feature_params, feather_stt needed for data conditioning module\n","#             #data_params['stt'], data_params['items_enc'], data_params['shops_enc'] = stt, items_enc, shops_enc\n","            \n","#     multiproc  ## \n","#             monthly_stt, robust_scalers, minmax_scalers = data_conditioning_function(\n","#                                                                 load_dfs['stt'],load_dfs['shops_enc'],load_dfs['items_enc'], **data_params)\n","#             data_cond_done = (eda_done and (data_iter+1 == len(data_conditioning_params_dict)))\n","#             if data_cond_done:\n","#                 del load_dfs    # (no more splits that will affect build of monthly_stt)\n","#                 # discard stt, items_enc, shops_enc, and any intermediate dfs\n","\n","#                 # merge with cartesian products, then with items_enc, shops_enc (do not create a new df; just use monthly_stt)\n","# \t\t\t\t# \tmultiprocess --> pool(merge,[months list]) do months in parallel?\n","# \t\t\t\t# \tmaybe split monthly_stt by months, then do the merges in parallel, then concatenate all the months back together\n","# \t\t\t\t# merge with lags (inplace with monthly_stt)\n","# \t\t\t\t# \tmultiprocess --> pool(merge,[lags list]) do lags in parallel?; check for proper column order / reset if necessary\n","# \t\t\t\t# \tcan I just add the shifted columns, and delete an N/A things where I don't have a shop-item match?, \n","#                 #         or make a big df with all the lags and then just one single merge (how=\"left\")\n","#                 # ?write to disk: monthly_stt.ftr feather? and inverse_scaler_xform?\n","#                 # outputs = monthly_stt (possibly on disk in ftr format), inverse_scaler_xform (in params df? or global? or on disk?)\n","#             for tvt_iter, tvt_split_params in train_val_split_params_dict.items():\n","#                 # ['feather_tvt_split','test_month','train_start_month','train_final_month','validate_months']\n","#                 # (train_X, feather_monthly_stt=True, feather_tvt_split=False, test_month=34, \n","#                 #        train_start_month=13, train_final_month=29, validate_months=999)\n","#                 tvt_split_params['feather_monthly_stt'] = data_params['feather_monthly_stt']\n","#                 tvt_split_params['feature_data_type'] = data_params['feature_data_type']\n","#                 tvt_split_params['use_categorical'] = data_params['use_categorical']\n","#                 tvt_split_params['categorical_features'] = model_params['feature_params']['categorical']\n","#                 tvt_split_params['model_type'] = model_params['model_type']\n","#                 tvt_xy_datasets = {'train_X':monthly_stt}\n","#                 # train_X,train_y,val_X,val_y,test_X, feature_names =\n","#     multiproc ##\n","#                 DataSets, feature_names = tvt_split_function(tvt_xy_datasets, **tvt_split_params)\n","#                     # ??? allow dropping of certain feature columns here???? need an entry in tvt params df \n","#                     #         (can't delete monthly_stt if plan to drop like this; would be best if saved to .ftr on disk)\n","#                     # ??? allow arbitrary setting of val months with a list?\n","#                     # read monthly_stt_temp.ftr into train_X \n","#                     # \ttest_X --> remove month 34 from train, rename into test_X; remove y_target column from test_X\n","#                     # \tval_X --> remove val months from train, rename into val_X\n","#                     # \tval_y --> pop y_target column from val_X, rename into val_y\n","#                     # \ttrain_y --> pop y_target column from train, rename into train_y\n","#                     # feature_names = train_X.columns\n","#                 for lgbm_setup_iter, lgbm_setup_params in lgbm_setup_params_dict.items():\n","#                     for lgbm_fit_iter, lgbm_fit_params in lgbm_fit_params_dict.items():\n","#                         model = lgbm_function(DataSets,**lgbm_setup_params,**lgbm_fit_params)\n","#                         for output_processing_iter, output_processing_params in output_processing_params_dict.items():\n","#                             OUTPUTS_df[RUN_n][feature_names] = feature_names\n","#                             OUTPUTS_df[RUN_n][other stuff] = model_output_function(model, DataSets,inverse_scaler_xform, output_processing_params)\n","#                                 # * do predictions\n","#                                 # * do inverse scaling\n","#                                 # * do clipping\n","#                                 # * compute rmse, r2 for train, val\n","#                                 # * compute test prediction / merge into proper shape/columns, and save to disk as model_filename+str(iter number)\n","#                                 # * compute feature importances\n","#                                 # save intermediate (or final) parameters+outputs dataframe to disk\n","#                                 # run = [model_iter*max(eda_iter)+eda_iter*max(data_iter)+data_iter*max(tvt_iter)+tvt_iter + setup + fit + process], or:\n","#                                 # RUN_n += 1\n","\n","# ens = ensembling_function(output file names):\n","#     # average, weighted-average, other method, to combine anything already saved to disk (default = straight avg of all runs in above loop)\n","\n","# next_runs = compute_trends(output  results):\n","#     # look at feature importances all together, and see if anything obvious good or bad\n","#     # look at splits and see if any parameters obviously good or bad (correlation matrix of parameters with output results?)\n","\n","\n","        # model_gbdt = lgb.LGBMRegressor(\n","        #     objective=param_df.at[iternum,'objective'],\n","        #     boosting_type=param_df.at[iternum,'boosting_type'],\n","        #     learning_rate=param_df.at[iternum,'learning_rate'],\n","        #     n_estimators=param_df.at[iternum,'n_estimators'],\n","        #     #metric=param_df.at[iternum,'metric'],\n","        #     subsample_for_bin=param_df.at[iternum,'subsample_for_bin'],\n","        #     num_leaves=param_df.at[iternum,'num_leaves'],\n","        #     max_depth=param_df.at[iternum,'max_depth'],\n","        #     min_split_gain=param_df.at[iternum,'min_split_gain'],\n","        #     min_child_weight=param_df.at[iternum,'min_child_weight'],\n","        #     min_child_samples=param_df.at[iternum,'min_child_samples'],\n","        #     colsample_bytree=param_df.at[iternum,'colsample_bytree'],\n","        #     random_state=param_df.at[iternum,'random_state'],\n","        #     silent=param_df.at[iternum,'silent'],\n","        #     importance_type=param_df.at[iternum,'importance_type'],\n","        #     reg_alpha=param_df.at[iternum,'reg_alpha'],\n","        #     reg_lambda=param_df.at[iternum,'reg_lambda'],\n","        #     n_jobs=param_df.at[iternum,'n_jobs'],\n","        #     subsample=param_df.at[iternum,'subsample'],\n","        #     subsample_freq=param_df.at[iternum,'subsample_freq']\n","        #     )\n","# 'eval_metric','early_stopping_rounds','init_score','eval_init_score','verbose',\n","#                                          'feature_name','categorical_feature','callbacks'\n","\n","        # model_gbdt.fit(\n","        #     data['X_train'],                                # Input feature matrix (array-like or sparse matrix of shape = [n_samples, n_features])\n","        #     data['y_train'],                                # The target values (class labels in classification, real numbers in regression) (array-like of shape = [n_samples])\n","        #     eval_set=[(data['X_val'], data['y_val'])],      # can have multiple tuples of validation data inside this list\n","        #     eval_names=None,                                # Names of eval_set (list of strings or None, optional (default=None))\n","        #     eval_metric=param_df.at[iternum,'eval_metric'],\n","        #     early_stopping_rounds=param_df.at[iternum,'early_stopping_rounds'],\n","        #     init_score=param_df.at[iternum,'init_score'],\n","        #     eval_init_score=param_df.at[iternum,'eval_init_score'],\n","        #     verbose=param_df.at[iternum,'verbose'],\n","        #     feature_name=param_df.at[iternum,'feature_name'],\n","        #     categorical_feature=param_df.at[iternum,'categorical_feature'],\n","        #     callbacks=param_df.at[iternum,'callbacks']\n","        #      )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x0jADK1Qs555"},"source":["###**Older method of defining features**"]},{"cell_type":"code","metadata":{"id":"-QoWKEW63ADk","cellView":"both"},"source":["\n","    # Lag Split Info:      months_list: [1, 2, 3, 4, 5, 6, 7, 8]\n","    #     params: OrderedDict([\n","    #     (1, OrderedDict([\n","    #         ('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum', 'median', 'count'], 'revenue': ['sum']}}), \n","    #         ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}}), \n","    #         ('shop_id_x_item_cluster', {'group': ['shop_id', 'item_cluster'], 'stats': {'sales': ['sum', 'median']}}), \n","    #         ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['sum', 'count']}}), \n","    #         ('item_id', {'group': ['item_id'], 'stats': {'sales': ['median', 'sum', 'count'], 'revenue': ['sum']}}), \n","    #         ('shop_group', {'group': ['shop_group'], 'stats': {'revenue': ['sum']}}), \n","    #         ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), \n","    #         ('item_group', {'group': ['item_group'], 'stats': {'sales': ['sum'], 'revenue': ['sum']}}), \n","    #         ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}})])), \n","    #     (2, OrderedDict([\n","    #         ('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), ...\n","    #     ...\n","\n","    #     stats_set: OrderedDict([\n","    #         ('shop_id_x_item_id', {'group': ['shop_id', 'item_id'], 'stats': {\n","    #             'shop_group': 'first', 'item_category_id': 'first', 'item_group': 'first', 'item_cluster': 'first', \n","    #             'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}}), \n","    #         ('shop_id_x_item_category_id', {'group': ['shop_id', 'item_category_id'], 'stats': {'sales': ['count', 'sum', 'median'], 'revenue': ['sum']}}), \n","    #         ('shop_id_x_item_cluster', {'group': ['shop_id', 'item_cluster'], 'stats': {'sales': ['sum', 'median']}}), \n","    #         ('shop_id', {'group': ['shop_id'], 'stats': {'sales': ['sum', 'count']}}), \n","    #         ('item_id', {'group': ['item_id'], 'stats': {'sales': ['median', 'sum', 'count'], 'revenue': ['sum']}}), \n","    #         ('shop_group', {'group': ['shop_group'], 'stats': {'revenue': ['sum']}}), \n","    #         ('item_category_id', {'group': ['item_category_id'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}}), \n","    #         ('item_group', {'group': ['item_group'], 'stats': {'sales': ['sum'], 'revenue': ['sum']}}), \n","    #         ('item_cluster', {'group': ['item_cluster'], 'stats': {'sales': ['sum', 'count'], 'revenue': ['sum']}})])\n","    #     stats_set_feature_names: [ #'shop_group', 'item_category_id', 'item_group', 'item_cluster',\n","    #              'shop_id_x_item_id_sales_sum', \n","    #             'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', \n","    #             'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', \n","    #             'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', 'shop_id_sales_sum', 'shop_id_sales_count', \n","    #             'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', 'shop_group_revenue_sum', \n","    #             'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', 'item_group_sales_sum', \n","    #             'item_group_revenue_sum', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum', \n","    #             'shop_id_x_item_category_id_revenue_sum']\n","    # stt_final: ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']\n","    # categorical: ['shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster']\n","    # integer: ['month', 'shop_id', 'shop_group', 'item_id', 'item_category_id', 'item_group', 'item_cluster']\n","\n","\n","\n","#apply(pd.to_numeric, downcast= 'np.float32')\n","\n","\n","# print(\"\\nModel Name Parameters:\");               pprint.pprint(SPLIT_PARAMS_module_dfs['MODEL'].iloc[0].to_dict()) # convert all of these dataframes to dicts for pretty printing\n","# print(\"\\nEDA Parameters:\");                      pprint.pprint(SPLIT_PARAMS_module_dfs['EDA'].iloc[0].to_dict())\n","# print(\"\\nData Conditioning Parameters:\");        pprint.pprint(SPLIT_PARAMS_module_dfs['DATA_CONDITIONING'].iloc[0].to_dict()) # print(params_data_conditioning_dict)\n","# print(\"\\nTrain-Val-Test Splitting Parameters:\"); pprint.pprint(SPLIT_PARAMS_module_dfs['TRAIN_VAL_SPLIT'].iloc[0].to_dict())\n","# print(\"\\nLGBM Regressor Setup Parameters:\");     pprint.pprint(SPLIT_PARAMS_module_dfs['LGBM_SETUP'].iloc[0].to_dict()) # pprint.pprint(PARS.filter(like='', axis=1).iloc[0].to_dict())\n","# print(\"\\nLGBM Regressor Fit Parameters:\");       pprint.pprint(SPLIT_PARAMS_module_dfs['LGBM_FIT'].iloc[0].to_dict())  # ,width=200,compact=True)\n","# print(\"\\nOutput Processing Parameters:\");        pprint.pprint(SPLIT_PARAMS_module_dfs['OUTPUT_PROCESSING'].iloc[0].to_dict())   \n","\n","# # convert to dict for pretty printing\n","# outputs_dict =              OUTPUTS_df.iloc[0].to_dict()\n","# params_model_dict =         PARAMS_MODEL_df.iloc[0].to_dict()\n","# params_lgbm_setup_dict =    PARAMS_LGBM_SETUP_df.iloc[0].to_dict()        \n","# params_lgbm_fit_dict =      PARAMS_LGBM_FIT_df.iloc[0].to_dict()\n","# params_eda_dict =           PARAMS_EDA_df.iloc[0].to_dict()\n","# params_tvt_split_dict =     PARAMS_TVT_SPLIT_df.iloc[0].to_dict()\n","# params_data_manip_dict =    PARAMS_DATA_MANIP_df.iloc[0].to_dict()        \n","\n","# FEATURES[\"LAGS_MONTHS\"] = [1,2,3,4,5,6,7,8]  # month lags to include in model \n","# FEATURES[\"LAG_FEATURES\"] = OrderedDict()\n","# for i in FEATURES[\"LAGS_MONTHS\"]:\n","#     FEATURES[\"LAG_FEATURES\"][i] = ['y_sales', 'shop_id_x_item_category_id_sales_sum', 'item_id_sales_sum', 'item_cluster_sales_sum'] \n","# # SECTION BELOW: manually remove some of the above-included features, as determined by feature importances to be likely unhelpful\n","# # keep at least the highest importance feature for each lag, but remove all others with < 20% importance (month 13-32 training)\n","# FEATURES[\"LAG_FEATURES\"][8] = [e for e in FEATURES[\"LAG_FEATURES\"][8] if e not in {'item_id_sales_sum','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","\n","\n","# # LAG_STATS_SET is SET of all aggregate statistics columns for all lags (allows us to shed the other stats, keeping memory requirements lower)\n","# LAG_STATS_SET = [] # FEATURES[\"LAG_FEATURES\"][1]\n","# FEATURES[\"ALL_FEATURES\"] = [] #FEATURES[\"LAG_FEATURES\"][1]\n","# for m in FEATURES[\"LAGS_MONTHS\"]: #[1:]:\n","#     LAG_STATS_SET = LAG_STATS_SET + [x for x in FEATURES[\"LAG_FEATURES\"][m] if x not in LAG_STATS_SET]\n","#     FEATURES[\"ALL_FEATURES\"] = FEATURES[\"ALL_FEATURES\"] + [x + \"_L\" + str(m) for x in FEATURES[\"LAG_FEATURES\"][m]]\n","# FEATURES[\"ALL_FEATURES\"] = INTEGER_COLS + FEATURES[\"ALL_FEATURES\"]\n","# N_FEATURES = len(FEATURES[\"ALL_FEATURES\"])\n","\n","# FEATURES[\"_CARTPROD_FILLNA0\"]         = ITERS[\"_cartprod_fillna0\"]\n","# FEATURES[\"_CARTPROD_FIRST_MONTH\"]     = ITERS[\"_cartprod_first_month\"]\n","# FEATURES[\"_CARTPROD_TEST_PAIRS\"]      = ITERS[\"_cartprod_test_pairs\"]\n","# FEATURES[\"_CLIP_TRAIN_H\"]             = ITERS[\"_clip_train_H\"]            \n","# FEATURES[\"_CLIP_TRAIN_L\"]             = ITERS[\"_clip_train_L\"]                   \n","# FEATURES[\"_CLIP_PREDICT_H\"]           = ITERS[\"_clip_predict_H\"]          \n","# FEATURES[\"_CLIP_PREDICT_L\"]           = ITERS[\"_clip_predict_L\"]    \n","# FEATURES[\"_EDA_DELETE_SHOPS\"]         = ITERS[\"_eda_delete_shops\"]\n","# FEATURES[\"_EDA_DELETE_ITEM_CATS\"]     = ITERS[\"_eda_delete_item_cats\"]\n","# FEATURES[\"_EDA_SCALE_MONTH\"]          = ITERS[\"_eda_scale_month\"]\n","# FEATURES[\"_FEATURE_DATA_TYPE\"]        = ITERS[\"_feature_data_type\"]\n","# FEATURES[\"_MINMAX_SCALER_RANGE\"]      = ITERS[\"_minmax_scaler_range\"]\n","# FEATURES[\"_MODEL_NAME_BASE\"]          = ITERS[\"_model_filename\"]\n","# FEATURES[\"_MODEL_TYPE\"]               = ITERS[\"_model_type\"]\n","# FEATURES[\"_ROBUST_SCALER_QUANTILES\"]  = ITERS[\"_robust_scaler_quantiles\"]     \n","# FEATURES[\"_TEST_MONTH\"]               = ITERS[\"_test_month\"]\n","# FEATURES[\"_TRAIN_START_MONTH\"]        = ITERS[\"_train_start_month\"]\n","# FEATURES[\"_TRAIN_FINAL_MONTH\"]        = ITERS[\"_train_final_month\"]\n","# FEATURES[\"_USE_CARTPROD_FILL\"]        = ITERS[\"_use_cartprod_fill\"]\n","# FEATURES[\"_USE_CATEGORICAL\"]          = ITERS[\"_use_categorical\"]         \n","# FEATURES[\"_USE_ROBUST_SCALER\"]        = ITERS[\"_use_robust_scaler\"]      \n","# FEATURES[\"_USE_MINMAX_SCALER\"]        = ITERS[\"_use_minmax_scaler\"]\n","# FEATURES[\"_validate_months\"]             = ITERS[\"_validate_months\"]\n","\n","# pprint.pprint(ITERS,width=200,compact=True)\n","# pprint.pprint(ALL_PARAMS,width=200,compact=True)\n","\n","# print(ITERS[['random_state','n_estimators','boosting_type','metric']].iloc[0].to_list())   # selecting only certain variables from a certain iteration line (0 in this case)\n","\n","\n","\n","# class Delete_Me:\n","#     def __init__(self, name, df):\n","#         self.name = name\n","#         self.df = df\n","#     def clear_memory(self,dataframe):\n","#         print(f'Removing {self.name}')\n","#         del dataframe\n","#         gc.collect()\n","#         #dataframe = pd.DataFrame(np.zeros((1,1),dtype=np.int8)) # not sure whether this line really helps\n","#         return True\n","\n","# def rm_df(rm_dict={'df':pd.DataFrame()}):\n","#     \"\"\"\n","#     try to save memory by deleting unneeded dataframes\n","#     input is a dictionary of dataframe string names as keys, and dataframes as values\n","#     \"\"\"\n","#     print(f'\\nPrior to df delete, Google Colab runtime is using {virtual_memory().used / 1e9:.1f} GB of {virtual_memory().total / 1e9:.1f} GB available RAM\\n')\n","#     for k,v in rm_dict.items():\n","#         rm_df_class = Delete_Me(k,v)\n","#         rm_df_class.clear_memory(v)\n","#         v = pd.DataFrame(np.zeros((1,1),dtype=np.int8))\n","#         # try: del v\n","#         # except: print(f'DataFrame {k} delete error.')\n","#     #gc.disable()\n","#     #gc.collect()\n","#     print(f'\\nAfter gc.collect(), Google Colab runtime is using {virtual_memory().used / 1e9:.1f} GB of {virtual_memory().total / 1e9:.1f} GB available RAM\\n')\n","#     return \n","\n","\n","nocode=True"],"execution_count":null,"outputs":[]}]}