{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MG_EDA_items only v1.0.ipynb","provenance":[{"file_id":"1y04qp_hoyBnsJQwkX67pk4iZsKGQIqNy","timestamp":1588805435380},{"file_id":"1b_K0QD9U6dofQ7VtTAtzUrqbKJMdj64l","timestamp":1588785261238},{"file_id":"1gcbeu-d1GUUzznZwTzfqYYbaD6cJ7EQ4","timestamp":1588238522691},{"file_id":"1pSGNRDJGzdeI69bw1zWefzPifBq-rv9H","timestamp":1588151557805},{"file_id":"1hq-ivO1BBtc5IC5xd-JdH8HNAQ81bkRf","timestamp":1587386702728},{"file_id":"1I7DWo2B7q7g9Ne2khD11YGTq_gD2FoaT","timestamp":1587321559573},{"file_id":"1fyZv-jgb8twsCBQwPxjgk_XSYA6dOa2t","timestamp":1587303588700},{"file_id":"1iKsplqpLQQZqdr3Trflk7TapksgkQXjX","timestamp":1587145642564},{"file_id":"https://github.com/migai/Kag/blob/master/Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587076517706}],"collapsed_sections":["ruw_WyRxhqpx","hPrPvh7sorJd","s0J5l5H98Xsh","lZfgOx-0_KXg","EIn77EZzoxkY","V7QY11R1QmlN","KvzvPqmCQ2ZM","amkSqIxGJy65","Uvreedy4YRVd","7TMDgN8xXlUy","_L1gjS_FZbFJ","xUIfCz4U4sVF"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn","colab_type":"text"},"source":["#**Investigation of *items* database and correlations between items**\n","\n","**EDA, NLP, Feature Generation**\n","\n","Andreas Theodoulou and Michael Gaidis (May, 2020)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ruw_WyRxhqpx"},"source":["#0. Configure Environment\n","**NOT OPTIONAL**"]},{"cell_type":"code","metadata":{"id":"sTVAxnMnenrB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"ac393e00-83e5-4586-dc27-3691eddd5dd3","executionInfo":{"status":"ok","timestamp":1588974584759,"user_tz":240,"elapsed":585,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# General python libraries/modules used throughout the notebook\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import MultipleLocator, FormatStrFormatter, AutoMinorLocator\n","import numpy as np\n","import seaborn as sns\n","\n","import os\n","from itertools import product\n","import re\n","import json\n","import time\n","from time import sleep, localtime, strftime\n","import pickle\n","\n","\n","# Magics\n","%matplotlib inline\n","\n","\n","# # NLP packages\n","# import nltk\n","# nltk.download('stopwords')\n","# from nltk.corpus import stopwords\n","\n","# # ML packages\n","# from sklearn.linear_model import LinearRegression\n","\n","# !pip install catboost\n","# from catboost import CatBoostRegressor \n","\n","# %tensorflow_version 2.x\n","# import tensorflow as tf\n","# import keras as K\n","\n","# # List of the modules we need to version-track for reference\n","# modules = ['pandas','matplotlib','numpy','seaborn','sklearn','tensorflow','keras','catboost','pip']"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NoOf7oi8Oe-Q","colab_type":"code","colab":{}},"source":["# Notebook formatting\n","# Adjust as per your preferences.  I'm using a FHD monitor with a full-screen browser window containing my IPynb notebook\n","\n","# format pandas output so we can see all the columns we care about (instead of \"col1  col2  ........ col8 col9\", we will see \"col1 col2 col3 col4 col5 col6 col7 col8 col9\" if it fits inside display.width parameter)\n","pd.set_option(\"display.max_columns\",30)  \n","pd.set_option(\"display.max_rows\",100)     # Override pandas choice of how many rows to show, so, for example, we can see the full 84-row item_category dataframe instead of the first few rows, then ...., then the last few rows\n","pd.set_option(\"display.width\", 300)       # Similar to the above for showing more rows than pandas defaults to, we can show more columns than default, if we tune this to our monitor window size\n","pd.set_option(\"max_colwidth\", None)\n","\n","#pd.set_option(\"display.precision\", 3)  # Nah, this is helpful, but below is even better\n","#Try to convince pandas to print without decimal places if a number is actually an integer (helps keep column width down, and highlights data types)\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"miCS3XqUhDXz"},"source":["#1. Load Data Files\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kjWzoXizEa5O","colab_type":"text"},"source":["##1.1) Enter Data File Names and Paths\n","\n","**NOT Optional**"]},{"cell_type":"code","metadata":{"id":"p9vsd3EynZLO","colab_type":"code","colab":{}},"source":["#  FYI, data is coming from a public repo on GitHub at github.com/migai/Kag\n","# List of the data files (path relative to GitHub master), to be loaded into pandas DataFrames\n","data_files = [  \"readonly/final_project_data/items.csv\",\n","                \"readonly/final_project_data/item_categories.csv\",\n","                \"readonly/final_project_data/shops.csv\",\n","                \"readonly/final_project_data/sample_submission.csv.gz\",\n","                \"readonly/final_project_data/sales_train.csv.gz\",\n","                \"readonly/final_project_data/test.csv.gz\",\n","                \"data_output/shops_transl.csv\",\n","                \"data_output/shops_augmented.csv\",\n","                \"data_output/item_categories_transl.csv\",\n","                \"data_output/item_categories_augmented.csv\",\n","                \"data_output/items_transl.csv\",\n","                \"data_output/item_vectors.csv.gz\",\n","                \"readonly/en_50k.csv\"  ]\n","\n","\n","# Dict of helper code files, to be loaded and imported {filepath : import_as}\n","code_files = {}  # not used at this time; example dict = {\"helper_code/kaggle_utils_at_mg.py\" : \"kag_utils\"}\n","\n","\n","# GitHub file location info\n","git_hub_url = \"https://raw.githubusercontent.com/migai\"\n","repo_name = 'Kag'\n","branch_name = 'master'\n","base_url = os.path.join(git_hub_url, repo_name, branch_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPrPvh7sorJd","colab_type":"text"},"source":["##1.2) Load Data Files"]},{"cell_type":"code","metadata":{"id":"CUIE1PVjSAmg","colab_type":"code","outputId":"9c212116-8c99-47b2-cf54-77407789f607","executionInfo":{"status":"ok","timestamp":1588974615175,"user_tz":240,"elapsed":30979,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, then paste it into the input box and hit 'enter' to complete mounting of the drive\n","from google.colab import drive  \n","drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dy5i7jl00oX-","colab_type":"code","outputId":"89071d95-2991-4769-de4b-26b53ba20208","executionInfo":{"status":"ok","timestamp":1588974637543,"user_tz":240,"elapsed":53339,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["'''\n","############################################################\n","############################################################\n","'''\n","# Replace this path with the path on *your* Google Drive where the repo master branch is stored\n","#   (on GitHub, the remote repo is located at github.com/migai/Kag --> below is my cloned repo location)\n","GDRIVE_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\"\n","'''\n","############################################################\n","############################################################\n","'''\n","\n","%cd \"{GDRIVE_REPO_PATH}\"\n","\n","print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","\n","# Loop to load the data files into appropriately-named pandas DataFrames\n","for path_name in data_files:\n","    filename = path_name.rsplit(\"/\")[-1]\n","    data_frame_name = filename.split(\".\")[0]\n","    exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","    if data_frame_name == 'sales_train':\n","        sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","    print(\"Data Frame: \" + data_frame_name)\n","    print(eval(data_frame_name).head(2))\n","    print(\"\\n\")\n","\n","\"\"\"\n","unused at this time...\n","\n","# Load in any helper functions from the code_files dictionary\n","#    dictionary key is the path (replace \"/\"\" with \".\" when using Google Drive + Colab), \n","#      and dictionary value is the module reference name\n","#    note that the directory chain on GitHub (and local repo) from current directory down to the .py file\n","#      must include a \"__init__.py\" file (it can be empty) in each of the directories\n","for filepath, module in code_files.items():\n","  path_name = filepath.replace(\"/\",\".\")[:-3]  # Google Drive reference does not use .py, and uses a \".\" instead of \"/\" for directory delineation\n","  exec(\"import \" + path_name + \" as \" + module)\n","\n","# Sanity check test\n","test1 = kag_utils.add_one(2)\n","print(test1)\n","\"\"\"\n","optional_code = True  # in a code block: (at top of cell = notice for you; at bottom = prevents Jupyter printing \"\"\" comments)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","Loading Files from Google Drive repo into Colab...\n","\n","Data Frame: items\n","                                                              item_name  item_id  item_category_id\n","0                             ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0                40\n","1  !ABBYY FineReader 12 Professional Edition Full [PC, Цифровая версия]        1                76\n","\n","\n","Data Frame: item_categories\n","        item_category_name  item_category_id\n","0  PC - Гарнитуры/Наушники                 0\n","1         Аксессуары - PS2                 1\n","\n","\n","Data Frame: shops\n","                       shop_name  shop_id\n","0  !Якутск Орджоникидзе, 56 фран        0\n","1  !Якутск ТЦ \"Центральный\" фран        1\n","\n","\n","Data Frame: sample_submission\n","   ID  item_cnt_month\n","0   0           0.500\n","1   1           0.500\n","\n","\n","Data Frame: sales_train\n","        date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0 2013-01-02               0       59    22154         999             1\n","1 2013-01-03               0       25     2552         899             1\n","\n","\n","Data Frame: test\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","\n","\n","Data Frame: shops_transl\n","                       shop_name  shop_id                       en_shop_name\n","0  !Якутск Орджоникидзе, 56 фран        0  ! Yakutsk Ordzhonikidze, 56 Franc\n","1  !Якутск ТЦ \"Центральный\" фран        1       ! Yakutsk TC \"Central\" Franc\n","\n","\n","Data Frame: shops_augmented\n","                       shop_name  shop_id                       en_shop_name shop_city shop_category shop_federal_district  shop_city_population  shop_tested\n","0  !Якутск Орджоникидзе, 56 фран        0  ! Yakutsk Ordzhonikidze, 56 Franc   Yakutsk          Shop               Eastern                235600        False\n","1  !Якутск ТЦ \"Центральный\" фран        1       ! Yakutsk TC \"Central\" Franc   Yakutsk          Mall               Eastern                235600        False\n","\n","\n","Data Frame: item_categories_transl\n","        item_category_name  item_category_id                 en_cat_name\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones\n","1         Аксессуары - PS2                 1           Accessories - PS2\n","\n","\n","Data Frame: item_categories_augmented\n","        item_category_name  item_category_id                 en_cat_name item_category1 item_category2 item_category3 item_category4  item_cat_tested\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones          Audio             PC    Accessories             PC             True\n","1         Аксессуары - PS2                 1           Accessories - PS2    Accessories    PlayStation    Accessories    PlayStation            False\n","\n","\n","Data Frame: items_transl\n","                                                              item_name  item_id  item_category_id                                                           en_item_name\n","0                             ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0                40                                           ! POWER IN glamor (PLAST.) D\n","1  !ABBYY FineReader 12 Professional Edition Full [PC, Цифровая версия]        1                76  ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]\n","\n","\n","Data Frame: item_vectors\n","   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  ...  4028  4029  4030  4031  4032  4033  4034  4035  4036  4037  4038  4039  4040  4041  4042\n","0  1  1  0  0  0  0  0  0  0  0   0   0   0   0   0  ...     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n","1  0  0  0  0  1  0  0  0  0  0   1   0   0   0   0  ...     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n","\n","[2 rows x 4043 columns]\n","\n","\n","Data Frame: en_50k\n","  word  word_count\n","0  you    28787591\n","1    i    27086011\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P99yjzAasJva"},"source":["#2. Explore Data (EDA), Clean Data, and Generate Features"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xr0FDeno_EUQ"},"source":["#2.5) ***items*** Dataset: EDA, Cleaning, Correlations, and Feature Generation\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s0J5l5H98Xsh"},"source":["###Thoughts regarding items dataframe\n","Let's first look at how many training examples we have to work with..."]},{"cell_type":"markdown","metadata":{"id":"1lg7NbEchkuM","colab_type":"text"},"source":["Many of the items have similar names, but slightly different punctuation, or only very slightly different version numbers or types.  (e.g., 'Call of Duty III' vs. 'Call of Duty III DVD')\n","\n","One can expect that these two items would have similar sales in general, and by grouping them into a single feature category, we can eliminate some of the overfitting that might come as a result of the relatively small ratio of (training set shop-item-date combinations = 2935849)/(total number of unique items = 22170).  (This is an average of about 132 rows in the sales_train data for each shop-item-date combination that we are using to train our model.  Our task is to produce a monthly estimate of sales (for November 2015), so it is relevant to consider training our model based on how many sales in a month vs. how many sales in the entire training set.  Given that the sales_train dataset covers the time period from January 2013 to October 2015 (34 months), we have on average fewer than 4 shop-item combinations in our training set for a given item in any given month.  Furthermore, as we are trying to predict for a particular month (*November* 2015), it is relevant to consider how many rows in our training set occur in the month of November.  The sales_train dataset contains data for two 'November' months out of the total 34 months of data.  Another simple calculation gives us an estimate that our training set contains on average 0.23 shop-item combinations per item for November months.\n","\n","To summarize:\n","\n","*  *sales_train* contains 34 months of data, including 2935849 shop-item-date combinations\n","*  *items* contains 22170 \"unique\" item_id values\n","\n","In the *sales_train* data, we therefore have:\n","*  on average, 132 rows with a given shop-item pair for a given item_id\n","*  on average, 4 rows with a given shop-item pair for a given item_id in a given month\n","*  on average, 0.23 rows with a given shop-item pair for a given item_id in all months named 'November'\n","\n","If we wish to improve our model predictions for the following month of November, it behooves us to use monthly grouping of sales, or, even better, November grouping of sales.  This smooths out day-to-day variations in sales for a better monthly prediction.  However, the sparse number of available rows in the *sales_train* data will contribute to inaccuracy in our model training and predictions.\n","\n","Imagine if we could reduce the number of item_id values from 22170 to perhaps half that or even less.  Given that the number of rows for training (per item, on a monthly or a November basis) is so small, then such a reduction in the number of item_id values would have a big impact.  (The same is true for creating features to supplement \"shop_id\" so as to group and reduce the individuality of each shop - and thus effectively create, on average, more rows of training data for each shop-item pair."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZfgOx-0_KXg"},"source":["###2.5.1) **Translate and Ruminate**\n","We will start by translating the Russian text in the dataframe, and add our ruminations on possible new features we can generate.\n","\n","The dataframe *items_transl* (equivalent to *items* plus a column for English translation) is saved as a .csv file so we do not have to repeat the translation process the next time we open a Google Colab runtime."]},{"cell_type":"code","metadata":{"id":"FhHSfXNxsKxQ","colab_type":"code","outputId":"c40c35c6-fff8-4b88-fabd-15d72d0bf332","executionInfo":{"status":"ok","timestamp":1588974637544,"user_tz":240,"elapsed":53333,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["print(items_transl.info())\n","print(\"\\n\")\n","print(items_transl.tail(10))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 22170 entries, 0 to 22169\n","Data columns (total 4 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   item_name         22170 non-null  object\n"," 1   item_id           22170 non-null  int64 \n"," 2   item_category_id  22170 non-null  int64 \n"," 3   en_item_name      22170 non-null  object\n","dtypes: int64(2), object(2)\n","memory usage: 692.9+ KB\n","None\n","\n","\n","                                                   item_name  item_id  item_category_id                                           en_item_name\n","22160                             ЯРМАРКА ТЩЕСЛАВИЯ (Регион)    22160                40                                   Vanity Fair (Region)\n","22161                       ЯРОСЛАВ. ТЫСЯЧУ ЛЕТ НАЗАД э (BD)    22161                37                YAROSLAV. Thousands of years ago e (BD)\n","22162                                                 ЯРОСТЬ    22162                40                                                   FURY\n","22163                                       ЯРОСТЬ ( регион)    22163                40                                          FURY (region)\n","22164                                            ЯРОСТЬ (BD)    22164                37                                              FURY (BD)\n","22165                 Ядерный титбит 2 [PC, Цифровая версия]    22165                31                 Nuclear titbit 2 [PC, Digital Version]\n","22166        Язык запросов 1С:Предприятия  [Цифровая версия]    22166                54     Language 1C queries: Enterprises [Digital Version]\n","22167  Язык запросов 1С:Предприятия 8 (+CD). Хрусталева Е.Ю.    22167                49  1C query language: Enterprise 8 (+ CD). Khrustalev EY\n","22168                                    Яйцо для Little Inu    22168                62                                     Egg for Little Inu\n","22169                          Яйцо дракона (Игра престолов)    22169                69                           Dragon egg (Game of Thrones)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9oSMeRVd7dvZ"},"source":["###2.5.2) **NLP for feature generation from items data**\n","Automate the search for commonality among items, and create new categorical feature to prevent overfitting from close similarity between many item names"]},{"cell_type":"markdown","metadata":{"id":"EIn77EZzoxkY","colab_type":"text"},"source":["####Investigate possibility of using NLP to reduce or regularize the items dataset\n","\n","---\n","\n","---\n"]},{"cell_type":"code","metadata":{"id":"_9w-KKpfvvos","colab_type":"code","outputId":"436efbf2-7f80-43c8-d64a-3a3905eb7cbc","executionInfo":{"status":"ok","timestamp":1588974640472,"user_tz":240,"elapsed":56255,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# much of the following is outdated... need to clean up\n","import nltk\n","# nltk.download('stopwords')\n","# from nltk.corpus import stopwords\n","# STOPWORDS = set(stopwords.words('english'))\n","\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer \n","lemmatizer = WordNetLemmatizer() \n","# # use wordnet??\n","# from nltk.corpus import wordnet as wn\n","# # example uses of wordnet\n","# w = \"volume iiii\"\n","# print(f\"Lemmatization of '{w}'':\", lemmatizer.lemmatize(w)) \n","w = \"gifts\"\n","print(f\"Lemmatization of '{w}'':\", lemmatizer.lemmatize(w)) \n","# wn.synsets('rus')\n","\n","# Here is the approach I plan to take to look at item name similarity:\n","#   1) vectorize item names, with vector elements chosen as follows:\n","#         a) uncommon words or part numbers found inside delimiters like () or [] or / / etc.\n","#               1.) large \"n\" n-grams\n","#               2.) part numbers and uncommon words\n","#         b) uncommon words in entire item name (not only delimited words)\n","#         c) words used in \"item_categories\" names, supplemented\n","#         d) special descriptors like edition number, english/russian, etc.  (keep all numbers as \"words\")\n","#   2) compute cosine similarity or other method giving special weight to the above tiers\n","#   3) manually investigate item names with very high similarity, and combine if actually the same name\n","#   4) set a certain similarity limit, and item groups above that limit will form new item categories (target: 2000 categories)\n","#   5) check any items not in one of these new categories, and see if they are tested... if so, assign to closest of the new categories\n","# Run Knn on corr matrix to find useful item groupings automatically?\n","#\n","# Idea is to then use the 2000 category list instead of item name as a key feature in fitting the model, both to help\n","#   regularize, and to help generalize to the items in test set that are not in train set\n","#\n","# What words are \"uncommon\"?\n","#   Use the top 50,000 words in 2018 database of movie/tv subtitles from https://github.com/hermitdave/FrequencyWords\n","#   The en_50k dataframe has the 50,000 most-commonly found words in this database, along with a number of \"counts\" \n","#   or appearances in that text corpus.  The word count gives us an idea of word popularity (higher count = higher popularity of use)\n","#   We can then do an inverse-frequency type of word characterization on our item names\n","# Why not use a pre-existing word vectorizer package to create our item_name vectors?...\n","#   because this 21,700 item database is somewhat unique in that it is heavily weighted \n","#   towards Russian entertainment sales.  We don't want word vectors that ignore things \n","#   like xbox versus playstation.  We want to \"tweak\" the vectors to help us form relevant \n","#   item groups, and not just use any word in the item name to dominate in group identification\n","#   (We know, for example, that the word \"rus\" is likely to mean \"Russian\" in our database,\n","#   whereas a standard vectorizer would either characterize it as \"rus\" or as \"ruthenium\" perhaps)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","Lemmatization of 'gifts'': gift\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CwfXcHsMJ0Yg","colab_type":"text"},"source":["####**Delimited Groups of Words**\n","\n","Investigating \"special\" delimited word groups (like this) or [here] or /hobbitville/ that are present in item names, and may be particularly important in creating n>1 n-grams for uniquely identifying items so that we can tell if two items are the same or nearly the same"]},{"cell_type":"markdown","metadata":{"id":"V7QY11R1QmlN","colab_type":"text"},"source":["#####Some Details on The Approach..."]},{"cell_type":"code","metadata":{"id":"jac_TColdsMf","colab_type":"code","colab":{}},"source":["# explanation of regex string I'm using to parse the item_name\n","'''\n","\n","^\\s+|\\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*|\\s+$\n","\n","gm\n","1st Alternative ^\\s+\n","^ asserts position at start of a line\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","2nd Alternative \\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","Match a single character present in the list below [,\\\"\\/\\(\\)\\[\\]]+\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n",", matches the character , literally (case sensitive)\n","\\\" matches the character \" literally (case sensitive)\n","\\/ matches the character / literally (case sensitive)\n","\\( matches the character ( literally (case sensitive)\n","\\) matches the character ) literally (case sensitive)\n","\\[ matches the character [ literally (case sensitive)\n","\\] matches the character ] literally (case sensitive)\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","3rd Alternative \\s+$\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","$ asserts position at the end of a line\n","\n","Global pattern flags\n","g modifier: global. All matches (don't return after first match)\n","m modifier: multi line. Causes ^ and $ to match the begin/end of each line (not only begin/end of string)\n","'''\n","commented_cell = True  # prevent Jupyter from printing triple-quoted comments"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rsc0yYJkiRBY","colab_type":"code","colab":{}},"source":["# This cell contains no code to run; it is simply a record of some inspections that were done on the items database\n","\n","# before removing undesirable characters / punctuation from the item name,\n","#   let's see if we can find n-grams or useful describers or common abbreviations by looking between the nasty characters\n","# first, let's see what characters are present in the en_item_name column\n","'''\n","nasty_symbols = re.compile('[^0-9a-zA-Z ]')\n","nasties = set()\n","for i in range(len(items_transl)):\n","  n = nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  nasties = nasties.union(set(n))\n","print(nasties)\n","{'[', '\\u200b', 'ñ', '(', ')', '.', 'à', '`', 'ó', '®', 'Á', \n","'\\\\', 'è', '&', '-', ':', 'ë', '_', 'û', '»', '=', '+', ']', ',', \n","'«', 'ú', \"'\", 'ö', '#', 'ä', ';', 'ü', '\"', 'ô', '/', '№', 'é', \n","'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","'''\n","# From the above set of nasty characters, it looks like slashes, single quotes, double quotes, parentheses, and square brackets might enclose relevant n-grams\n","# Let's pull everything from en_item_name that is inside ' ', \" \", (), or [] and see how many unique values we get, and if they are n-grams or abbreviations, for example\n","# It also seems that many of the item names end in a single character \"D\" for example, which should be converted to DVD\n","\n","# ignore the :&+' stuff for now...\n","# Let's set up columns for ()[]-grams, for last string in the name, and for first string in name, and for text that precedes \":\", and for text that surrounds \"&\" or \"+\"\n","#   but first, we will strip out every nasty character except ()[]:&+'\"/ and replace the nasties with spaces, then eliminating double spaces\n","\n","'''\n","# sanity check:\n","really_nasty_symbols = re.compile('[^0-9a-zA-Z \\(\\)\\[\\]:&+\\'\"/]')\n","really_nasties = set()\n","for i in range(len(items_transl)):\n","  rn = really_nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  really_nasties = really_nasties.union(set(rn))\n","print(really_nasties)\n","{'\\u200b', 'ñ', '.', 'à', '`', 'ó', '®', 'Á', '\\\\', 'è', '-', 'ë', '_', 'û', '»', '=', ',', '«', 'ú', 'ö', '#', 'ä', ';', 'ü', 'ô', '№', 'é', 'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","OK, looks good\n","'''\n","commented_cell = True  # prevent Jupyter from printing triple-quoted comments"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvzvPqmCQ2ZM","colab_type":"text"},"source":["#####Add 'delimited' and 'cleaned' data columns; shorten the titles of other columns so dataframe fits better on the screen"]},{"cell_type":"code","metadata":{"id":"Z35pqOYCtyZ7","colab_type":"code","outputId":"1fbcff65-ec1f-4910-fec3-566a22bf109e","executionInfo":{"status":"ok","timestamp":1588974643277,"user_tz":240,"elapsed":59040,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":479}},"source":["%%time\n","items_delimited = items_transl.copy(deep=True)\n","# delete the wide \"item_name\" column so we can read more of the data table width-wise\n","items_delimited = items_delimited.drop(\"item_name\", axis=1).rename(columns = {'en_item_name':'item_name','item_category_id':'i_cat_id'})\n","#print(items_delimited.head())\n","items_in_test_set = test.item_id.unique()\n","items_delimited[\"i_tested\"] = False\n","for i in items_in_test_set:\n","  items_delimited.at[i,\"i_tested\"] = True\n","\n","# stopwords to remove from item names\n","stop_words = \"a,the,an,only,more,are,any,on,your,just,it,its,has,with,for,by,from\".split(\",\")\n","\n","# nasty_symbols_re = re.compile('[^0-9a-zA-Z \\+\\:\\&]')  # remove all punctuation and crazy characters, except that we keep \"+\", \":\" and \"&\"\n","# really_nasty_symbols_re = re.compile('[^0-9a-zA-Z \\(\\)\\[\\]\\:\\&\\+\\'\"/]')\n","# conjunctions_re = re.compile('\\s*[\\+\\&]\\s*')\n","nasty_symbols_re = re.compile('[^0-9a-zA-Z ]')  # remove all punctuation\n","really_nasty_symbols_re = re.compile('[^0-9a-zA-Z ,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@]')  # remove nasties, but leave behind the delimiters\n","delimiters_re = re.compile('[,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@]')\n","delim_pattern_re = re.compile('^\\s+|\\s*[,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@]+\\s*|\\s+$') # special symbols indicating a delimiter --> a space at start or end of item name will be removed at split time, along with ,;/()[]:\"-@\n","d_to_dvd_re = re.compile('\\s+d$')  #several item names end in \"d\" -- which actually seems to indicate dvd (because the items I see are in category 40: Movies-DVD)... standardize so d --> dvd\n","digitalin_to_digitalversion_re = re.compile('digital in$') # several items seem to end in \"digital in\"... maybe in = internet?, but looking at nearby items/categories, 'digital version' looks standard\n","multiple_whitespace_re = re.compile('[ ]{2,}')\n","\n","def text_total_clean(text):\n","    #text: the original en_item_name\n","    #return: en_item_name made lowercase, stripped of \"really_nasties\" and multiple spaces\n","    text = text.lower()\n","    text = d_to_dvd_re.sub(\" dvd\", text)\n","    text = digitalin_to_digitalversion_re.sub(\"digital version\",text)\n","    text = text.replace(\"&\",\" and \")\n","    text = text.replace(\"+\",\" and \")\n","    text = delimiters_re.sub(\" \", text)  # replace all delimiters with a space; other nasties get simply deleted\n","    text = nasty_symbols_re.sub(\"\", text)  # delete anything other than letters, numbers, and spaces\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = text.strip() # remove whitespace around string\n","    # lemmatize each word\n","    text = \" \".join([lemmatizer.lemmatize(w) for w in text.split(\" \") if w not in stop_words])\n","    return text\n","\n","def text_clean_delimited(text):\n","    #text: the original en_item_name\n","    #return: en_item_name made lowercase, stripped of \"really_nasties\" and multiple spaces, in a list of strings that had been separated by one of the above \"delimiters\"\n","    text = text.lower()\n","    text = d_to_dvd_re.sub(\" dvd\", text)\n","    text = digitalin_to_digitalversion_re.sub(\"digital version\",text)\n","    text = text.replace(\"&\",\" and \")\n","    text = text.replace(\"+\",\" and \")\n","    text = really_nasty_symbols_re.sub(\"\", text)  # just delete the nasty symbols\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = delim_pattern_re.split(text)           # split item_name at all delimiters, irrespective of number of spaces before or after the string or delimiter\n","    text = [x.strip() for x in text if x != \"\"]           # remove empty strings \"\" from the list of split items in text, and remove whitespace outside text n-gram\n","    # lemmatize each word\n","    lemtext = []\n","    for ngram in text:\n","        lemtext.append(\" \".join([lemmatizer.lemmatize(w) for w in ngram.split(\" \") if w not in stop_words]))\n","    return lemtext\n","\n","# add item_category name with delimiter to the item_name, as this will be useful info for grouping similar items\n","items_delimited['item_name'] = items_delimited.apply(lambda x: item_categories_augmented.at[x.i_cat_id,'en_cat_name'] + \" : \" + x.item_name, axis=1)\n","\n","# add a column of simply cleaned text without any undesired punctuation or delimiters\n","items_delimited['clean_item_name'] = items_delimited['item_name'].apply(text_total_clean)\n","\n","# now add a column of lists of delimited (cleaned) text\n","items_delimited['delim_name_list'] = items_delimited['item_name'].apply(text_clean_delimited)\n","\n","# have a look at what we got with our delimited text globs\n","def maxgram(gramlist):\n","    maxg = 0\n","    for g in gramlist:\n","        maxg = max(maxg,len(g.split()))\n","    return maxg\n","items_delimited['d_len'] = items_delimited.delim_name_list.apply(lambda x: len(x))\n","items_delimited['d_maxgram'] = items_delimited.delim_name_list.apply(maxgram)\n","print(items_delimited.head())\n","print(\"\\n\")\n","print(items_delimited.describe())\n","\n","#items_delimited.to_csv(\"data_output/items_delimited.csv\", index=False)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["   item_id  i_cat_id                                                                                                  item_name  i_tested                                                                                   clean_item_name  \\\n","0        0        40                                                                 Movie - DVD : ! POWER IN glamor (PLAST.) D     False                                                               movie dvd power in glamor plast dvd   \n","1        1        76  Program - Home & Office (Digital) : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]     False  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2        2        40                                                                     Movie - DVD : *** In the glory (UNV) D     False                                                                        movie dvd in glory unv dvd   \n","3        3        40                                                                       Movie - DVD : *** BLUE WAVE (Univ) D     False                                                                      movie dvd blue wave univ dvd   \n","4        4        40                                                                            Movie - DVD : *** BOX (GLASS) D     False                                                                           movie dvd box glass dvd   \n","\n","                                                                                           delim_name_list  d_len  d_maxgram  \n","0                                                                [movie, dvd, power in glamor, plast, dvd]      5          3  \n","1  [program, home and office, digital, abbyy finereader 12 professional edition full, pc, digital version]      6          6  \n","2                                                                         [movie, dvd, in glory, unv, dvd]      5          2  \n","3                                                                       [movie, dvd, blue wave, univ, dvd]      5          2  \n","4                                                                            [movie, dvd, box, glass, dvd]      5          1  \n","\n","\n","         item_id  i_cat_id  d_len  d_maxgram\n","count      22170     22170  22170      22170\n","mean  11,084.500    46.291  4.599      4.205\n","std    6,400.072    15.941  1.462      2.168\n","min            0         0      2          1\n","25%    5,542.250        37      3          3\n","50%   11,084.500        40      4          4\n","75%   16,626.750        58      5          5\n","max        22169        83     15         17\n","CPU times: user 3.07 s, sys: 16.1 ms, total: 3.08 s\n","Wall time: 3.09 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IUA58xQZ72NC","colab_type":"code","colab":{}},"source":["# make item df easier to read for the following stuff\n","items_clean_delimited = items_delimited.copy(deep=True).drop(\"item_name\", axis=1).rename(columns = {'clean_item_name':'item_name'})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"amkSqIxGJy65","colab_type":"text"},"source":["#####Look at the characteristics of different length n-grams in our delimited set"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"ae8de158-7c75-4cb8-e260-d3210a07db1e","executionInfo":{"status":"ok","timestamp":1588929127082,"user_tz":240,"elapsed":54250,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"a4bwEr91S2Wz","colab":{"base_uri":"https://localhost:8080/","height":632}},"source":["# # Inspect the delimited 1-grams\n","\n","# items_delimited_1gram = items_clean_delimited.copy(deep=True)\n","# items_delimited_1gram[\"d_1grams\"] = items_delimited_1gram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == 1]) # column contains all \"delimited\" 1-grams in the translation\n","\n","# g1 = items_delimited_1gram.d_1grams.apply(pd.Series,1).stack()\n","# g1.index = g1.index.droplevel(-1)\n","# g1.name = 'd_1grams'\n","# del items_delimited_1gram['d_1grams']\n","# items_delimited_1gram = items_delimited_1gram.join(g1)\n","\n","# print(items_delimited_1gram.head())\n","# print(\"\\n\")\n","# freq_1grams = items_delimited_1gram.d_1grams.value_counts()\n","# print(f'Number of unique delimited 1-grams: {len(freq_1grams)}')\n","# print(f'Number of unique delimited 1-grams that are duplicated at least once: {len(freq_1grams[freq_1grams > 1])}')\n","# print(freq_1grams[:10])\n","# singles = [x for x in freq_1grams.index if len(x) == 1]\n","# print(f'\\nNumber of unique delimited unit-length 1-grams: {len(singles)}')\n","# print(f'Number of unique delimited unit-length 1-grams that are duplicated at least once: {len(freq_1grams.loc[singles][freq_1grams > 1])}')\n","# print(freq_1grams.loc[singles][:10])"],"execution_count":17,"outputs":[{"output_type":"stream","text":["   item_id  i_cat_id  i_tested                                                                                         item_name                                                                                          delim_name_list  d_len  d_maxgram d_1grams\n","0        0        40     False                                                               movie dvd power in glamor plast dvd                                                                [movie, dvd, power in glamor, plast, dvd]      5          3    movie\n","0        0        40     False                                                               movie dvd power in glamor plast dvd                                                                [movie, dvd, power in glamor, plast, dvd]      5          3      dvd\n","0        0        40     False                                                               movie dvd power in glamor plast dvd                                                                [movie, dvd, power in glamor, plast, dvd]      5          3    plast\n","0        0        40     False                                                               movie dvd power in glamor plast dvd                                                                [movie, dvd, power in glamor, plast, dvd]      5          3      dvd\n","1        1        76     False  program home and office digital abbyy finereader 12 professional edition full pc digital version  [program, home and office, digital, abbyy finereader 12 professional edition full, pc, digital version]      6          6  program\n","\n","\n","Number of unique delimited 1-grams: 2908\n","Number of unique delimited 1-grams that are duplicated at least once: 1258\n","movie     7138\n","dvd       5319\n","music     4333\n","gift      3530\n","pc        2746\n","blu       2097\n","game      1954\n","region    1835\n","ray       1824\n","bd        1706\n","Name: d_1grams, dtype: int64\n","\n","Number of unique delimited unit-length 1-grams: 31\n","Number of unique delimited unit-length 1-grams that are duplicated at least once: 25\n","v    283\n","f    163\n","1    119\n","2     64\n","4     49\n","t     45\n","3     44\n","x     38\n","9     35\n","8     24\n","Name: d_1grams, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aaezqsyDUHca","outputId":"7e7499f6-8a13-4e33-83ca-7b33c5501701","executionInfo":{"status":"ok","timestamp":1588929131815,"user_tz":240,"elapsed":58977,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":360}},"source":["# # Inspect the delimited 2-grams\n","\n","# items_delimited_2gram = items_clean_delimited.copy(deep=True)\n","# items_delimited_2gram[\"d_2grams\"] = items_delimited_2gram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == 2]) # column contains all \"delimited\" 2-grams in the translation\n","\n","# g2 = items_delimited_2gram.d_2grams.apply(pd.Series,1).stack()\n","# g2.index = g2.index.droplevel(-1)\n","# g2.name = 'd_2grams'\n","# del items_delimited_2gram['d_2grams']\n","# items_delimited_2gram = items_delimited_2gram.join(g2)\n","\n","# print(items_delimited_2gram.tail())\n","# print(\"\\n\")\n","# freq_2grams = items_delimited_2gram.d_2grams.value_counts()\n","# print(f'Number of unique delimited 2-grams: {len(freq_2grams)}')\n","# print(f'Number of unique delimited 2-grams that are duplicated at least once: {len(freq_2grams[freq_2grams > 1])}')\n","# print(freq_2grams[:8])"],"execution_count":18,"outputs":[{"output_type":"stream","text":["       item_id  i_cat_id  i_tested                                                                        item_name                                                                         delim_name_list  d_len  d_maxgram       d_2grams\n","22167    22167        49      True  book methodical material 1c 1c query language enterprise 8 and cd khrustalev ey  [book, methodical material 1c, 1c query language, enterprise 8, and cd, khrustalev ey]      6          3   enterprise 8\n","22167    22167        49      True  book methodical material 1c 1c query language enterprise 8 and cd khrustalev ey  [book, methodical material 1c, 1c query language, enterprise 8, and cd, khrustalev ey]      6          3         and cd\n","22167    22167        49      True  book methodical material 1c 1c query language enterprise 8 and cd khrustalev ey  [book, methodical material 1c, 1c query language, enterprise 8, and cd, khrustalev ey]      6          3  khrustalev ey\n","22168    22168        62     False                                           gift gadget robot sport egg little inu                                            [gift, gadget, robot, sport, egg little inu]      5          3            NaN\n","22169    22169        69     False                                          gift souvenir dragon egg game of throne                                            [gift, souvenir, dragon egg, game of throne]      4          3     dragon egg\n","\n","\n","Number of unique delimited 2-grams: 4311\n","Number of unique delimited 2-grams that are duplicated at least once: 1221\n","game pc             2167\n","digital version     1960\n","russian version      977\n","xbox 360             888\n","standard edition     760\n","russian subtitle     401\n","soft toy             382\n","english version      337\n","Name: d_2grams, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"7585f71e-aa28-4983-d01a-3dd943ad068f","executionInfo":{"status":"ok","timestamp":1588929136373,"user_tz":240,"elapsed":63530,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"USoJANp3U9Ei","colab":{"base_uri":"https://localhost:8080/","height":425}},"source":["# %%time\n","# # Inspect the delimited 4-grams (4.64sec to run this cell without GPU, 4.01sec with GPU)\n","\n","# items_delimited_4gram = items_clean_delimited.copy(deep=True)\n","# items_delimited_4gram[\"d_4grams\"] = items_delimited_4gram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == 4]) # column contains all \"delimited\" 4-grams in the translation\n","\n","# g4 = items_delimited_4gram.d_4grams.apply(pd.Series,1).stack()\n","# g4.index = g4.index.droplevel(-1)\n","# g4.name = 'd_4grams'\n","# del items_delimited_4gram['d_4grams']\n","# items_delimited_4gram = items_delimited_4gram.join(g4)\n","\n","# print(items_delimited_4gram.tail())\n","# print(\"\\n\")\n","# freq_4grams = items_delimited_4gram.d_4grams.value_counts()\n","# print(f'Number of unique delimited 4-grams: {len(freq_4grams)}')\n","# print(f'Number of unique delimited 4-grams that are duplicated at least once: {len(freq_4grams[freq_4grams > 1])}')\n","# print(freq_4grams[1:12])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["       item_id  i_cat_id  i_tested                                                                        item_name                                                                         delim_name_list  d_len  d_maxgram d_4grams\n","22165    22165        31     False                              game pc digital nuclear titbit 2 pc digital version                               [game pc, digital, nuclear titbit 2, pc, digital version]      5          3      NaN\n","22166    22166        54      True                        book digital language 1c query enterprise digital version                         [book, digital, language 1c query, enterprise, digital version]      5          3      NaN\n","22167    22167        49      True  book methodical material 1c 1c query language enterprise 8 and cd khrustalev ey  [book, methodical material 1c, 1c query language, enterprise 8, and cd, khrustalev ey]      6          3      NaN\n","22168    22168        62     False                                           gift gadget robot sport egg little inu                                            [gift, gadget, robot, sport, egg little inu]      5          3      NaN\n","22169    22169        69     False                                          gift souvenir dragon egg game of throne                                            [gift, souvenir, dragon egg, game of throne]      4          3      NaN\n","\n","\n","Number of unique delimited 4-grams: 3540\n","Number of unique delimited 4-grams that are duplicated at least once: 366\n","xbox 360 russian version             151\n","3d bd and bd                          92\n","kaspersky internet security multi     30\n","xbox 360 record russian               18\n","device russian edition 2              14\n","tom clancys splinter cell             11\n","device russian edition 3              11\n","grand theft auto v                    10\n","metal gear solid v                    10\n","dvd and 3d bd                          9\n","and cartoon a gift                     9\n","Name: d_4grams, dtype: int64\n","CPU times: user 4.6 s, sys: 63.1 ms, total: 4.67 s\n","Wall time: 4.65 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Uvreedy4YRVd"},"source":["#####Gather all info for duplicated n-grams in our delimited set"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"127708c8-cb36-4388-8050-ad6dc4ac78cf","executionInfo":{"status":"ok","timestamp":1588929216082,"user_tz":240,"elapsed":143233,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"7H2VxLddVqlJ","colab":{"base_uri":"https://localhost:8080/","height":952}},"source":["# %%time\n","# # Get all of the delimited n-grams that are duplicated at least once in item names (1min 24sec no gpu vs. 1min 11sec with gpu)\n","# #  range of sizes of delimited phrases (number of 'words'):\n","# min_gram = items_delimited.d_maxgram.min()\n","# max_gram = items_delimited.d_maxgram.max()\n","\n","# total_dupe_grams = 0\n","# gram_freqs = {}   # dict will hold elements that are pd.Series with index = phrase, value = number of repeats in items database item names\n","# for n in range(min_gram,max_gram+1):\n","#     item_ngram = items_clean_delimited.copy(deep=True)\n","#     item_ngram['delim_ngrams'] = item_ngram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == n])\n","\n","#     grams = item_ngram.delim_ngrams.apply(pd.Series,1).stack()\n","#     grams.index = grams.index.droplevel(-1)\n","#     grams.name = 'delim_ngrams'\n","#     del item_ngram['delim_ngrams']\n","#     item_ngram = item_ngram.join(grams)\n","\n","#     freq_grams = item_ngram.delim_ngrams.value_counts()\n","#     print(f'Number of unique delimited {n}-grams: {len(freq_grams)}')\n","#     grams_dupe = len(freq_grams[freq_grams > 1])\n","#     print(f'Number of unique delimited {n}-grams that are duplicated at least once: {grams_dupe}\\n')\n","#     if grams_dupe > 0:\n","#         gram_freqs[n] = freq_grams[freq_grams > 1].copy(deep=True)\n","#         total_dupe_grams += grams_dupe\n","# print(f'\\nTotal number of unique, delimited, duplicated n-grams for all n: {total_dupe_grams}')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Number of unique delimited 1-grams: 2908\n","Number of unique delimited 1-grams that are duplicated at least once: 1258\n","\n","Number of unique delimited 2-grams: 4311\n","Number of unique delimited 2-grams that are duplicated at least once: 1221\n","\n","Number of unique delimited 3-grams: 3909\n","Number of unique delimited 3-grams that are duplicated at least once: 725\n","\n","Number of unique delimited 4-grams: 3540\n","Number of unique delimited 4-grams that are duplicated at least once: 366\n","\n","Number of unique delimited 5-grams: 2809\n","Number of unique delimited 5-grams that are duplicated at least once: 222\n","\n","Number of unique delimited 6-grams: 1922\n","Number of unique delimited 6-grams that are duplicated at least once: 132\n","\n","Number of unique delimited 7-grams: 1238\n","Number of unique delimited 7-grams that are duplicated at least once: 66\n","\n","Number of unique delimited 8-grams: 760\n","Number of unique delimited 8-grams that are duplicated at least once: 23\n","\n","Number of unique delimited 9-grams: 482\n","Number of unique delimited 9-grams that are duplicated at least once: 13\n","\n","Number of unique delimited 10-grams: 246\n","Number of unique delimited 10-grams that are duplicated at least once: 10\n","\n","Number of unique delimited 11-grams: 132\n","Number of unique delimited 11-grams that are duplicated at least once: 6\n","\n","Number of unique delimited 12-grams: 63\n","Number of unique delimited 12-grams that are duplicated at least once: 1\n","\n","Number of unique delimited 13-grams: 24\n","Number of unique delimited 13-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 14-grams: 10\n","Number of unique delimited 14-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 15-grams: 9\n","Number of unique delimited 15-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 16-grams: 2\n","Number of unique delimited 16-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 17-grams: 1\n","Number of unique delimited 17-grams that are duplicated at least once: 0\n","\n","\n","Total number of unique, delimited, duplicated n-grams for all n: 4043\n","CPU times: user 1min 17s, sys: 1.2 s, total: 1min 18s\n","Wall time: 1min 17s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yxeofuYfm3CS","colab_type":"text"},"source":["There are roughly 4000 n-grams to consider in forming \"item name\" word vectors.\n","\n","Questions:\n","\n","1. Should we use all 4000, or will this take too much computation time?  21,700 items * 4000 vector positions = 87 million elements to compute and possibly store.\n","2. Do we assign weights to different index positions in item_name_word_vectors to reflect more important (longer or more descriptive) n-grams or rarer words?\n","3. Do we make these vectors and then try to find similarity between items using standard NLP techniques, or use pandas .corr method, or do Knn to group items into useful co-predictive bunches\n","4. Should we add other information beyond item name words, such as a) month of median number of items sold (is it weighted towards 2013 or towards 2015?), b) median price of item over all months, c) number of item units sold, d) variance in sales (perhaps with special consideration to months of November vs. other months), e) sales trend (slope of units sold vs. time, or revenue vs. time)?  And, if we use something like this to group items that are *not* in the training set, do we assign some kind of average values for these items.\n","5. Am I overthinking this, and not getting anything done?\n","\n","OK, so answer to #5 is 'yes,' and let's try encoding just a few items to see how long it will take with 4000-long vector encoding..."]},{"cell_type":"code","metadata":{"id":"hCAAjSqbQmlU","colab_type":"code","colab":{}},"source":["# # encode an item name using all 'delimited' n-grams:\n","# item_vectors = pd.DataFrame(np.zeros((len(items_clean_delimited),total_dupe_grams), dtype = np.int16))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrWd5n4Fnq7R","colab_type":"code","outputId":"fa5492ef-3594-4e40-8aeb-de1c8ff287cf","executionInfo":{"status":"ok","timestamp":1588937886827,"user_tz":240,"elapsed":539049,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# %%time\n","\n","# gram_idx = 0\n","# for n in range(1,len(gram_freqs)+1):\n","#     grams = gram_freqs[n].index\n","#     for g in grams:\n","#         gram_pattern = re.compile(r'\\b' + g + r'\\b')  # I'm not looking for partial words; n-grams must match at word boundaries\n","#         for i in range(len(items_clean_delimited)):\n","#             if gram_pattern.search(items_clean_delimited.at[i,'item_name']):\n","#                 item_vectors.iloc[i,gram_idx] = n  # set equal to length n of n-gram for weighting (perhaps?)\n","#         gram_idx += 1\n","\n"],"execution_count":34,"outputs":[{"output_type":"stream","text":["CPU times: user 8min 58s, sys: 61.9 ms, total: 8min 58s\n","Wall time: 8min 58s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-GFD62B9i1DO","colab_type":"code","colab":{}},"source":["# # save the word vectors as a compressed csv file\n","# #   (170MB uncompressed... won't push to GitHub simply)\n","# #   gzip -->  400 kB !!\n","\n","# compression_opts = dict(method='gzip',\n","#                         archive_name='item_vectors.csv')  \n","# item_vectors.to_csv('data_output/item_vectors.csv.gz', index=False, compression=compression_opts)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7TMDgN8xXlUy","colab_type":"text"},"source":["######Let's see if pandas can handle correlation matrix for this huge dataframe..."]},{"cell_type":"code","metadata":{"id":"RbCCH9Fm80rY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":233},"outputId":"06cd68db-d294-4e30-aa5a-601507e37104","executionInfo":{"status":"ok","timestamp":1588939759649,"user_tz":240,"elapsed":605,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["item_vectors.head()"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>...</th>\n","      <th>4028</th>\n","      <th>4029</th>\n","      <th>4030</th>\n","      <th>4031</th>\n","      <th>4032</th>\n","      <th>4033</th>\n","      <th>4034</th>\n","      <th>4035</th>\n","      <th>4036</th>\n","      <th>4037</th>\n","      <th>4038</th>\n","      <th>4039</th>\n","      <th>4040</th>\n","      <th>4041</th>\n","      <th>4042</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 4043 columns</p>\n","</div>"],"text/plain":["   0     1     2     3     4     5     6     7     8     9     10    11    12    13    14    ...  4028  4029  4030  4031  4032  4033  4034  4035  4036  4037  4038  4039  4040  4041  4042\n","0     1     1     0     0     0     0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n","1     0     0     0     0     1     0     0     0     0     0     1     0     0     0     0  ...     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n","2     1     1     0     0     0     0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n","3     1     1     0     0     0     0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n","4     1     1     0     0     0     0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n","\n","[5 rows x 4043 columns]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"1dYnH7NGN5mg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"c33a0970-4531-4992-d04d-fd1ed80e699c","executionInfo":{"status":"ok","timestamp":1588947721495,"user_tz":240,"elapsed":7353291,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# %%time\n","# iv_T = item_vectors.transpose()\n","# dp = item_vectors.dot(iv_T)\n","\n","# dp.to_csv('data_output/item_dot_prod.csv', index=False)\n","# dp.head()"],"execution_count":44,"outputs":[{"output_type":"stream","text":["CPU times: user 2h 2min 19s, sys: 3.02 s, total: 2h 2min 22s\n","Wall time: 2h 2min 32s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IS-9L4Y0LPS8","colab_type":"text"},"source":["OK, in hindsight, maybe not that great an idea... 946MB csv file.\n","\n","Maybe break it into groups by item_category_3, and run each group separately\n","And, see if I can stick with np.int16 dot products (make sure they aren't \"object\" or float64, for example)"]},{"cell_type":"code","metadata":{"id":"WsC2rGdkMH6c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"b7aa1b59-eeb8-44ad-f039-c63d2a111af2","executionInfo":{"status":"ok","timestamp":1588974294669,"user_tz":240,"elapsed":39281,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# %%time\n","# # run a test on 1000 items, then work up from there --> with GPU, 40 sec for 1000 items x 4043 n-grams\n","# df = item_vectors.iloc[:1000][:].transpose()\n","# def dotprod(a, b):\n","#     d = a.dot(b.transpose())\n","#     v = np.int16(d)\n","#     return v\n","    \n","# c = df.corr(method=dotprod)\n","# print(c.head())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["   0    1    2    3    4    5    6    7    8    9    10   11   12   13   14   ...  985  986  987  988  989  990  991  992  993  994  995  996  997  998  999\n","0    1    0    3    2    2    2    2    2    2    2    2    2    0    2    2  ...    2    2    2    1    0    0    0    0    0    0    0    0    0    0    0\n","1    0    1    0    0    0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","2    3    0    1    2    2    2    2    2    2    2    2    2    0    2    2  ...    2    2    2    1    0    0    0    0    0    0    0    0    0    0    0\n","3    2    0    2    1    2    2    2    2    2    2    2    2    0    2    2  ...    2    2    2    1    0    0    0    0    0    0    0    0    0    0    0\n","4    2    0    2    2    1    2    2    2    2    2    2    2    0    2    2  ...    2    2    2    1    0    0    0    0    0    0    0    0    0    0    0\n","\n","[5 rows x 1000 columns]\n","CPU times: user 38.9 s, sys: 0 ns, total: 38.9 s\n","Wall time: 38.9 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_B2ZZPQigJmh"},"source":["#####Use scipy instead of pandas..."]},{"cell_type":"code","metadata":{"id":"oWtui0wwF1Yp","colab_type":"code","colab":{}},"source":["from scipy import sparse\n","\n","item_vec_matrix = sparse.csr_matrix(item_vectors.values)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQEF0onOUtxz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"83740d33-addb-4049-dd10-575716467d6d","executionInfo":{"status":"ok","timestamp":1588976859779,"user_tz":240,"elapsed":2209,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["%%time\n","# <2sec for 21,700 items x 4034 ngrams; output is a csr matrix of type int64\n","dots = item_vec_matrix.dot(item_vec_matrix.transpose())    #.todense()\n","#print('pairwise dot product output:\\n {}\\n'.format(dots))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["CPU times: user 1.37 s, sys: 1.7 ms, total: 1.37 s\n","Wall time: 1.37 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d3CtdtxXbUcc","colab_type":"code","colab":{}},"source":["# !pip install numba"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFPX5PouV_aw","colab_type":"code","colab":{}},"source":["# wicked fast way to get top K # of items by dot product value (i.e., closest K items to the item of interest)\n","# https://stackoverflow.com/questions/31790819/scipy-sparse-csr-matrix-how-to-get-top-ten-values-and-indices\n","# also, great reference for speeding up python here: https://colab.research.google.com/drive/1nMDtWcVZCT9q1VWen5rXL8ZHVlxn2KnL\n","\n","from numba import jit, prange\n","@jit(cache=True)\n","def row_topk_csr(data, indices, indptr, K):\n","    m = indptr.shape[0] - 1\n","    max_indices = np.zeros((m, K), dtype=indices.dtype)\n","    max_values = np.zeros((m, K), dtype=data.dtype)\n","\n","    for i in prange(m):\n","        top_inds = np.argsort(data[indptr[i] : indptr[i + 1]])[::-1][:K]\n","        max_indices[i] = indices[indptr[i] : indptr[i + 1]][top_inds]\n","        max_values[i] = data[indptr[i] : indptr[i + 1]][top_inds]\n","\n","    return max_indices, max_values\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3D9OVzjXDlGm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"3aecc263-a9af-4325-9433-e042b6e9d91b","executionInfo":{"status":"ok","timestamp":1588977898347,"user_tz":240,"elapsed":6468,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["%%time\n","dots.setdiag(0)\n","closest3_indices, highest_values = row_topk_csr(dots.data, dots.indices, dots.indptr, K=3)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["CPU times: user 5.58 s, sys: 13.6 ms, total: 5.59 s\n","Wall time: 5.59 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-xjDRQcM9odF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"e8c54b42-e2f1-40e4-e45d-3bd33dc05906","executionInfo":{"status":"ok","timestamp":1588980225325,"user_tz":240,"elapsed":867,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["print(closest3_indices.shape)\n","print(closest3_indices[:10,:])"],"execution_count":34,"outputs":[{"output_type":"stream","text":["(22170, 3)\n","[[ 1072 16691  8635]\n"," [ 1155  1156  1154]\n"," [10113  8136 14885]\n"," [10463 19630 20027]\n"," [ 5220  7848  7845]\n"," [10344 10468  8964]\n"," [    7 10344 10468]\n"," [    6  9931  8404]\n"," [15597 15596 15595]\n"," [10344 10468     5]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OfOzk_xWnF7q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"outputId":"84005b75-ee4a-4677-f1fe-eaa1d729c871","executionInfo":{"status":"ok","timestamp":1588982454768,"user_tz":240,"elapsed":1293,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["similar_items = pd.DataFrame({'item_id':range(22170)}) #,'close_item_idx':closest3_indices,'close_item_dot':highest_values})\n","similar_items['close_item_idx'] = [closest3_indices[x] for x in range(22170)]\n","similar_items['close_item_dot'] = [highest_values[x] for x in range(22170)]\n","similar_items = similar_items.merge(items[['item_id','item_category_id']], on='item_id')\n","similar_items['close_item_cat'] = similar_items.close_item_idx.apply(lambda x: [items.at[i,'item_category_id'] for i in x])\n","print(similar_items.head())\n","\n","ids = [0,1072,16691,8635,1,1155,1156,1154,4,5220,7848,7845]\n","a =0 \n","for r in range(3):\n","    print(ids[a],items_clean_delimited.at[ids[a],'item_name'])\n","    a+=1\n","    print(ids[a],items_clean_delimited.at[ids[a],'item_name'])\n","    a+=1\n","    print(ids[a],items_clean_delimited.at[ids[a],'item_name'])\n","    a+=1\n","    print(ids[a],items_clean_delimited.at[ids[a],'item_name'])\n","    a+=1\n","    print('\\n')\n"],"execution_count":54,"outputs":[{"output_type":"stream","text":["   item_id         close_item_idx close_item_dot  item_category_id close_item_cat\n","0        0    [1072, 16691, 8635]      [3, 3, 3]                40   [40, 40, 40]\n","1        1     [1155, 1156, 1154]   [54, 24, 24]                76   [75, 76, 76]\n","2        2   [10113, 8136, 14885]      [3, 3, 3]                40   [40, 40, 40]\n","3        3  [10463, 19630, 20027]      [3, 3, 3]                40   [40, 40, 40]\n","4        4     [5220, 7848, 7845]      [6, 6, 6]                40   [28, 28, 28]\n","0 movie dvd power in glamor plast dvd\n","1072 movie dvd 4 day in may region\n","16691 movie dvd holiday in september rem\n","8635 movie dvd barbie ballerina in pointe shoe pink\n","\n","\n","1 program home and office digital abbyy finereader 12 professional edition full pc digital version\n","1155 program home and office abbyy finereader 12 professional edition full box\n","1156 program home and office digital abbyy finereader 12 professional edition 1 year version download pc digital version\n","1154 program home and office digital abbyy finereader 12 professional edition 1 year pc digital version\n","\n","\n","4 movie dvd box glass dvd\n","5220 game pc additional publication need speed carbon classic pycv pc dvd dvd box\n","7848 game pc additional publication world of warcraft online rusv 30 day pc dvd dvd box\n","7845 game pc additional publication world in conflict soviet assault dvd box\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L6ErXnphuc2t","colab_type":"text"},"source":["##**summary so far (May 8, 2020):**\n","\n","something interesting... why would 1155 have higher dot product with item 1 than 1156 or 1154... maybe n-gram delimiters are different for the different items; and \"full\" is only in 1 and 1155, so maybe we're ok\n","\n","item 4 doesn't seem to match terribly well with 5220,7848,7845... \n","\n","**should perhaps take the top 10 or 20 close item matches, and if all have same dot product, pick close items to be only those from same category**\n","\n","if dot product is < 3 (?), set close items = 0, and use average over category's items perhaps\n","\n","if dot product variance is large, and/or there is a dot product > 20, keep just the high items, whether n=1 or n=20"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_L1gjS_FZbFJ"},"source":["###Continue on the work to vectorize item names and compute similarities between items (this section should be grouped with the above section eventually, but I have it set apart for now, so it is easy to find)\n","\n","---\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TqtqyzJG8qRn"},"source":["######Consider as special the n-grams that follow certain descriptive keywords such as \"by\" or \"for\" or \"from\" (? TBD)"]},{"cell_type":"code","metadata":{"id":"UJ4sm2JcYAhe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ec5048df-b6a6-4ffe-a417-a14246f1be74","executionInfo":{"status":"ok","timestamp":1588935430666,"user_tz":240,"elapsed":889,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# these are words we could consider to highlight n-grams that follow these words, similar to the thinking that \"delimited\" words may be special\n","key_modifiers = \"by,for,from\"  \n","print(lemmatizer.lemmatize('as'))\n"],"execution_count":26,"outputs":[{"output_type":"stream","text":["a\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mi0smegojy0-","colab_type":"code","colab":{}},"source":["# Inspecting the \"delimited\" n-grams, I've considered the following for use in vectorizing our item names\n","\n","# Regarding the use of NLTK stopwords, meh... a lot of these stopwords are irrelevant regarding excess text in our item names... \n","#    We really should make our own stopword list\n","s = \"a,the,an,only,more,are,any,on,your,just,it,its,it's,has,with,for,by,from\"\n","\n","# make some dictionaries to \"lemmatize\" certain keywords into standardized 1-grams\n","# volumes/editions\n","key_descriptors = {'1': 'first,1st,1ed,v1,vol1,volume1,ver1,version1,edition1,original,standard'}\n","key_descriptors['2'] = 'second,2nd,ii,2ed,v2,vol2,volume2,ver2,version2,edition2,updated,revised'\n","key_descriptors['3'] = 'third,3rd,iii,3ed,v3,vol3,volume3,ver3,version3,edition3'\n","key_descriptors['4'] = 'fourth,4th,iiii,4ed,v4,vol4,volume4,ver4,version4,edition4'\n","# special, highlighted items\n","key_descriptors['special'] = 'collector,collection,premium,platinum,gold,silver,black,boxset,classic,trilogy'\n","# items dealing with business\n","key_descriptors['biz'] = 'company,business,enterprise,firm,professional'\n","# items likely to be a Russian version, or translated into Russian\n","key_descriptors['rus'] = 'russian,rus,rusv,translate,translated'\n","# items likely to be in English\n","key_descriptors['eng'] = 'english,eng,engl'\n","# relatively frequent descriptors that should have substantial weight in the item name vectorization\n","key_descriptors['0'] = 'essential,preorder,supplement,addition,catalog,reprint'\n","\n","key_platforms = '1c,1cpublishing,playstation,psp,ps2,ps3,ps4,psvita,psn,pc,android,iphone,phone,tablet,'\n","key_platforms += 'ipad,xbox,xbox 360,xbox one,xbox360,xboxone,xbox1,xone,apple,mac,microsoft,windows,98,live,marvel,cech4008c'\n","\n","key_formats = {'dvd': 'pcdvd,dvd,2dvd,3dvd,4dvd,5dvd,6dvd,dvddigipack,dvdbox,mp3dvd,dvdbook,dvdcd'}\n","key_formats['cd'] = 'cd,mp3cd,2cd,cdbox,compactdisk,audiocd,cddigipack'\n","key_formats['movie'] = 'bd,2bd,3bd,bluray,blueray,4k,movie,dvdmovie,moviedvd,dvdbd,bddvd,2dvdbd,tv'\n","key_formats['0'] = 'book,audiobook,digibook,wb,wd,mp3,digipack,jewel,umdcase,keychain,ac,6cm,8cm,12cm,14cm,dicomp,glass,delivery,online,gb,tb,mb'\n","\n","special_words = 'komplvoprsertekzam1s,softklab,transformer,hobbit,barbie,middleearth,'  # plus any 4-digit numbers like 1812, 2014,...\n","special_words += 'red30,red20,alien,toy,region,bbc,batman,scee,cyrus,mtg,berserk,disney,spell,'\n","special_words += 'buh8,b01,retribution,metodmaterialy,destiny,wolfenstein,hhkata,luntik,'\n","special_words += 'paragon,42,72,buka,injustice,call,action,universal,zhelboks,fallout,cuh1108a,16,cechzc2e,'\n","special_words += 'pch1108za01,dinosaur,braveheart,titanic,infamous,spiderman,armageddon,universe,'\n","special_words += 'rezhversiya,outcast,wheelbarrow,yellow,inquisition,pch1008za01,zolushka,xmen,'\n","special_words += 'metro,battlefront,cuh1208b,wargame,operation,cech3008b,poirot,hhkatv,kitchen,'\n","special_words += 'icecrown,underworld,shepherd,thief,sensor,shrek,oceans,castlevania,murdered,'\n","special_words += 'madagascar,shelter,devastation,deadpool,incredibles,jungle,titanfall,pspe1008,cech2508b,skyrim,avatar,maleficent,invizimals,heracles,phantom,toysmlk,aladdin'\n","\n","#key_categories = 1-grams from all item_categories_augmented (0,1,2,3,4 columns)\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JC-Tlq1Wp2UV","colab_type":"code","colab":{}},"source":["# Get word counts from en_50k for the delimited terms:\n","#   combine all words in delim_name_list, and search one-by-one through en_50k\n","#   create a column for words with zero counts\n","#   create a column for words with 1 to 1000 counts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QqUhu9EmUu-v","colab_type":"code","outputId":"17632705-5524-4942-ded7-b006ae51a54a","executionInfo":{"status":"ok","timestamp":1588789043195,"user_tz":240,"elapsed":106927,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["en50k_word_list = en_50k.word.to_numpy(dtype='str')\n","swfreq = []\n","for sw in special_words.split(\",\"):\n","    sw_50k_rank = np.where(en50k_word_list == sw)[0]\n","    if len(sw_50k_rank) > 0:\n","        swfreq.append(en_50k.at[sw_50k_rank[0],'word_count'])\n","    else:\n","        swfreq.append(0)\n","\n","special_wds_df = pd.DataFrame({\"word\": special_words.split(\",\"), \"counts_50k\": swfreq}).sort_values('counts_50k')\n","print(special_wds_df.head())\n","print(special_wds_df.describe())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["                    word  counts_50k\n","0   komplvoprsertekzam1s           0\n","24           wolfenstein           0\n","25                hhkata           0\n","26                luntik           0\n","28                    42           0\n","       counts_50k\n","count          91\n","mean   10,899.868\n","std    60,340.351\n","min             0\n","25%             0\n","50%           624\n","75%          3637\n","max        572997\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"268aa003-ceb1-4289-91e8-44fc39c8b358","executionInfo":{"status":"ok","timestamp":1588789203625,"user_tz":240,"elapsed":480,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"aZcys-YW_4dW","colab":{"base_uri":"https://localhost:8080/","height":901}},"source":["# inspect item names more closely to see how they relate to the rare delimited words, and to \n","#    help understand what certain unusual terms mean \n","# \"firms\" = big record label \n","# \"figure\" = digital version; not hardcopy\n","#  komplvoprsertekzam1s = Complete Set of Certification Exam. 1C\n","\n","# enter the desired item_ids here:\n","to_plot = [617,618]\n","\n","item_plot_df = sales_train[sales_train.item_id.isin(to_plot)] \n","print(item_plot_df.head())\n","\n","# what does \"figure\" mean in category name? --> DIGITAL VERSION (as opposed to hardcopy)\n","# whereas, \"Digital Version\" in item name actually seems to indicate it is downloaded rather than obtained from DVD, for example\n","\n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([77,78])][['item_id','en_item_name','item_category_id','category']] \n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([43,44])][['item_id','en_item_name','item_category_id','category']] \n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([75,76])][['item_id','en_item_name','item_category_id','category']] \n","fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([6])][['item_id','en_item_name','item_category_id','category']] \n","print(fig_cat_df.tail())\n","\n","# see what some of these 'rare' word 1-grams are:\n","# get item names from rare words\n","rare_words = 'cech2508b,komplvoprsertekzam1s'\n","#reg_pattern = re.compile('|'.join(rare_words.split(',')), flags=re.IGNORECASE)\n","any_rare_words = '|'.join(rare_words.split(','))\n","rare_items_df = items_delimited[items_delimited.clean_item_name.str.contains(any_rare_words, case=False, na=False)][['item_id','i_cat_id','i_tested','item_name','clean_item_name']] \n","print(rare_items_df.head(50))\n","print(items[items['item_id']==13380])\n","\n","# \"certification exam\" is not present in the cleaned, translated item names, nor is \"certification\"\n","# \"exam \" is present in only 26 items... so, komplvoprsertekzam1s is broader than 'exam'\n","certification = items_delimited[items_delimited.clean_item_name.str.contains('exam ', case=False, na=False)][['item_id','i_cat_id','i_tested','item_name','clean_item_name']] \n","print(len(certification))\n","print(certification.head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","505355  2013-05-07               4       55      618         172             1\n","933109  2013-09-27               8       55      618         172             1\n","933124  2013-09-06               8       55      618         172             1\n","1088977 2013-11-24              10       55      618         172             1\n","1088978 2013-11-03              10       55      618         172             1\n","       item_id                                                                        en_item_name  item_category_id                category\n","8448      8448                                   Accessory: Xbox 360 Hard Drive 500 GB (6FM-00003)                 6  Accessories - XBOX 360\n","8449      8449                  Accessory: Xbox 360 Wireless Controller Chrome magenta (43G-00062)                 6  Accessories - XBOX 360\n","8450      8450                    Accessory: Xbox 360 Wireless Controller Black Chrome (43G-00059)                 6  Accessories - XBOX 360\n","11305    11305  Holder ARTPLAYS Camera Clip 2 to 1 to Kinect / PS3 camera sensor (SR-70102), black                 6  Accessories - XBOX 360\n","11306    11306                             Holder for Xbox360 Kinect and PS3 Move Camera (HHC-008)                 6  Accessories - XBOX 360\n","       item_id  i_cat_id  i_tested                                                                                        item_name                                                                 clean_item_name\n","13367    13367        49      True                          Kompl.vopr.sert.ekzam.1S: ERP Enterprise Management 2.0 (1C Publishing)                 komplvoprsertekzam1s erp enterprise management 20 1c publishing\n","13368    13368        49     False                Kompl.vopr.sert.ekzam.1S: Buch. state. institutions (August 2011) (1C-Publishing)           komplvoprsertekzam1s buch state institutions august 2011 1cpublishing\n","13369    13369        49      True         Kompl.vopr.sert.ekzam.1S: Buch. state. ed institutions. 2.0 (April 2014) (1C-Publishing)      komplvoprsertekzam1s buch state ed institutions 20 april 2014 1cpublishing\n","13370    13370        49      True                     Kompl.vopr.sert.ekzam.1S: Accounting 8 (red.3.0), March 2013 (1C Publishing)                komplvoprsertekzam1s accounting 8 red30 march 2013 1c publishing\n","13371    13371        49     False                         Kompl.vopr.sert.ekzam.1S: Accounting for Belarus (1.6 ed.), October 2011                  komplvoprsertekzam1s accounting for belarus 16 ed october 2011\n","13372    13372        49     False                                  Kompl.vopr.sert.ekzam.1S: Budgetary reporting 8 (February 2011)                        komplvoprsertekzam1s budgetary reporting 8 february 2011\n","13373    13373        49     False                      Kompl.vopr.sert.ekzam.1S: Document 8 (red.1.2), August 2013 (1C Publishing)                 komplvoprsertekzam1s document 8 red12 august 2013 1c publishing\n","13374    13374        49     False                      Kompl.vopr.sert.ekzam.1S: Document 8 (red.1.3), August 2014 (1C Publishing)                 komplvoprsertekzam1s document 8 red13 august 2014 1c publishing\n","13375    13375        49      True                   Kompl.vopr.sert.ekzam.1S: Document 8 (red.2.0), September 2015 (1C-Publishing)               komplvoprsertekzam1s document 8 red20 september 2015 1cpublishing\n","13376    13376        49     False                                  Kompl.vopr.sert.ekzam.1S: Document 8, June 2011 (1C Publishing)                         komplvoprsertekzam1s document 8 june 2011 1c publishing\n","13377    13377        49     False                       Kompl.vopr.sert.ekzam.1S: Salary and Personnel budget entity 8, April 2010            komplvoprsertekzam1s salary and personnel budget entity 8 april 2010\n","13378    13378        49     False                     Kompl.vopr.sert.ekzam.1S: Salary and Personnel budget entity 8 November 2013         komplvoprsertekzam1s salary and personnel budget entity 8 november 2013\n","13379    13379        49      True              Kompl.vopr.sert.ekzam.1S: Salary and Personnel state. institutions 8, February 2015    komplvoprsertekzam1s salary and personnel state institutions 8 february 2015\n","13380    13380        49     False          Kompl.vopr.sert.ekzam.1S: Salary and upr.pers.8 (red.2.5), January 2010 (1C Publishing)       komplvoprsertekzam1s salary and uprpers8 red25 january 2010 1c publishing\n","13381    13381        49      True          Kompl.vopr.sert.ekzam.1S: Salary and upr.pers.8 (red.3.0), October 2014 (1C Publishing)       komplvoprsertekzam1s salary and uprpers8 red30 october 2014 1c publishing\n","13382    13382        49     False                                              Kompl.vopr.sert.ekzam.1S: Consolidation 8 Feb. 2009                                   komplvoprsertekzam1s consolidation 8 feb 2009\n","13383    13383        49      True                                           Kompl.vopr.sert.ekzam.1S: Pr.8.UPP (red.1.3), May 2012                                      komplvoprsertekzam1s pr8upp red13 may 2012\n","13384    13384        49     False                 Kompl.vopr.sert.ekzam.1S: Pr.8.Upr.torg. (Red.11, February 2011) (1C-Publishing)                komplvoprsertekzam1s pr8uprtorg red11 february 2011 1cpublishing\n","13385    13385        49      True               Kompl.vopr.sert.ekzam.1S: Pr.8.Upr.torg. (Red.11.1, December 2013) (1C-Publishing)               komplvoprsertekzam1s pr8uprtorg red111 december 2013 1cpublishing\n","13386    13386        49     False                      Kompl.vopr.sert.ekzam.1S: Pr.8.Upravlenie construction company (April 2008)              komplvoprsertekzam1s pr8upravlenie construction company april 2008\n","13387    13387        49      True                      Kompl.vopr.sert.ekzam.1S: Professional on technology issues (1C Publishing)            komplvoprsertekzam1s professional on technology issues 1c publishing\n","13388    13388        49      True                               Kompl.vopr.sert.ekzam.1S: Retail 8 September 2011. (1S-Publishing)                       komplvoprsertekzam1s retail 8 september 2011 1spublishing\n","13389    13389        49     False                       Kompl.vopr.sert.ekzam.1S: Managing a small firm 8 (red.1.4), November 2012                komplvoprsertekzam1s managing a small firm 8 red14 november 2012\n","13390    13390        49      True                        Kompl.vopr.sert.ekzam.1S: Managing a small firm 8 (red.1.5), January 2015                 komplvoprsertekzam1s managing a small firm 8 red15 january 2015\n","13391    13391        49     False                     Kompl.vopr.sert.ekzam.1S: Managing a small firm 8, June 2011 (1C Publishing)            komplvoprsertekzam1s managing a small firm 8 june 2011 1c publishing\n","13468    13468        11     False                                     Kit «Sony PS3 (320 GB) (CECH-2508B)» + game «Battlefield 3 '                               kit sony ps3 320 gb cech2508b game battlefield 3 \n","13469    13469        11     False  Kit «Sony PS3 (320 GB) (CECH-2508B)» + game «God of War 3 (Platinum)» + game «Uncharted 2 (Plat  kit sony ps3 320 gb cech2508b game god of war 3 platinum game uncharted 2 plat\n","13474    13474        11     False                                            Kit «Sony PS3 (320 Gb) (CECH-2508B) + game« FIFA 12 \"                                     kit sony ps3 320 gb cech2508b game fifa 12 \n","13475    13475        11     False                                       Kit «Sony PS3 (320 Gb) (CECH-2508B) + game« Resistance 3 '                                kit sony ps3 320 gb cech2508b game resistance 3 \n","13476    13476        11     False                                         Kit «Sony PS3 (320 Gb) (CECH-2508B)» + game \"inFamous 2\"                                   kit sony ps3 320 gb cech2508b game infamous 2\n","                                                                                  item_name  item_id  item_category_id\n","13380  Компл.вопр.серт.экзам.1С:Зарплата и упр.перс.8 (ред.2.5), январь 2010 (1С-Паблишинг)    13380                49\n","26\n","     item_id  i_cat_id  i_tested                                                          item_name                                              clean_item_name\n","810      810        77     False                  1C: tutor. Maths. We pass the exam (2013) (Jewel)                   1c tutor maths we pass the exam 2013 jewel\n","813      813        77     False      1C: tutor. Russian language. We pass the exam in 2012 (Jewel)     1c tutor russian language we pass the exam in 2012 jewel\n","814      814        77     False      1C: tutor. Russian language. We pass the exam in 2013 (Jewel)     1c tutor russian language we pass the exam in 2013 jewel\n","815      815        78     False  1C: tutor. We pass the exam 2010. Chemistry [PC, Digital Version]  1c tutor we pass the exam 2010 chemistry pc digital version\n","816      816        77     False          1C: tutor. We pass the exam in mathematics (2013) (Jewel)          1c tutor we pass the exam in mathematics 2013 jewel\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UVtotq7VK0nB","colab_type":"text"},"source":["######Items containing \"1C\" in their name"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"c21d970c-17ed-40fe-a934-6f49c2ca648f","executionInfo":{"status":"ok","timestamp":1588784840405,"user_tz":240,"elapsed":303,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"hXM-ICo7FPGn","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# I'm curious about items containing \"1C\" ... there seem to be quite a few, and if I'm not mistaken,\n","#    1C is an ERP like Oracle or SAP, but 1C is based in Russia.  Let's see how many items have 1C\n","#    in their item_name, and what categories these belong to, and what the item names look like (what are common words that accompany \"1C\"?)\n","items_delimited_1c = items_clean_delimited.copy(deep=True)\n","# delete the wide \"item_name\" column so we can read more of the data table width-wise\n","items_delimited_1c = items_delimited_1c.drop(['delim_name_list','d_len','d_maxgram'], axis=1)\n","items_delimited_1c.head()\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>i_cat_id</th>\n","      <th>i_tested</th>\n","      <th>item_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>power in glamor plast dvd</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>False</td>\n","      <td>abbyy finereader 12 professional edition full pc digital version</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>in the glory unv dvd</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>blue wave univ dvd</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>box glass dvd</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id  i_cat_id  i_tested                                                          item_name\n","0        0        40     False                                          power in glamor plast dvd\n","1        1        76     False   abbyy finereader 12 professional edition full pc digital version\n","2        2        40     False                                               in the glory unv dvd\n","3        3        40     False                                                 blue wave univ dvd\n","4        4        40     False                                                      box glass dvd"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xUIfCz4U4sVF"},"source":["#XX) Below this markdown cell can be ignored\n","\n","These are just code snippets I used to debug how to accomplish some of the tasks above.  I may want to revisit them some day, so I am too scared to just delete them. :)"]},{"cell_type":"markdown","metadata":{"id":"gKW9VvCoZakg","colab_type":"text"},"source":["Below is a code snippet to plot item sales vs. month for one or more item ids"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"cbf26d54-1fae-4612-c658-b555cf2fcfef","executionInfo":{"status":"ok","timestamp":1588785157671,"user_tz":240,"elapsed":451,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"GDca22GLZYlD","colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["# enter the desired item_ids here:\n","to_plot = [617,618]\n","\n","item_plot_df = sales_train[sales_train.item_id.isin(to_plot)] \n","print(item_plot_df.head())\n","\n","# what does \"figure\" mean in category name?\n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([77,78])][['item_id','en_item_name','item_category_id','category']] \n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([43,44])][['item_id','en_item_name','item_category_id','category']] \n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([75,76])][['item_id','en_item_name','item_category_id','category']] \n","fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([6])][['item_id','en_item_name','item_category_id','category']] \n","print(fig_cat_df.tail())\n","\n","# get item names from rare words\n","rare_words = 'pch1008za01' #,hhkatv,pspe1008,cech2508b,komplvoprsertekzam1s'\n","#reg_pattern = re.compile('|'.join(rare_words.split(',')), flags=re.IGNORECASE)\n","any_rare_words = '|'.join(rare_words.split(','))\n","rare_items_df = items_clean_delimited[items_clean_delimited.item_name.str.contains(any_rare_words, case=False, na=False)][['item_id','i_cat_id','i_tested','item_name']] \n","print(rare_items_df.head(50))\n","'''\n","\n","# seems to be one particular bad player... item 2973 from shop 6 in row 484683\n","#   look more at this item, shop, and item-shop combo:\n","print(\"\\n\")\n","print(2973 in test.item_id.to_list())\n","# this item is not in the test set... does it make up a significant amount of the train set?\n","\n","print(f\"Shop 32 = {shops_augmented.at[32,'en_shop_name']}\", end=\"\")\n","print(f\", Item 2973 = {items_transl.at[2973,'en_item_name']}\\n\")\n","\n","item2973 = sales_train[sales_train.item_id == 2973]\n","print(item2973.describe())\n","print(\"\\n\")\n","print(item2973.sort_values('item_price').head(10))\n","# only one sales_train row entry with price < 1000, and it is this negative outlier\n","\n","print(\"\\n\")\n","item2973shop32 = item2973[item2973.shop_id == 32]\n","print(item2973shop32.describe())\n","print(\"\\n\")\n","print(item2973shop32.sort_values('date').head(15))\n","# it looks like perhaps this shop had a discount clearance sale in summer 2013, and\n","#  then never sold the item again\n","\n","plt.rcParams[\"figure.figsize\"] = [16,5]\n","fig = plt.figure() #figsize=(16,9))\n","#item2973.sort_values('date_block_num')['date_block_num'].value_counts().plot(kind='bar')\n","item2973.date_block_num.value_counts().reset_index().drop('index',axis=1).rename(columns={'date_block_num':'n_train_rows'}).plot(kind='bar')\n","plt.title('by MONTH (date_block_num), number of rows in sales_train for item 2973')\n","plt.grid(b=True, which='major', axis='y', color='#666666', linestyle='-')\n","\n","plt.show()\n","\n","# looks like this item has little bearing on sales in Nov. 2015, as its sales\n","#  died off (among all shops) by January 2015\n","# and, the one entry at row 484683 in sales_train with the negative price can be safely deleted\n","'''\n","a=1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","505355  2013-05-07               4       55      618         172             1\n","933109  2013-09-27               8       55      618         172             1\n","933124  2013-09-06               8       55      618         172             1\n","1088977 2013-11-24              10       55      618         172             1\n","1088978 2013-11-03              10       55      618         172             1\n","       item_id                                                                        en_item_name  item_category_id                category\n","8448      8448                                   Accessory: Xbox 360 Hard Drive 500 GB (6FM-00003)                 6  Accessories - XBOX 360\n","8449      8449                  Accessory: Xbox 360 Wireless Controller Chrome magenta (43G-00062)                 6  Accessories - XBOX 360\n","8450      8450                    Accessory: Xbox 360 Wireless Controller Black Chrome (43G-00059)                 6  Accessories - XBOX 360\n","11305    11305  Holder ARTPLAYS Camera Clip 2 to 1 to Kinect / PS3 camera sensor (SR-70102), black                 6  Accessories - XBOX 360\n","11306    11306                             Holder for Xbox360 Kinect and PS3 Move Camera (HHC-008)                 6  Accessories - XBOX 360\n","       item_id  i_cat_id  i_tested                                                                                    item_name\n","6665      6665        14     False                                                      sony ps vita wifi black rus pch1008za01\n","13429    13429        14     False     kit sony ps vita wifi black rus pch1008za01 psn activation code assassins creed released\n","13430    13430        14     False   kit sony ps vita wifi black rus pch1008za01 psn activation code call of duty black ops dec\n","13431    13431        14     False  kit sony ps vita wifi black rus pch1008za01 psn activation code littlebigplanet memory card\n","13432    13432        14     False       kit sony ps vita wifi black rus pch1008za01 4gb memory card cod black ops declassified\n","13467    13467        14     False        kit sony ps vita wifi black rus pch1008za01 memory card 16 gb disney mega pack psn to\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hMwVqnoC_aje","colab_type":"text"},"source":["######Below here are some code snippets for future reference; ignore please\n","\n","</br>\n","\n","</br>\n","\n","---\n","---\n","\n","</br>\n","\n","</br>\n"]},{"cell_type":"code","metadata":{"id":"bY5u83Wmw5-m","colab_type":"code","colab":{}},"source":["'''\n","REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","MULTIPLE_WHITESPACE_RE = re.compile('[ ]{2,}')\n","STOPWORDS = set(stopwords.words('english'))  #using \"set\" speeds things up a little; note all stopwords are in lowercase\n","#print(\".\" in STOPWORDS)\n","def text_prepare(text):\n","    \"\"\"\n","        text: a string\n","        \n","        return: modified initial string\n","    \"\"\"\n","    text = text.lower() # lowercase text... need to do this before removing stopwords because stopwords are all lowercase\n","    text = REPLACE_BY_SPACE_RE.sub(' ',text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n","    text = BAD_SYMBOLS_RE.sub('',text) # delete symbols which are in BAD_SYMBOLS_RE from text\n","    text = MULTIPLE_WHITESPACE_RE.sub(' ',text)\n","    text = \" \".join([word for word in text.split(\" \") if word not in stopwords.words('english')]) # delete stopwords from text\n"," \n","    return text\n","'''\n","\n","\n","#item_vectors.to_csv(\"data_output/item_vectors.csv\", index=False)\n","# item_vectors[:][:2000].to_csv(\"data_output/item_vectors1.csv\", index=False)\n","# item_vectors[:][2000:].to_csv(\"data_output/item_vectors2.csv\", index=False)\n","\n","\n","# item_vectors = pd.DataFrame(np.zeros((21700,4043), dtype = np.int16))\n","\n","# compression_opts = dict(method='gzip',\n","#                         archive_name='item_vectors.csv')  \n","# item_vectors.to_csv('item_vectors.csv.gz', compression=compression_opts)\n","\n","# keep this cell for future reference\n","keep = True"],"execution_count":0,"outputs":[]}]}