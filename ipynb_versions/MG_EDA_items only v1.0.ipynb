{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MG_EDA_items only v1.0.ipynb","provenance":[{"file_id":"1y04qp_hoyBnsJQwkX67pk4iZsKGQIqNy","timestamp":1588805435380},{"file_id":"1b_K0QD9U6dofQ7VtTAtzUrqbKJMdj64l","timestamp":1588785261238},{"file_id":"1gcbeu-d1GUUzznZwTzfqYYbaD6cJ7EQ4","timestamp":1588238522691},{"file_id":"1pSGNRDJGzdeI69bw1zWefzPifBq-rv9H","timestamp":1588151557805},{"file_id":"1hq-ivO1BBtc5IC5xd-JdH8HNAQ81bkRf","timestamp":1587386702728},{"file_id":"1I7DWo2B7q7g9Ne2khD11YGTq_gD2FoaT","timestamp":1587321559573},{"file_id":"1fyZv-jgb8twsCBQwPxjgk_XSYA6dOa2t","timestamp":1587303588700},{"file_id":"1iKsplqpLQQZqdr3Trflk7TapksgkQXjX","timestamp":1587145642564},{"file_id":"https://github.com/migai/Kag/blob/master/Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587076517706}],"collapsed_sections":["s0J5l5H98Xsh","lZfgOx-0_KXg","EIn77EZzoxkY","V7QY11R1QmlN","xUIfCz4U4sVF","hMwVqnoC_aje"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn","colab_type":"text"},"source":["#**Investigation of *items* database and correlations between items**\n","\n","**EDA, NLP, Feature Generation**\n","\n","Andreas Theodoulou and Michael Gaidis (May, 2020)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ruw_WyRxhqpx"},"source":["#0. Configure Environment\n","**NOT OPTIONAL**"]},{"cell_type":"code","metadata":{"id":"sTVAxnMnenrB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"25e2e5ee-36be-4fa2-bbc1-e9c131e71740","executionInfo":{"status":"ok","timestamp":1588810153777,"user_tz":240,"elapsed":822,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# General python libraries/modules used throughout the notebook\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import MultipleLocator, FormatStrFormatter, AutoMinorLocator\n","import numpy as np\n","import seaborn as sns\n","\n","import os\n","from itertools import product\n","import re\n","import json\n","import time\n","from time import sleep, localtime, strftime\n","import pickle\n","\n","\n","# Magics\n","%matplotlib inline\n","\n","\n","# # NLP packages\n","# import nltk\n","# nltk.download('stopwords')\n","# from nltk.corpus import stopwords\n","\n","# # ML packages\n","# from sklearn.linear_model import LinearRegression\n","\n","# !pip install catboost\n","# from catboost import CatBoostRegressor \n","\n","# %tensorflow_version 2.x\n","# import tensorflow as tf\n","# import keras as K\n","\n","# # List of the modules we need to version-track for reference\n","# modules = ['pandas','matplotlib','numpy','seaborn','sklearn','tensorflow','keras','catboost','pip']"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NoOf7oi8Oe-Q","colab_type":"code","colab":{}},"source":["# Notebook formatting\n","# Adjust as per your preferences.  I'm using a FHD monitor with a full-screen browser window containing my IPynb notebook\n","\n","# format pandas output so we can see all the columns we care about (instead of \"col1  col2  ........ col8 col9\", we will see \"col1 col2 col3 col4 col5 col6 col7 col8 col9\" if it fits inside display.width parameter)\n","pd.set_option(\"display.max_columns\",30)  \n","pd.set_option(\"display.max_rows\",100)     # Override pandas choice of how many rows to show, so, for example, we can see the full 84-row item_category dataframe instead of the first few rows, then ...., then the last few rows\n","pd.set_option(\"display.width\", 300)       # Similar to the above for showing more rows than pandas defaults to, we can show more columns than default, if we tune this to our monitor window size\n","pd.set_option(\"max_colwidth\", None)\n","\n","#pd.set_option(\"display.precision\", 3)  # Nah, this is helpful, but below is even better\n","#Try to convince pandas to print without decimal places if a number is actually an integer (helps keep column width down, and highlights data types)\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"miCS3XqUhDXz"},"source":["#1. Load Data Files\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kjWzoXizEa5O","colab_type":"text"},"source":["##1.1) Enter Data File Names and Paths\n","\n","**NOT Optional**"]},{"cell_type":"code","metadata":{"id":"p9vsd3EynZLO","colab_type":"code","colab":{}},"source":["#  FYI, data is coming from a public repo on GitHub at github.com/migai/Kag\n","# List of the data files (path relative to GitHub master), to be loaded into pandas DataFrames\n","data_files = [  \"readonly/final_project_data/items.csv\",\n","                \"readonly/final_project_data/item_categories.csv\",\n","                \"readonly/final_project_data/shops.csv\",\n","                \"readonly/final_project_data/sample_submission.csv.gz\",\n","                \"readonly/final_project_data/sales_train.csv.gz\",\n","                \"readonly/final_project_data/test.csv.gz\",\n","                \"data_output/shops_transl.csv\",\n","                \"data_output/shops_augmented.csv\",\n","                \"data_output/item_categories_transl.csv\",\n","                \"data_output/item_categories_augmented.csv\",\n","                \"data_output/items_transl.csv\",\n","                \"readonly/en_50k.csv\"  ]\n","\n","\n","# Dict of helper code files, to be loaded and imported {filepath : import_as}\n","code_files = {}  # not used at this time; example dict = {\"helper_code/kaggle_utils_at_mg.py\" : \"kag_utils\"}\n","\n","\n","# GitHub file location info\n","git_hub_url = \"https://raw.githubusercontent.com/migai\"\n","repo_name = 'Kag'\n","branch_name = 'master'\n","base_url = os.path.join(git_hub_url, repo_name, branch_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPrPvh7sorJd","colab_type":"text"},"source":["##1.2) Load Data Files"]},{"cell_type":"code","metadata":{"id":"CUIE1PVjSAmg","colab_type":"code","outputId":"041e44fe-9298-4c75-a584-4d17e7eec772","executionInfo":{"status":"ok","timestamp":1588810179525,"user_tz":240,"elapsed":26540,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, then paste it into the input box and hit 'enter' to complete mounting of the drive\n","from google.colab import drive  \n","drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dy5i7jl00oX-","colab_type":"code","outputId":"961e2cb1-e8b6-4a07-876f-8dce17ec2ad5","executionInfo":{"status":"ok","timestamp":1588810187244,"user_tz":240,"elapsed":34244,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["'''\n","############################################################\n","############################################################\n","'''\n","# Replace this path with the path on *your* Google Drive where the repo master branch is stored\n","#   (on GitHub, the remote repo is located at github.com/migai/Kag --> below is my cloned repo location)\n","GDRIVE_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\"\n","'''\n","############################################################\n","############################################################\n","'''\n","\n","%cd \"{GDRIVE_REPO_PATH}\"\n","\n","print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","\n","# Loop to load the data files into appropriately-named pandas DataFrames\n","for path_name in data_files:\n","    filename = path_name.rsplit(\"/\")[-1]\n","    data_frame_name = filename.split(\".\")[0]\n","    exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","    if data_frame_name == 'sales_train':\n","        sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","    print(\"Data Frame: \" + data_frame_name)\n","    print(eval(data_frame_name).head(2))\n","    print(\"\\n\")\n","\n","\"\"\"\n","unused at this time...\n","\n","# Load in any helper functions from the code_files dictionary\n","#    dictionary key is the path (replace \"/\"\" with \".\" when using Google Drive + Colab), \n","#      and dictionary value is the module reference name\n","#    note that the directory chain on GitHub (and local repo) from current directory down to the .py file\n","#      must include a \"__init__.py\" file (it can be empty) in each of the directories\n","for filepath, module in code_files.items():\n","  path_name = filepath.replace(\"/\",\".\")[:-3]  # Google Drive reference does not use .py, and uses a \".\" instead of \"/\" for directory delineation\n","  exec(\"import \" + path_name + \" as \" + module)\n","\n","# Sanity check test\n","test1 = kag_utils.add_one(2)\n","print(test1)\n","\"\"\"\n","optional_code = True  # in a code block: (at top of cell = notice for you; at bottom = prevents Jupyter printing \"\"\" comments)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","Loading Files from Google Drive repo into Colab...\n","\n","Data Frame: items\n","                                                              item_name  item_id  item_category_id\n","0                             ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0                40\n","1  !ABBYY FineReader 12 Professional Edition Full [PC, Цифровая версия]        1                76\n","\n","\n","Data Frame: item_categories\n","        item_category_name  item_category_id\n","0  PC - Гарнитуры/Наушники                 0\n","1         Аксессуары - PS2                 1\n","\n","\n","Data Frame: shops\n","                       shop_name  shop_id\n","0  !Якутск Орджоникидзе, 56 фран        0\n","1  !Якутск ТЦ \"Центральный\" фран        1\n","\n","\n","Data Frame: sample_submission\n","   ID  item_cnt_month\n","0   0           0.500\n","1   1           0.500\n","\n","\n","Data Frame: sales_train\n","        date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0 2013-01-02               0       59    22154         999             1\n","1 2013-01-03               0       25     2552         899             1\n","\n","\n","Data Frame: test\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","\n","\n","Data Frame: shops_transl\n","                       shop_name  shop_id                       en_shop_name\n","0  !Якутск Орджоникидзе, 56 фран        0  ! Yakutsk Ordzhonikidze, 56 Franc\n","1  !Якутск ТЦ \"Центральный\" фран        1       ! Yakutsk TC \"Central\" Franc\n","\n","\n","Data Frame: shops_augmented\n","                       shop_name  shop_id                       en_shop_name shop_city shop_category shop_federal_district  shop_city_population  shop_tested\n","0  !Якутск Орджоникидзе, 56 фран        0  ! Yakutsk Ordzhonikidze, 56 Franc   Yakutsk          Shop               Eastern                235600        False\n","1  !Якутск ТЦ \"Центральный\" фран        1       ! Yakutsk TC \"Central\" Franc   Yakutsk          Mall               Eastern                235600        False\n","\n","\n","Data Frame: item_categories_transl\n","        item_category_name  item_category_id                 en_cat_name\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones\n","1         Аксессуары - PS2                 1           Accessories - PS2\n","\n","\n","Data Frame: item_categories_augmented\n","        item_category_name  item_category_id                 en_cat_name item_category1 item_category2 item_category3 item_category4  item_cat_tested\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones          Audio             PC    Accessories             PC             True\n","1         Аксессуары - PS2                 1           Accessories - PS2    Accessories    PlayStation    Accessories    PlayStation            False\n","\n","\n","Data Frame: items_transl\n","                                                              item_name  item_id  item_category_id                                                           en_item_name\n","0                             ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0                40                                           ! POWER IN glamor (PLAST.) D\n","1  !ABBYY FineReader 12 Professional Edition Full [PC, Цифровая версия]        1                76  ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]\n","\n","\n","Data Frame: en_50k\n","  word  word_count\n","0  you    28787591\n","1    i    27086011\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P99yjzAasJva"},"source":["#2. Explore Data (EDA), Clean Data, and Generate Features"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xr0FDeno_EUQ"},"source":["#2.5) ***items*** Dataset: EDA, Cleaning, Correlations, and Feature Generation\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s0J5l5H98Xsh"},"source":["###Thoughts regarding items dataframe\n","Let's first look at how many training examples we have to work with..."]},{"cell_type":"markdown","metadata":{"id":"1lg7NbEchkuM","colab_type":"text"},"source":["Many of the items have similar names, but slightly different punctuation, or only very slightly different version numbers or types.  (e.g., 'Call of Duty III' vs. 'Call of Duty III DVD')\n","\n","One can expect that these two items would have similar sales in general, and by grouping them into a single feature category, we can eliminate some of the overfitting that might come as a result of the relatively small ratio of (training set shop-item-date combinations = 2935849)/(total number of unique items = 22170).  (This is an average of about 132 rows in the sales_train data for each shop-item-date combination that we are using to train our model.  Our task is to produce a monthly estimate of sales (for November 2015), so it is relevant to consider training our model based on how many sales in a month vs. how many sales in the entire training set.  Given that the sales_train dataset covers the time period from January 2013 to October 2015 (34 months), we have on average fewer than 4 shop-item combinations in our training set for a given item in any given month.  Furthermore, as we are trying to predict for a particular month (*November* 2015), it is relevant to consider how many rows in our training set occur in the month of November.  The sales_train dataset contains data for two 'November' months out of the total 34 months of data.  Another simple calculation gives us an estimate that our training set contains on average 0.23 shop-item combinations per item for November months.\n","\n","To summarize:\n","\n","*  *sales_train* contains 34 months of data, including 2935849 shop-item-date combinations\n","*  *items* contains 22170 \"unique\" item_id values\n","\n","In the *sales_train* data, we therefore have:\n","*  on average, 132 rows with a given shop-item pair for a given item_id\n","*  on average, 4 rows with a given shop-item pair for a given item_id in a given month\n","*  on average, 0.23 rows with a given shop-item pair for a given item_id in all months named 'November'\n","\n","If we wish to improve our model predictions for the following month of November, it behooves us to use monthly grouping of sales, or, even better, November grouping of sales.  This smooths out day-to-day variations in sales for a better monthly prediction.  However, the sparse number of available rows in the *sales_train* data will contribute to inaccuracy in our model training and predictions.\n","\n","Imagine if we could reduce the number of item_id values from 22170 to perhaps half that or even less.  Given that the number of rows for training (per item, on a monthly or a November basis) is so small, then such a reduction in the number of item_id values would have a big impact.  (The same is true for creating features to supplement \"shop_id\" so as to group and reduce the individuality of each shop - and thus effectively create, on average, more rows of training data for each shop-item pair."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZfgOx-0_KXg"},"source":["###2.5.1) **Translate and Ruminate**\n","We will start by translating the Russian text in the dataframe, and add our ruminations on possible new features we can generate.\n","\n","The dataframe *items_transl* (equivalent to *items* plus a column for English translation) is saved as a .csv file so we do not have to repeat the translation process the next time we open a Google Colab runtime."]},{"cell_type":"code","metadata":{"id":"FhHSfXNxsKxQ","colab_type":"code","outputId":"a2ebbf2d-4dde-4d0b-c276-85b1c873614f","executionInfo":{"status":"ok","timestamp":1588810187245,"user_tz":240,"elapsed":34227,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["print(items_transl.info())\n","print(\"\\n\")\n","print(items_transl.tail(10))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 22170 entries, 0 to 22169\n","Data columns (total 4 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   item_name         22170 non-null  object\n"," 1   item_id           22170 non-null  int64 \n"," 2   item_category_id  22170 non-null  int64 \n"," 3   en_item_name      22170 non-null  object\n","dtypes: int64(2), object(2)\n","memory usage: 692.9+ KB\n","None\n","\n","\n","                                                   item_name  item_id  item_category_id                                           en_item_name\n","22160                             ЯРМАРКА ТЩЕСЛАВИЯ (Регион)    22160                40                                   Vanity Fair (Region)\n","22161                       ЯРОСЛАВ. ТЫСЯЧУ ЛЕТ НАЗАД э (BD)    22161                37                YAROSLAV. Thousands of years ago e (BD)\n","22162                                                 ЯРОСТЬ    22162                40                                                   FURY\n","22163                                       ЯРОСТЬ ( регион)    22163                40                                          FURY (region)\n","22164                                            ЯРОСТЬ (BD)    22164                37                                              FURY (BD)\n","22165                 Ядерный титбит 2 [PC, Цифровая версия]    22165                31                 Nuclear titbit 2 [PC, Digital Version]\n","22166        Язык запросов 1С:Предприятия  [Цифровая версия]    22166                54     Language 1C queries: Enterprises [Digital Version]\n","22167  Язык запросов 1С:Предприятия 8 (+CD). Хрусталева Е.Ю.    22167                49  1C query language: Enterprise 8 (+ CD). Khrustalev EY\n","22168                                    Яйцо для Little Inu    22168                62                                     Egg for Little Inu\n","22169                          Яйцо дракона (Игра престолов)    22169                69                           Dragon egg (Game of Thrones)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9oSMeRVd7dvZ"},"source":["###2.5.2) **NLP for feature generation from items data**\n","Automate the search for commonality among items, and create new categorical feature to prevent overfitting from close similarity between many item names"]},{"cell_type":"markdown","metadata":{"id":"EIn77EZzoxkY","colab_type":"text"},"source":["####Investigate possibility of using NLP to reduce or regularize the items dataset\n","\n","---\n","\n","---\n"]},{"cell_type":"code","metadata":{"id":"_9w-KKpfvvos","colab_type":"code","colab":{}},"source":["# The following is commented out for now; not sure if I'm going to use nltk or wordnet\n","\n","# import nltk\n","# nltk.download('stopwords')\n","# from nltk.corpus import stopwords\n","# STOPWORDS = set(stopwords.words('english'))\n","\n","# nltk.download('wordnet')\n","# from nltk.stem import WordNetLemmatizer \n","# lemmatizer = WordNetLemmatizer() \n","# # use wordnet??\n","# from nltk.corpus import wordnet as wn\n","# # example uses of wordnet\n","# w = \"volume iiii\"\n","# print(f\"Lemmatization of '{w}'':\", lemmatizer.lemmatize(w)) \n","# w = \"rocks\"\n","# print(f\"Lemmatization of '{w}'':\", lemmatizer.lemmatize(w)) \n","# wn.synsets('rus')\n","\n","# Here is the approach I plan to take to look at item name similarity:\n","#   1) vectorize item names, with vector elements chosen as follows:\n","#         a) uncommon words or part numbers found inside delimiters like () or [] or / / etc.\n","#               1.) large \"n\" n-grams\n","#               2.) part numbers and uncommon words\n","#         b) uncommon words in entire item name (not only delimited words)\n","#         c) words used in \"item_categories\" names, supplemented\n","#         d) special descriptors like edition number, english/russian, etc.  (keep all numbers as \"words\")\n","#   2) compute cosine similarity or other method giving special weight to the above tiers\n","#   3) manually investigate item names with very high similarity, and combine if actually the same name\n","#   4) set a certain similarity limit, and item groups above that limit will form new item categories (target: 2000 categories)\n","#   5) check any items not in one of these new categories, and see if they are tested... if so, assign to closest of the new categories\n","#\n","# Idea is to then use the 2000 category list instead of item name as a key feature in fitting the model, both to help\n","#   regularize, and to help generalize to the items in test set that are not in train set\n","#\n","# What words are \"uncommon\"?\n","#   Use the top 50,000 words in 2018 database of movie/tv subtitles from https://github.com/hermitdave/FrequencyWords\n","#   The en_50k dataframe has the 50,000 most-commonly found words in this database, along with a number of \"counts\" \n","#   or appearances in that text corpus.  The word count gives us an idea of word popularity (higher count = higher popularity of use)\n","#   We can then do an inverse-frequency type of word characterization on our item names\n","# Why not use a pre-existing word vectorizer package to create our item_name vectors?...\n","#   because this 21,700 item database is somewhat unique in that it is heavily weighted \n","#   towards Russian entertainment sales.  We don't want word vectors that ignore things \n","#   like xbox versus playstation.  We want to \"tweak\" the vectors to help us form relevant \n","#   item groups, and not just use any word in the item name to dominate in group identification\n","#   (We know, for example, that the word \"rus\" is likely to mean \"Russian\" in our database,\n","#   whereas a standard vectorizer would either characterize it as \"rus\" or as \"ruthenium\" perhaps)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CwfXcHsMJ0Yg","colab_type":"text"},"source":["####**Delimited Groups of Words**\n","\n","Investigating \"special\" delimited word groups (like this) or [here] or /hobbitville/ that are present in item names, and may be particularly important in creating n>1 n-grams for uniquely identifying items so that we can tell if two items are the same or nearly the same"]},{"cell_type":"markdown","metadata":{"id":"V7QY11R1QmlN","colab_type":"text"},"source":["#####Some Details on The Approach..."]},{"cell_type":"code","metadata":{"id":"jac_TColdsMf","colab_type":"code","colab":{}},"source":["# explanation of regex string I'm using to parse the item_name\n","'''\n","\n","^\\s+|\\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*|\\s+$\n","\n","gm\n","1st Alternative ^\\s+\n","^ asserts position at start of a line\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","2nd Alternative \\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","Match a single character present in the list below [,\\\"\\/\\(\\)\\[\\]]+\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n",", matches the character , literally (case sensitive)\n","\\\" matches the character \" literally (case sensitive)\n","\\/ matches the character / literally (case sensitive)\n","\\( matches the character ( literally (case sensitive)\n","\\) matches the character ) literally (case sensitive)\n","\\[ matches the character [ literally (case sensitive)\n","\\] matches the character ] literally (case sensitive)\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","3rd Alternative \\s+$\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","$ asserts position at the end of a line\n","\n","Global pattern flags\n","g modifier: global. All matches (don't return after first match)\n","m modifier: multi line. Causes ^ and $ to match the begin/end of each line (not only begin/end of string)\n","'''\n","commented_cell = True  # prevent Jupyter from printing triple-quoted comments"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rsc0yYJkiRBY","colab_type":"code","colab":{}},"source":["# This cell contains no code to run; it is simply a record of some inspections that were done on the items database\n","\n","# before removing undesirable characters / punctuation from the item name,\n","#   let's see if we can find n-grams or useful describers or common abbreviations by looking between the nasty characters\n","# first, let's see what characters are present in the en_item_name column\n","'''\n","nasty_symbols = re.compile('[^0-9a-zA-Z ]')\n","nasties = set()\n","for i in range(len(items_transl)):\n","  n = nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  nasties = nasties.union(set(n))\n","print(nasties)\n","{'[', '\\u200b', 'ñ', '(', ')', '.', 'à', '`', 'ó', '®', 'Á', \n","'\\\\', 'è', '&', '-', ':', 'ë', '_', 'û', '»', '=', '+', ']', ',', \n","'«', 'ú', \"'\", 'ö', '#', 'ä', ';', 'ü', '\"', 'ô', '/', '№', 'é', \n","'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","'''\n","# From the above set of nasty characters, it looks like slashes, single quotes, double quotes, parentheses, and square brackets might enclose relevant n-grams\n","# Let's pull everything from en_item_name that is inside ' ', \" \", (), or [] and see how many unique values we get, and if they are n-grams or abbreviations, for example\n","# It also seems that many of the item names end in a single character \"D\" for example, which should be converted to DVD\n","# Let's set up columns for ()[]-grams, for last string in the name, and for first string in name, and for text that precedes \":\", and for text that surrounds \"&\" or \"+\"\n","#   but first, we will strip out every nasty character except ()[]:&+'\"/ and replace the nasties with spaces, then eliminating double spaces\n","# And, let's add a boolean column for whether or not that item is in the test set\n","\n","'''\n","# sanity check:\n","really_nasty_symbols = re.compile('[^0-9a-zA-Z \\(\\)\\[\\]:&+\\'\"/]')\n","really_nasties = set()\n","for i in range(len(items_transl)):\n","  rn = really_nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  really_nasties = really_nasties.union(set(rn))\n","print(really_nasties)\n","{'\\u200b', 'ñ', '.', 'à', '`', 'ó', '®', 'Á', '\\\\', 'è', '-', 'ë', '_', 'û', '»', '=', ',', '«', 'ú', 'ö', '#', 'ä', ';', 'ü', 'ô', '№', 'é', 'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","OK, looks good\n","'''\n","commented_cell = True  # prevent Jupyter from printing triple-quoted comments"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvzvPqmCQ2ZM","colab_type":"text"},"source":["#####Add 'delimited' and 'cleaned' data columns; shorten the titles of other columns so dataframe fits better on the screen"]},{"cell_type":"code","metadata":{"id":"Z35pqOYCtyZ7","colab_type":"code","outputId":"137ff322-68da-4b09-dc97-2f8180c7c72c","executionInfo":{"status":"ok","timestamp":1588813925836,"user_tz":240,"elapsed":1387,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["items_delimited = items_transl.copy(deep=True)\n","# delete the wide \"item_name\" column so we can read more of the data table width-wise\n","items_delimited = items_delimited.drop(\"item_name\", axis=1).rename(columns = {'en_item_name':'item_name','item_category_id':'i_cat_id'})\n","#print(items_delimited.head())\n","items_in_test_set = test.item_id.unique()\n","items_delimited[\"i_tested\"] = False\n","for i in items_in_test_set:\n","  items_delimited.at[i,\"i_tested\"] = True\n","\n","# nasty_symbols_re = re.compile('[^0-9a-zA-Z \\+\\:\\&]')  # remove all punctuation and crazy characters, except that we keep \"+\", \":\" and \"&\"\n","# really_nasty_symbols_re = re.compile('[^0-9a-zA-Z \\(\\)\\[\\]\\:\\&\\+\\'\"/]')\n","conjunctions_re = re.compile('\\s*[\\+\\&]\\s*')\n","nasty_symbols_re = re.compile('[^0-9a-zA-Z ]')  # remove all punctuation\n","really_nasty_symbols_re = re.compile('[^0-9a-zA-Z \\(\\)\\[\\]\\:\\\"\\/]')\n","delimiters_re = re.compile('[\\(\\)\\[\\]\\:\\\"\\/\\+\\&]')\n","multiple_whitespace_re = re.compile('[ ]{2,}')\n","delim_pattern_re = re.compile('^\\s+|\\s*[,\\\"\\/\\(\\)\\[\\]\\:]+\\s*|\\s+$') # special symbols indicating a delimiter --> a space at start or end of item name is considered a delimiter, along with ,/()[]:\"\n","d_to_dvd_re = re.compile('\\s+d$')  #several item names end in \"d\" -- which actually seems to indicate dvd (because the items I see are in category 40: Movies-DVD)... standardize so d --> dvd\n","digitalin_to_digitalversion_re = re.compile('digital in$') # several items seem to end in \"digital in\"... maybe in = internet?, but looking at nearby items/categories, 'digital version' looks standard\n","\n","def text_total_clean(text):\n","    #text: the original en_item_name\n","    #return: en_item_name made lowercase, stripped of \"really_nasties\" and multiple spaces\n","    #     NOTE: this is not stripping stopwords\n","    text = text.lower()\n","    text = d_to_dvd_re.sub(\" dvd\", text)\n","    text = digitalin_to_digitalversion_re.sub(\"digital version\",text)\n","    text = delimiters_re.sub(\" \", text)  # replace all delimiters with a space\n","    text = nasty_symbols_re.sub(\"\", text)  # delete anything other than letters, numbers, and spaces\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    return text\n","\n","def text_clean_delimited(text):\n","    #text: the original en_item_name\n","    #return: en_item_name made lowercase, stripped of \"really_nasties\" and multiple spaces, in a list of strings that had been separated by one of the above \"delimiters\"\n","    #     NOTE: this is not stripping out all punctuation, and not stripping stopwords... we are first getting a look at commonly found phrases\n","    text = text.lower()\n","    text = d_to_dvd_re.sub(\" dvd\", text)\n","    text = digitalin_to_digitalversion_re.sub(\"digital version\",text)\n","    text = conjunctions_re.sub(\" \", text)         # replace conjunctions + and & with a space\n","    text = really_nasty_symbols_re.sub(\"\", text)  # just delete the nasty symbols\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = delim_pattern_re.split(text)           # split item_name at all delimiters, irrespective of number of spaces before or after the string or delimiter\n","    text = [x for x in text if x]                 # remove empty strings \"\" from the list of split items in text\n","    return text\n","\n","# add a column of simply cleaned text without any undesired punctuation or delimiters\n","items_delimited['clean_item_name'] = items_delimited['item_name'].apply(text_total_clean)\n","\n","# now add a column of lists of delimited (cleaned) text\n","items_delimited['delim_name_list'] = items_delimited['item_name'].apply(text_clean_delimited)\n","\n","# have a look at what we got with our delimited text globs\n","def maxgram(gramlist):\n","    maxg = 0\n","    for g in gramlist:\n","        maxg = max(maxg,len(g.split()))\n","    return maxg\n","items_delimited['d_len'] = items_delimited.delim_name_list.apply(lambda x: len(x))\n","items_delimited['d_maxgram'] = items_delimited.delim_name_list.apply(maxgram)\n","print(items_delimited.head())\n","print(\"\\n\")\n","print(items_delimited.describe())\n","\n","#items_delimited.to_csv(\"data_output/items_delimited.csv\", index=False)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["   item_id  i_cat_id                                                              item_name  i_tested                                                     clean_item_name                                                      delim_name_list  d_len  d_maxgram\n","0        0        40                                           ! POWER IN glamor (PLAST.) D     False                                           power in glamor plast dvd                                        [power in glamor, plast, dvd]      3          3\n","1        1        76  ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]     False   abbyy finereader 12 professional edition full pc digital version   [abbyy finereader 12 professional edition full, pc digital version]      2          6\n","2        2        40                                               *** In the glory (UNV) D     False                                                in the glory unv dvd                                             [in the glory, unv, dvd]      3          3\n","3        3        40                                                 *** BLUE WAVE (Univ) D     False                                                  blue wave univ dvd                                               [blue wave, univ, dvd]      3          2\n","4        4        40                                                      *** BOX (GLASS) D     False                                                       box glass dvd                                                    [box, glass, dvd]      3          1\n","\n","\n","         item_id  i_cat_id  d_len  d_maxgram\n","count      22170     22170  22170      22170\n","mean  11,084.500    46.291  1.977      4.630\n","std    6,400.072    15.941  0.951      2.449\n","min            0         0      1          1\n","25%    5,542.250        37      1          3\n","50%   11,084.500        40      2          4\n","75%   16,626.750        58      3          6\n","max        22169        83     10         19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IUA58xQZ72NC","colab_type":"code","colab":{}},"source":["# make item df easier to read for the following stuff\n","items_clean_delimited = items_delimited.copy(deep=True).drop(\"item_name\", axis=1).rename(columns = {'clean_item_name':'item_name'})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"amkSqIxGJy65","colab_type":"text"},"source":["#####Look at the characteristics of different length n-grams in our delimited set"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"9ad5ff97-c528-42ab-bfae-c0bbc38436b8","executionInfo":{"status":"ok","timestamp":1588813948548,"user_tz":240,"elapsed":5434,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"a4bwEr91S2Wz","colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["# Inspect the delimited 1-grams\n","\n","items_delimited_1gram = items_clean_delimited.copy(deep=True)\n","items_delimited_1gram[\"d_1grams\"] = items_delimited_1gram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == 1]) # column contains all \"delimited\" 1-grams in the translation\n","\n","g1 = items_delimited_1gram.d_1grams.apply(pd.Series,1).stack()\n","g1.index = g1.index.droplevel(-1)\n","g1.name = 'd_1grams'\n","del items_delimited_1gram['d_1grams']\n","items_delimited_1gram = items_delimited_1gram.join(g1)\n","\n","print(items_delimited_1gram.head())\n","print(\"\\n\")\n","freq_1grams = items_delimited_1gram.d_1grams.value_counts()\n","print(f'Number of unique delimited 1-grams: {len(freq_1grams)}')\n","print(f'Number of unique delimited 1-grams that are duplicated at least once: {len(freq_1grams[freq_1grams > 1])}')\n","print(freq_1grams[1:10])  # can ignore index 0, as it is the empty string"],"execution_count":26,"outputs":[{"output_type":"stream","text":["   item_id  i_cat_id  i_tested                                                          item_name                                                      delim_name_list  d_len  d_maxgram d_1grams\n","0        0        40     False                                          power in glamor plast dvd                                        [power in glamor, plast, dvd]      3          3    plast\n","0        0        40     False                                          power in glamor plast dvd                                        [power in glamor, plast, dvd]      3          3      dvd\n","1        1        76     False   abbyy finereader 12 professional edition full pc digital version  [abbyy finereader 12 professional edition full, pc digital version]      2          6      NaN\n","2        2        40     False                                               in the glory unv dvd                                             [in the glory, unv, dvd]      3          3      unv\n","2        2        40     False                                               in the glory unv dvd                                             [in the glory, unv, dvd]      3          3      dvd\n","\n","\n","Number of unique delimited 1-grams: 2300\n","Number of unique delimited 1-grams that are duplicated at least once: 899\n","bd            1706\n","1c             886\n","jewel          818\n","mp3cd          777\n","firms          587\n","digipack       512\n","v              283\n","rem            194\n","essentials     164\n","Name: d_1grams, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aaezqsyDUHca","outputId":"b7c0793d-18c8-490b-eed5-3af292f44140","executionInfo":{"status":"ok","timestamp":1588813959171,"user_tz":240,"elapsed":5246,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["# Inspect the delimited 2-grams\n","\n","items_delimited_2gram = items_clean_delimited.copy(deep=True)\n","items_delimited_2gram[\"d_2grams\"] = items_delimited_2gram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == 2]) # column contains all \"delimited\" 2-grams in the translation\n","\n","g2 = items_delimited_2gram.d_2grams.apply(pd.Series,1).stack()\n","g2.index = g2.index.droplevel(-1)\n","g2.name = 'd_2grams'\n","del items_delimited_2gram['d_2grams']\n","items_delimited_2gram = items_delimited_2gram.join(g2)\n","\n","print(items_delimited_2gram.tail())\n","print(\"\\n\")\n","freq_2grams = items_delimited_2gram.d_2grams.value_counts()\n","print(f'Number of unique delimited 2-grams: {len(freq_2grams)}')\n","print(f'Number of unique delimited 2-grams that are duplicated at least once: {len(freq_2grams[freq_2grams > 1])}')\n","print(freq_2grams[1:8])"],"execution_count":27,"outputs":[{"output_type":"stream","text":["       item_id  i_cat_id  i_tested                                        item_name                                       delim_name_list  d_len  d_maxgram         d_2grams\n","22166    22166        54      True  language 1c queries enterprises digital version   [language 1c queries, enterprises, digital version]      3          3  digital version\n","22167    22167        49      True  1c query language enterprise 8 cd khrustalev ey  [1c query language, enterprise 8, cd, khrustalev ey]      4          3     enterprise 8\n","22167    22167        49      True  1c query language enterprise 8 cd khrustalev ey  [1c query language, enterprise 8, cd, khrustalev ey]      4          3    khrustalev ey\n","22168    22168        62     False                               egg for little inu                                  [egg for little inu]      1          4              NaN\n","22169    22169        69     False                       dragon egg game of thrones                         [dragon egg, game of thrones]      2          3       dragon egg\n","\n","\n","Number of unique delimited 2-grams: 3349\n","Number of unique delimited 2-grams that are duplicated at least once: 943\n","3d bd              213\n","pc jewel           152\n","xbox 360            68\n","dvd bd              62\n","assassins creed     55\n","total war           40\n","500 gb              35\n","Name: d_2grams, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"2226116f-f673-4318-cae7-19a7cdcf0f2f","executionInfo":{"status":"ok","timestamp":1588813973937,"user_tz":240,"elapsed":5345,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"USoJANp3U9Ei","colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["# Inspect the delimited 4-grams\n","\n","items_delimited_4gram = items_clean_delimited.copy(deep=True)\n","items_delimited_4gram[\"d_4grams\"] = items_delimited_4gram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == 4]) # column contains all \"delimited\" 4-grams in the translation\n","\n","g4 = items_delimited_4gram.d_4grams.apply(pd.Series,1).stack()\n","g4.index = g4.index.droplevel(-1)\n","g4.name = 'd_4grams'\n","del items_delimited_4gram['d_4grams']\n","items_delimited_4gram = items_delimited_4gram.join(g4)\n","\n","print(items_delimited_4gram.tail())\n","print(\"\\n\")\n","freq_4grams = items_delimited_4gram.d_4grams.value_counts()\n","print(f'Number of unique delimited 4-grams: {len(freq_4grams)}')\n","print(f'Number of unique delimited 4-grams that are duplicated at least once: {len(freq_4grams[freq_4grams > 1])}')\n","print(freq_4grams[1:12])"],"execution_count":28,"outputs":[{"output_type":"stream","text":["       item_id  i_cat_id  i_tested                                        item_name                                       delim_name_list  d_len  d_maxgram            d_4grams\n","22165    22165        31     False              nuclear titbit 2 pc digital version                [nuclear titbit 2, pc digital version]      2          3                 NaN\n","22166    22166        54      True  language 1c queries enterprises digital version   [language 1c queries, enterprises, digital version]      3          3                 NaN\n","22167    22167        49      True  1c query language enterprise 8 cd khrustalev ey  [1c query language, enterprise 8, cd, khrustalev ey]      4          3                 NaN\n","22168    22168        62     False                               egg for little inu                                  [egg for little inu]      1          4  egg for little inu\n","22169    22169        69     False                       dragon egg game of thrones                         [dragon egg, game of thrones]      2          3                 NaN\n","\n","\n","Number of unique delimited 4-grams: 3392\n","Number of unique delimited 4-grams that are duplicated at least once: 397\n","xbox 360 russian version          161\n","xbox 360 english version          107\n","pc jewel russian subtitles         91\n","xbox 360 russian subtitles         78\n","xbox one russian version           57\n","xbox 360 russian documentation     56\n","xbox one russian subtitles         34\n","only for ps move                   29\n","ps vita russian version            28\n","only for ms kinect                 27\n","with support for 3d                25\n","Name: d_4grams, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"b2d320e7-8723-46ce-f7fa-133322230bf6","executionInfo":{"status":"ok","timestamp":1588814068743,"user_tz":240,"elapsed":85507,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"7H2VxLddVqlJ","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Get all of the delimited n-grams that are duplicated at least once in item names\n","#  range of sizes of delimited phrases (number of 'words'):\n","min_gram = items_delimited.d_maxgram.min()\n","max_gram = items_delimited.d_maxgram.max()\n","\n","gram_freqs = {}   # dict will hold elements that are pd.Series with index = phrase, value = number of repeats in items database item names\n","for n in range(min_gram,max_gram+1):\n","    item_ngram = items_clean_delimited.copy(deep=True)\n","    item_ngram['delim_ngrams'] = item_ngram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == n])\n","\n","    grams = item_ngram.delim_ngrams.apply(pd.Series,1).stack()\n","    grams.index = grams.index.droplevel(-1)\n","    grams.name = 'delim_ngrams'\n","    del item_ngram['delim_ngrams']\n","    item_ngram = item_ngram.join(grams)\n","\n","    freq_grams = item_ngram.delim_ngrams.value_counts()\n","    print(f'Number of unique delimited {n}-grams: {len(freq_grams)}')\n","    grams_dupe = len(freq_grams[freq_grams > 1])\n","    print(f'Number of unique delimited {n}-grams that are duplicated at least once: {grams_dupe}\\n')\n","    if grams_dupe > 0:\n","        gram_freqs[n] = freq_grams.copy(deep=True)\n","print('Done')"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Number of unique delimited 1-grams: 2300\n","Number of unique delimited 1-grams that are duplicated at least once: 899\n","\n","Number of unique delimited 2-grams: 3349\n","Number of unique delimited 2-grams that are duplicated at least once: 943\n","\n","Number of unique delimited 3-grams: 3453\n","Number of unique delimited 3-grams that are duplicated at least once: 617\n","\n","Number of unique delimited 4-grams: 3392\n","Number of unique delimited 4-grams that are duplicated at least once: 397\n","\n","Number of unique delimited 5-grams: 2906\n","Number of unique delimited 5-grams that are duplicated at least once: 235\n","\n","Number of unique delimited 6-grams: 2112\n","Number of unique delimited 6-grams that are duplicated at least once: 110\n","\n","Number of unique delimited 7-grams: 1460\n","Number of unique delimited 7-grams that are duplicated at least once: 84\n","\n","Number of unique delimited 8-grams: 1034\n","Number of unique delimited 8-grams that are duplicated at least once: 40\n","\n","Number of unique delimited 9-grams: 652\n","Number of unique delimited 9-grams that are duplicated at least once: 24\n","\n","Number of unique delimited 10-grams: 406\n","Number of unique delimited 10-grams that are duplicated at least once: 10\n","\n","Number of unique delimited 11-grams: 234\n","Number of unique delimited 11-grams that are duplicated at least once: 5\n","\n","Number of unique delimited 12-grams: 158\n","Number of unique delimited 12-grams that are duplicated at least once: 7\n","\n","Number of unique delimited 13-grams: 96\n","Number of unique delimited 13-grams that are duplicated at least once: 6\n","\n","Number of unique delimited 14-grams: 34\n","Number of unique delimited 14-grams that are duplicated at least once: 1\n","\n","Number of unique delimited 15-grams: 22\n","Number of unique delimited 15-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 16-grams: 10\n","Number of unique delimited 16-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 17-grams: 3\n","Number of unique delimited 17-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 18-grams: 1\n","Number of unique delimited 18-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 19-grams: 2\n","Number of unique delimited 19-grams that are duplicated at least once: 0\n","\n","Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yxeofuYfm3CS","colab_type":"text"},"source":["OK, so the gram_freqs dictionary holds a bunch of informative stuff that will help us group items together.\n","Let's also add every unique word from the item_categories names"]},{"cell_type":"code","metadata":{"id":"QrWd5n4Fnq7R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"340e91fe-e1a8-47cd-f797-33758b62d063","executionInfo":{"status":"ok","timestamp":1588814351585,"user_tz":240,"elapsed":324,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["def cat_name_clean(text):\n","    #text: the original en_cat_name\n","    #return: en_cat_name made lowercase, stripped of punctuation and multiple spaces\n","    #     NOTE: this is not stripping stopwords\n","    text = text.lower()\n","    text = delimiters_re.sub(\" \", text)  # replace all delimiters with a space\n","    text = nasty_symbols_re.sub(\"\", text)  # delete anything other than letters, numbers, and spaces\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = text + \" \"  # serves as a delimiter when we combine rows\n","    return text\n","\n","catwords = set(item_categories_augmented.en_cat_name.apply(cat_name_clean).sum().split(\" \"))\n","print(catwords)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["{'', 'movie', 'certificates', 'additional', 'dvd', 'albums', 'one', 'fiction', 'live', 'robots', 'toys', 'compact', 'mats', 'local', 'and', 'gifts', 'pc', 'android', 'artbook', '8', 'sports', 'movies', 'in', 'bags', 'development', 'of', 'educational', 'bluray', '3d', 'tickets', 'materials', '1c', 'souvenirs', 'carriers', 'firm', 'attributes', 'ps4', 'computer', 'office', 'spire', 'literature', 'cards', 'goods', 'board', 'collectors', 'informative', 'card', 'business', 'postcards', 'services', 'for', 'xbox', 'headphones', 'psvita', 'music', 'encyclopedia', 'headsets', 'net', 'vinyl', 'soft', 'enterprise', 'mouse', 'gift', 'programs', 'batteries', 'tools', 'other', 'audiobooks', 'figure', 'system', 'digital', 'windows', 'cinema', 'delivery', 'program', 'stickers', 'publications', 'manga', '360', 'mp3', '4k', 'production', 'accessories', 'guides', 'ps3', 'travel', 'gadgets', 'edition', 'mac', 'payment', 'ps2', 'numeral', 'collector', 'consoles', 'digits', 'game', 'standard', 'piece', 'video', 'methodical', 'psp', 'books', 'comics', 'figures', 'weighed', 'games', 'utilities', 'cd', 'd', 'psn', 'home'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7TMDgN8xXlUy","colab_type":"text"},"source":["######Consider as special the n-grams that follow certain descriptive keywords such as \"by\" or \"for\" or \"from\" (? TBD)"]},{"cell_type":"code","metadata":{"id":"UJ4sm2JcYAhe","colab_type":"code","colab":{}},"source":["# these are words we could consider to highlight n-grams that follow these words, similar to the thinking that \"delimited\" words may be special\n","key_modifiers = \"by,for,from\"  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_L1gjS_FZbFJ"},"source":["###Continue on the work to vectorize item names and compute similarities between items (this section should be grouped with the above section eventually, but I have it set apart for now, so it is easy to find)\n","\n","---\n","\n","---\n"]},{"cell_type":"code","metadata":{"id":"mi0smegojy0-","colab_type":"code","colab":{}},"source":["# Inspecting the \"delimited\" n-grams, I've considered the following for use in vectorizing our item names\n","\n","# Regarding the use of NLTK stopwords, meh... a lot of these stopwords are irrelevant regarding excess text in our item names... \n","#    We really should make our own stopword list\n","s = \"a,the,an,only,more,are,any,on,your,just,it,its,it's,has,with,for,by,from\"\n","\n","# make some dictionaries to \"lemmatize\" certain keywords into standardized 1-grams\n","# volumes/editions\n","key_descriptors = {'1': 'first,1st,1ed,v1,vol1,volume1,ver1,version1,edition1,original,standard'}\n","key_descriptors['2'] = 'second,2nd,ii,2ed,v2,vol2,volume2,ver2,version2,edition2,updated,revised'\n","key_descriptors['3'] = 'third,3rd,iii,3ed,v3,vol3,volume3,ver3,version3,edition3'\n","key_descriptors['4'] = 'fourth,4th,iiii,4ed,v4,vol4,volume4,ver4,version4,edition4'\n","# special, highlighted items\n","key_descriptors['special'] = 'collector,collection,premium,platinum,gold,silver,black,boxset,classic,trilogy'\n","# items dealing with business\n","key_descriptors['biz'] = 'company,business,enterprise,firm,professional'\n","# items likely to be a Russian version, or translated into Russian\n","key_descriptors['rus'] = 'russian,rus,rusv,translate,translated'\n","# items likely to be in English\n","key_descriptors['eng'] = 'english,eng,engl'\n","# relatively frequent descriptors that should have substantial weight in the item name vectorization\n","key_descriptors['0'] = 'essential,preorder,supplement,addition,catalog,reprint'\n","\n","key_platforms = '1c,1cpublishing,playstation,psp,ps2,ps3,ps4,psvita,psn,pc,android,iphone,phone,tablet,'\n","key_platforms += 'ipad,xbox,xbox 360,xbox one,xbox360,xboxone,xbox1,xone,apple,mac,microsoft,windows,98,live,marvel,cech4008c'\n","\n","key_formats = {'dvd': 'pcdvd,dvd,2dvd,3dvd,4dvd,5dvd,6dvd,dvddigipack,dvdbox,mp3dvd,dvdbook,dvdcd'}\n","key_formats['cd'] = 'cd,mp3cd,2cd,cdbox,compactdisk,audiocd,cddigipack'\n","key_formats['movie'] = 'bd,2bd,3bd,bluray,blueray,4k,movie,dvdmovie,moviedvd,dvdbd,bddvd,2dvdbd,tv'\n","key_formats['0'] = 'book,audiobook,digibook,wb,wd,mp3,digipack,jewel,umdcase,keychain,ac,6cm,8cm,12cm,14cm,dicomp,glass,delivery,online,gb,tb,mb'\n","\n","special_words = 'komplvoprsertekzam1s,softklab,transformer,hobbit,barbie,middleearth,'  # plus any 4-digit numbers like 1812, 2014,...\n","special_words += 'red30,red20,alien,toy,region,bbc,batman,scee,cyrus,mtg,berserk,disney,spell,'\n","special_words += 'buh8,b01,retribution,metodmaterialy,destiny,wolfenstein,hhkata,luntik,'\n","special_words += 'paragon,42,72,buka,injustice,call,action,universal,zhelboks,fallout,cuh1108a,16,cechzc2e,'\n","special_words += 'pch1108za01,dinosaur,braveheart,titanic,infamous,spiderman,armageddon,universe,'\n","special_words += 'rezhversiya,outcast,wheelbarrow,yellow,inquisition,pch1008za01,zolushka,xmen,'\n","special_words += 'metro,battlefront,cuh1208b,wargame,operation,cech3008b,poirot,hhkatv,kitchen,'\n","special_words += 'icecrown,underworld,shepherd,thief,sensor,shrek,oceans,castlevania,murdered,'\n","special_words += 'madagascar,shelter,devastation,deadpool,incredibles,jungle,titanfall,pspe1008,cech2508b,skyrim,avatar,maleficent,invizimals,heracles,phantom,toysmlk,aladdin'\n","\n","#key_categories = 1-grams from all item_categories_augmented (0,1,2,3,4 columns)\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JC-Tlq1Wp2UV","colab_type":"code","colab":{}},"source":["# Get word counts from en_50k for the delimited terms:\n","#   combine all words in delim_name_list, and search one-by-one through en_50k\n","#   create a column for words with zero counts\n","#   create a column for words with 1 to 1000 counts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QqUhu9EmUu-v","colab_type":"code","outputId":"17632705-5524-4942-ded7-b006ae51a54a","executionInfo":{"status":"ok","timestamp":1588789043195,"user_tz":240,"elapsed":106927,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["en50k_word_list = en_50k.word.to_numpy(dtype='str')\n","swfreq = []\n","for sw in special_words.split(\",\"):\n","    sw_50k_rank = np.where(en50k_word_list == sw)[0]\n","    if len(sw_50k_rank) > 0:\n","        swfreq.append(en_50k.at[sw_50k_rank[0],'word_count'])\n","    else:\n","        swfreq.append(0)\n","\n","special_wds_df = pd.DataFrame({\"word\": special_words.split(\",\"), \"counts_50k\": swfreq}).sort_values('counts_50k')\n","print(special_wds_df.head())\n","print(special_wds_df.describe())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["                    word  counts_50k\n","0   komplvoprsertekzam1s           0\n","24           wolfenstein           0\n","25                hhkata           0\n","26                luntik           0\n","28                    42           0\n","       counts_50k\n","count          91\n","mean   10,899.868\n","std    60,340.351\n","min             0\n","25%             0\n","50%           624\n","75%          3637\n","max        572997\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"268aa003-ceb1-4289-91e8-44fc39c8b358","executionInfo":{"status":"ok","timestamp":1588789203625,"user_tz":240,"elapsed":480,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"aZcys-YW_4dW","colab":{"base_uri":"https://localhost:8080/","height":901}},"source":["# inspect item names more closely to see how they relate to the rare delimited words, and to \n","#    help understand what certain unusual terms mean \n","# \"firms\" = big record label \n","# \"figure\" = digital version; not hardcopy\n","#  komplvoprsertekzam1s = Complete Set of Certification Exam. 1C\n","\n","# enter the desired item_ids here:\n","to_plot = [617,618]\n","\n","item_plot_df = sales_train[sales_train.item_id.isin(to_plot)] \n","print(item_plot_df.head())\n","\n","# what does \"figure\" mean in category name? --> DIGITAL VERSION (as opposed to hardcopy)\n","# whereas, \"Digital Version\" in item name actually seems to indicate it is downloaded rather than obtained from DVD, for example\n","\n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([77,78])][['item_id','en_item_name','item_category_id','category']] \n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([43,44])][['item_id','en_item_name','item_category_id','category']] \n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([75,76])][['item_id','en_item_name','item_category_id','category']] \n","fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([6])][['item_id','en_item_name','item_category_id','category']] \n","print(fig_cat_df.tail())\n","\n","# see what some of these 'rare' word 1-grams are:\n","# get item names from rare words\n","rare_words = 'cech2508b,komplvoprsertekzam1s'\n","#reg_pattern = re.compile('|'.join(rare_words.split(',')), flags=re.IGNORECASE)\n","any_rare_words = '|'.join(rare_words.split(','))\n","rare_items_df = items_delimited[items_delimited.clean_item_name.str.contains(any_rare_words, case=False, na=False)][['item_id','i_cat_id','i_tested','item_name','clean_item_name']] \n","print(rare_items_df.head(50))\n","print(items[items['item_id']==13380])\n","\n","# \"certification exam\" is not present in the cleaned, translated item names, nor is \"certification\"\n","# \"exam \" is present in only 26 items... so, komplvoprsertekzam1s is broader than 'exam'\n","certification = items_delimited[items_delimited.clean_item_name.str.contains('exam ', case=False, na=False)][['item_id','i_cat_id','i_tested','item_name','clean_item_name']] \n","print(len(certification))\n","print(certification.head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","505355  2013-05-07               4       55      618         172             1\n","933109  2013-09-27               8       55      618         172             1\n","933124  2013-09-06               8       55      618         172             1\n","1088977 2013-11-24              10       55      618         172             1\n","1088978 2013-11-03              10       55      618         172             1\n","       item_id                                                                        en_item_name  item_category_id                category\n","8448      8448                                   Accessory: Xbox 360 Hard Drive 500 GB (6FM-00003)                 6  Accessories - XBOX 360\n","8449      8449                  Accessory: Xbox 360 Wireless Controller Chrome magenta (43G-00062)                 6  Accessories - XBOX 360\n","8450      8450                    Accessory: Xbox 360 Wireless Controller Black Chrome (43G-00059)                 6  Accessories - XBOX 360\n","11305    11305  Holder ARTPLAYS Camera Clip 2 to 1 to Kinect / PS3 camera sensor (SR-70102), black                 6  Accessories - XBOX 360\n","11306    11306                             Holder for Xbox360 Kinect and PS3 Move Camera (HHC-008)                 6  Accessories - XBOX 360\n","       item_id  i_cat_id  i_tested                                                                                        item_name                                                                 clean_item_name\n","13367    13367        49      True                          Kompl.vopr.sert.ekzam.1S: ERP Enterprise Management 2.0 (1C Publishing)                 komplvoprsertekzam1s erp enterprise management 20 1c publishing\n","13368    13368        49     False                Kompl.vopr.sert.ekzam.1S: Buch. state. institutions (August 2011) (1C-Publishing)           komplvoprsertekzam1s buch state institutions august 2011 1cpublishing\n","13369    13369        49      True         Kompl.vopr.sert.ekzam.1S: Buch. state. ed institutions. 2.0 (April 2014) (1C-Publishing)      komplvoprsertekzam1s buch state ed institutions 20 april 2014 1cpublishing\n","13370    13370        49      True                     Kompl.vopr.sert.ekzam.1S: Accounting 8 (red.3.0), March 2013 (1C Publishing)                komplvoprsertekzam1s accounting 8 red30 march 2013 1c publishing\n","13371    13371        49     False                         Kompl.vopr.sert.ekzam.1S: Accounting for Belarus (1.6 ed.), October 2011                  komplvoprsertekzam1s accounting for belarus 16 ed october 2011\n","13372    13372        49     False                                  Kompl.vopr.sert.ekzam.1S: Budgetary reporting 8 (February 2011)                        komplvoprsertekzam1s budgetary reporting 8 february 2011\n","13373    13373        49     False                      Kompl.vopr.sert.ekzam.1S: Document 8 (red.1.2), August 2013 (1C Publishing)                 komplvoprsertekzam1s document 8 red12 august 2013 1c publishing\n","13374    13374        49     False                      Kompl.vopr.sert.ekzam.1S: Document 8 (red.1.3), August 2014 (1C Publishing)                 komplvoprsertekzam1s document 8 red13 august 2014 1c publishing\n","13375    13375        49      True                   Kompl.vopr.sert.ekzam.1S: Document 8 (red.2.0), September 2015 (1C-Publishing)               komplvoprsertekzam1s document 8 red20 september 2015 1cpublishing\n","13376    13376        49     False                                  Kompl.vopr.sert.ekzam.1S: Document 8, June 2011 (1C Publishing)                         komplvoprsertekzam1s document 8 june 2011 1c publishing\n","13377    13377        49     False                       Kompl.vopr.sert.ekzam.1S: Salary and Personnel budget entity 8, April 2010            komplvoprsertekzam1s salary and personnel budget entity 8 april 2010\n","13378    13378        49     False                     Kompl.vopr.sert.ekzam.1S: Salary and Personnel budget entity 8 November 2013         komplvoprsertekzam1s salary and personnel budget entity 8 november 2013\n","13379    13379        49      True              Kompl.vopr.sert.ekzam.1S: Salary and Personnel state. institutions 8, February 2015    komplvoprsertekzam1s salary and personnel state institutions 8 february 2015\n","13380    13380        49     False          Kompl.vopr.sert.ekzam.1S: Salary and upr.pers.8 (red.2.5), January 2010 (1C Publishing)       komplvoprsertekzam1s salary and uprpers8 red25 january 2010 1c publishing\n","13381    13381        49      True          Kompl.vopr.sert.ekzam.1S: Salary and upr.pers.8 (red.3.0), October 2014 (1C Publishing)       komplvoprsertekzam1s salary and uprpers8 red30 october 2014 1c publishing\n","13382    13382        49     False                                              Kompl.vopr.sert.ekzam.1S: Consolidation 8 Feb. 2009                                   komplvoprsertekzam1s consolidation 8 feb 2009\n","13383    13383        49      True                                           Kompl.vopr.sert.ekzam.1S: Pr.8.UPP (red.1.3), May 2012                                      komplvoprsertekzam1s pr8upp red13 may 2012\n","13384    13384        49     False                 Kompl.vopr.sert.ekzam.1S: Pr.8.Upr.torg. (Red.11, February 2011) (1C-Publishing)                komplvoprsertekzam1s pr8uprtorg red11 february 2011 1cpublishing\n","13385    13385        49      True               Kompl.vopr.sert.ekzam.1S: Pr.8.Upr.torg. (Red.11.1, December 2013) (1C-Publishing)               komplvoprsertekzam1s pr8uprtorg red111 december 2013 1cpublishing\n","13386    13386        49     False                      Kompl.vopr.sert.ekzam.1S: Pr.8.Upravlenie construction company (April 2008)              komplvoprsertekzam1s pr8upravlenie construction company april 2008\n","13387    13387        49      True                      Kompl.vopr.sert.ekzam.1S: Professional on technology issues (1C Publishing)            komplvoprsertekzam1s professional on technology issues 1c publishing\n","13388    13388        49      True                               Kompl.vopr.sert.ekzam.1S: Retail 8 September 2011. (1S-Publishing)                       komplvoprsertekzam1s retail 8 september 2011 1spublishing\n","13389    13389        49     False                       Kompl.vopr.sert.ekzam.1S: Managing a small firm 8 (red.1.4), November 2012                komplvoprsertekzam1s managing a small firm 8 red14 november 2012\n","13390    13390        49      True                        Kompl.vopr.sert.ekzam.1S: Managing a small firm 8 (red.1.5), January 2015                 komplvoprsertekzam1s managing a small firm 8 red15 january 2015\n","13391    13391        49     False                     Kompl.vopr.sert.ekzam.1S: Managing a small firm 8, June 2011 (1C Publishing)            komplvoprsertekzam1s managing a small firm 8 june 2011 1c publishing\n","13468    13468        11     False                                     Kit «Sony PS3 (320 GB) (CECH-2508B)» + game «Battlefield 3 '                               kit sony ps3 320 gb cech2508b game battlefield 3 \n","13469    13469        11     False  Kit «Sony PS3 (320 GB) (CECH-2508B)» + game «God of War 3 (Platinum)» + game «Uncharted 2 (Plat  kit sony ps3 320 gb cech2508b game god of war 3 platinum game uncharted 2 plat\n","13474    13474        11     False                                            Kit «Sony PS3 (320 Gb) (CECH-2508B) + game« FIFA 12 \"                                     kit sony ps3 320 gb cech2508b game fifa 12 \n","13475    13475        11     False                                       Kit «Sony PS3 (320 Gb) (CECH-2508B) + game« Resistance 3 '                                kit sony ps3 320 gb cech2508b game resistance 3 \n","13476    13476        11     False                                         Kit «Sony PS3 (320 Gb) (CECH-2508B)» + game \"inFamous 2\"                                   kit sony ps3 320 gb cech2508b game infamous 2\n","                                                                                  item_name  item_id  item_category_id\n","13380  Компл.вопр.серт.экзам.1С:Зарплата и упр.перс.8 (ред.2.5), январь 2010 (1С-Паблишинг)    13380                49\n","26\n","     item_id  i_cat_id  i_tested                                                          item_name                                              clean_item_name\n","810      810        77     False                  1C: tutor. Maths. We pass the exam (2013) (Jewel)                   1c tutor maths we pass the exam 2013 jewel\n","813      813        77     False      1C: tutor. Russian language. We pass the exam in 2012 (Jewel)     1c tutor russian language we pass the exam in 2012 jewel\n","814      814        77     False      1C: tutor. Russian language. We pass the exam in 2013 (Jewel)     1c tutor russian language we pass the exam in 2013 jewel\n","815      815        78     False  1C: tutor. We pass the exam 2010. Chemistry [PC, Digital Version]  1c tutor we pass the exam 2010 chemistry pc digital version\n","816      816        77     False          1C: tutor. We pass the exam in mathematics (2013) (Jewel)          1c tutor we pass the exam in mathematics 2013 jewel\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UVtotq7VK0nB","colab_type":"text"},"source":["######Items containing \"1C\" in their name"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"c21d970c-17ed-40fe-a934-6f49c2ca648f","executionInfo":{"status":"ok","timestamp":1588784840405,"user_tz":240,"elapsed":303,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"hXM-ICo7FPGn","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# I'm curious about items containing \"1C\" ... there seem to be quite a few, and if I'm not mistaken,\n","#    1C is an ERP like Oracle or SAP, but 1C is based in Russia.  Let's see how many items have 1C\n","#    in their item_name, and what categories these belong to, and what the item names look like (what are common words that accompany \"1C\"?)\n","items_delimited_1c = items_clean_delimited.copy(deep=True)\n","# delete the wide \"item_name\" column so we can read more of the data table width-wise\n","items_delimited_1c = items_delimited_1c.drop(['delim_name_list','d_len','d_maxgram'], axis=1)\n","items_delimited_1c.head()\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>i_cat_id</th>\n","      <th>i_tested</th>\n","      <th>item_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>power in glamor plast dvd</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>False</td>\n","      <td>abbyy finereader 12 professional edition full pc digital version</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>in the glory unv dvd</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>blue wave univ dvd</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>box glass dvd</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id  i_cat_id  i_tested                                                          item_name\n","0        0        40     False                                          power in glamor plast dvd\n","1        1        76     False   abbyy finereader 12 professional edition full pc digital version\n","2        2        40     False                                               in the glory unv dvd\n","3        3        40     False                                                 blue wave univ dvd\n","4        4        40     False                                                      box glass dvd"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xUIfCz4U4sVF"},"source":["#XX) Below this markdown cell can be ignored\n","\n","These are just code snippets I used to debug how to accomplish some of the tasks above.  I may want to revisit them some day, so I am too scared to just delete them. :)"]},{"cell_type":"markdown","metadata":{"id":"gKW9VvCoZakg","colab_type":"text"},"source":["Below is a code snippet to plot item sales vs. month for one or more item ids"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"cbf26d54-1fae-4612-c658-b555cf2fcfef","executionInfo":{"status":"ok","timestamp":1588785157671,"user_tz":240,"elapsed":451,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"GDca22GLZYlD","colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["# enter the desired item_ids here:\n","to_plot = [617,618]\n","\n","item_plot_df = sales_train[sales_train.item_id.isin(to_plot)] \n","print(item_plot_df.head())\n","\n","# what does \"figure\" mean in category name?\n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([77,78])][['item_id','en_item_name','item_category_id','category']] \n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([43,44])][['item_id','en_item_name','item_category_id','category']] \n","#fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([75,76])][['item_id','en_item_name','item_category_id','category']] \n","fig_cat_df = items_transl_with_cat[items_transl_with_cat.item_category_id.isin([6])][['item_id','en_item_name','item_category_id','category']] \n","print(fig_cat_df.tail())\n","\n","# get item names from rare words\n","rare_words = 'pch1008za01' #,hhkatv,pspe1008,cech2508b,komplvoprsertekzam1s'\n","#reg_pattern = re.compile('|'.join(rare_words.split(',')), flags=re.IGNORECASE)\n","any_rare_words = '|'.join(rare_words.split(','))\n","rare_items_df = items_clean_delimited[items_clean_delimited.item_name.str.contains(any_rare_words, case=False, na=False)][['item_id','i_cat_id','i_tested','item_name']] \n","print(rare_items_df.head(50))\n","'''\n","\n","# seems to be one particular bad player... item 2973 from shop 6 in row 484683\n","#   look more at this item, shop, and item-shop combo:\n","print(\"\\n\")\n","print(2973 in test.item_id.to_list())\n","# this item is not in the test set... does it make up a significant amount of the train set?\n","\n","print(f\"Shop 32 = {shops_augmented.at[32,'en_shop_name']}\", end=\"\")\n","print(f\", Item 2973 = {items_transl.at[2973,'en_item_name']}\\n\")\n","\n","item2973 = sales_train[sales_train.item_id == 2973]\n","print(item2973.describe())\n","print(\"\\n\")\n","print(item2973.sort_values('item_price').head(10))\n","# only one sales_train row entry with price < 1000, and it is this negative outlier\n","\n","print(\"\\n\")\n","item2973shop32 = item2973[item2973.shop_id == 32]\n","print(item2973shop32.describe())\n","print(\"\\n\")\n","print(item2973shop32.sort_values('date').head(15))\n","# it looks like perhaps this shop had a discount clearance sale in summer 2013, and\n","#  then never sold the item again\n","\n","plt.rcParams[\"figure.figsize\"] = [16,5]\n","fig = plt.figure() #figsize=(16,9))\n","#item2973.sort_values('date_block_num')['date_block_num'].value_counts().plot(kind='bar')\n","item2973.date_block_num.value_counts().reset_index().drop('index',axis=1).rename(columns={'date_block_num':'n_train_rows'}).plot(kind='bar')\n","plt.title('by MONTH (date_block_num), number of rows in sales_train for item 2973')\n","plt.grid(b=True, which='major', axis='y', color='#666666', linestyle='-')\n","\n","plt.show()\n","\n","# looks like this item has little bearing on sales in Nov. 2015, as its sales\n","#  died off (among all shops) by January 2015\n","# and, the one entry at row 484683 in sales_train with the negative price can be safely deleted\n","'''\n","a=1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","505355  2013-05-07               4       55      618         172             1\n","933109  2013-09-27               8       55      618         172             1\n","933124  2013-09-06               8       55      618         172             1\n","1088977 2013-11-24              10       55      618         172             1\n","1088978 2013-11-03              10       55      618         172             1\n","       item_id                                                                        en_item_name  item_category_id                category\n","8448      8448                                   Accessory: Xbox 360 Hard Drive 500 GB (6FM-00003)                 6  Accessories - XBOX 360\n","8449      8449                  Accessory: Xbox 360 Wireless Controller Chrome magenta (43G-00062)                 6  Accessories - XBOX 360\n","8450      8450                    Accessory: Xbox 360 Wireless Controller Black Chrome (43G-00059)                 6  Accessories - XBOX 360\n","11305    11305  Holder ARTPLAYS Camera Clip 2 to 1 to Kinect / PS3 camera sensor (SR-70102), black                 6  Accessories - XBOX 360\n","11306    11306                             Holder for Xbox360 Kinect and PS3 Move Camera (HHC-008)                 6  Accessories - XBOX 360\n","       item_id  i_cat_id  i_tested                                                                                    item_name\n","6665      6665        14     False                                                      sony ps vita wifi black rus pch1008za01\n","13429    13429        14     False     kit sony ps vita wifi black rus pch1008za01 psn activation code assassins creed released\n","13430    13430        14     False   kit sony ps vita wifi black rus pch1008za01 psn activation code call of duty black ops dec\n","13431    13431        14     False  kit sony ps vita wifi black rus pch1008za01 psn activation code littlebigplanet memory card\n","13432    13432        14     False       kit sony ps vita wifi black rus pch1008za01 4gb memory card cod black ops declassified\n","13467    13467        14     False        kit sony ps vita wifi black rus pch1008za01 memory card 16 gb disney mega pack psn to\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hMwVqnoC_aje","colab_type":"text"},"source":["######Below here are some code snippets for future reference; ignore please\n","\n","</br>\n","\n","</br>\n","\n","---\n","---\n","\n","</br>\n","\n","</br>\n"]},{"cell_type":"code","metadata":{"id":"bY5u83Wmw5-m","colab_type":"code","colab":{}},"source":["'''\n","REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","MULTIPLE_WHITESPACE_RE = re.compile('[ ]{2,}')\n","STOPWORDS = set(stopwords.words('english'))  #using \"set\" speeds things up a little; note all stopwords are in lowercase\n","#print(\".\" in STOPWORDS)\n","def text_prepare(text):\n","    \"\"\"\n","        text: a string\n","        \n","        return: modified initial string\n","    \"\"\"\n","    text = text.lower() # lowercase text... need to do this before removing stopwords because stopwords are all lowercase\n","    text = REPLACE_BY_SPACE_RE.sub(' ',text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n","    text = BAD_SYMBOLS_RE.sub('',text) # delete symbols which are in BAD_SYMBOLS_RE from text\n","    text = MULTIPLE_WHITESPACE_RE.sub(' ',text)\n","    text = \" \".join([word for word in text.split(\" \") if word not in stopwords.words('english')]) # delete stopwords from text\n"," \n","    return text\n","'''\n","# keep this cell for future reference\n","keep = True"],"execution_count":0,"outputs":[]}]}