{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nlp_clustering_item_names_v1_june2020_mg.ipynb","provenance":[{"file_id":"1vW7OzJ1sZPtkXjCk2C6euLEfuY3leJLV","timestamp":1590764973097},{"file_id":"1XyM72BkhI503rOTsWAWIDyYyR-Gi1HWH","timestamp":1590658110496},{"file_id":"14t_SkT4SYL-JAcrbJIJ60cbNgCrJlvIN","timestamp":1589069755220},{"file_id":"1mFtJLElc2hyopq6yrPAoi0zQVUegsAP5","timestamp":1589018631041},{"file_id":"1y04qp_hoyBnsJQwkX67pk4iZsKGQIqNy","timestamp":1588805435380},{"file_id":"1b_K0QD9U6dofQ7VtTAtzUrqbKJMdj64l","timestamp":1588785261238},{"file_id":"1gcbeu-d1GUUzznZwTzfqYYbaD6cJ7EQ4","timestamp":1588238522691},{"file_id":"1pSGNRDJGzdeI69bw1zWefzPifBq-rv9H","timestamp":1588151557805},{"file_id":"1hq-ivO1BBtc5IC5xd-JdH8HNAQ81bkRf","timestamp":1587386702728},{"file_id":"1I7DWo2B7q7g9Ne2khD11YGTq_gD2FoaT","timestamp":1587321559573},{"file_id":"1fyZv-jgb8twsCBQwPxjgk_XSYA6dOa2t","timestamp":1587303588700},{"file_id":"1iKsplqpLQQZqdr3Trflk7TapksgkQXjX","timestamp":1587145642564},{"file_id":"https://github.com/migai/Kag/blob/master/Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587076517706}],"collapsed_sections":["ruw_WyRxhqpx","iiCw8FkBFStA","FX9bOxkfr55N","s0J5l5H98Xsh","lZfgOx-0_KXg","V7QY11R1QmlN","KvzvPqmCQ2ZM","Uvreedy4YRVd","_B2ZZPQigJmh","2fBOqdalFBx1"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn","colab_type":"text"},"source":["#**EDA and Feature Generation**\n","## shops, items, item_categories\n","\n","Andreas Theodoulou and Michael Gaidis (June, 2020)"]},{"cell_type":"markdown","metadata":{"id":"hPrPvh7sorJd","colab_type":"text"},"source":["#**0. Mount Google Drive (Local File Storage/Repo For Colab)**"]},{"cell_type":"code","metadata":{"id":"CUIE1PVjSAmg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1592483269209,"user_tz":240,"elapsed":24950,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"715bd3b4-bd63-4b8f-ab0e-6d123272a035"},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, then paste it into the input box and hit 'enter' to complete mounting of the drive\n","from google.colab import drive  \n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ruw_WyRxhqpx"},"source":["#**1. Configure Environment and Load Data Files**"]},{"cell_type":"code","metadata":{"id":"sTVAxnMnenrB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1592483272768,"user_tz":240,"elapsed":28473,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"d70b54bc-4717-4df9-e3b4-821f052c2b58"},"source":["# python libraries/modules used throughout this notebook (with some holdovers from other, similar notebooks)\n","'''\n","NOTE: selecting a group of code lines and pressing ctrl-/ will toggle commenting of the code lines, for fast and easy disabling/enabling of stuff\n","'''\n","\n","# pandas data(database) storage, EDA, and manipulation\n","import pandas as pd\n","### pandas formatting\n","### Adjust as per your preferences.  Here's what I find works well when using a FHD monitor with a full-screen browser window containing my IPynb notebook:\n","# pd.set_option(\"display.max_rows\",100)     # Override pandas choice of how many rows to show, so we can see the full 84-row item_category df instead of '...' in the middle\n","# pd.set_option(\"display.max_columns\",30)   # Similar to row code above, we can show more columns than default\n","# pd.set_option(\"display.width\", 250)       # Tune this to our monitor window size to avoid horiz scroll bars in output windows (but, the drawback is that we will get output text wrapping)\n","# pd.set_option(\"max_colwidth\", None)       # This is done, for example, so we can see full item name and not '...' in the middle\n","### Here's what I find works well for this particular IPynb, when using a FHD laptop monitor with a full-screen browser window containing my IPynb notebook:\n","pd.set_option(\"display.max_rows\",120)     # Override pandas choice of how many rows to show, so, for example, we can see the full 84-row item_category dataframe instead of the first few rows, then ...., then the last few rows\n","pd.set_option(\"display.max_columns\",26)   # Similar to row code above, we can show more columns than default  \n","pd.set_option(\"display.width\", 230)       # Tune this to our monitor window size to avoid horiz scroll bars in output windows (but, the drawback is that we will get output text wrapping)\n","pd.set_option(\"max_colwidth\", None)       # This is done, for example, so we can see full item name and not '...' in the middle\n","\n","# pd.set_option(\"display.precision\", 3)  # Nah, this is helpful, but below is even better\n","# Try to convince pandas to print without decimal places if a number is actually an integer (helps keep column width down, and highlights data types), or with precision = 3 decimals if a float\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)\n","\n","# Pandas additional enhancements\n","pd.set_option('compute.use_bottleneck', False)  # speed up operation when using NaNs\n","pd.set_option('compute.use_numexpr', False)     # speed up boolean operations, large dataframes; DataFrame.query() and pandas.eval() will evaluate the subexpressions that can be evaluated by numexpr\n","\n","\n","# data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# ipynb magic command to allow interactive matplotlib graphics in ipynb notebook\n","%matplotlib inline  \n","# a useful reference of contrasting color choices to use when plotting multiple things on a single axis\n","pltcolors = ['blue','red','green','black','darkorange','fuchsia','teal','gold','violet','olive','firebrick','gray','cyan','sienna','dodgerblue','lime','darkorchid','deeppink','turquoise','tan']\n","from matplotlib.ticker import MultipleLocator, FormatStrFormatter, AutoMinorLocator\n","from IPython.display import Javascript      # used to properly code the creation of sns heatmaps in IPynb with Google Colab\n","from IPython.display import display_html    # used to print out side-by-side dataframes, for example\n","\n","# computations\n","import numpy as np\n","from scipy import sparse\n","from numba import jit, njit, prange  # speedup for appropriate functions and datatypes (no sets, lists, dictionaries, string functions; use np arrays rather than pandas series or dataframes)\n","#  If you want Numba to throw an error if it cannot compile a function in a way that speeds up your code, pass the argument nopython=True (e.g. @jit(nopython=True))\n","from numba import vectorize  # speed up row-wise operations like .apply() --> https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html\n","# can also do np.vectorize (see pandas 1.0.3 documentation section on enhancing speed of pandas operations)\n","\n","# file operations\n","import os\n","import feather   # this is 3x to 8x faster than pd.read_csv and pd.to_hdf, but file size is 2x hdf and 10x csv.gz\n","import pickle\n","import json\n","from urllib.parse import urlunparse\n","from pathlib import Path\n","\n","# misc. python enhancements\n","# note: for a quick look at what's available for magic commands in this ipynb, enter this into a code cell: '%quickref'\n","import re\n","import string\n","from itertools import product\n","from collections import OrderedDict\n","import time\n","import datetime\n","from time import sleep, localtime, strftime, tzset, strptime\n","os.environ['TZ'] = 'EST+05EDT,M4.1.0,M10.5.0'   # allows user to simply print a formatted version of the local date and time; helps keep track of what cells were run, and when\n","tzset()\n","\n","\n","# Specialized packages\n","# -- for network analysis / graphs / clustering (a reasonable alternative to pca, tSNE, or Knn clustering when number of dimensions is huge)\n","import networkx as nx\n","from networkx.algorithms import community, cluster\n","# -- NLP packages ... for now, as of 5/29/20, only using the lemmatizer, due to the \n","import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer \n","lemmatizer = WordNetLemmatizer() \n","\n","# ML packages\n","import sklearn\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","# from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","# !pip install catboost\n","# from catboost import CatBoostRegressor\n","# %tensorflow_version 2.x\n","# import tensorflow as tf\n","# import keras as K\n","\n","# # List of the modules we need to version-track for reference\n","modules = ['pandas','matplotlib','numpy','scipy','numba','seaborn','sklearn','tensorflow','keras','catboost','pip','nltk','networkx']\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","done: Thu 08:27:52 06/18/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p9vsd3EynZLO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":969},"executionInfo":{"status":"ok","timestamp":1592483284577,"user_tz":240,"elapsed":40268,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"0aa3dc02-2865-4c74-abfb-4a5ebc93ceb5"},"source":["#  Except for fast-loading (large filesize) feather format files, \n","#   the data is coming from a public repo on GitHub at github.com/migai/Kag that has been synced to my local repo on Google Drive\n","\n","'''\n","############################################################\n","############################################################\n","'''\n","# Replace this path with the path on *your* Google Drive where the repo master branch is stored\n","#   (on GitHub, the remote repo is located at github.com/migai/Kag --> below is my cloned repo location)\n","GDRIVE_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\"\n","OUT_OF_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final\"   # place > 100MB files here, because they won't sync with GitHub\n","\n","traintest_loaded = True   # set this to True if you plan to load the .ftr or the .csv.gz version of the traintest dataframe, and skip the calculations below that generated it\n","ftr_file_load_employed = True #False #True  # set to True if you wish to load the .ftr version or the .csv.gz version... it's faster, but its a 10x larger file, and won't work in the GitHub repo push\n","\n","\n","# if using large feather file for fast loading, use the routine here\n","#   note that this is too large to push to GitHub, so if you want to go this route, \n","#   you'll first have to load (more slowly) the 'data_output/traintest.csv.gz' file \n","#   with pandas read_csv, and then store the file as feather type (outside your local GitHub repo)\n","#   Or, you can just recreate the dataframe by running the first few code cells that do merging and data manipulation\n","# load feather files manually for now\n","if (traintest_loaded and ftr_file_load_employed):\n","    print('ftr files source directory: ', end='')\n","    %cd \"{OUT_OF_REPO_PATH}\"\n","    traintest = pd.read_feather('traintest.ftr', columns=None, use_threads=True)\n","    print(\"Loading ftr Files from Google Drive (outside repo) into Colab... \\n\\nData Frame: traintest (from ftr)\")\n","    print(traintest.head(2))\n","\n","'''\n","############################################################\n","############################################################\n","'''\n","\n","data_files = []\n","# List of the data files (path relative to GitHub master), to be loaded into pandas DataFrames\n","if (traintest_loaded and not ftr_file_load_employed):\n","    data_files = [ \"data_output/traintest.csv.gz\" ]\n","                \n","data_files += [  #\"readonly/final_project_data/shops.csv\",\n","                #\"data_output/shops_transl.csv\",\n","                \"data_output/shops_augmented.csv\",\n","                \"data_output/shops_new.csv\",\n","               \n","                #\"readonly/final_project_data/items.csv\",\n","                #\"data_output/items_transl.csv\",\n","                \"data_output/items_augmented.csv\",\n","                #\"data_output/items_new.csv\",\n","                #\"data_output/items_clustered_22170.csv.gz\",\n","               \n","                #\"readonly/final_project_data/item_categories.csv\",\n","                #\"data_output/item_categories_transl.csv\",\n","                \"data_output/item_categories_augmented.csv\",\n","                #\"readonly/en_50k.csv\",\n","               \n","                \"readonly/final_project_data/sales_train.csv.gz\",\n","                #\"data_output/sales_train_cleaned.csv.gz\",\n","               \n","                #\"readonly/final_project_data/sample_submission.csv.gz\",\n","                \"readonly/final_project_data/test.csv.gz\"\n","                ]\n","\n","\n","# Dict of helper code files, to be loaded and imported {filepath : import_as}\n","code_files = {}  # not used at this time; example dict = {\"helper_code/kaggle_utils_at_mg.py\" : \"kag_utils\"}\n","\n","\n","# GitHub file location info\n","git_hub_url = \"https://raw.githubusercontent.com/migai\"\n","repo_name = 'Kag'\n","branch_name = 'master'\n","base_url = os.path.join(git_hub_url, repo_name, branch_name)\n","\n","if data_files:\n","    print('\\n\\ncsv files source directory: ', end='')\n","    %cd \"{GDRIVE_REPO_PATH}\"\n","\n","    print(\"\\nLoading csv Files from Google Drive repo into Colab...\\n\")\n","\n","    # Loop to load the data files into appropriately-named pandas DataFrames\n","    for path_name in data_files:\n","        filename = path_name.rsplit(\"/\")[-1]\n","        data_frame_name = filename.split(\".\")[0]\n","        exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","        # if data_frame_name == 'sales_train':\n","        #     sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","        print(f'DataFrame {data_frame_name}, shape = {eval(data_frame_name).shape} :')\n","        print(eval(data_frame_name).head(2))\n","        print(\"\\n\")\n","else: \n","    %cd \"{GDRIVE_REPO_PATH}\"\n","    \n","print(f'\\nDataFrame Loading Complete: {strftime(\"%a %X %x\")}\\n')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["ftr files source directory: /content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final\n","Loading ftr Files from Google Drive (outside repo) into Colab... \n","\n","Data Frame: traintest (from ftr)\n","   day  DoW  DoM  week  qtr  season  month  price  sales  shop_id  item_id                                  item_name  it_test  item_category_id   item_category_name  it_cat_test item_cat3 item_cat4         shop_name sh_cat  \\\n","0    0  Tue    1     0    0       2      0     99      1        2      991    3d action puzzle dinosaur tyrannosaurus    False                67  Gifts - Development         True     Gifts     Gifts  Adygea TC \"Mega\"   Mega   \n","1    0  Tue    1     0    0       2      0   2599      1        2     1472  assassin creed 3 xbox 360 russian version    False                23     Games - XBOX 360         True     Games      Xbox  Adygea TC \"Mega\"   Mega   \n","\n","   sh_test district    city  \n","0     True    South  Adygea  \n","1     True    South  Adygea  \n","\n","\n","csv files source directory: /content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","\n","Loading csv Files from Google Drive repo into Colab...\n","\n","DataFrame shops_augmented, shape = (60, 8) :\n","                       shop_name  shop_id                       en_shop_name shop_city shop_category shop_federal_district  shop_city_population  shop_tested\n","0  !Якутск Орджоникидзе, 56 фран        0  ! Yakutsk Ordzhonikidze, 56 Franc   Yakutsk          Shop               Eastern                235600        False\n","1  !Якутск ТЦ \"Центральный\" фран        1       ! Yakutsk TC \"Central\" Franc   Yakutsk          Mall               Eastern                235600        False\n","\n","\n","DataFrame shops_new, shape = (60, 14) :\n","   shop_id  shop_tested shop_type  shop_type_enc shop_city  shop_city_enc shop_federal_district  shop_federal_district_enc s_type_broad  s_type_broad_enc fd_popdens  fd_popdens_enc        fd_gdp  fd_gdp_enc\n","0        0        False      Shop             20   Yakutsk             54               Eastern                         16         Shop                10     Remote               5  Intermediate          43\n","1        1        False      Mall             50   Yakutsk             54               Eastern                         16         Mall                60     Remote               5  Intermediate          43\n","\n","\n","DataFrame items_augmented, shape = (22170, 5) :\n","   item_id                                                         item_name  item_tested  item_category_id                                                   orig_eng_name_transl\n","0        0                                         power in glamor plast dvd        False                40                                           ! POWER IN glamor (PLAST.) D\n","1        1  abbyy finereader 12 professional edition full pc digital version        False                76  ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]\n","\n","\n","DataFrame item_categories_augmented, shape = (84, 8) :\n","        item_category_name  item_category_id                 en_cat_name item_category1 item_category2 item_category3 item_category4  item_cat_tested\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones          Audio             PC    Accessories             PC             True\n","1         Аксессуары - PS2                 1           Accessories - PS2    Accessories    PlayStation    Accessories    PlayStation            False\n","\n","\n","DataFrame sales_train, shape = (2935849, 6) :\n","         date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0  02.01.2013               0       59    22154         999             1\n","1  03.01.2013               0       25     2552         899             1\n","\n","\n","DataFrame test, shape = (214200, 3) :\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","\n","\n","\n","DataFrame Loading Complete: Thu 08:28:04 06/18/20\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iiCw8FkBFStA","colab_type":"text"},"source":["#**2. Merge data sets and create day, week, quarter, and season feature columns**"]},{"cell_type":"code","metadata":{"id":"YY7-Idw86ST2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483284760,"user_tz":240,"elapsed":40439,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"eb24bcb8-8723-4cac-98b9-30cf7b1ed47e"},"source":["def clean_merge_augment(day0 = datetime.datetime(2013,1,1),\n","                        delete_rows = [2909818,2909401,2326930,2257299,1163158,484683],\n","                        merge_shops = {0: 57, 1: 58, 11: 10},\n","                        delete_shops = []):\n","    \"\"\"\n","    Parameters:\n","    day0 = datetime.datetime object representing the day you wish to use as your reference when creating time-based features\n","    delete_rows = list of integer row numbers that you wish to delete from the sales_train data set\n","    merge_shops = dictionary of integer shop_id key:value pairs where shop(=key) is merged into shop(=value)\n","    delete_shops = list of integer shop_id numbers that you wish to fully delete from the sales_train data set\n","        # looks like it could be safe to delete these shops from sales_train: [8, 13, 23, 32, 33, 40]\n","        # should probably delete categories 8, 80 (= 'tickets') and probably 81,82 (= 'net carriers')\n","\n","    Global Variables: this function assumes you have the following pandas dataframes available globally:\n","    1) unaltered sales_train\n","    2) unaltered test\n","    3) items_augmented (contains 'item_id', 'item_tested', 'item_category_id', and 'orig_eng_name_transl')\n","    4) item_categories_augmented ('item_category_id','en_cat_name','item_cat_tested','item_category3','item_category4')\n","    5) shops_augmented ('shop_id', 'en_shop_name', 'shop_city', 'shop_federal_district',  'shop_city_population',  'shop_tested')\n","    6) shops_new ('shop_id', 'shop_type', 'fd_popdens',  'fd_gdp')\n","\n","    This function does the following:\n","    1) cleans (deletes) outlier rows from the training set that appear to be erroneous or irrelevant entries\n","    2) merge 3 shops into other shops where it appears that the sales_train set simply has different names for the \n","        same shop at different time periods (shop 0 absorbed by 57; shop 1 absorbed by 58, shop 11 absorbed by 10)\n","    3) optionally delete shops entirely from the sales_train data set (e.g., for irrelevant shops)\n","    4) append the test set rows to the sales_train rows, using a date of November 1, 2015 for test\n","    5) adjust the 'date' column on the merged dataset to be in datetime format, so it looks like a string of format: 'YYYY-M-D'\n","    6) merge the aforementioned data sets into the merged sales_train + test DataFrame\n","\n","    Then, creates and inserts new time-based feature columns as follows:\n","    Given a dataframe with a 'date' column containing strings like '2015-10-30', create new time-series columns:\n","    1. 'day'    = integer value of day number, starting at day = 0 for parameter day0, and incrementing by calendar day number (not by transaction day number)... \n","                    Thus, 'day' may not include all possible integers from start to finish.  It only assigns integer values (based on the calendar) to days when \n","                    there are transactions in the input dataframe --> if the input dataframe has no transactions on a particular day, that day's 'calendar' integer \n","                    value will not be present in the column (will be = 0)\n","    2. 'DoW'    = day of week = 3-character text string of weekday by name (Sun, Mon, ...)\n","    3. 'DoM'    = day of month = 1-31\n","    4. 'week'   = integer value of week number, with week = 0 at time= parameter day0.  However, unlike 'day', the 'week' number is aligned not to start at day0, but rather\n","                    so that there is a full 'week' of 7 days that ends on Oct. 31, 2015 (the final day of training data).  This results in week = 0 having only 5 days in it.\n","                    n.b., the final week of October, 2015 is assigned 'week' number = 147.  Artifically assigning test to Nov. 1, 2015 results in test week = 148\n","    5. 'month'  = renamed from \"date_block_num\" of original data set (no changes).  Integer values from 0 to 33 represent the months starting at day0.  Test month=34 is Nov. 2015.\n","    6. 'qtr'    = quarter = integer number of 3-month chunks of time, aligned with the end of October, 2015.  day0 is included in 'qtr' = 0, but 'qtr'=0 only contains 1 month (Jan 2013) of data due to the alignment\n","                    The months of August, Sept, Oct 2015 form 'qtr' = 11.  \"qtr\" in this sense is just 3-month chunks... it is not the traditional Q1,Q2,Q3,Q4 beginning Jan 1, but instead is more like\n","                    date_block_num in that it is monotonically increasing integers, incremented every 3 months such that #11 ends at the end of our training data\n","    7. 'season' = integer number of 3-month chunks of time, reset each year (allowed values = 0,1,2,3)... not quite the same as spring-summer-winter-fall, or Q1,Q2,Q3,Q4, but instead shifted to \n","                    better capture seasonal spending trends aligned in particular with high December spending\n","                    2 = Dec 1 to Feb 28 (biggest spending season), 3 = Mar 1 to May 31, 0 = June 1 to Aug 30 (lowest spending season), 1 = Sept 1 to Nov 30\n","\n","    Finally, drop the date column from the dataframe, and sort the dataframe by ['day','shop_id','item_id']  (original dataframe seems to be sorted by month, but unsorted within each month)\n","\n","    returns: the cleaned/dated/merged/feature-augmented DataFrame\n","    \"\"\"\n","\n","    print(f'Shape of original sales_train data set = {sales_train.shape}')\n","\n","    # remove outlier rows from training set (first make a DataFrame copy so we can reuse sales_train later, if we need to)\n","    sales_train_cleaned = sales_train.copy(deep=True)\n","    print('Rows being deleted:')\n","    for i in sorted(delete_rows, reverse=True):   # delete the rows in reverse order to be sure we don't run into issues with indexing\n","        print(f'  {i}')\n","        sales_train_cleaned.drop(sales_train_cleaned.index[i],inplace=True)\n","    print(f'Shape of sales_train_cleaned after {len(delete_rows)} outlier rows were removed: {sales_train_cleaned.shape}')\n","    \n","    # Merge the 3 shops we are nearly certain must correctly fit into the other shops' dropout regions:\n","    sales_train_cleaned.shop_id = sales_train_cleaned.shop_id.replace(merge_shops)\n","    print(f'Shape of sales_train_cleaned after merging shops as in {merge_shops}: {sales_train_cleaned.shape}')\n","\n","    # Remove irrelevant shops entirely from the sales_train_cleaned DataFrame:\n","    if delete_shops:\n","        sales_train_cleaned = sales_train_cleaned.query('shop_id != @delete_shops')\n","        print(f'Shape of sales_train_cleaned after deleting shops {delete_shops}: {sales_train_cleaned.shape}')\n","\n","    # sales_train_cleaned = sales_train_cleaned[sales_train_cleaned.shop_id != 9]\n","    # sales_train_cleaned = sales_train_cleaned[sales_train_cleaned.shop_id != 13]\n","    # print(f'Shape of sales_train_cleaned after removal of shops: {sales_train_cleaned.shape})\n","    # print(f'{sales_train_cleaned.shop_id.nunique()} shops remaining in sales_train_cleaned DataFrame: {sorted(sales_train_cleaned.shop_id.unique())})\n","\n","    sales_train_cleaned = sales_train_cleaned.astype({'date_block_num':np.int8,'shop_id':np.int8,'item_id':np.int16,\n","                                                    'item_price':np.float32,'item_cnt_day':np.int16}).reset_index(drop=True)\n","\n","    # merge dataframes so we optionally include test elements in our EDA and feature generation\n","    test_prep = test.copy(deep=True)\n","    test_prep['date_block_num'] = 34\n","    test_prep['date'] = '1.11.2015' #pd.Timestamp(year=2015, month=11, day=1)\n","    sales_traintest_cleaned = sales_train_cleaned.append(test_prep).fillna(0)\n","\n","    traintest = sales_traintest_cleaned.merge(items_augmented[['item_id','item_category_id','item_tested','item_name']],on='item_id',how='left').reset_index(drop=True)\n","    traintest = traintest.merge(item_categories_augmented[['item_category_id','en_cat_name','item_cat_tested','item_category3','item_category4']],on='item_category_id',how='left').reset_index(drop=True)\n","    traintest = traintest.merge(shops_augmented[['shop_id', 'en_shop_name', 'shop_city', 'shop_federal_district', 'shop_tested']], on='shop_id',how='left').reset_index(drop=True)  \n","    traintest = traintest.merge(shops_new[['shop_id', 'shop_type']], on='shop_id',how='left').reset_index(drop=True)\n","    traintest = traintest[['date', 'date_block_num', 'item_price', 'item_cnt_day', 'shop_id', 'item_id', 'item_name', 'item_tested', 'item_category_id', 'en_cat_name', 'item_cat_tested',\n","                                'item_category3', 'item_category4', 'en_shop_name', 'shop_type','shop_tested', 'shop_federal_district', 'shop_city']]\n","    traintest.columns = ['date', 'month', 'price', 'sales', 'shop_id', 'item_id', 'item_name', 'it_test', 'item_category_id', 'item_category_name', 'it_cat_test', 'item_cat3', 'item_cat4', \n","                            'shop_name', 'sh_cat', 'sh_test', 'district', 'city']\n","    traintest.item_name.astype(str)\n","    print(f'Shape of traintest after merging: {traintest.shape}')\n","        \n","    # Add in the time-based feature columns\n","    traintest.date =  pd.to_datetime(traintest.date, dayfirst=True, infer_datetime_format=True)\n","    traintest.insert(1,'day', traintest.date.apply(lambda x: (x - day0).days))\n","    traintest.insert(2,'DoW', traintest.date.apply(lambda x: x.strftime('%a')))  # lambda x: (x.weekday()+1)%7 )  # 0=Sun, 1=Mon, ... 6= Sat   # use x.strftime('%A') to get full text string of day (Sunday, Monday, ...)\n","    traintest.insert(3,'DoM', traintest.date.apply(lambda x: x.day))\n","    traintest.insert(4,'week', (traintest.day+2) // 7 )             # add the 2 days so we have end of a week coinciding with end of training data Oct. 31, 2015\n","    traintest.insert(5,'qtr', (traintest.month + 2) // 3 )          # add the 2 months so we have end of a quarter aligning with end of training data Oct. 31, 2015\n","    traintest.insert(6,'season', (traintest.month + 2) % 4 ) \n","    traintest.drop('date',axis=1,inplace=True)\n","    traintest = traintest.sort_values(['day','shop_id','item_id']).reset_index(drop=True)  # note that the train dataset is sorted by month, but nothing obvious within the month; we sort it here for consistent results in calculations below\n","    print(f'Shape of traintest after creating time-based feature columns: {traintest.shape}')\n","    print(f'traintest DataFrame creation done: {strftime(\"%a %X %x\")}\\n')\n","    return traintest\n","\n","print(f'\\nDone: {strftime(\"%a %X %x\")}\\n')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","Done: Thu 08:28:04 06/18/20\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j83GHWtrz_Gy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483285276,"user_tz":240,"elapsed":40944,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"d85d2234-9df6-4b00-ff92-058dd18e208e"},"source":["if not traintest_loaded:\n","    print(f'traintest dataframe creation started: {strftime(\"%a %X %x\")}\\n')\n","    traintest = clean_merge_augment()\n","\n","    # optional save file as feather type (big file; don't store inside repo) and/or csv.gz type (inside repo)\n","    %cd \"{OUT_OF_REPO_PATH}\"\n","    traintest.to_feather('traintest.ftr')\n","    print(\"traintest.ftr feather file stored on google drive, outside repo\")\n","    %cd \"{GDRIVE_REPO_PATH}\"\n","    # alternative, or, in addition, can save as csv.gz for < 100 MB storage and sync with GitHub\n","    compression_opts = dict(method='gzip',\n","                            archive_name='traintest.csv')  \n","    traintest.to_csv('data_output/traintest.csv.gz', index=False, compression=compression_opts)\n","    print(\"traintest.csv.gz file stored on google drive in data_output directory\")\n","    print(f'traintest file save done: {strftime(\"%a %X %x\")}')\n","\n","display(traintest[traintest.week == 102].tail(2))\n","\n","# Copy in case we screw up tt; don't want to recreate traintest\n","tt = traintest.copy(deep=True)\n","\n","print(f'\\ntraintest done: {strftime(\"%a %X %x\")}')"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>day</th>\n","      <th>DoW</th>\n","      <th>DoM</th>\n","      <th>week</th>\n","      <th>qtr</th>\n","      <th>season</th>\n","      <th>month</th>\n","      <th>price</th>\n","      <th>sales</th>\n","      <th>shop_id</th>\n","      <th>item_id</th>\n","      <th>item_name</th>\n","      <th>it_test</th>\n","      <th>item_category_id</th>\n","      <th>item_category_name</th>\n","      <th>it_cat_test</th>\n","      <th>item_cat3</th>\n","      <th>item_cat4</th>\n","      <th>shop_name</th>\n","      <th>sh_cat</th>\n","      <th>sh_test</th>\n","      <th>district</th>\n","      <th>city</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2257039</th>\n","      <td>718</td>\n","      <td>Sat</td>\n","      <td>20</td>\n","      <td>102</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>399</td>\n","      <td>1</td>\n","      <td>59</td>\n","      <td>21970</td>\n","      <td>shar predictor soccer ball</td>\n","      <td>False</td>\n","      <td>69</td>\n","      <td>Gifts - Souvenirs</td>\n","      <td>True</td>\n","      <td>Gifts</td>\n","      <td>Gifts</td>\n","      <td>Yaroslavl shopping center \"Altair\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Yaroslavl</td>\n","    </tr>\n","    <tr>\n","      <th>2257040</th>\n","      <td>718</td>\n","      <td>Sat</td>\n","      <td>20</td>\n","      <td>102</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>499</td>\n","      <td>1</td>\n","      <td>59</td>\n","      <td>22060</td>\n","      <td>epic bluray dvd</td>\n","      <td>True</td>\n","      <td>37</td>\n","      <td>Movie - Blu-Ray</td>\n","      <td>True</td>\n","      <td>Movies</td>\n","      <td>Movies</td>\n","      <td>Yaroslavl shopping center \"Altair\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Yaroslavl</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         day  DoW  DoM  week  qtr  season  month  price  sales  shop_id  item_id                   item_name  it_test  item_category_id item_category_name  it_cat_test item_cat3 item_cat4                           shop_name  \\\n","2257039  718  Sat   20   102    8       1     23    399      1       59    21970  shar predictor soccer ball    False                69  Gifts - Souvenirs         True     Gifts     Gifts  Yaroslavl shopping center \"Altair\"   \n","2257040  718  Sat   20   102    8       1     23    499      1       59    22060             epic bluray dvd     True                37    Movie - Blu-Ray         True    Movies    Movies  Yaroslavl shopping center \"Altair\"   \n","\n","        sh_cat  sh_test district       city  \n","2257039    SEC     True  Central  Yaroslavl  \n","2257040    SEC     True  Central  Yaroslavl  "]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","traintest done: Thu 08:28:04 06/18/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xr0FDeno_EUQ"},"source":["#2.5) ***items*** Dataset: EDA, Cleaning, Correlations, and Feature Generation\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FX9bOxkfr55N","colab_type":"text"},"source":["###2.5.1) Initial data exploration and Russian -> English translation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s0J5l5H98Xsh"},"source":["####Thoughts regarding items dataframe\n","Let's first look at how many training examples we have to work with..."]},{"cell_type":"markdown","metadata":{"id":"1lg7NbEchkuM","colab_type":"text"},"source":["Many of the items have similar names, but slightly different punctuation, or only very slightly different version numbers or types.  (e.g., 'Call of Duty III' vs. 'Call of Duty III DVD')\n","\n","One can expect that these two items would have similar sales in general, and by grouping them into a single feature category, we can eliminate some of the overfitting that might come as a result of the relatively small ratio of (training set shop-item-date combinations = 2935849)/(total number of unique items = 22170).  (This is an average of about 132 rows in the sales_train data for each shop-item-date combination that we are using to train our model.  Our task is to produce a monthly estimate of sales (for November 2015), so it is relevant to consider training our model based on how many sales in a month vs. how many sales in the entire training set.  Given that the sales_train dataset covers the time period from January 2013 to October 2015 (34 months), we have on average fewer than 4 shop-item combinations in our training set for a given item in any given month.  Furthermore, as we are trying to predict for a particular month (*November* 2015), it is relevant to consider how many rows in our training set occur in the month of November.  The sales_train dataset contains data for two 'November' months out of the total 34 months of data.  Another simple calculation gives us an estimate that our training set contains on average 0.23 shop-item combinations per item for November months.\n","\n","To summarize:\n","\n","*  *sales_train* contains 34 months of data, including 2935849 shop-item-date combinations\n","*  *items* contains 22170 \"unique\" item_id values\n","\n","In the *sales_train* data, we therefore have:\n","*  on average, 132 rows with a given shop-item pair for a given item_id\n","*  on average, 4 rows with a given shop-item pair for a given item_id in a given month\n","*  on average, 0.23 rows with a given shop-item pair for a given item_id in all months named 'November'\n","\n","If we wish to improve our model predictions for the following month of November, it behooves us to use monthly grouping of sales, or, even better, November grouping of sales.  This smooths out day-to-day variations in sales for a better monthly prediction.  However, the sparse number of available rows in the *sales_train* data will contribute to inaccuracy in our model training and predictions.\n","\n","Imagine if we could reduce the number of item_id values from 22170 to perhaps half that or even less.  Given that the number of rows for training (per item, on a monthly or a November basis) is so small, then such a reduction in the number of item_id values would have a big impact.  (The same is true for creating features to supplement \"shop_id\" so as to group and reduce the individuality of each shop - and thus effectively create, on average, more rows of training data for each shop-item pair."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZfgOx-0_KXg"},"source":["####Translate and Ruminate\n","We will start by translating the Russian text in the dataframe, and add our ruminations on possible new features we can generate.\n","\n","The dataframe *items_transl* (equivalent to *items* plus a column for English translation) is saved as a .csv file so we do not have to repeat the translation process the next time we open a Google Colab runtime."]},{"cell_type":"code","metadata":{"id":"FhHSfXNxsKxQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483285276,"user_tz":240,"elapsed":40938,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"51979a20-9572-4ea5-df3d-f97ff8f3f374"},"source":["print(items_augmented.info())\n","print(\"\\n\")\n","print(items_augmented.tail(10))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 22170 entries, 0 to 22169\n","Data columns (total 5 columns):\n"," #   Column                Non-Null Count  Dtype \n","---  ------                --------------  ----- \n"," 0   item_id               22170 non-null  int64 \n"," 1   item_name             22169 non-null  object\n"," 2   item_tested           22170 non-null  bool  \n"," 3   item_category_id      22170 non-null  int64 \n"," 4   orig_eng_name_transl  22170 non-null  object\n","dtypes: bool(1), int64(2), object(2)\n","memory usage: 714.6+ KB\n","None\n","\n","\n","       item_id                                            item_name  item_tested  item_category_id                                   orig_eng_name_transl\n","22160    22160                                   vanity fair region        False                40                                   Vanity Fair (Region)\n","22161    22161           yaroslav thousand of year ago e bluray dvd        False                37                YAROSLAV. Thousands of years ago e (BD)\n","22162    22162                                                 fury         True                40                                                   FURY\n","22163    22163                                          fury region         True                40                                          FURY (region)\n","22164    22164                                      fury bluray dvd         True                37                                              FURY (BD)\n","22165    22165                  nuclear titbit 2 pc digital version        False                31                 Nuclear titbit 2 [PC, Digital Version]\n","22166    22166         language 1c query enterprise digital version         True                54     Language 1C queries: Enterprises [Digital Version]\n","22167    22167  1c query language enterprise 8 and cd khrustalev ey         True                49  1C query language: Enterprise 8 (+ CD). Khrustalev EY\n","22168    22168                                       egg little inu        False                62                                     Egg for Little Inu\n","22169    22169                            dragon egg game of throne        False                69                           Dragon egg (Game of Thrones)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9oSMeRVd7dvZ"},"source":["###2.5.2) **NLP for feature generation from items data**\n","Automate the search for commonality among items, and create new categorical feature to prevent overfitting from close similarity between many item names"]},{"cell_type":"markdown","metadata":{"id":"CwfXcHsMJ0Yg","colab_type":"text"},"source":["####**Delimited Groups of Words**\n","\n","Investigating \"special\" delimited word groups (like this) or [here] or /hobbitville/ that are present in item names, and may be particularly important in creating n>1 n-grams for uniquely identifying items so that we can tell if two items are the same or nearly the same"]},{"cell_type":"markdown","metadata":{"id":"V7QY11R1QmlN","colab_type":"text"},"source":["#####Some details on the approach, and code for helper functions to clean and separate the text:"]},{"cell_type":"code","metadata":{"id":"jac_TColdsMf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483285278,"user_tz":240,"elapsed":40933,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"9443fa32-acbc-4348-f92f-d4bdf3792bc0"},"source":["# explanation of regex string I'm using to parse the item_name\n","'''\n","\n","^\\s+|\\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*|\\s+$\n","\n","gm\n","1st Alternative ^\\s+\n","^ asserts position at start of a line\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","2nd Alternative \\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","Match a single character present in the list below [,\\\"\\/\\(\\)\\[\\]]+\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n",", matches the character , literally (case sensitive)\n","\\\" matches the character \" literally (case sensitive)\n","\\/ matches the character / literally (case sensitive)\n","\\( matches the character ( literally (case sensitive)\n","\\) matches the character ) literally (case sensitive)\n","\\[ matches the character [ literally (case sensitive)\n","\\] matches the character ] literally (case sensitive)\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","3rd Alternative \\s+$\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","$ asserts position at the end of a line\n","\n","Global pattern flags\n","g modifier: global. All matches (don't return after first match)\n","m modifier: multi line. Causes ^ and $ to match the begin/end of each line (not only begin/end of string)\n","'''\n","print(f'done: {strftime(\"%a %X %x\")}')  # prevent Jupyter from printing triple-quoted comments"],"execution_count":7,"outputs":[{"output_type":"stream","text":["done: Thu 08:28:04 06/18/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rsc0yYJkiRBY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483285278,"user_tz":240,"elapsed":40925,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"eb4e2511-33b7-4836-b289-a4d498627705"},"source":["# This cell contains no code to run; it is simply a record of some inspections that were done on the items database\n","\n","# before removing undesirable characters / punctuation from the item name,\n","#   let's see if we can find n-grams or useful describers or common abbreviations by looking between the nasty characters\n","# first, let's see what characters are present in the en_item_name column\n","'''\n","nasty_symbols = re.compile('[^0-9a-zA-Z ]')\n","nasties = set()\n","for i in range(len(items_transl)):\n","  n = nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  nasties = nasties.union(set(n))\n","print(nasties)\n","{'[', '\\u200b', 'ñ', '(', ')', '.', 'à', '`', 'ó', '®', 'Á', \n","'\\\\', 'è', '&', '-', ':', 'ë', '_', 'û', '»', '=', '+', ']', ',', \n","'«', 'ú', \"'\", 'ö', '#', 'ä', ';', 'ü', '\"', 'ô', '/', '№', 'é', \n","'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","'''\n","# From the above set of nasty characters, it looks like slashes, single quotes, double quotes, parentheses, and square brackets might enclose relevant n-grams\n","# Let's pull everything from en_item_name that is inside ' ', \" \", (), or [] and see how many unique values we get, and if they are n-grams or abbreviations, for example\n","# It also seems that many of the item names end in a single character \"D\" for example, which should be converted to DVD\n","\n","# ignore the :&+' stuff for now...\n","# Let's set up columns for ()[]-grams, for last string in the name, and for first string in name, and for text that precedes \":\", and for text that surrounds \"&\" or \"+\"\n","#   but first, we will strip out every nasty character except ()[]:&+'\"/ and replace the nasties with spaces, then eliminating double spaces\n","\n","'''\n","# sanity check:\n","really_nasty_symbols = re.compile('[^0-9a-zA-Z \\(\\)\\[\\]:&+\\'\"/]')\n","really_nasties = set()\n","for i in range(len(items_transl)):\n","  rn = really_nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  really_nasties = really_nasties.union(set(rn))\n","print(really_nasties)\n","{'\\u200b', 'ñ', '.', 'à', '`', 'ó', '®', 'Á', '\\\\', 'è', '-', 'ë', '_', 'û', '»', '=', ',', '«', 'ú', 'ö', '#', 'ä', ';', 'ü', 'ô', '№', 'é', 'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","OK, looks good\n","'''\n","print(f'done: {strftime(\"%a %X %x\")}')  # prevent Jupyter from printing triple-quoted comments"],"execution_count":8,"outputs":[{"output_type":"stream","text":["done: Thu 08:28:04 06/18/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5fb3tLqLtcey","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483285279,"user_tz":240,"elapsed":40921,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"376ef134-1795-47b4-c9dc-9f513d0308e3"},"source":["#  Start by defining stopwords and delimiters and punctuation that we wish to remove\n","#  Then, create a couple of functions to use for text cleaning, and for extracting delimited text n-grams\n","\n","# stopwords to remove from item names (these are only a bit better than arbitrary selections from large stopwords lists -- may be worth adjusting them)\n","stop_words = \"a,the,an,only,more,are,any,on,your,just,it,its,has,with,for,by,from\".split(\",\")\n","\n","# pre-compile regex strings to use for fast symbol removal or delimiting\n","nasty_symbols_re = re.compile(r'[^0-9a-zA-Z ]')  # remove all punctuation\n","really_nasty_symbols_re = re.compile(r'[^0-9a-zA-Z ,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@]')  # remove nasties, but leave behind the delimiters\n","delimiters_re = re.compile(r'[,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@\\u00AB\\u00BB~<>]')  # unicodes are << and >> thingies\n","# special symbols indicating a delimiter --> a space at start or end of item name will be removed at split time, along with ,;/()[]:\"-@~<<>><>\n","delim_pattern_re = re.compile(r'^\\s+|\\s*[,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@\\u00AB\\u00BB~<>]+\\s*|\\s+$') \n","multiple_whitespace_re = re.compile(r'[ ]{2,}')\n","\n","# pre-compile some specific regex strings to deal with inconsistencies in item names (more of this will be done later, after delimiting)\n","cleanup_text = {}\n","cleanup_text['preorder'] = re.compile(r'pre.?order')\n","cleanup_text[' dvd'] = re.compile(r'\\s+d$')  #several item names end in \"d\" -- which actually seems to indicate dvd (because the items I see are in category 40: Movies-DVD)... standardize so d --> dvd\n","cleanup_text['digital version'] = re.compile(r'digital in$') # several items seem to end in \"digital in\"... maybe in = internet?, but looking at nearby items/categories, 'digital version' looks standard\n","cleanup_text['bluray dvd'] = re.compile(r'\\bbd\\b|\\bblu\\s+ray\\b|\\bblu\\-ray\\b|\\bblueray\\b|\\bblue\\s+ray\\b|\\bblue\\-ray\\b')\n","cleanup_text['007 : james bond : skyfall'] = re.compile(r'\\bskyfall\\b|\\bskayfoll\\b')\n","cleanup_text[' and '] = re.compile(r'[\\&\\+]')\n","cleanup_text[' xbox'] = re.compile(r'\\bx[^0-9a-zA-Z ]box')  # anything like \"x box\" or \"x-box\" or \"x%box\" gets converted to a standard \"xbox\"\n","cleanup_text[' ps'] = re.compile(r'\\bp[^0-9a-zA-Z ]s')      # attempt to do the same with \"p-s4\" --> \"ps4\"\n","cleanup_text['sim city'] = re.compile(r'\\bsim(s)?(\\s)?(city)?\\b')\n","cleanup_text['watchdog'] = re.compile(r'\\bwatch(\\s)?dog\\b')\n","cleanup_text['bloodborne'] = re.compile(r'\\bbloodborn(e)?\\b')\n","cleanup_text['plant vs zombie'] = re.compile(r'\\bplant\\b.*zombie\\b')\n","cleanup_text['tom clancy'] = re.compile(r'\\btom clancy(s)?\\b')\n","cleanup_text['pirate caribbean'] = re.compile(r'\\bpirate\\b.*car\\w*\\b')\n","cleanup_text['one of you'] = re.compile(r'\\bone of (yo)?u\\b')\n","cleanup_text['titanfall'] = re.compile(r'\\btitan(\\s)?fall\\b')\n","\n","def maid_service(text):\n","    \"\"\"\n","    Compact routine to implement multiple regex substitutions using the above 'cleanup_text' dictionary\n","    \"\"\"\n","    text = text.lower()\n","    for repl_text, pattern in cleanup_text.items():\n","        text = pattern.sub(repl_text, text)\n","    #r = re.compile(r'\\bskayfoll\\b')   # can add 'quickie' items here if you don't want to add to above dictionary, or if you want to perform something other than re.sub\n","    #text = r.sub('skyfall',text)  \n","    return text\n","\n","def text_total_clean(text):\n","    \"\"\"\n","    Gives a punctuation-free, cleaned, lemmatized version of the original English translation\n","    inputs: (text): the original en_item_name single-string, uncleaned, translated version of the Russian item name\n","    returns: single-string text, made lowercase, stripped of \"really_nasties\" and multiple spaces, and every word lemmatized\n","    \"\"\"\n","    text = maid_service(text)\n","    text = delimiters_re.sub(\" \", text)  # replace all delimiters with a space; other nasties get simply deleted\n","    text = nasty_symbols_re.sub(\"\", text)  # delete anything other than letters, numbers, and spaces\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = text.strip() # remove whitespace around string\n","    # lemmatize each word\n","    text = \" \".join([lemmatizer.lemmatize(w) for w in text.split(\" \") if w not in stop_words])\n","    text = maid_service(text)\n","    return text\n","\n","def text_clean_delimited(text):\n","    \"\"\"\n","    Gives a punctuation-free, cleaned version of the original English translation, \n","        but the function returns a list of strings instead of a single string,\n","        with each element in the list corresponding to text that was separated from neighboring\n","        text with one of the above-defined 'delimiter' characters\n","        (so, rather than analyzing the full item name for n-grams, we define an item's important\n","        n-grams as being separated by such delimiters.  It greatly reduces the number of n-grams we need to analyze)\n","    inputs: (text): the original en_item_name single-string, uncleaned, translated version of the Russian item name\n","    returns: en_item_name made lowercase, stripped of \"really_nasties\" and multiple spaces, \n","        in a list of strings that had been separated by one of the above 'delimiters',\n","        and, with every word in every string lemmatized \n","    \"\"\"\n","    text = maid_service(text)\n","    text = really_nasty_symbols_re.sub(\"\", text)  # just delete the nasty symbols\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    t = delim_pattern_re.split(text)           # split item_name at all delimiters, irrespective of number of spaces before or after the string or delimiter\n","    text = []\n","    for i in t:\n","        text.append(maid_service(i))\n","    text = [x.strip() for x in text if x != \"\"]           # remove empty strings \"\" from the list of split items in text, and remove whitespace outside text n-gram\n","    # lemmatize each word\n","    lemtext = []\n","    for ngram in text:\n","        lemtext.append(\" \".join([lemmatizer.lemmatize(w) for w in ngram.split(\" \") if w not in stop_words]))\n","    return lemtext\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["done: Thu 08:28:04 06/18/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KvzvPqmCQ2ZM","colab_type":"text"},"source":["#####Add 'delimited' and 'cleaned' data columns; shorten the titles of other columns so dataframe fits better on the screen"]},{"cell_type":"code","metadata":{"id":"Z35pqOYCtyZ7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483293313,"user_tz":240,"elapsed":48947,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"f8c7399a-e7fa-4f5d-989f-afd5d96dc466"},"source":["items_delimited = items_augmented[['item_id','orig_eng_name_transl','item_tested','item_category_id']].copy(deep=True)\n","# delete the wide \"item_name\" column so we can read more of the data table width-wise\n","items_delimited = items_delimited.rename(columns = {'orig_eng_name_transl':'item_name','item_category_id':'i_cat_id','item_tested':'i_tested'})\n","items_in_test_set = test.item_id.unique()\n","# items_delimited[\"i_tested\"] = False\n","# for i in items_in_test_set:\n","#   items_delimited.at[i,\"i_tested\"] = True\n","\n","\n","# add item_category name with delimiter to the item_name, as this will be useful info for grouping similar items (remove delimiting punctuation from cat names first, so it stays as one chunk of text)\n","items_delimited['item_name'] = items_delimited.apply(lambda x: text_total_clean(item_categories_augmented.at[x.i_cat_id,'en_cat_name']) + \" : \" + x.item_name, axis=1)\n","\n","# add a column of simply cleaned text without any undesired punctuation or delimiters\n","items_delimited['clean_item_name'] = items_delimited['item_name'].apply(text_total_clean)\n","\n","# now add a column of lists of delimited (cleaned) text\n","items_delimited['delim_name_list'] = items_delimited['item_name'].apply(text_clean_delimited)\n","\n","# remove duplicate entries and single-character 1-grams to assist with operations to come later in this notebook\n","alphnum = list(string.ascii_lowercase) + list('1234567890')  # get rid of all length=1 1-grams\n","def remove_dupes_singles(gramlist):\n","    unwanted = set(alphnum)\n","    dupe_gramset = unwanted\n","    return [x for x in gramlist if x not in dupe_gramset and not dupe_gramset.add(x)]\n","items_delimited.delim_name_list = items_delimited.delim_name_list.apply(lambda x: remove_dupes_singles(x) )\n","\n","\n","# have a look at what we got with our delimited text globs\n","def maxgram(gramlist):\n","    maxg = 0\n","    for g in gramlist:\n","        maxg = max(maxg,len(g.split()))\n","    return maxg\n","items_delimited['d_len'] = items_delimited.delim_name_list.apply(lambda x: len(x))\n","items_delimited['d_maxgram'] = items_delimited.delim_name_list.apply(maxgram)\n","\n","#items_delimited.to_csv(\"data_output/items_delimited.csv\", index=False)\n","\n","print(f'done: {strftime(\"%a %X %x\")}')\n","print(\"\\n\")\n","print(items_delimited.describe())\n","print(\"\\n\")\n","print(items_delimited.iloc[31][:])\n","print(\"\\n\")\n","items_delimited.head()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["done: Thu 08:28:12 06/18/20\n","\n","\n","         item_id  i_cat_id  d_len  d_maxgram\n","count      22170     22170  22170      22170\n","mean  11,084.500    46.291  3.347      4.497\n","std    6,400.072    15.941  1.336      1.984\n","min            0         0      1          2\n","25%    5,542.250        37      2          3\n","50%   11,084.500        40      3          4\n","75%   16,626.750        58      4          5\n","max        22169        83     13         17\n","\n","\n","item_id                                                                                                 31\n","item_name                                              movie bluray dvd : 007: COORDINATES \"SKAYFOLL» (BD)\n","i_tested                                                                                              True\n","i_cat_id                                                                                                37\n","clean_item_name       movie bluray dvd 007 coordinate 007 james bond 007 : james bond : skyfall bluray dvd\n","delim_name_list    [movie bluray dvd, 007, coordinate, james bond, 007 : james bond : skyfall, bluray dvd]\n","d_len                                                                                                    6\n","d_maxgram                                                                                                6\n","Name: 31, dtype: object\n","\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>item_name</th>\n","      <th>i_tested</th>\n","      <th>i_cat_id</th>\n","      <th>clean_item_name</th>\n","      <th>delim_name_list</th>\n","      <th>d_len</th>\n","      <th>d_maxgram</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>movie dvd : ! POWER IN glamor (PLAST.) D</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>[movie dvd, power in glamor, plast, dvd]</td>\n","      <td>4</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>program home and office digital : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]</td>\n","      <td>False</td>\n","      <td>76</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version]</td>\n","      <td>4</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>movie dvd : *** In the glory (UNV) D</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>[movie dvd, in glory, unv, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>movie dvd : *** BLUE WAVE (Univ) D</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd blue wave univ dvd</td>\n","      <td>[movie dvd, blue wave, univ, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>movie dvd : *** BOX (GLASS) D</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd box glass dvd</td>\n","      <td>[movie dvd, box, glass, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id                                                                                                item_name  i_tested  i_cat_id  \\\n","0        0                                                                 movie dvd : ! POWER IN glamor (PLAST.) D     False        40   \n","1        1  program home and office digital : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]     False        76   \n","2        2                                                                     movie dvd : *** In the glory (UNV) D     False        40   \n","3        3                                                                       movie dvd : *** BLUE WAVE (Univ) D     False        40   \n","4        4                                                                            movie dvd : *** BOX (GLASS) D     False        40   \n","\n","                                                                                    clean_item_name                                                                                        delim_name_list  d_len  d_maxgram  \n","0                                                               movie dvd power in glamor plast dvd                                                               [movie dvd, power in glamor, plast, dvd]      4          3  \n","1  program home and office digital abbyy finereader 12 professional edition full pc digital version  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version]      4          6  \n","2                                                                        movie dvd in glory unv dvd                                                                        [movie dvd, in glory, unv, dvd]      4          2  \n","3                                                                      movie dvd blue wave univ dvd                                                                      [movie dvd, blue wave, univ, dvd]      4          2  \n","4                                                                           movie dvd box glass dvd                                                                           [movie dvd, box, glass, dvd]      4          2  "]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Rq0hR-jbnjBz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592483293314,"user_tz":240,"elapsed":48946,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# # adjust items_augmented to remove delimited name list, and instead put a clean item name in the file (but with no prefix of item category)\n","# # add a column of simply cleaned text without any undesired punctuation or delimiters\n","# items_augmented.delim_name_list = items_augmented.item_name\n","# items_delimited.item_name = items_augmented.delim_name_list.apply(text_total_clean)\n","# items_augmented.columns = ['item_id','item_category_id','item_name','item_tested','orig_eng_name_transl']\n","# items_augmented = items_augmented[['item_id','item_name','item_tested','item_category_id','orig_eng_name_transl']]\n","# # items_augmented.to_csv(\"data_output/items_augmented.csv\", index=False)\n","# items_augmented.head()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"qc0D5gkeuk2i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483293315,"user_tz":240,"elapsed":48938,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"53c29501-5d0c-4b29-f1d0-d94d6d8566e9"},"source":["# do some more text manipulation to help ensure items are properly grouped\n","#   also, expand the breadth of n-gram matches to ignore things like version number, in an effort to reduce the final number of clusters that are generated\n","#   (looking for perhaps 200 clusters instead of 2000+ that we get without this extra treatment (see 'items_nlp_clusters_v3...ipynb' ))\n","\n","highlight_roots = OrderedDict()\n","cleanup_sub = OrderedDict()\n","cleanup_final = OrderedDict()\n","#cleanup_complete_replace = OrderedDict()\n","\n","# for some matches, I want to only make a new entry in the list to standardize games to root values (e.g., \"assasin creed special ops\" = \"assasin creed part 2\")\n","#     The new list element will be a 5-gram, for example, to give it substantial weight when grouping items\n","#     The original list of delimited text will remain the same, so as to catch matches like \"assasin creed special ops dvd bluray english version\"\n","\n","# use replacement text = '' if you want the create operation to use the match as a base string (possibly adding to it with fill_strings until n_gram size is reached)\n","games1 = \"adventure of tintin|advanced warfare|army of two|assasin creed|angry bird|batman|battlefield|behind enemy line|black ops|bloodborne|borderland|call of duty|chaggington funny train\"\n","games2 = \"child of light|dark soul|dead space|diablo|disney infinity|dragon age|elder scroll|far cry|fifa|final fantasy|fury|game of throne|god of war|gran turismo|grand theft auto|harry potter|hobbit|injustice god|james bond\"\n","games3 = \"jurassic|lord of ring|mario|masha and bear|max payne|medal of honor|men in black|metal gear solid|mickey mouse|might and magic|modern warfare|mortal kombat|nba|need speed|nhl|ninja storm|one of you|pirate caribbean\"\n","games4 = \"plant vs zombie|pro evolution|resident evil|secret of unicorn|shadow of mordor|sherlock holmes|sid meiers civilization|simcity|skylander|sniper elite|star war|stick of truth|street fighter\"\n","games5 = \"tiger wood|titanfall|tom clancy|tomb raider|total war|transformer|walking dead|warhammer|watchdog|witcher|world of warcraft\"\n","popular_games = re.compile(rf'\\b({games1}|{games2}|{games3}|{games4}|{games5})\\b')\n","highlight_roots['compress game names to root values'] =     {'optype':['create'], 'reg_pattern':popular_games, \n","                                                                'replacement_text':'', 'final_gram_n':5, \n","                                                                'fill_strings':['game','computer','electronic','multirelease']}\n","\n","lego = re.compile(r'\\blego\\b')\n","#lego = re.compile(rf'\\b(lego.*({games1}|{games2}|{games3}|{games4}|{games5})?|({games1}|{games2}|{games3}|{games4}|{games5}).*lego)\\b')\n","highlight_roots['lego products'] =                          {'optype':['create'], 'reg_pattern':lego,\n","                                                                'replacement_text':'lego brand lego style game'}\n","\n","popular_companies = re.compile(rf'\\b(1c|abbyy)\\b')\n","highlight_roots['highlight product origins'] =              {'optype':['create'], 'reg_pattern':popular_companies, \n","                                                                'replacement_text':'', 'final_gram_n':4, \n","                                                                'fill_strings':['educational','software','learning']}\n","\n","# for some of the matches, I just want to do an inplace substitution\n","fix_accessory_game = re.compile(r'\\baccessory game\\b')\n","cleanup_sub['game accessory'] =         {'optype':['sub'], 'replacement_text':'game accessory', 'reg_pattern':fix_accessory_game}\n","\n","biz = re.compile(r'\\b(firm|enterprise|company|corporation|shop|store|outlet)\\b')\n","cleanup_sub['standardize biz'] =        {'optype':['sub'], 'replacement_text':'business', 'reg_pattern':biz}\n","\n","digit = re.compile(r'\\b(digital|download|online)(\\s?(version|edition|set|box set))?\\b')\n","cleanup_sub['special edition'] =        {'optype':['sub'], 'replacement_text':'online digital version', 'reg_pattern':digit}\n","\n","special = re.compile(r'\\b(collector|premier|platinum|special|suite)(.*(version|edition|set|box set|suite))?\\b')\n","cleanup_sub['special edition'] =        {'optype':['sub'], 'replacement_text':'special version', 'reg_pattern':special}\n","\n","std = re.compile(r'\\b(standard|std)(\\s?(edition|version|set|box set))?\\b')\n","cleanup_sub['standard edition'] =       {'optype':['sub'], 'replacement_text':'standard version', 'reg_pattern':std}\n","\n","russia = re.compile(r'\\b(russian|ru)(\\s?(edition|version|set|box set|documentation|instruction|language|format|subtitle|feature))?\\b')\n","cleanup_sub['russian version'] =        {'optype':['sub'], 'replacement_text':'russian language version', 'reg_pattern':russia}\n","\n","engl = re.compile(r'\\b(english|en|eng|engl)(\\s?(edition|version|set|box set|documentation|instruction|language|format|subtitle|feature))?\\b')\n","cleanup_sub['english version'] =        {'optype':['sub'], 'replacement_text':'english language version', 'reg_pattern':engl}\n","\n","\n","# for other matches, I want to create a new n-gram and insert it into the list, and also do an inplace substitution\n","#   substitution text is made longer or shorter, depending on rough importance to matching (longer matching n-grams get more weight)\n","yo = re.compile(r'\\b(yo|yoyo|yo yo)\\b')\n","cleanup_final['yo yo yo'] =         {'optype':['sub','create'], 'replacement_text':'yo yo toy game fun', 'reg_pattern':yo}\n","\n","music = re.compile(r'\\b(cd mirex|mirex cd|cd mirex cd|vinyl|cd.*production firm|cd.*local production|mp3)(\\s?(cd mirex|mirex cd|cd mirex cd|vinyl|cd.*production firm|cd.*local production|mp3))?\\b')\n","cleanup_final['music media'] =      {'optype':['sub','create'], 'replacement_text':'music media', 'reg_pattern': music}\n","\n","dvdclean = re.compile(r'\\b(\\d\\s?)?(disc\\s?)?(\\d\\s?)?dvd\\b')\n","cleanup_final['dvd'] =             {'optype':['sub','create'], 'replacement_text':'dvd', 'reg_pattern':dvdclean}\n","\n","brdvd = re.compile(r'\\b(4k\\s?)?(\\d\\s?)?(bluray\\s?)?(\\d\\s?)?(dvd\\s?)?(and\\s?)?(\\d\\s?)?(disc\\s?)?(4k\\s?)?(\\d\\s?)?bluray(\\s?and)?(\\s?4k)?(\\s?(\\d\\s?)?dvd)?(\\s?4k)?(\\s?and)?(\\s?(\\d\\s?)?dvd)?\\b|\\b2bd\\b')\n","cleanup_final['bluray dvd'] =      {'optype':['sub','create'], 'replacement_text':'bluray dvd', 'reg_pattern':brdvd}\n","\n","br3d=re.compile(r'\\b(\\d\\s?)?(disc)?\\s?(\\d\\s?)?(dvd)?\\s?(and)?\\s?(3d\\s?(\\d\\s?)?(dvd)?\\s?(\\d\\s?)?bluray\\s?(\\d\\s?)?(dvd)?|(\\d\\s?)?bluray\\s?(\\d\\s?)?(dvd)?\\s?3d)\\s?(\\d\\s?)?(bluray dvd)?\\s?(3d)?\\s?(and)?\\s?(\\d\\s?)?(dvd)?\\s?(3d)?\\b')\n","cleanup_final['3d bluray dvd'] =   {'optype':['sub','create'], 'replacement_text':'3d bluray dvd', 'reg_pattern':br3d}\n","\n","macregx = re.compile(r'\\b(support\\s?)?(mac|ipad|macbook|powerbook|imac|apple)(\\s?support)?\\b')\n","cleanup_final['pc'] =              {'optype':['create'], 'replacement_text':'mac computing platform product', 'reg_pattern':macregx}\n","\n","pcregx = re.compile(r'\\b(support\\s?)?(pc|windows|microsoft windows)(\\s?support)?\\b')\n","cleanup_final['pc'] =              {'optype':['create'], 'replacement_text':'pc computing platform product', 'reg_pattern':pcregx}\n","\n","playsta = re.compile(r'\\b(support\\s?)?p(laystation|sp|\\s?s|\\s?s?\\s?(move|2|3|4|pro|vita|vita 1000))\\b')\n","cleanup_final['sony playstn'] =    {'optype':['create'], 'replacement_text':'sony playstation gaming platform', 'reg_pattern':playsta}\n","\n","xbox = re.compile(r'\\bx?\\s?box\\s?(one|360|live)(.*(kinect|knect))?\\b')\n","cleanup_final['microsoft xbox'] =  {'optype':['create'], 'replacement_text':'microsoft xbox gaming platform','reg_pattern':xbox}\n","\n","kinect = re.compile(r'\\b(support)?\\s?m?\\s?s?\\s?(kinect|knect)\\b')\n","cleanup_final['microsoft knect'] = {'optype':['create'], 'replacement_text':'microsoft xbox gaming platform', 'reg_pattern':kinect}\n","\n","msoffice = re.compile(r'\\b(microsoft office|ms office|m office|office mac|office home|office professional|home and office|office student|office enterprise)\\b')\n","cleanup_final['ms office'] =       {'optype':['create'], 'replacement_text':'microsoft office productivity software', 'reg_pattern':msoffice}\n","\n","educate = re.compile(r'\\b(education|educational|development|course|school|history|lesson|accounting|b8)\\b')\n","cleanup_final['educational dev'] = {'optype':['create'], 'replacement_text':'educational development training lessons', 'reg_pattern':educate}\n","\n","paycard = re.compile(r'\\b(payment|card|ticket|debit)(\\s?(card|ticket|debit))?\\b')\n","cleanup_final['payment card'] =    {'optype':['create'], 'replacement_text':'payment card ticket', 'reg_pattern':paycard}\n","\n","licenses = re.compile(r'\\b(subscription|renewal|1 year|extension|license)(.*(subscription|renewal|1 year|extension|license))?(.*(subscription|renewal|1 year|extension|license))?\\b')\n","cleanup_final['licenses'] =        {'optype':['create'], 'replacement_text':'license renewal subscription extension', 'reg_pattern':licenses}\n","\n","download = re.compile(r'\\b(online|digital|download|access|without disc|without disk|epay)(.*(online|digital|download|access|without disc|without disk|epay|version|edition))?\\b')\n","cleanup_final['downloads'] =       {'optype':['create'], 'replacement_text':'online download version', 'reg_pattern':download}\n","\n","ship = re.compile(r'\\b(courier|delivery|deliver|postage|mail|send|ship|shipment)(.*(courier|delivery|deliver|postage|mail|send|ship|shipment))?\\b')\n","cleanup_final['shipping'] =        {'optype':['create'], 'replacement_text':'shipping delivery postage mail', 'reg_pattern':ship}\n","\n","virus = re.compile(r'\\b(point pixel|kaspersky|panda|drweb|eset nod32|security|antivirus|virus)(.*(point pixel|kaspersky|panda|drweb|eset nod32|security|antivirus|virus|software))?\\b')\n","cleanup_final['antivirus'] =       {'optype':['create'], 'replacement_text':'antivirus defender internet security software', 'reg_pattern':virus}\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["done: Thu 08:28:13 06/18/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RzArCadlmhe4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483293479,"user_tz":240,"elapsed":49093,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"fdfdd48e-a9ab-4d96-9ce7-867d14cb9bf4"},"source":["# here are the routines to implement the above pattern-matching instructions, and modify the delim_items_list column of the dataframe\n","\n","def expand_gram(gram,final_gram_n,fill_strings):\n","    for f in range(final_gram_n - len(gram.split())):\n","        gram = gram + \" \" + fill_strings[f]\n","    return gram\n","\n","def cleanup_service(gramlist=[\"word1 this is a 6 gram\", \"word1\", \"two gram\", \"three gram string\"], \n","                    pattern_dict=OrderedDict({'replace 0007 with 007':{'optype':['sub'],'reg_pattern':re.compile(r'\\b0007\\b'), 'replacement_text':'007', \n","                                                                       'final_gram_n':4, 'fill_strings':['game','computer','electronic']},\n","                                             'replace skayfall with skyfall':{'optype':['sub','create'],'reg_pattern':re.compile(r'\\bskayfall\\b'), 'replacement_text':'skyfall'}})):\n","    \"\"\"\n","    for text modification in the items_delimited dataframe, in an effort to help standardize terms to better highlight similarities between items,\n","    and to help group items a bit more broadly in some cases, so we create fewer clusters with the following graph/network analysis.\n","\n","    gramlist = list of delimited n-grams provided typically from a single cell from 'items_delimited' DF, at a single row, in column = 'delim_name_list'\n","    pattern_dict = ordered dictionary of lists where operations are done in the order created by user (e.g., clean up \"dvd\" variants before cleaning up \"bluray dvd\" variants, so the latter becomes simpler in regex)\n","        keys = representative text, describing what is being done (somewhat irrelevant to this function)\n","        values = dict{  \n","                    optype = list of strings indicating if one wants to do one or more of the following 4 types of operation on the gramlist\n","                            'sub': (sub)stitute regex matches, searching each element in the gramlist,  (len(gramlist) remains the same, but each string in gramlist may shrink or grow or remain unchanged)\n","                            'create': (create) new \"standardized\" list elements if a match is found within the gramlist (so len(gramlist) grows by 1 for each match); original gramlist strings remain unchanged\n","                            'complete_replace': wherever you have matching elements in the gramlist, replace the entire gramlist element with the pattern_dict key (len(gramlist) remains the same, but n in each n-gram may change)\n","                    reg_pattern = regex patterns to find/substitute/create/replace, \n","                    replacement_text = the text to put in place of the reg_pattern match, or to use when creating a new gramlist list element\n","                    final_gram_n = integer; desired final n-gram length (if desired) \n","                    fill_strings= list(padding strings used to append on to the shorter regex matches to make final string = n grams in length, in order from most important to least)... must be long enough!\n","                    }\n","    \"\"\"\n","    print_counter = 0\n","    previous_gramlist = gramlist.copy()\n","    for key_text, op_details in pattern_dict.items():\n","        optype = op_details['optype']\n","        do_sub = 'sub' in optype\n","        do_create = 'create' in optype\n","        do_replace = 'complete_replace' in optype\n","        do_regfind = do_create or do_replace\n","        reg_pattern = op_details['reg_pattern']\n","        replacement_text = op_details['replacement_text']\n","        gram_set_n = False  # don't try to expand the n-gram to an (n+x)-gram unless the information is provided\n","        if 'final_gram_n' in op_details.keys():\n","            gram_set_n = True\n","            final_gram_n = op_details['final_gram_n']\n","        if 'fill_strings' in op_details.keys():\n","            fill_strings = op_details['fill_strings']\n","        else:\n","            gram_set_n = False\n","\n","        updated_gramlist = previous_gramlist.copy()\n","\n","        if do_sub:   # do substitutions first (cleanup), then do create, then do full replace\n","            for idx, gram in enumerate(previous_gramlist):\n","                updated_gramlist[idx] = reg_pattern.sub(replacement_text, gram)\n","            previous_gramlist = updated_gramlist.copy()\n","\n","        if do_regfind:\n","            for idx, gram in enumerate(previous_gramlist):\n","                patt_find = reg_pattern.findall(previous_gramlist[idx])                                 # ['education ', ('', '', '20', '500'), ('21', 'failed', '', '')]\n","                patt_find = [[match.strip()] if type(match) == str else match for match in patt_find]   # [['education'], ['20', '500'], ['21', 'failed']]\n","                patt_find = [[x.strip() for x in match if (x != \"\")] for match in patt_find]            # converts findall from mix of strings and tuples(including empty strings) to compact list of lists of string matches\n","                flat_patt_find = [x for p in patt_find for x in p]                                      # ['education', '20', '500', '21', 'failed']\n","                if len(flat_patt_find) > 0: #find_list:  # proceed only if we have found some matches\n","                    if do_create:  # do creations before full replacements\n","                        new_grams = []\n","                        for nmatch, match_str in enumerate(flat_patt_find):\n","                            if match_str:  # make sure it's not an empty list that was found as one of the matching groups\n","                                if replacement_text:\n","                                    new_grams.append(replacement_text)\n","                                elif gram_set_n:\n","                                    new_grams.append(expand_gram(match_str, final_gram_n, fill_strings))\n","                                else:\n","                                    new_grams.append(match_str)\n","                        updated_gramlist += new_grams\n","\n","                \n","                    if do_replace:\n","                        print('You should not be in replace; not employed at this time')\n","                        modlist[idx] = replacement_text\n","\n","        previous_gramlist = updated_gramlist.copy()\n","    return updated_gramlist\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["done: Thu 08:28:13 06/18/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u7w-nOQNQuqO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483298996,"user_tz":240,"elapsed":54602,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"6ad5632f-70d1-4a7c-cd27-9f1451b5d236"},"source":["%%time\n","# Test it on a few rows  highlight_roots, cleanup_sub, cleanup_sub_create\n","# dlist = items_delimited.at[36,'delim_name_list'].copy()\n","\n","# for i in range(16000,16030):\n","#     dlist = items_delimited.at[i,'delim_name_list'].copy()\n","#     # print(dlist)\n","#     for clean_dict in [highlight_roots,cleanup_sub,cleanup_final]:\n","#         dlist = cleanup_service(dlist,clean_dict)\n","#     #print(dlist)\n","\n","# Let's remove duplicate entries and unwanted stuff\n","clean_items_delim = items_delimited.copy(deep=True)\n","alphnum = list(string.ascii_lowercase) + list('1234567890')  # get rid of all length=1 1-grams\n","def remove_dupes(gramlist):\n","    unwanted = set(['and','weighed in','given y'] + alphnum)\n","    gramset = unwanted\n","    return [x for x in gramlist if x not in gramset and not gramset.add(x)]\n","\n","for clean_dict in [highlight_roots,cleanup_sub,cleanup_final]: #[cleanup_games,cleanup_sub,cleanup_sub_create]:\n","    clean_items_delim.delim_name_list = clean_items_delim.delim_name_list.apply(lambda x: remove_dupes(cleanup_service(x,clean_dict)))\n","\n","clean_items_delim['d_len'] = clean_items_delim.delim_name_list.apply(lambda x: len(x))\n","clean_items_delim['d_maxgram'] = clean_items_delim.delim_name_list.apply(maxgram)\n","\n","print(f'done: {strftime(\"%a %X %x\")}\\n')\n","display(clean_items_delim.head())"],"execution_count":14,"outputs":[{"output_type":"stream","text":["done: Thu 08:28:18 06/18/20\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>item_name</th>\n","      <th>i_tested</th>\n","      <th>i_cat_id</th>\n","      <th>clean_item_name</th>\n","      <th>delim_name_list</th>\n","      <th>d_len</th>\n","      <th>d_maxgram</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>movie dvd : ! POWER IN glamor (PLAST.) D</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>[movie dvd, power in glamor, plast, dvd]</td>\n","      <td>4</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>program home and office digital : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]</td>\n","      <td>False</td>\n","      <td>76</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>9</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>movie dvd : *** In the glory (UNV) D</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>[movie dvd, in glory, unv, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>movie dvd : *** BLUE WAVE (Univ) D</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd blue wave univ dvd</td>\n","      <td>[movie dvd, blue wave, univ, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>movie dvd : *** BOX (GLASS) D</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd box glass dvd</td>\n","      <td>[movie dvd, box, glass, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id                                                                                                item_name  i_tested  i_cat_id  \\\n","0        0                                                                 movie dvd : ! POWER IN glamor (PLAST.) D     False        40   \n","1        1  program home and office digital : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]     False        76   \n","2        2                                                                     movie dvd : *** In the glory (UNV) D     False        40   \n","3        3                                                                       movie dvd : *** BLUE WAVE (Univ) D     False        40   \n","4        4                                                                            movie dvd : *** BOX (GLASS) D     False        40   \n","\n","                                                                                    clean_item_name  \\\n","0                                                               movie dvd power in glamor plast dvd   \n","1  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2                                                                        movie dvd in glory unv dvd   \n","3                                                                      movie dvd blue wave univ dvd   \n","4                                                                           movie dvd box glass dvd   \n","\n","                                                                                                                                                                                                                                                                        delim_name_list  \\\n","0                                                                                                                                                                                                                                              [movie dvd, power in glamor, plast, dvd]   \n","1  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]   \n","2                                                                                                                                                                                                                                                       [movie dvd, in glory, unv, dvd]   \n","3                                                                                                                                                                                                                                                     [movie dvd, blue wave, univ, dvd]   \n","4                                                                                                                                                                                                                                                          [movie dvd, box, glass, dvd]   \n","\n","   d_len  d_maxgram  \n","0      4          3  \n","1      9          6  \n","2      4          2  \n","3      4          2  \n","4      4          2  "]},"metadata":{"tags":[]}},{"output_type":"stream","text":["CPU times: user 4.95 s, sys: 1.65 ms, total: 4.95 s\n","Wall time: 4.95 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IUA58xQZ72NC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483298997,"user_tz":240,"elapsed":54594,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"4e0a4836-a8ab-4dab-cb91-b0b1439d0a2a"},"source":["# make item df easier to read for the following stuff\n","items_clean_delimited = clean_items_delim.copy(deep=True).drop(\"item_name\", axis=1).rename(columns = {'clean_item_name':'item_name'})\n","\n","display(items_clean_delimited.describe())\n","print(\"\\n\")\n","display(items_clean_delimited.head())\n","\n","print(f'\\ndone: {strftime(\"%a %X %x\")}')"],"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>i_cat_id</th>\n","      <th>d_len</th>\n","      <th>d_maxgram</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>22170</td>\n","      <td>22170</td>\n","      <td>22170</td>\n","      <td>22170</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>11,084.500</td>\n","      <td>46.291</td>\n","      <td>3.991</td>\n","      <td>4.628</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>6,400.072</td>\n","      <td>15.941</td>\n","      <td>2.092</td>\n","      <td>1.993</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>5,542.250</td>\n","      <td>37</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>11,084.500</td>\n","      <td>40</td>\n","      <td>3</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>16,626.750</td>\n","      <td>58</td>\n","      <td>5</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>22169</td>\n","      <td>83</td>\n","      <td>15</td>\n","      <td>18</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         item_id  i_cat_id  d_len  d_maxgram\n","count      22170     22170  22170      22170\n","mean  11,084.500    46.291  3.991      4.628\n","std    6,400.072    15.941  2.092      1.993\n","min            0         0      1          2\n","25%    5,542.250        37      2          3\n","50%   11,084.500        40      3          4\n","75%   16,626.750        58      5          5\n","max        22169        83     15         18"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>i_tested</th>\n","      <th>i_cat_id</th>\n","      <th>item_name</th>\n","      <th>delim_name_list</th>\n","      <th>d_len</th>\n","      <th>d_maxgram</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>[movie dvd, power in glamor, plast, dvd]</td>\n","      <td>4</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>76</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>9</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>[movie dvd, in glory, unv, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd blue wave univ dvd</td>\n","      <td>[movie dvd, blue wave, univ, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd box glass dvd</td>\n","      <td>[movie dvd, box, glass, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id  i_tested  i_cat_id                                                                                         item_name  \\\n","0        0     False        40                                                               movie dvd power in glamor plast dvd   \n","1        1     False        76  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2        2     False        40                                                                        movie dvd in glory unv dvd   \n","3        3     False        40                                                                      movie dvd blue wave univ dvd   \n","4        4     False        40                                                                           movie dvd box glass dvd   \n","\n","                                                                                                                                                                                                                                                                        delim_name_list  \\\n","0                                                                                                                                                                                                                                              [movie dvd, power in glamor, plast, dvd]   \n","1  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]   \n","2                                                                                                                                                                                                                                                       [movie dvd, in glory, unv, dvd]   \n","3                                                                                                                                                                                                                                                     [movie dvd, blue wave, univ, dvd]   \n","4                                                                                                                                                                                                                                                          [movie dvd, box, glass, dvd]   \n","\n","   d_len  d_maxgram  \n","0      4          3  \n","1      9          6  \n","2      4          2  \n","3      4          2  \n","4      4          2  "]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","done: Thu 08:28:18 06/18/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"USoJANp3U9Ei","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483298997,"user_tz":240,"elapsed":54586,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"50de0e3a-34d4-40fb-d59a-e75e468bef43"},"source":["#%%time\n","# Inspect a single n, gathered from all possible delimited n-grams (4.64sec to run this cell without GPU, 4.01sec with GPU)\n","n_in_ngram = 4    # look at, e.g. length-4 (4-grams) strings of words\n","print_top_f = 10  # printout the top xx ngram strings, sorted by frequency of occurrence in the data\n","\n","total_dupe_grams = 0\n","item_ngram = items_clean_delimited.copy(deep=True)\n","item_ngram['delim_ngrams'] = item_ngram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == n_in_ngram])\n","display(item_ngram.head())\n","item_ngram = item_ngram.explode('delim_ngrams').reset_index(drop=True) # < 0.2sec this method (CPU)\n","\n","freq_grams = item_ngram.delim_ngrams.value_counts()\n","grams_dupe = len(freq_grams[freq_grams > 1])\n","print(f'done: {strftime(\"%a %X %x\")}')\n","print('\\n')\n","print(f'Number of unique delimited {n_in_ngram}-grams: {len(freq_grams)}')\n","print(f'Number of unique delimited {n_in_ngram}-grams that are duplicated at least once: {grams_dupe}\\n')\n","print(f'Top {print_top_f:d} {n_in_ngram:d}-grams by frequency of appearance in item names:')\n","print(freq_grams[:print_top_f])\n","print('\\n')\n","display(item_ngram.head())"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>i_tested</th>\n","      <th>i_cat_id</th>\n","      <th>item_name</th>\n","      <th>delim_name_list</th>\n","      <th>d_len</th>\n","      <th>d_maxgram</th>\n","      <th>delim_ngrams</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>[movie dvd, power in glamor, plast, dvd]</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>76</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>[abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>[movie dvd, in glory, unv, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd blue wave univ dvd</td>\n","      <td>[movie dvd, blue wave, univ, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd box glass dvd</td>\n","      <td>[movie dvd, box, glass, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id  i_tested  i_cat_id                                                                                         item_name  \\\n","0        0     False        40                                                               movie dvd power in glamor plast dvd   \n","1        1     False        76  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2        2     False        40                                                                        movie dvd in glory unv dvd   \n","3        3     False        40                                                                      movie dvd blue wave univ dvd   \n","4        4     False        40                                                                           movie dvd box glass dvd   \n","\n","                                                                                                                                                                                                                                                                        delim_name_list  \\\n","0                                                                                                                                                                                                                                              [movie dvd, power in glamor, plast, dvd]   \n","1  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]   \n","2                                                                                                                                                                                                                                                       [movie dvd, in glory, unv, dvd]   \n","3                                                                                                                                                                                                                                                     [movie dvd, blue wave, univ, dvd]   \n","4                                                                                                                                                                                                                                                          [movie dvd, box, glass, dvd]   \n","\n","   d_len  d_maxgram                                                                                                                                            delim_ngrams  \n","0      4          3                                                                                                                                                      []  \n","1      9          6  [abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons]  \n","2      4          2                                                                                                                                                      []  \n","3      4          2                                                                                                                                                      []  \n","4      4          2                                                                                                                                                      []  "]},"metadata":{"tags":[]}},{"output_type":"stream","text":["done: Thu 08:28:18 06/18/20\n","\n","\n","Number of unique delimited 4-grams: 3464\n","Number of unique delimited 4-grams that are duplicated at least once: 378\n","\n","Top 10 4-grams by frequency of appearance in item names:\n","pc computing platform product               2882\n","educational development training lessons    1736\n","sony playstation gaming platform            1262\n","1c educational software learning            1042\n","microsoft xbox gaming platform               773\n","game pc standard version                     756\n","microsoft office productivity software       619\n","music cd production business                 397\n","gift gadget robot sport                      295\n","program home and office                      277\n","Name: delim_ngrams, dtype: int64\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>i_tested</th>\n","      <th>i_cat_id</th>\n","      <th>item_name</th>\n","      <th>delim_name_list</th>\n","      <th>d_len</th>\n","      <th>d_maxgram</th>\n","      <th>delim_ngrams</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>40</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>[movie dvd, power in glamor, plast, dvd]</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>76</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>abbyy educational software learning</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>76</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>pc computing platform product</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>76</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>microsoft office productivity software</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>76</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>educational development training lessons</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id  i_tested  i_cat_id                                                                                         item_name  \\\n","0        0     False        40                                                               movie dvd power in glamor plast dvd   \n","1        1     False        76  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2        1     False        76  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","3        1     False        76  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","4        1     False        76  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","\n","                                                                                                                                                                                                                                                                        delim_name_list  \\\n","0                                                                                                                                                                                                                                              [movie dvd, power in glamor, plast, dvd]   \n","1  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]   \n","2  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]   \n","3  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]   \n","4  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, abbyy educational software learning, pc computing platform product, microsoft office productivity software, educational development training lessons, online download version]   \n","\n","   d_len  d_maxgram                              delim_ngrams  \n","0      4          3                                       NaN  \n","1      9          6       abbyy educational software learning  \n","2      9          6             pc computing platform product  \n","3      9          6    microsoft office productivity software  \n","4      9          6  educational development training lessons  "]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Uvreedy4YRVd"},"source":["#####Gather all info for duplicated n-grams in our delimited set"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7H2VxLddVqlJ","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483301196,"user_tz":240,"elapsed":56777,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"41ecad74-7895-4aa3-faf3-102857996306"},"source":["%%time \n","# Should take < 4sec on CPU\n","\n","# Get all of the delimited n-grams that are duplicated at least once in item names\n","#  range of sizes of delimited phrases (number of 'words'):\n","\n","min_gram = 1\n","max_gram = items_delimited.d_maxgram.max()\n","\n","total_dupe_grams = 0\n","gram_freqs = {}   # dict will hold elements that are pd.Series with index = phrase, value = number of repeats in items database item names\n","for n in range(min_gram,max_gram+1):\n","    item_ngram = items_clean_delimited.copy(deep=True)\n","    item_ngram['delim_ngrams'] = item_ngram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == n])\n","    item_ngram = item_ngram.explode('delim_ngrams').reset_index(drop=True)  \n","    freq_grams = item_ngram.delim_ngrams.value_counts()\n","    print(f'Number of unique delimited {n}-grams: {len(freq_grams)}')\n","    grams_dupe = len(freq_grams[freq_grams > 1])\n","    print(f'Number of unique delimited {n}-grams that are duplicated at least once: {grams_dupe}\\n')\n","    if grams_dupe > 0:\n","        gram_freqs[n] = freq_grams[freq_grams > 1].copy(deep=True)\n","        total_dupe_grams += grams_dupe\n","print(f'\\nTotal number of unique, delimited, duplicated n-grams for all n: {total_dupe_grams}')\n","print(f'\\nGram Processing Done: {strftime(\"%a %X %x\")}\\n')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Number of unique delimited 1-grams: 2817\n","Number of unique delimited 1-grams that are duplicated at least once: 1169\n","\n","Number of unique delimited 2-grams: 4236\n","Number of unique delimited 2-grams that are duplicated at least once: 1191\n","\n","Number of unique delimited 3-grams: 3867\n","Number of unique delimited 3-grams that are duplicated at least once: 751\n","\n","Number of unique delimited 4-grams: 3464\n","Number of unique delimited 4-grams that are duplicated at least once: 378\n","\n","Number of unique delimited 5-grams: 2852\n","Number of unique delimited 5-grams that are duplicated at least once: 295\n","\n","Number of unique delimited 6-grams: 1936\n","Number of unique delimited 6-grams that are duplicated at least once: 144\n","\n","Number of unique delimited 7-grams: 1259\n","Number of unique delimited 7-grams that are duplicated at least once: 68\n","\n","Number of unique delimited 8-grams: 829\n","Number of unique delimited 8-grams that are duplicated at least once: 31\n","\n","Number of unique delimited 9-grams: 522\n","Number of unique delimited 9-grams that are duplicated at least once: 18\n","\n","Number of unique delimited 10-grams: 264\n","Number of unique delimited 10-grams that are duplicated at least once: 10\n","\n","Number of unique delimited 11-grams: 150\n","Number of unique delimited 11-grams that are duplicated at least once: 5\n","\n","Number of unique delimited 12-grams: 75\n","Number of unique delimited 12-grams that are duplicated at least once: 5\n","\n","Number of unique delimited 13-grams: 34\n","Number of unique delimited 13-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 14-grams: 18\n","Number of unique delimited 14-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 15-grams: 9\n","Number of unique delimited 15-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 16-grams: 2\n","Number of unique delimited 16-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 17-grams: 2\n","Number of unique delimited 17-grams that are duplicated at least once: 0\n","\n","\n","Total number of unique, delimited, duplicated n-grams for all n: 4065\n","\n","Gram Processing Done: Thu 08:28:20 06/18/20\n","\n","CPU times: user 2.52 s, sys: 19.4 ms, total: 2.54 s\n","Wall time: 2.54 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hnkJJylUh55B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483301375,"user_tz":240,"elapsed":56950,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"77b5b5fc-3434-4250-9420-0100d865d02f"},"source":["'''\n","May 25: try adjusting code to incude ngrams in range 1 and up, but reduce weight for n-grams that contain many common words\n","'''\n","start_n = 0\n","finish_n = 10\n","# first, inspect data to see what are the common n-grams of little value in determining cluster coupling\n","df_busy_grams=pd.DataFrame({'n3_names':gram_freqs[3].index[start_n:finish_n], 'n3_counts':gram_freqs[3].values[start_n:finish_n],\n","                 'n4_names':gram_freqs[4].index[start_n:finish_n], 'n4_counts':gram_freqs[4].values[start_n:finish_n],\n","                 'n5_names':gram_freqs[5].index[start_n:finish_n], 'n5_counts':gram_freqs[5].values[start_n:finish_n]\n","                 })\n","display(df_busy_grams)"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>n3_names</th>\n","      <th>n3_counts</th>\n","      <th>n4_names</th>\n","      <th>n4_counts</th>\n","      <th>n5_names</th>\n","      <th>n5_counts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>music music media</td>\n","      <td>3582</td>\n","      <td>pc computing platform product</td>\n","      <td>2882</td>\n","      <td>program home and office digital</td>\n","      <td>333</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>online download version</td>\n","      <td>2097</td>\n","      <td>educational development training lessons</td>\n","      <td>1736</td>\n","      <td>antivirus defender internet security software</td>\n","      <td>245</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>movie bluray dvd</td>\n","      <td>1787</td>\n","      <td>sony playstation gaming platform</td>\n","      <td>1262</td>\n","      <td>xbox 360 russian language version</td>\n","      <td>151</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>russian language version</td>\n","      <td>1688</td>\n","      <td>1c educational software learning</td>\n","      <td>1042</td>\n","      <td>batman game computer electronic multirelease</td>\n","      <td>136</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>game pc digital</td>\n","      <td>1125</td>\n","      <td>microsoft xbox gaming platform</td>\n","      <td>773</td>\n","      <td>call of duty game computer</td>\n","      <td>135</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>game xbox 360</td>\n","      <td>501</td>\n","      <td>game pc standard version</td>\n","      <td>756</td>\n","      <td>warhammer game computer electronic multirelease</td>\n","      <td>115</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>payment card ticket</td>\n","      <td>371</td>\n","      <td>microsoft office productivity software</td>\n","      <td>619</td>\n","      <td>angry bird game computer electronic</td>\n","      <td>97</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>gift soft toy</td>\n","      <td>366</td>\n","      <td>music cd production business</td>\n","      <td>397</td>\n","      <td>star war game computer electronic</td>\n","      <td>93</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>english language version</td>\n","      <td>346</td>\n","      <td>gift gadget robot sport</td>\n","      <td>295</td>\n","      <td>lego brand lego style game</td>\n","      <td>91</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>cinema special version</td>\n","      <td>332</td>\n","      <td>program home and office</td>\n","      <td>277</td>\n","      <td>borderland game computer electronic multirelease</td>\n","      <td>82</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   n3_names  n3_counts                                  n4_names  n4_counts                                          n5_names  n5_counts\n","0         music music media       3582             pc computing platform product       2882                   program home and office digital        333\n","1   online download version       2097  educational development training lessons       1736     antivirus defender internet security software        245\n","2          movie bluray dvd       1787          sony playstation gaming platform       1262                 xbox 360 russian language version        151\n","3  russian language version       1688          1c educational software learning       1042      batman game computer electronic multirelease        136\n","4           game pc digital       1125            microsoft xbox gaming platform        773                        call of duty game computer        135\n","5             game xbox 360        501                  game pc standard version        756   warhammer game computer electronic multirelease        115\n","6       payment card ticket        371    microsoft office productivity software        619               angry bird game computer electronic         97\n","7             gift soft toy        366              music cd production business        397                 star war game computer electronic         93\n","8  english language version        346                   gift gadget robot sport        295                        lego brand lego style game         91\n","9    cinema special version        332                   program home and office        277  borderland game computer electronic multirelease         82"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"sw9OnsLGv5P0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483301861,"user_tz":240,"elapsed":57431,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"b8d5f76d-d9a3-4ab5-a335-5a71b2eacea0"},"source":["# format data for feeding into word vector creator\n","\n","count_bins = [0, 2, 4, 8, 16, 32, 128, 1024, 32768]\n","idf_weights = [8,7,6,5,4,3,2,1]  # more weight for ngrams with lower counts\n","\n","notfirst = False\n","for n,s in gram_freqs.items():\n","    a=len(s)\n","    n_array = np.ones(a,dtype=np.int32)*n\n","    gram_count = s.values.astype(np.int32)\n","    gram_string0 = s.index.to_numpy(dtype='str')\n","    gram_string = [re.compile(r'\\b' + gs + r'\\b') for gs in gram_string0]  # I'm not looking for partial words; n-grams must match at word boundaries\n","    weight_bin = pd.cut(s,count_bins,labels=idf_weights,retbins=False).astype(np.int32)\n","\n","    if notfirst:\n","        n_arrays = np.concatenate((n_arrays,n_array))\n","        gram_counts = np.concatenate((gram_counts,gram_count))\n","        gram_strings = np.concatenate((gram_strings,gram_string))\n","        weight_bins = np.concatenate((weight_bins,weight_bin))\n","    else:\n","        n_arrays = n_array\n","        gram_counts = gram_count\n","        gram_strings = gram_string\n","        weight_bins = weight_bin\n","        notfirst = True\n","\n","print(n_arrays[:5],gram_counts[:5],gram_strings[:5],weight_bins[:5])\n","print(len(n_arrays))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[1 1 1 1 1] [2742 1835 1360  893  816] [re.compile('\\\\bpc\\\\b') re.compile('\\\\bregion\\\\b')\n"," re.compile('\\\\bjewel\\\\b') re.compile('\\\\b1c\\\\b') re.compile('\\\\bcd\\\\b')] [1 1 1 2 2]\n","4065\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EdrBr4rN7wr8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592483301862,"user_tz":240,"elapsed":57430,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# use np matrix storage to speed this up... the following code cell takes about 3 min using np, vs. 8 min with pandas dataframe calculations\n","#   also, reducing np matrix to hold only ngrams of size 3 or greater (5/25/20) takes 48 sec on CPU\n","def make_word_vecs(item_names, ngram_re_patterns, ngram_ns, ngram_weights):\n","    \"\"\"Output is word vectors for input containing item names (english transl)\"\"\"\n","\n","    # create np zeros array of size (number of items, word vector length)\n","    n_items = len(item_names)\n","    wv_len = len(ngram_ns)\n","    item_vec_array = np.zeros((n_items, wv_len), dtype = np.int32)\n","\n","    for g in range(wv_len):\n","        gram_pattern = ngram_re_patterns[g] \n","        gram_len = ngram_ns[g]\n","        gram_weight = ngram_weights[g]\n","        for i in range(n_items):\n","            if gram_pattern.search(item_names[i]):\n","                item_vec_array[i,g] = 2 * gram_len * gram_weight  # use weighting function 2 * (n= length of ngram) * (idf weight from binning above)\n","    return item_vec_array\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrNVLGs1nLmR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483470527,"user_tz":240,"elapsed":226089,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"35b7b884-cadc-4d5e-d378-bbbb3e0072e7"},"source":["%%time\n","item_word_vectors = make_word_vecs(items_clean_delimited.loc[:,'item_name'].to_numpy(dtype='str'), gram_strings,n_arrays,weight_bins)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["CPU times: user 2min 48s, sys: 233 ms, total: 2min 48s\n","Wall time: 2min 48s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_p1QepqQp1kv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592483470527,"user_tz":240,"elapsed":226087,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# # intermediate point: can save word vectors here for the 22170 items\n","#np.savez_compressed('data_output/item_word_vectorsCompressed.npz', arrayname = item_word_vectors)\n","# # ...\n","# iwv = np.load(\"data_output/item_word_vectors.npz\")\n","# item_word_vectors = iwv['arrayname']\n","# print(item_word_vectors.shape)"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_B2ZZPQigJmh"},"source":["#####Use scipy sparse matrices instead of pandas... faster, and less memory use"]},{"cell_type":"code","metadata":{"id":"oWtui0wwF1Yp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592483471025,"user_tz":240,"elapsed":226583,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["item_vec_matrix = sparse.csr_matrix(item_word_vectors)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQEF0onOUtxz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483472910,"user_tz":240,"elapsed":228462,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"34b9f49f-9d71-4f72-db39-8af54bc9d49e"},"source":["%%time\n","# <2sec for 21,700 items x 4000+ ngrams; output is a csr matrix of type int64\n","dots = item_vec_matrix.dot(item_vec_matrix.transpose()) "],"execution_count":24,"outputs":[{"output_type":"stream","text":["CPU times: user 1.45 s, sys: 443 ms, total: 1.89 s\n","Wall time: 1.9 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yFPX5PouV_aw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592483472911,"user_tz":240,"elapsed":228461,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# wicked fast way to get top K # of items by dot product value (i.e., closest K items to the item of interest)\n","# https://stackoverflow.com/questions/31790819/scipy-sparse-csr-matrix-how-to-get-top-ten-values-and-indices\n","# also, great reference for speeding up python here: https://colab.research.google.com/drive/1nMDtWcVZCT9q1VWen5rXL8ZHVlxn2KnL\n","\n","@jit(cache=True)\n","def row_topk_csr(data, indices, indptr, K):\n","    \"\"\"Take a sparse scipy csr matrix, and for each column, find the K largest \n","    values in that column (like argmax or argsort[:K]).  Return the row indices \n","    and associated values for each column as two separate np arrays of \n","    length = number of columns in sparse matrix.  Inputs are data/indices/indptr\n","    of csr matrix, and integer K.  Call function like this:\n","    rows, vals = row_topk_csr(csr_name.data, csr_name.indices, csr_name.indptr, K)\n","    Use jit by importing jit and prange from numba, and decorating with\n","    @jit(cache=True) immediately before this function definition\n","    (adopted from https://stackoverflow.com/users/3924566/deepak-saini ) \"\"\"\n","\n","    m = indptr.shape[0] - 1\n","    max_indices = np.zeros((m, K), dtype=indices.dtype)\n","    max_values = np.zeros((m, K), dtype=data.dtype)\n","    # for i in prange(m):\n","    #     top_inds = np.argsort(data[indptr[i] : indptr[i + 1]])[::-1][:K]\n","    #     max_indices[i] = indices[indptr[i] : indptr[i + 1]][top_inds]\n","    #     max_values[i] = data[indptr[i] : indptr[i + 1]][top_inds]\n","    for i in prange(m):\n","        top_inds = np.arange(22190-K,22190)\n","        tops = np.argsort(data[indptr[i] : indptr[i + 1]])[::-1][:K]\n","        top_inds[:len(tops)] = tops\n","        #print(i,top_inds)\n","        max_indices[i] = indices[indptr[i] : indptr[i + 1]][top_inds]\n","        max_values[i] = data[indptr[i] : indptr[i + 1]][top_inds]\n","\n","    return max_indices, max_values\n"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"3D9OVzjXDlGm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483479901,"user_tz":240,"elapsed":235446,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"5fb3170e-e090-468d-ddf0-5a54ae0f4904"},"source":["%%time\n","dots.setdiag(0)\n","print(dots.indptr.shape)\n","kval = 20\n","closest_indices, highest_values = row_topk_csr(dots.data, dots.indices, dots.indptr, K=kval)  # Changed K from 10 to 2 on 5/25/20 to 3 on 5/26 to 15 with fakedots"],"execution_count":26,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n"],"name":"stderr"},{"output_type":"stream","text":["(22171,)\n","CPU times: user 6.4 s, sys: 485 ms, total: 6.88 s\n","Wall time: 6.92 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-xjDRQcM9odF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483479901,"user_tz":240,"elapsed":235441,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"44237f99-8256-4ba6-e582-ae124dbeff9a"},"source":["#print(closest_indices.shape)\n","print(closest_indices[:5,:7]) #[:10,:])\n","print(highest_values[8000:8008,:7])"],"execution_count":27,"outputs":[{"output_type":"stream","text":["[[ 9920  9922 16973 21346 21661  9932 21667]\n"," [ 1155  1154  1156  1152  1153  1157  1182]\n"," [17212 16518 16519 16521 16616 16691 16692]\n"," [19630  9633 20027 10463 10462  9029 12427]\n"," [ 9172  2716  9449  9450  9451  7732  7845]]\n","[[ 656  480  480  480  480  480  480]\n"," [ 864  864  864  864  864  864  656]\n"," [ 224  224  224  224  224  224  224]\n"," [1024 1024 1024 1024 1024  224  224]\n"," [ 152  152  152  152  152  152  152]\n"," [2448  928  864  864  864  864  864]\n"," [2448  736  736  528  480  480  480]\n"," [ 772  720  708  708  708  672  672]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OfOzk_xWnF7q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483482154,"user_tz":240,"elapsed":237690,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"e247babf-eda9-4463-abd9-d42fc797cf81"},"source":["similar_items = pd.DataFrame({'item_id':range(22170)}) #,'close_item_idx':closest_indices,'close_item_dot':highest_values})\n","similar_items['close_item_idx'] = [closest_indices[x][:kval] for x in range(22170)]\n","similar_items['close_item_dot'] = [highest_values[x][:kval] for x in range(22170)]\n","similar_items = similar_items.merge(items_clean_delimited[['item_id','i_tested','i_cat_id']], on='item_id')\n","similar_items['close_item_cat'] = similar_items.close_item_idx.apply(lambda x: [items_augmented.at[i,'item_category_id'] for i in x])\n","print(similar_items.head())\n"],"execution_count":28,"outputs":[{"output_type":"stream","text":["   item_id                                                                                                                                close_item_idx  \\\n","0        0        [9920, 9922, 16973, 21346, 21661, 9932, 21667, 12449, 20043, 21420, 15819, 16616, 13950, 10811, 8125, 10290, 14864, 17831, 8635, 8631]   \n","1        1                      [1155, 1154, 1156, 1152, 1153, 1157, 1182, 1177, 1174, 1172, 1184, 1170, 1181, 5730, 3873, 3876, 3878, 3877, 3875, 3874]   \n","2        2  [17212, 16518, 16519, 16521, 16616, 16691, 16692, 16973, 17125, 17251, 18732, 17265, 17352, 17518, 17831, 17910, 18530, 18531, 18532, 16513]   \n","3        3                [19630, 9633, 20027, 10463, 10462, 9029, 12427, 13115, 2486, 4662, 5639, 5638, 5603, 5600, 5582, 5163, 4668, 4661, 4569, 4660]   \n","4        4         [9172, 2716, 9449, 9450, 9451, 7732, 7845, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17732, 17738, 17739, 17740, 17741, 17742]   \n","\n","                                                                                                close_item_dot  i_tested  i_cat_id                                                                    close_item_cat  \n","0         [288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288]     False        40  [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]  \n","1  [10564, 1832, 1832, 1768, 1284, 1048, 984, 984, 984, 984, 984, 984, 964, 948, 932, 932, 928, 928, 928, 928]     False        76  [75, 76, 76, 76, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 29, 28, 23, 19, 23, 19]  \n","2         [288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288]     False        40  [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]  \n","3         [132, 132, 132, 132, 116, 116, 116, 116, 116, 116, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]     False        40       [40, 40, 40, 40, 37, 37, 55, 43, 56, 59, 2, 2, 5, 5, 5, 67, 55, 58, 55, 58]  \n","4         [816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816]     False        40  [43, 28, 43, 43, 43, 23, 28, 43, 43, 43, 43, 43, 43, 43, 28, 75, 75, 75, 75, 75]  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OWuI-nw27XFH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483483442,"user_tz":240,"elapsed":238972,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"911af5f5-f9b4-40a9-cccc-7cd8b13bc443"},"source":["# create a graph with nodes = item ids in test set, and edge weights = dot product values\n","\n","# we will use the \"community\" algorithms to determine useful groupings of other items around/including the test items\n","# ##### start with a graph containing the 5100 items in the test set as starter nodes, and add in the 10 highest-match wordvector items if dot product > threshold\n","# TAKING A LEAP... gonna try with 21700 full items dataset / top 10 matches\n","\n","edge_threshold = 100  # dot product (edge weight) must be greater than this for two item_ids to be connected in the graph\n","\n","graph_items = similar_items[['item_id','close_item_idx']].copy(deep=True).explode('close_item_idx').reset_index(drop=True)\n","graph_weights = similar_items[['item_id','close_item_dot']].copy(deep=True).explode('close_item_dot').reset_index(drop=True)\n","graph_items['weight'] = graph_weights.loc[:]['close_item_dot']\n","graph_items.columns = ['item1_id','item2_id','weight']\n","\n","print(len(graph_items))\n","graph_items = graph_items[graph_items.weight > edge_threshold]\n","print(len(graph_items))\n","graph_items.head()\n","# depending on threshold, we may end up dropping some of the test items (for example, we lose item 22154 if threshold = 150, but not if threshold = 100)\n","# K=15\n","# 332550\n","# x\n","# 284132 (thresh 100)\n","# 265912 (200)\n","# 143099 (500)\n","\n","# K=10\n","# 221700\n","# x\n","# 192094 (100)\n","\n","# K = 20\n","# 443400 -> 374990 (100)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["443400\n","375055\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item1_id</th>\n","      <th>item2_id</th>\n","      <th>weight</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>9920</td>\n","      <td>288</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>9922</td>\n","      <td>288</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>16973</td>\n","      <td>288</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>21346</td>\n","      <td>288</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>21661</td>\n","      <td>288</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item1_id item2_id weight\n","0         0     9920    288\n","1         0     9922    288\n","2         0    16973    288\n","3         0    21346    288\n","4         0    21661    288"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"cNTMKGOREZGL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483487945,"user_tz":240,"elapsed":243471,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"8bd29033-1983-4e65-982f-1ba9e33fc284"},"source":["%%time\n","# import pandas df into weighted-edge graph:\n","G = nx.from_pandas_edgelist(graph_items, 'item1_id', 'item2_id', ['weight'])"],"execution_count":30,"outputs":[{"output_type":"stream","text":["CPU times: user 4.59 s, sys: 82.9 ms, total: 4.67 s\n","Wall time: 4.68 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iR2U0g8iY0AN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483532453,"user_tz":240,"elapsed":287973,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"85540945-f836-4982-db2e-edb92fa9fe3d"},"source":["%%time\n","# employ a clustering method that utilizes the edge weights\n","communities2 = community.asyn_lpa_communities(G, weight='weight', seed=42)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["CPU times: user 44.2 s, sys: 14.3 ms, total: 44.2 s\n","Wall time: 44.2 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2fBOqdalFBx1"},"source":["#####Extract and display community (cluster) information"]},{"cell_type":"code","metadata":{"id":"dtbKGa2CcjJa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483535467,"user_tz":240,"elapsed":290980,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"8d746135-1c3a-4f9d-83cc-989fdb14d546"},"source":["num_communities = 0\n","community_items = set()\n","cluster_nodes = []\n","n_nodes = []\n","weight_avgs = []\n","weight_sums = []\n","weight_maxs = []\n","weight_mins = []\n","weight_stds = []\n","for i,c in enumerate(communities2):\n","    num_communities += 1\n","    community_items = community_items | set(c)\n","    nodelist = list(c)\n","    n_nodes.append(len(nodelist))\n","    edgeweights = []\n","    for m in range(n_nodes[-1]-1):\n","        for n in range(m+1,n_nodes[-1]):\n","            try:\n","                edgeweights.append(G.edges[nodelist[m], nodelist[n]]['weight'])\n","            except:\n","                pass\n","    cluster_nodes.append(nodelist)\n","    weight_avgs.append(np.mean(edgeweights))\n","    weight_sums.append(np.sum(edgeweights))\n","    weight_maxs.append(np.max(edgeweights))\n","    weight_mins.append(np.min(edgeweights))\n","    weight_stds.append(np.std(edgeweights))\n","\n","print(num_communities)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["1403\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vw2SkxaFd1qm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483535468,"user_tz":240,"elapsed":290971,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"152aba22-4461-4501-87af-3036aa993416"},"source":["weight_avgs = [round(x) for x in weight_avgs]\n","community_df = pd.DataFrame({'n_nodes':n_nodes,'w_avg':weight_avgs,'w_sum':weight_sums,'w_max':weight_maxs,'w_min':weight_mins,'w_std':weight_stds,'cluster_members':cluster_nodes})\n","print(community_df.head())\n","print(\"\\n\")\n","print(community_df.describe())"],"execution_count":33,"outputs":[{"output_type":"stream","text":["   n_nodes  w_avg   w_sum  w_max  w_min     w_std  \\\n","0      111    351  472856  21220    212   651.611   \n","1       33   2599  956500   4000    160 1,568.929   \n","2       27    330   78836   4368    256   341.139   \n","3        4   3424   20544   3424   3424         0   \n","4       21   1537  195140   6284    104 1,473.718   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               cluster_members  \n","0  [0, 2, 20, 12828, 14366, 14885, 14381, 1070, 1071, 10288, 1072, 10290, 19520, 12354, 20043, 16973, 10833, 12883, 18530, 18531, 18532, 17518, 9843, 19060, 19062, 13950, 16513, 16518, 16519, 20102, 16521, 19611, 21661, 12449, 11940, 14522, 14523, 9920, 9922, 10442, 9932, 14545, 11492, 17125, 14054, 14055, 16616, 14053, 14581, 14078, 13058, 14084, 14597, 17156, 11015, 9993, 9486, 9487, 9488, 9489, 9490, 15635, 9492, 15634, 9500, 9502, 11039, 15653, 9515, 9519, 9520, 16691, 16692, 17212, 17213, 9027, 20308, 9557, 12630, 12631, 12632, 21346, 17251, 10083, 9065, 17265, 18805, 21880, 10113, 9604, 9617, 18843, 14243, 17831, 9643, 21420, 8631, 17352, 8136, 11209, ...]  \n","1                                                                                                                                                                                                                                                                                                                                                                                                                                                           [15751, 12436, 21664, 21665, 21666, 21667, 21668, 21669, 21670, 21671, 21672, 21674, 21675, 21676, 21677, 21678, 21679, 21680, 21681, 21682, 21683, 21684, 14901, 14902, 12352, 8406, 8407, 13290, 13291, 9068, 9070, 19568, 6908]  \n","2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [11412, 16410, 16411, 19232, 21539, 9512, 10793, 10794, 9513, 10797, 10798, 10800, 10801, 10805, 10806, 10807, 12344, 12343, 10810, 10811, 10814, 10815, 18635, 11239, 2923, 2925, 21876]  \n","3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [8123, 8124, 8125, 8126]  \n","4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [14864, 16402, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 8624, 14128, 8635, 14789, 11471, 11472, 21715]  \n","\n","\n","       n_nodes     w_avg       w_sum     w_max     w_min     w_std\n","count     1403      1403        1403      1403      1403      1403\n","mean    14.544 2,073.382 106,694.218 3,762.831 1,791.387   345.491\n","std     43.566 3,094.246 377,171.862 4,876.017 3,103.257   751.800\n","min          2       112         132       112       104         0\n","25%          2       439        2336       944       256         0\n","50%          4      1040        9440      2064       576    13.459\n","75%         13      2336       56738      4644      2048   368.779\n","max       1177     31264     7393596     41760     31264 9,408.742\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k1Ch9mz6BNZa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1592483535635,"user_tz":240,"elapsed":291126,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"34e23203-e286-4984-d7e3-9658df6cca08"},"source":["cluster_items = community_df[['n_nodes','cluster_members']].copy(deep=True).explode('cluster_members').reset_index(drop=True)\n","print(f'community_df length: {len(community_df)}')\n","print(f'cluster_items df length: {len(cluster_items)}')\n","print(f'number of unique item ids contained in clusters: {cluster_items.cluster_members.nunique()}')\n","for nn in [9,24,49]:\n","    nn_community = community_df.query(\"n_nodes > @nn\").copy(deep=True)\n","    print(f'number of clusters with at least {nn+1} items as members: {len(nn_community)}')\n","    print(nn_community.describe())\n","    print('\\n')"],"execution_count":34,"outputs":[{"output_type":"stream","text":["community_df length: 1403\n","cluster_items df length: 20405\n","number of unique item ids contained in clusters: 20405\n","number of clusters with at least 10 items as members: 423\n","       n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","count      423       423         423       423     423       423\n","mean    40.201   991.073 331,544.217 5,495.102 382.809   686.413\n","std     73.157 1,041.634 631,964.521 6,309.470 404.129   902.919\n","min         10       142        9572       168     104         0\n","25%         15   445.500       65140      1328     196   149.370\n","50%         23       678      135700      3452     256   399.994\n","75%     41.500 1,130.500      359504      7088     400   879.486\n","max       1177     11251     7393596     41760    3192 9,408.742\n","\n","\n","number of clusters with at least 25 items as members: 195\n","       n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","count      195       195         195       195     195       195\n","mean    68.687   882.395 585,480.205 6,718.790 289.928   629.107\n","std    100.541   885.414 848,690.400 6,904.979 229.663   907.604\n","min         25       159       39676       520     104     8.582\n","25%         30   440.500      191580      2068     166   163.425\n","50%         43       652      348460      4488     256   399.994\n","75%         74 1,037.500      658216      8072     340   751.046\n","max       1177      9639     7393596     41760    2448 9,408.742\n","\n","\n","number of clusters with at least 50 items as members: 84\n","       n_nodes   w_avg         w_sum     w_max   w_min     w_std\n","count       84      84            84        84      84        84\n","mean   115.690 698.024   903,738.476 7,035.857 239.048   431.044\n","std    140.131 507.148 1,054,408.702 6,395.942 100.216   396.648\n","min         50     159         57832       564     104     9.500\n","25%         60 386.250        381956      2497     152   150.409\n","50%         77 575.500        632448      5196     212   290.940\n","75%    120.250 816.250       1024385      9687     288   547.277\n","max       1177    3497       7393596     32132     592 1,793.495\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BvopMozVAK2W","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592483535636,"user_tz":240,"elapsed":291125,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# with K=15 and threshold = 100, we get 1624 clusters, quantiles of n_nodes = 2 min, 2, 4 med, 11, 1161 max; 479 clusters with at least 10 items 479/10... 182/25...73/50;  20405 items actually clustered (out of 21700)\n","# with K=15 and threshold = 200, we get 1664 clusters, quantiles of n_nodes = 2 min, 2, 4 med, 10, 1164 max; 431 clusters with at least 10 items, 19787 items actually clustered (out of 21700)\n","# with K=15 and threshold = 500, we get 1843 clusters, quantiles of n_nodes = 2 min, 2, 3 med,  6,  236 max; 319 clusters with at least 10 items 319/10... 103/25...31/50;  13422 items actually clustered (out of 21700)\n","# with K=10 and threshold = 100, we get 1962 clusters, quantiles of n_nodes = 2 min, 2, 4 med, 10, 1096 max; 529 clusters with at least 10 items 529/10... 152/25...56/50;  20404 items actually clustered (out of 21700)\n","# community_df length: 1962\n","# cluster_items df length: 20404\n","# number of unique item ids contained in clusters: 20404\n","# number of clusters with at least 10 items as members: 529\n","#        n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","# count      529       529         529       529     529       529\n","# mean    28.371 1,118.951 151,510.904 4,402.389 423.274   719.163\n","# std     56.233 1,174.116 292,205.958 5,415.598 359.267   967.964\n","# min         10       132        2904       132     104         0\n","# 25%         12       509       36548      1108     228   149.643\n","# 50%         17       791       75604      2604     320   385.788\n","# 75%         27      1293      165472      5668     504   987.902\n","# max       1096     11827     3816308     41760    2940 9,873.388\n","\n","\n","# number of clusters with at least 25 items as members: 152\n","#        n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","# count      152       152         152       152     152       152\n","# mean    61.704   917.283 309,274.921 4,552.974 339.474   537.762\n","# std     97.197 1,102.951 480,567.401 5,888.855 251.088   947.367\n","# min         25       167       21352       244     104         0\n","# 25%         30   436.250      108438      1088     196   106.725\n","# 50%         39       639      191836      2586     272   255.362\n","# 75%         58 1,001.750      317200      5629     400   671.011\n","# max       1096     11495     3816308     41760    2448 9,873.388\n","\n","\n","# number of clusters with at least 50 items as members: 56\n","#        n_nodes   w_avg       w_sum     w_max   w_min     w_std\n","# count       56      56          56        56      56        56\n","# mean   110.143 670.875 458,976.500 4,891.786 268.071   423.299\n","# std    148.554 403.079 562,950.003 5,501.423 108.325   516.756\n","# min         50     167       84276       372     104     5.652\n","# 25%     56.500 403.500      198484      1288     196    95.334\n","# 50%         72 558.500      314638      2720     256   255.362\n","# 75%    106.250 823.500      534489      6053     320   575.131\n","# max       1096    1989     3392540     22140     608 2,978.267\n","#\n","\n","\n","#####################################################\n","## use clusters n>49, k=20, thresh = 100\n","###################################\n","\n","# # with K=20 and threshold = 100, we get 1387 clusters, quantiles of n_nodes = 2 min, 2, 4 med, 13, 1319 max; 422 clusters with at least 10 items 422/10... 179/25...78/50;  20406 items actually clustered (out of 21700)\n","# community_df length: 1387\n","# cluster_items df length: 20406\n","# number of unique item ids contained in clusters: 20406\n","# number of clusters with at least 10 items as members: 422\n","#        n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","# count      422       422         422       422     422       422\n","# mean    40.405   987.488 334,640.844 5,371.365 382.237   706.833\n","# std     79.321 1,002.201 663,848.830 6,042.819 366.626   930.412\n","# min         10       142        9836       168     104         0\n","# 25%         15   445.500       61230      1387     196   168.636\n","# 50%         22   688.500      130628      3394     256   424.621\n","# 75%         39 1,149.750      363573      6901     416   910.528\n","# max       1319     11251     7558860     41760    3192 9,339.161\n","\n","\n","# number of clusters with at least 25 items as members: 179\n","#        n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","# count      179       179         179       179     179       179\n","# mean    73.682   878.994 633,793.497 6,350.324 295.330   606.438\n","# std    113.674   860.866 924,124.723 6,362.418 230.057   891.521\n","# min         25       165       36576       484     104     9.963\n","# 25%         33   435.500      198778      2168     164   180.056\n","# 50%         44       623      388704      4276     256   377.799\n","# 75%         75 1,072.500      667122      7824     342   767.243\n","# max       1319      8846     7558860     41760    2448 9,339.161\n","\n","\n","# number of clusters with at least 50 items as members: 78\n","#        n_nodes     w_avg         w_sum     w_max   w_min     w_std\n","# count       78        78            78        78      78        78\n","# mean   124.269   822.628 1,054,526.103 7,313.077 248.205   559.201\n","# std    158.805 1,056.614 1,253,915.801 7,405.189 106.790 1,093.614\n","# min         50       179         68600       528     104     9.963\n","# 25%     63.250   405.250        453204      2467     167   154.897\n","# 50%         83   562.500        666060      4136     244   300.057\n","# 75%    138.750   891.250       1162252      9802     272   628.492\n","# max       1319      8846       7558860     41760     592 9,339.161\n","\n","# updated regexes, 6/16/20... k=20, thr=100\n","# community_df length: 1403\n","# cluster_items df length: 20405\n","# number of unique item ids contained in clusters: 20405\n","# number of clusters with at least 10 items as members: 429\n","#        n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","# count      429       429         429       429     429       429\n","# mean    39.816   964.718 330,242.284 5,215.291 363.347   666.080\n","# std     71.759 1,017.564 639,627.695 5,820.533 306.239   891.461\n","# min         10       146        7288       168     104         0\n","# 25%         15       445       62944      1348     196   171.419\n","# 50%         23       660      126668      3328     260   384.643\n","# 75%         40      1121      355724      6904     408   843.593\n","# max       1149     11251     7195364     41760    2940 9,400.753\n","\n","\n","# number of clusters with at least 25 items as members: 190\n","#        n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","# count      190       190         190       190     190       190\n","# mean    69.747   898.179 609,067.453 6,547.221 283.116   629.240\n","# std    100.097   913.846 870,781.482 6,344.391 156.825   935.361\n","# min         25       160       40240       244     104         0\n","# 25%         31       442      195290      2121     185   208.694\n","# 50%         43   641.500      357134      4500     256   383.428\n","# 75%         69 1,068.750      669944      8082     356   766.780\n","# max       1149      9381     7195364     41760    1296 9,400.753\n","\n","\n","# number of clusters with at least 50 items as members: 77\n","#        n_nodes   w_avg         w_sum     w_max   w_min     w_std\n","# count       77      77            77        77      77        77\n","# mean       122 719.208   992,967.844 7,237.299 259.273   467.077\n","# std    142.094 527.528 1,099,653.689 6,194.954 127.151   446.270\n","# min         50     160         81852       564     104     9.931\n","# 25%         60     421        413644      2592     152   184.071\n","# 50%         81     545        672344      5248     224   314.437\n","# 75%        140     894       1159600      9896     320   645.293\n","# max       1149    3379       7195364     27240     784 2,267.530"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XupYfvwUFLGI"},"source":["#####Encode cluster categories\n","* I could use the index number as item_cluster_id to feed into training of the model & predictions (simplest, easiest)\n","\n","* Or, I could try to give the clusters a bit more meaning... perhaps higher cluster numbers if higher weights between nodes on average, but\n","```\n","community_df.w_avg.nunique()\n","``` \n","shows there are not enough unique average weights to use this as a cluster category code.\n","\n","* I could sort on w_avg, then on number of nodes as perhaps the next most important defining characteristic of a given cluster.  Then, to make the categorization unique, I could take the w_avg value and sum it with the index (row number)... (with the sorting, this favors even more the clusters with high average item-to-item similarity)\n","```\n","community_df = community_df.sort_values(['w_avg','n_nodes']).reset_index(drop=True)\n","community_df['item_cluster_id'] = community_df.index + community_df['w_avg']\n","community_df.head()\n","```\n","This is not too difficult, but does make cluster category codes rather large, and requires more RAM for storage of training data\n","\n","\n","* Lastly, I could do some sort of mean encoding for sales, such as using the average of cluster sales over a certain number of important months (such as months 25 to 33) using the training data with outliers removed.  It's similar to the above \"weighted\" method, but a bit more complex\n","\n","Since I am not sure if any of this will help, particularly with decision tree models, I will just use index number for now, and perhaps revisit this in the future.  However, since there are many unclustered items, I will deal with them as follows:\n","* clustered items cluster code = 100 + index_row_number\n","* unclustered items cluster code = original item_category_id (0-83)"]},{"cell_type":"code","metadata":{"id":"OSnYLCamXr2S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1592486899561,"user_tz":240,"elapsed":605169,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"086f4ca0-f87e-4a3a-be7f-ec213ad7644f"},"source":["# unravel / explode the cluster node lists... we know this will not duplicate item ids, from the counting we did above (nodes are not duplicated in clusters)\n","item_clusters = community_df.copy(deep=True).explode('cluster_members').reset_index().rename(columns = {'index':'item_cluster','cluster_members':'item_id'})\n","display(item_clusters.head())\n","print(f't1: {strftime(\"%a %X %x\")}')\n","\n","n_cluster_codes = item_clusters.item_cluster.nunique()\n","all_items = items_augmented.item_id.unique()\n","n_items_total = len(all_items)\n","items_in_clusters = item_clusters.item_id.unique()\n","items_in_test = test.item_id.unique()\n","items_in_train = tt.query('month < 34').item_id.unique()\n","items_in_train2533 = tt.query('((month < 34) & (month > 24))').item_id.unique()\n","print(f't2: {strftime(\"%a %X %x\")}')\n","\n","items_unclustered = [x for x in all_items if x not in items_in_clusters]\n","print(f't3: {strftime(\"%a %X %x\")}')\n","test_unclustered = [x for x in items_in_test if x in items_unclustered]\n","print(f't4: {strftime(\"%a %X %x\")}')\n","train_unclustered = [x for x in items_in_train if x in items_unclustered]\n","print(f't5: {strftime(\"%a %X %x\")}')\n","train2533_unclustered = [x for x in items_in_train2533 if x in items_unclustered]\n","print(f't6: {strftime(\"%a %X %x\")}')\n","test_trained = [x for x in items_in_test if x in items_in_train]\n","print(f't7: {strftime(\"%a %X %x\")}')\n","test_trained2533 = [x for x in items_in_test if x in items_in_train2533]\n","print(f't8: {strftime(\"%a %X %x\")}')\n","\n","# Merge cluster information with item dataset and utilize original item_category_id if a particular item is not in one of the NLP-defined clusters\n","items_clustered = items_clean_delimited[['item_id','i_cat_id','i_tested','item_name']].merge(item_clusters,on='item_id',how='left')\n","items_clustered = items_clustered[['item_id','item_name','i_cat_id','i_tested','item_cluster','n_nodes']] #,'w_avg','w_sum','w_max','w_min','w_std']]\n","items_clustered.columns = ['item_id','item_name','item_category_id','item_tested','item_cluster','n_items_in_cluster'] #'w_avg','w_sum','w_max','w_min','w_std']\n","\n","items_clustered.item_cluster = items_clustered.apply(lambda x: x.item_cluster+100 if x.item_cluster >= 0 else x.item_category_id, axis = 1)\n","\n","print(f'done: {strftime(\"%a %X %x\")}')\n"],"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_cluster</th>\n","      <th>n_nodes</th>\n","      <th>w_avg</th>\n","      <th>w_sum</th>\n","      <th>w_max</th>\n","      <th>w_min</th>\n","      <th>w_std</th>\n","      <th>item_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>111</td>\n","      <td>351</td>\n","      <td>472856</td>\n","      <td>21220</td>\n","      <td>212</td>\n","      <td>651.611</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>111</td>\n","      <td>351</td>\n","      <td>472856</td>\n","      <td>21220</td>\n","      <td>212</td>\n","      <td>651.611</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>111</td>\n","      <td>351</td>\n","      <td>472856</td>\n","      <td>21220</td>\n","      <td>212</td>\n","      <td>651.611</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>111</td>\n","      <td>351</td>\n","      <td>472856</td>\n","      <td>21220</td>\n","      <td>212</td>\n","      <td>651.611</td>\n","      <td>12828</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>111</td>\n","      <td>351</td>\n","      <td>472856</td>\n","      <td>21220</td>\n","      <td>212</td>\n","      <td>651.611</td>\n","      <td>14366</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_cluster  n_nodes  w_avg   w_sum  w_max  w_min   w_std item_id\n","0             0      111    351  472856  21220    212 651.611       0\n","1             0      111    351  472856  21220    212 651.611       2\n","2             0      111    351  472856  21220    212 651.611      20\n","3             0      111    351  472856  21220    212 651.611   12828\n","4             0      111    351  472856  21220    212 651.611   14366"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["t1: Thu 09:18:14 06/18/20\n","t2: Thu 09:18:14 06/18/20\n","t3: Thu 09:28:16 06/18/20\n","t4: Thu 09:28:17 06/18/20\n","t5: Thu 09:28:17 06/18/20\n","t6: Thu 09:28:18 06/18/20\n","t7: Thu 09:28:18 06/18/20\n","t8: Thu 09:28:18 06/18/20\n","done: Thu 09:28:19 06/18/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZrxFHkMDksGD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"ok","timestamp":1592486964069,"user_tz":240,"elapsed":879,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"7460fdf8-5b32-4a8c-a634-8089b77fd753"},"source":["print(f'\\nNumber of \"cluster category\" codes: {n_cluster_codes:,d}')\n","print(f'Number of \"cluster category\" codes after merging unclustered items with original category codes: {items_clustered.item_cluster.nunique():,d}\\n')\n","\n","print(f'Number of items not included in clusters: {len(items_unclustered):,d} (out of {n_items_total:,d})')\n","print(f'Number of test items not included in clusters: {len(test_unclustered):,d} (out of {len(items_in_test):,d})')\n","print(f'Number of sales_train items not included in clusters: {len(train_unclustered):,d} (out of {len(items_in_train):,d})')\n","print(f'Number of sales_train items from only months including 25-33 not included in clusters: {len(train2533_unclustered):,d} (out of {len(items_in_train2533):,d})\\n')\n","\n","print(f'Number of test items not included in sales_train: {(len(items_in_test) - len(test_trained)):,d} (out of {len(items_in_test):,d} test items)')\n","print(f'Number of test items not included in sales_train months 25-33: {(len(items_in_test) - len(test_trained2533)):,d} (out of {len(items_in_test):,d} test items)\\n')\n","\n","# Perhaps calculate size of clusters with % of test items and with % of test items not in train set... i.e., better determination of \"coverage\" of clusters for test items\n","#    then, maybe run a loop to optimize threshold and k\n","\n","display(items_clustered.head())"],"execution_count":39,"outputs":[{"output_type":"stream","text":["\n","Number of \"cluster category\" codes: 1,403\n","Number of \"cluster category\" codes after merging unclustered items with original category codes: 1,409\n","\n","Number of items not included in clusters: 1,765 (out of 22,170)\n","Number of test items not included in clusters: 251 (out of 5,100)\n","Number of sales_train items not included in clusters: 1,739 (out of 21,806)\n","Number of sales_train items from only months including 25-33 not included in clusters: 704 (out of 10,747)\n","\n","Number of test items not included in sales_train: 363 (out of 5,100 test items)\n","Number of test items not included in sales_train months 25-33: 485 (out of 5,100 test items)\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>item_name</th>\n","      <th>item_category_id</th>\n","      <th>item_tested</th>\n","      <th>item_cluster</th>\n","      <th>n_items_in_cluster</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>100</td>\n","      <td>111</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>76</td>\n","      <td>False</td>\n","      <td>105</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>100</td>\n","      <td>111</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>movie dvd blue wave univ dvd</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>110</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>movie dvd box glass dvd</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>116</td>\n","      <td>84</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  item_id                                                                                         item_name  item_category_id  item_tested  item_cluster  n_items_in_cluster\n","0       0                                                               movie dvd power in glamor plast dvd                40        False           100                 111\n","1       1  program home and office digital abbyy finereader 12 professional edition full pc digital version                76        False           105                  19\n","2       2                                                                        movie dvd in glory unv dvd                40        False           100                 111\n","3       3                                                                      movie dvd blue wave univ dvd                40        False           110                   5\n","4       4                                                                           movie dvd box glass dvd                40        False           116                  84"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"KtvuccfmrWJy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592487484129,"user_tz":240,"elapsed":327,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# # how many test items are represented by clusters?\n","# tested_clustered = items_clustered[items_clustered.item_tested==True][['item_id','item_category_id','item_cluster_id','item_name']]\n","# tested_clustered['unclustered'] = tested_clustered.apply(lambda x: np.NaN if x.item_cluster_id > 0  else x.item_id, axis = 1)\n","# print(tested_clustered.head(10))\n","# print('\\n')\n","# print(tested_clustered.item_id.nunique())\n","# unclustered = tested_clustered.unclustered.unique()\n","# unclustered = [x for x in unclustered if x > 0]\n","# print(len(unclustered))\n","# train_items = sales_train.item_id.unique()\n","# print(len(train_items))\n","# print(len(items_augmented))\n","# untrained = [x for x in unclustered if x not in train_items]\n","# print(len(untrained))\n","# print(len(items_augmented) - len(train_items) - len(untrained))"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFs7GZWs6bkF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592487226799,"user_tz":240,"elapsed":840,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"470cf2ab-0774-454e-f0e0-dd46703291eb"},"source":["# save what we have; maybe refine later\n","\n","compression_opts = dict(method='gzip',\n","                        archive_name='items_clustered_21700c.csv')  \n","items_clustered.to_csv('data_output/items_clustered_21700c.csv.gz', index=False, compression=compression_opts)\n","\n","print(f'File Saved at: {strftime(\"%a %X %x\")}')"],"execution_count":40,"outputs":[{"output_type":"stream","text":["File Saved at: Thu 09:33:46 06/18/20\n"],"name":"stdout"}]}]}