{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_mg_v1p0.ipynb","provenance":[{"file_id":"1zoSvdwl9M_wq03QHaMQuQdIcdEN0JRSY","timestamp":1594910950502},{"file_id":"12KA-rL8-rk29NOHJN8-DbZ8fGUESsfgV","timestamp":1589287670669},{"file_id":"1nzPRIdf4UB-3biwx8fCJlTnx8bL126aO","timestamp":1588242465890},{"file_id":"https://github.com/migai/Kag/blob/master/template_Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587141076973}],"collapsed_sections":["YPp_Nesy2yxn","-YA__znazThG","i_NGdlH8zbz8","r_Oe76PW3aoN","ufy-J0xC2efV","WLdAjg45wEne","UlH3NopEv1Ha","DP8AZkYQvtaj","LyLQLqBcOnLt","_Ldvqgyw0_Hw","Isean25B9FjA","fz81jlrrjz55","W7uoRFv-uSC2","Jhj7bbgCceh8","nLmjmqw1oihf","aWEbp9hBVIqd","4kjKndgDM8Wi","nVew2KThVzWw","jtZ0iBdUCD-Z","9TJk9bzeCjqF","HprRItOZV8o6","8aPOnrqM9y6r","FPzNWMRAyx6M","lKGrJUG7f50F","a4U1U6nZy8wd","c98tKblpRWya","zPM4RLKcn3RE","LF91nOlf3Km7","qG665uxzQ5J5","x4YcaYylQ0d8"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"twk8LgLIzEAI","colab_type":"text"},"source":["# **Intro and Setup**"]},{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn","colab_type":"text"},"source":["##**Final Project for Coursera's 'How to Win a Data Science Competition'**\n","April, 2020;  Andreas Theodoulou and Michael Gaidis;  (Competition Info last updated:  3 years ago)"]},{"cell_type":"markdown","metadata":{"id":"-YA__znazThG","colab_type":"text"},"source":["###**About this Competition**\n","\n","You are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n","\n","Evaluation: root mean squared error (RMSE). True target values are clipped into [0,20] range."]},{"cell_type":"markdown","metadata":{"id":"i_NGdlH8zbz8","colab_type":"text"},"source":["###**File descriptions**\n","\n","***sales_train.csv*** - the training set. Daily historical data from January 2013 to October 2015.\n","\n","***test.csv*** - the test set. You need to forecast the sales for these shops and products for November 2015.\n","\n","***sample_submission.csv*** - a sample submission file in the correct format.\n","\n","***items.csv*** - supplemental information about the items/products.\n","\n","***item_categories.csv***  - supplemental information about the items categories.\n","\n","***shops.csv***- supplemental information about the shops."]},{"cell_type":"markdown","metadata":{"id":"r_Oe76PW3aoN","colab_type":"text"},"source":["###**Data fields**\n","\n","***ID*** - an Id that represents a (Shop, Item) tuple within the test set\n","\n","***shop_id*** - unique identifier of a shop\n","\n","***item_id*** - unique identifier of a product\n","\n","***item_category_id*** - unique identifier of item category\n","\n","***item_cnt_day*** - number of products sold. You are predicting a monthly amount of this measure\n","\n","***item_price*** - current price of an item\n","\n","***date*** - date in format dd/mm/yyyy\n","\n","***month*** - a consecutive month number. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n","\n","***item_name*** - name of item\n","\n","***shop_name*** - name of shop\n","\n","***item_category_name*** - name of item category"]},{"cell_type":"markdown","metadata":{"id":"ufy-J0xC2efV","colab_type":"text"},"source":["## **Colab Prep** for those using Google Colab\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WLdAjg45wEne","colab_type":"text"},"source":["### **Save Previous Work**\n","* Click **File -> Save a copy in Drive** and click **Open in new tab** in the pop-up window to save your progress in Google Drive. (This places the copy at the top level of Colab directory.)\n","* Or, in Google Drive before opening this notebook, right-click on this ipynb and select ***make a copy***, then with the copy in the same directory, right-click and select ***rename*** to update the version number.  Finally, right-click on the new version and ***open in colab***."]},{"cell_type":"markdown","metadata":{"id":"UlH3NopEv1Ha","colab_type":"text"},"source":["### **Select Runtime Type** *before* running notebook:\n","* Click **Runtime -> Change runtime type** and select **GPU** or **TPU** in Hardware accelerator box to enable faster training."]},{"cell_type":"markdown","metadata":{"id":"DP8AZkYQvtaj","colab_type":"text"},"source":["### **Keep Colab Active**\n","* To keep Colab connected by clicking on Colab window once every minute, go to Chrome Dev Tools --> Console Tab --> run the following code (April 2020):\n","</br>Take note that this should prevent disconnecting after each 1.5 hours of inactivity, but each runtime, if you don't have Colab Pro, will be terminated after 12 hours. (Pro = 24 hours) (Interval below is in millisec.)\n","```\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"#ok\").click()\n","}\n","setInterval(ClickConnect,60000)\n","```\n","Note that it will throw an error, its ok, it means that the Disconnection notification is not shown. Once it appear it will be clicked to reconnect.\n","\n","* If that doesn't work, try this in the console:\n","```\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"colab-connect-button\").click()\n","}\n","setInterval(ClickConnect,60000)\n","```\n","* Lastly, can try this (older):\n","```\n","function KeepClicking(){\n","   console.log(\"Clicking\");\n","   document.querySelector(\"colab-toolbar-button#connect\").click()\n","}setInterval(KeepClicking,600000)\n","```"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SNR_OjCZcfxf"},"source":["### **Save Previous Work**\n","* Click **File -> Save a copy in Drive** and click **Open in new tab** in the pop-up window to save your progress in Google Drive. (This places the copy at the top level of Colab directory.)\n","* Or, in Google Drive before opening this notebook, right-click on this ipynb and select ***make a copy***, then with the copy in the same directory, right-click and select ***rename*** to update the version number.  Finally, right-click on the new version and ***open in colab***."]},{"cell_type":"markdown","metadata":{"id":"LyLQLqBcOnLt","colab_type":"text"},"source":["#**Set Up Environment**\n","import python modules, and set up pandas environment options"]},{"cell_type":"code","metadata":{"id":"naC94KtXOnLt","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1595940590749,"user_tz":240,"elapsed":1996,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"9d1dc55c-10f7-4ba5-e03e-548460a0194f"},"source":["from google.colab import drive  \n","\n","# General python libraries/modules used throughout the notebook\n","from   itertools import product\n","from   collections import OrderedDict\n","import pprint\n","import re\n","import os\n","import sys\n","from   pathlib import Path\n","import platform                 # determine the active version of python\n","import pkg_resources            # determine the active versions of imported packages\n","import psutil                   # from psutil import virtual_memory   # find how much RAM you have left in Colab VM\n","import gc                       # garbage collection... ok with np arrays, not useful with python objects\n","import multiprocessing          # help to delete unnecessary pandas dataframes when you are done with them\n","\n","# timing\n","import time\n","from   time import strftime, tzset, perf_counter\n","import datetime\n","from   datetime import timedelta\n","os.environ['TZ'] = 'EST+05EDT,M4.1.0,M10.5.0'   # allows user to simply print a formatted version of the local date and time; helps keep track of what cells were run, and when\n","tzset()                                         # set the time zone\n","\n","# Helpful packages for EDA, cleaning, data manipulation\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# ML packages\n","import lightgbm as lgb  # LGBMRegressor\n","import sklearn\n","from   sklearn.preprocessing import MinMaxScaler, RobustScaler\n","from   sklearn.experimental import enable_hist_gradient_boosting  # noqa # explicitly require this experimental feature before importing HistGradientBoostingRegressor\n","from   sklearn.ensemble import HistGradientBoostingRegressor\n","from   sklearn.metrics import mean_squared_error\n","from   sklearn.metrics import r2_score\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","\n","# local file location information\n","GDRIVE_HOME = Path(\"/content\" + \"/drive\")                                       # initial directory when mounting Google Drive in Colab\n","COLAB_DIR = GDRIVE_HOME / \"My Drive/Colab Notebooks\"                            # default Colab directory\n","GDRIVE_REPO_PATH = COLAB_DIR / \"NRUHSE_2_Kaggle_Coursera/final/Kag\"             # location of local GitHub repo: content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","\n","# For finding and reporting the versions of modules used in this notebook\n","modules = ['pandas','matplotlib','numpy','scikit-learn','lightgbm','tensorflow']  # ,'keras','catboost','seaborn','nltk','graphx'\n","\n","print(f'\\n{strftime(\"%a %X %x\")}')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\n","Tue 08:49:49 07/28/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P8S5AW9bcfxe"},"source":["## **Pandas Prep**\n","\n"]},{"cell_type":"code","metadata":{"id":"JBudx4r9tO5s","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595940590751,"user_tz":240,"elapsed":1989,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# Pandas additional enhancements / formatting\n","pd.set_option('compute.use_bottleneck', False)  # speed up operation when using NaNs\n","pd.set_option('compute.use_numexpr', False)     # speed up boolean operations, large dataframes; DataFrame.query() and pandas.eval() will evaluate the subexpressions that can be evaluated by numexpr\n","pd.set_option(\"display.max_rows\",60)            # Override pandas choice of how many rows to show, so we can see the full 84-row item_category df instead of '...' in the middle\n","pd.set_option(\"display.max_columns\",60)         # Similar to row code above, we can show more columns than default\n","pd.set_option(\"display.width\", 220)             # Tune this to our monitor window size to avoid horiz scroll bars in output windows (but, will get output text wrapping)\n","pd.set_option(\"max_colwidth\", None)             # This is done, for example, so we can see full item name and not '...' in the middle\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)   # print without decimals if a number is an integer; use 3 decimals if a float\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9uV9AzFbtdJZ"},"source":["## **Analysis and Descriptive Functions**\n","\n"]},{"cell_type":"code","metadata":{"id":"duCrNE7ZpnzG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1595940590751,"user_tz":240,"elapsed":1983,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"1d8a9b58-5c36-4fb7-e5fe-7e829933c555"},"source":["def get_memory_stats(n=\"0: \"):\n","    # to print number indicating when this is called, set n='5.)  ' or similar (space after an integer, in a string)\n","    pid = os.getpid()\n","    py = psutil.Process(pid)\n","    pid_memory_use = py.memory_info()[0] / 2. ** 30\n","    vm_used = psutil.virtual_memory().used/ 1e9\n","    vm_total = psutil.virtual_memory().total/ 1e9\n","    vm_available = vm_total - vm_used\n","    print(f'{n}Memory Use: {pid_memory_use:.2f} | {vm_used:.2f} | {vm_available:.2f} | {vm_total:.2f} GB:  pid, vm_used, vm_available, vm_total.  {strftime(\"%a %X %x\")}.')\n","    return pid_memory_use,vm_used,vm_total,vm_available,f'{strftime(\"%a %X %x\")}'\n","\n","# Check if you are indeed connected to a GPU-enabled runtime, or even possibly a TPU:  (particularly relevant for people using Google Colab)\n","def get_runtime_type():\n","    runtime_type_dict = {}\n","    try:\n","        gpu_device_name = tf.test.gpu_device_name()\n","        if gpu_device_name != '/device:GPU:0':  #' ' means CPU whereas '/device:G:0' means GPU\n","            try:\n","                tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","                runtime_type_dict['TPU'] = f'Colab using a TPU runtime at: {tpu.cluster_spec().as_dict()[\"worker\"]}'\n","            except ValueError:\n","                runtime_type_dict['CPU'] = 'Colab using a CPU runtime.'\n","        else:\n","            runtime_type_dict['GPU'] = f'Colab using a GPU runtime at: {gpu_device_name}'\n","    except:\n","        runtime_type_dict['Unknown'] = 'Unknown Colab runtime type'\n","    return runtime_type_dict\n","\n","# Find number of CPUs available:\n","def get_n_cpu():\n","    try:\n","        n_cpu = multiprocessing.cpu_count()\n","        print(f'{n_cpu} CPUs are available for use.')\n","    except:\n","        pass\n","\n","# Find the versions of modules used in this notebook\n","def get_module_versions(module_list=modules):\n","    module_version_dict = OrderedDict()\n","    module_version_dict[\"Python\"] = platform.python_version()\n","    for mod in sorted(module_list): \n","        try:\n","            module_version_dict[mod] = pkg_resources.get_distribution(mod).version\n","        except:\n","            pass\n","    return module_version_dict\n","\n","RUNTIME_TYPE = get_runtime_type()\n","for k,v in RUNTIME_TYPE.items():\n","    print(v)\n","get_n_cpu()\n","print('\\n')\n","MODULE_VERSIONS = get_module_versions(modules)\n","for k,v in MODULE_VERSIONS.items():\n","    print(f'{k} version: {v}')\n","print('\\n')\n","mu,vu,vt,va,t = get_memory_stats(\"After Setup: \")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Colab using a CPU runtime.\n","4 CPUs are available for use.\n","\n","\n","Python version: 3.6.9\n","lightgbm version: 2.2.3\n","matplotlib version: 3.2.2\n","numpy version: 1.18.5\n","pandas version: 1.0.5\n","scikit-learn version: 0.22.2.post1\n","tensorflow version: 2.2.0\n","\n","\n","After Setup: Memory Use: 0.38 | 1.95 | 25.44 | 27.39 GB:  pid, vm_used, vm_available, vm_total.  Tue 08:49:49 07/28/20.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MY4tx3FwtVq7"},"source":["#**Mount Google Drive for access to Google Drive local repo; Load Data**"]},{"cell_type":"code","metadata":{"id":"CUIE1PVjSAmg","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595940590752,"user_tz":240,"elapsed":1978,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"11acad9e-6715-401b-f579-e8e2cc51d9bb"},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, then paste it into the input box and hit 'enter' to complete mounting of the drive\n","\n","drive.mount('/content/drive')\n","mu,vu,vt,va,t = get_memory_stats(\"After Mounting Google Drive: \")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","After Mounting Google Drive: Memory Use: 0.38 | 1.95 | 25.44 | 27.39 GB:  pid, vm_used, vm_available, vm_total.  Tue 08:49:49 07/28/20.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RHbcGFx1sp-g"},"source":["##Load Data Files into Similarly-Named pd DataFrames"]},{"cell_type":"code","metadata":{"id":"WaoqJmt42Yo4","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":969},"executionInfo":{"status":"ok","timestamp":1595940615535,"user_tz":240,"elapsed":3666,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"35f91d3a-36b5-4ee9-8cbe-4f72ddbb6ebc"},"source":["# List of the *data* files (path relative to GitHub branch), to be loaded into pandas DataFrames\n","data_files = [  \"data_output/items_enc.csv\",\n","                \"data_output/shops_enc.csv\",\n","                \"data_output/date_adjustments.csv\",\n","                \"data_output/train_test_base.csv.gz\",\n","                \"readonly/final_project_data/test.csv.gz\"]\n","\n","def load_dfs(arg_dict={}):\n","    print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","    %cd \"{arg_dict['repo_path']}\"\n","    dfs = {}\n","    for path_name in arg_dict['data_file_list']:\n","        filename = path_name.rsplit(\"/\")[-1]\n","        data_frame_name = filename.split(\".\")[0]\n","        exec(data_frame_name + ' = pd.read_csv(path_name)')\n","        print(f'Data Frame: {data_frame_name}; n_rows = {len(eval(data_frame_name))}, n_cols = ',end=\"\")\n","        print(f'{len(eval(data_frame_name).columns)}') #\\nData Types: {eval(data_frame_name).dtypes}\\n')\n","        print(f'Column Names: {eval(data_frame_name).columns.to_list()}')\n","        print(eval(data_frame_name).head(2))\n","        dfs[data_frame_name] = eval(data_frame_name)\n","    return dfs\n","\n","# Issue: freeing up memory used by pandas dataframes that are no longer required by the program (del + gc.collect() does not reliably do this, nor does redefining the df as pd.DataFrame())\n","txt1 = \"1) After Defining Data Files:   \"\n","txt2 = \"2) After Pool Returns dfs dict: \"\n","txt3 = \"3) After Pool Close and Join:   \"\n","txt4 = \"4) After Setting dfs to Empty:  \"\n","mu1,vu1,vt1,va1,t1 = get_memory_stats(txt1)\n","\n","args_dict = {'repo_path':GDRIVE_REPO_PATH,'data_file_list':data_files}\n","with multiprocessing.Pool(None,maxtasksperchild = 1) as pool:  # None = use all available processors, could use Pool(1, maxtasksperchild = 1)\n","    dfs = pool.map(load_dfs, [args_dict])[0]\n","mu2,vu2,vt2,va2,t2 = get_memory_stats(txt2)\n","pool.close()  # pool.terminate()\n","pool.join()\n","mu3,vu3,vt3,va3,t3 = get_memory_stats(txt3)\n","\n","print(dfs['items_enc'].head(2))\n","print(dfs['shops_enc'].head(2))\n","print(dfs['date_adjustments'].head(2))\n","print(dfs['train_test_base'].head(2))\n","print(dfs['test'].head(2))\n","dfs = {}; print(\"dfs = \", dfs)\n","print(f'\\nMultiprocessing Active Children: {multiprocessing.active_children()}')  # should be zero (empty list) if all processes are terminated and memory is returned to system\n","mu4,vu4,vt4,va4,t4 = get_memory_stats(txt4)\n","print('\\n')\n","print(f'{txt1} | {mu1:.2f} | {vu1:.2f} | {vt1:.2f} | {va1:.2f} |  {t1}')\n","print(f'{txt2} | {mu2:.2f} | {vu2:.2f} | {vt2:.2f} | {va2:.2f} |  {t2}')\n","print(f'{txt3} | {mu3:.2f} | {vu3:.2f} | {vt3:.2f} | {va3:.2f} |  {t3}')\n","print(f'{txt4} | {mu4:.2f} | {vu4:.2f} | {vt4:.2f} | {va4:.2f} |  {t4}')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["1) After Defining Data Files:   Memory Use: 0.82 | 2.42 | 24.97 | 27.39 GB:  pid, vm_used, vm_available, vm_total.  Tue 08:50:10 07/28/20.\n","Loading Files from Google Drive repo into Colab...\n","\n","/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","Data Frame: items_enc; n_rows = 22170, n_cols = 10\n","Column Names: ['item_id', 'item_tested', 'item_cluster', 'item_category_id', 'item_cat_tested', 'item_group', 'item_category1', 'item_category2', 'item_category3', 'item_category4']\n","   item_id  item_tested  item_cluster  item_category_id  item_cat_tested  item_group  item_category1  item_category2  item_category3  item_category4\n","0        0            0           100                40                1           6               8               3               7               3\n","1        1            0           105                76                1           6              11               6              10               5\n","Data Frame: shops_enc; n_rows = 60, n_cols = 9\n","Column Names: ['shop_id', 'shop_tested', 'shop_group', 'shop_type', 's_type_broad', 'shop_federal_district', 'fd_popdens', 'fd_gdp', 'shop_city']\n","   shop_id  shop_tested  shop_group  shop_type  s_type_broad  shop_federal_district  fd_popdens  fd_gdp  shop_city\n","0        0            0           7          5             2                      1           3       1         26\n","1        1            0           7          1             0                      1           3       1         26\n","Data Frame: date_adjustments; n_rows = 35, n_cols = 8\n","Column Names: ['month', 'year', 'season', 'MoY', 'days_in_M', 'weekday_weight', 'retail_sales', 'week_retail_weight']\n","   month  year  season  MoY  days_in_M  weekday_weight  retail_sales  week_retail_weight\n","0      0  2013       2    1         31           0.979         1.052               1.030\n","1      1  2013       3    2         28           1.069         1.072               1.146\n","Data Frame: train_test_base; n_rows = 3150043, n_cols = 9\n","Column Names: ['day', 'week', 'qtr', 'season', 'month', 'price', 'sales', 'shop_id', 'item_id']\n","   day  week  qtr  season  month  price  sales  shop_id  item_id\n","0    0     0    0       2      0     99      1        2      991\n","1    0     0    0       2      0   2599      1        2     1472\n","Data Frame: test; n_rows = 214200, n_cols = 3\n","Column Names: ['ID', 'shop_id', 'item_id']\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","2) After Pool Returns dfs dict: Memory Use: 0.82 | 2.42 | 24.97 | 27.39 GB:  pid, vm_used, vm_available, vm_total.  Tue 08:50:14 07/28/20.\n","3) After Pool Close and Join:   Memory Use: 0.82 | 2.42 | 24.97 | 27.39 GB:  pid, vm_used, vm_available, vm_total.  Tue 08:50:14 07/28/20.\n","   item_id  item_tested  item_cluster  item_category_id  item_cat_tested  item_group  item_category1  item_category2  item_category3  item_category4\n","0        0            0           100                40                1           6               8               3               7               3\n","1        1            0           105                76                1           6              11               6              10               5\n","   shop_id  shop_tested  shop_group  shop_type  s_type_broad  shop_federal_district  fd_popdens  fd_gdp  shop_city\n","0        0            0           7          5             2                      1           3       1         26\n","1        1            0           7          1             0                      1           3       1         26\n","   month  year  season  MoY  days_in_M  weekday_weight  retail_sales  week_retail_weight\n","0      0  2013       2    1         31           0.979         1.052               1.030\n","1      1  2013       3    2         28           1.069         1.072               1.146\n","   day  week  qtr  season  month  price  sales  shop_id  item_id\n","0    0     0    0       2      0     99      1        2      991\n","1    0     0    0       2      0   2599      1        2     1472\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","dfs =  {}\n","\n","Multiprocessing Active Children: []\n","4) After Setting dfs to Empty:  Memory Use: 0.82 | 2.42 | 24.97 | 27.39 GB:  pid, vm_used, vm_available, vm_total.  Tue 08:50:14 07/28/20.\n","\n","\n","1) After Defining Data Files:    | 0.82 | 2.42 | 27.39 | 24.97 |  Tue 08:50:10 07/28/20\n","2) After Pool Returns dfs dict:  | 0.82 | 2.42 | 27.39 | 24.97 |  Tue 08:50:14 07/28/20\n","3) After Pool Close and Join:    | 0.82 | 2.42 | 27.39 | 24.97 |  Tue 08:50:14 07/28/20\n","4) After Setting dfs to Empty:   | 0.82 | 2.42 | 27.39 | 24.97 |  Tue 08:50:14 07/28/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZA5YyPKB7QAt","colab_type":"text"},"source":["#**Define Various Constants, Parameters, Variables that Determine Feature Creation, Model Configuration, etc.**"]},{"cell_type":"code","metadata":{"id":"1c8M1ONK7txo","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595898316733,"user_tz":240,"elapsed":993,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"a0bf9eff-2c07-4ac5-fdeb-3d1d3ee2b5f2"},"source":["# # for reference, column names of the loaded dataframes:\n","# items_enc_cols        = ['item_id', 'item_tested', 'item_cluster', 'item_category_id', 'item_cat_tested', 'item_group', 'item_category1', 'item_category2', 'item_category3', 'item_category4']\n","# shops_enc_cols        = ['shop_id', 'shop_tested', 'shop_group', 'shop_type', 's_type_broad', 'shop_federal_district', 'fd_popdens', 'fd_gdp', 'shop_city']\n","# date_adj_cols         = ['month', 'year', 'season', 'MoY', 'days_in_M', 'weekday_weight', 'retail_sales', 'week_retail_weight']\n","# train_test_base_cols  = ['day', 'week', 'qtr', 'season', 'month', 'price', 'sales', 'shop_id', 'item_id']\n","# test_cols             = ['ID', 'shop_id', 'item_id']\n","\n","# FEATURES Dictionary stores everything for dumping to file later\n","FEATURES = OrderedDict()\n","\n","FEATURES[\"AGG_STATS\"] = OrderedDict()\n","FEATURES[\"AGG_STATS\"][\"sales\"]        = ['sum', 'median', 'count']\n","FEATURES[\"AGG_STATS\"][\"revenue\"]      = ['sum']                      # revenue can handle fillna(0) cartesian product; price doesn't make sense with fillna(0), so don't use that at this time\n","#FEATURES[\"AGG_STATS\"][\"price\"]        = ['median','std']\n","\n","# columns to keep for this round of modeling (dropping some of the less important features to save memory):\n","FEATURES[\"COLS_KEEP_BASE_TRAIN_TEST\"]   = ['month', 'price', 'sales', 'shop_id', 'item_id']\n","FEATURES[\"COLS_KEEP_DATE_SCALING\"]      = ['month', 'days_in_M', 'weekday_weight', 'retail_sales', 'week_retail_weight']\n","FEATURES[\"COLS_KEEP_ITEMS\"]             = ['item_id', 'item_group', 'item_cluster', 'item_category_id']  #, 'item_category4']\n","FEATURES[\"COLS_KEEP_SHOPS\"]             = ['shop_id','shop_group']\n","FEATURES[\"COLS_ORDER_STT\"]              = ['month'] + list(FEATURES[\"AGG_STATS\"].keys()) + ['shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']  # main dataframe colums before stats\n","\n","CATEGORICAL_COLS    = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in ['month']+ list(FEATURES[\"AGG_STATS\"].keys()) ]   # remove these from STT df to give just categorical columns\n","INTEGER_COLS        = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in list(FEATURES[\"AGG_STATS\"].keys()) ]              # remove these from STT df to give just intrinsically integer columns\n","\n","FEATURES[\"LAGS_MONTHS\"] = [1,2,3,4,5,6,7,8]  # month lags to include in model \n","\n","# aggregate statistics columns (initial computation shall be 'sales per month' prediction target for shop_id-item_id pair grouping; multiple category groupings should be placed together in a list)\n","FEATURES[\"LAGGED_FEATURE_GROUPS\"] = [['shop_id', 'item_id'], ['shop_id', 'item_category_id'], ['shop_id', 'item_cluster']] + CATEGORICAL_COLS\n","\n","FEATURES[\"LAG_FEATURES\"] = OrderedDict()\n","for i in FEATURES[\"LAGS_MONTHS\"]:\n","    FEATURES[\"LAG_FEATURES\"][i] = ['y_sales', 'shop_id_x_item_category_id_sales_sum', 'item_id_sales_sum', 'item_cluster_sales_sum'] \n","FEATURES[\"LAG_FEATURES\"][1] = ['y_sales', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', \n","                     'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', \n","                     'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', \n","                     'shop_id_sales_sum', 'shop_id_sales_count', \n","                     'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', \n","                     'shop_group_revenue_sum', \n","                     'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', \n","                     'item_group_sales_sum', 'item_group_revenue_sum', \n","                     'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","\n","FEATURES[\"LAG_FEATURES\"][2] = ['y_sales', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', \n","                     'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_revenue_sum', \n","                     'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_count', \n","                     'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', \n","                     'item_category_id_sales_sum', 'item_category_id_sales_count', \n","                     'item_group_sales_sum', \n","                     'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","\n","FEATURES[\"LAG_FEATURES\"][3] = ['y_sales', 'shop_id_x_item_id_sales_count', \n","                     'shop_id_x_item_category_id_sales_sum', \n","                     'shop_id_sales_sum', \n","                     'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', \n","                     'item_category_id_sales_sum', 'item_category_id_sales_count', \n","                     'item_cluster_sales_sum', 'item_cluster_sales_count']\n","\n","# SECTION BELOW: manually remove some of the above-included features, as determined by feature importances to be likely unhelpful\n","# keep at least the highest importance feature for each lag, but remove all others with < 20% importance (month 13-32 training)\n","FEATURES[\"LAG_FEATURES\"][2] = [e for e in FEATURES[\"LAG_FEATURES\"][2] if e not in {'item_group_sales_sum','shop_id_x_item_category_id_sales_sum','shop_id_x_item_cluster_sales_sum','shop_id_x_item_cluster_sales_count'}]\n","FEATURES[\"LAG_FEATURES\"][3] = [e for e in FEATURES[\"LAG_FEATURES\"][3] if e not in {'item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum','shop_id_x_item_id_sales_count'}]\n","FEATURES[\"LAG_FEATURES\"][4] = [e for e in FEATURES[\"LAG_FEATURES\"][4] if e not in {'shop_id_x_item_category_id_sales_sum','y_sales','item_cluster_sales_sum'}]\n","FEATURES[\"LAG_FEATURES\"][5] = [e for e in FEATURES[\"LAG_FEATURES\"][5] if e not in {'item_cluster_sales_sum','item_id_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","FEATURES[\"LAG_FEATURES\"][6] = [e for e in FEATURES[\"LAG_FEATURES\"][6] if e not in {'item_id_sales_sum','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","FEATURES[\"LAG_FEATURES\"][7] = [e for e in FEATURES[\"LAG_FEATURES\"][7] if e not in {'y_sales','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","FEATURES[\"LAG_FEATURES\"][8] = [e for e in FEATURES[\"LAG_FEATURES\"][8] if e not in {'item_id_sales_sum','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","\n","# LAG_STATS_SET is SET of all aggregate statistics columns for all lags (allows us to shed the other stats, keeping memory requirements lower)\n","LAG_STATS_SET = [] # FEATURES[\"LAG_FEATURES\"][1]\n","FEATURES[\"ALL_FEATURES\"] = [] #FEATURES[\"LAG_FEATURES\"][1]\n","for m in FEATURES[\"LAGS_MONTHS\"]: #[1:]:\n","    LAG_STATS_SET = LAG_STATS_SET + [x for x in FEATURES[\"LAG_FEATURES\"][m] if x not in LAG_STATS_SET]\n","    FEATURES[\"ALL_FEATURES\"] = FEATURES[\"ALL_FEATURES\"] + [x + \"_L\" + str(m) for x in FEATURES[\"LAG_FEATURES\"][m]]\n","FEATURES[\"ALL_FEATURES\"] = INTEGER_COLS + FEATURES[\"ALL_FEATURES\"]\n","N_FEATURES = len(FEATURES[\"ALL_FEATURES\"])\n","\n","# Define various constants that drive the attributes of the various features\n","# Define hyperparameters for modeling and feature generation, including those that we might want to loop over multiple choices\n","# Put in a DataFrame so we can \"explode\" and generate rows for every possible combination of variable inputs\n","ALL_RUN_PARAMETERS = pd.DataFrame({\n","        ###### below are regresssor setup/fitting parameters, data parameters, and output data slots ########\n","        'boosting_type':'gbdt',\n","        'metric':[['rmse']],\n","        'learning_rate':[[0.05]],                                   # default = 0.1\n","        'n_estimators':[[200]],\n","        'colsample_bytree':[[0.4]],                                 # = feature_fraction;  default = 1 for LGBM, 0 for HGBR (these models use inverse forms of regularization)\n","        'random_state':[[42]],\n","        'subsample_for_bin':[[200000,800000]],\n","        'num_leaves':[[31]],\n","        'max_depth':[[-1]],\n","        'min_split_gain':[[0.0]],\n","        'min_child_weight':[[0.001]],\n","        'min_child_samples':[[20]],\n","        'silent':[[False]],\n","        'importance_type':[['split']],                              # 'split' = num of times the feature is used in model; 'gain' = total gains of splits which use the feature\n","        'reg_alpha':[[0.0]],\n","        'reg_lambda':[[0.0]],\n","        'n_jobs':-1,\n","        'subsample':1.0,\n","        'subsample_freq':0,\n","        'objective':'regression', \n","        ###### last regressor setup parameter above ######\n","        ###### below are fitting parameters, data parameters, and output data slots ########\n","        'early_stopping_rounds':[[20]],\n","        'eval_metric':[['rmse']],                                   # if multiple metrics are desired, use [['rmse',['rmse','l2']]] to do one iteration with 'rmse' and one iteration with ['rmse','l2']\n","        'init_score':None,\n","        'eval_init_score':None,\n","        'verbose':[[True]],                                         # True # (int=4 four is to print every 4th iteration); True is every iteration; False is no print except best and last\n","        'feature_name':[['auto']],\n","        'categorical_feature':[['auto']],\n","        'callbacks':None,\n","        ###### last fitting parameter above ######\n","        ###### below are data manipulation parameters, and output results slots ########\n","        'cartprod_fillna0':[[True]],                               # fill n/a cartesian additions with zeros (not good for price-based stats, however)\n","        'cartprod_first_month':[[13]],                             # month number + max lag to start adding Cartesian product rows (i.e., maxlag=6mo and cartprod_first_month=10 will cartesian fill from 4 to 33)\n","        'cartprod_test_pairs':[[False]],                           # include all of test set 'shop-item pairs' with each month's normal cartesian product fill\n","        'clip_train_H':[[20]],                                     # this clips sales after doing monthly groupings (monthly_stt dataframe)\n","        'clip_train_L':[[0]],                                      # this clips sales after doing monthly groupings (monthly_stt dataframe)\n","        'clip_predict_H':[[20]],                                   # this clips the final result before submission to Coursera/Kaggle\n","        'clip_predict_L':[[0]],                                    # this clips the final result before submission to Coursera/Kaggle\n","        'eda_delete_shops':[[[9,20]]],                             # [0,1,8,9,11,13,17,20,23,27,29,30,32,33,40,43,51,54]   # [8, 9, 13, 20, 23, 32, 33, 40]  # False  # online, untested, and early-termination shops\n","        'eda_delete_item_cats':[[[8, 10, 32, 59, 80, 81, 82]]],    # [1,4,8,10,13,14,17,18,32,39,46,48,50,51,52,53,59,66,68,80,81,82]  # [8, 80, 81, 82]  # False     # hokey categories, untested categories\n","        'eda_scale_month':[['week_retail_weight']],                # False # scale sales by days in month, number of each weekday, and Russian recession retail sales index\n","        'feature_data_type':[[np.int16]],                          # np.float32 np.uint16      # if df contains np.NaN entries, python integer type cannot represent it, and we need to use memory-hogging float32\n","        'minmax_scaler_range':[[(0,16000)]],                       # int16 = (0,32700); uint16 = (0,65500)  --> keep range positive for best results with LGBM; keep to smaller numbers for faster LGBM fitting\n","        'model_name_base':'v13_15b_ens',                           # Name of file substring to save data submission to (= False if user to input it below); concatenated with model_type before save\n","        'model_type':[['LGBM']],                                   # 'HGBR' for sklearn version\n","        'robust_scaler_quantiles':[[(20,80)]],                     # parameter determines how sklearn robust scaler will do it's bizness\n","        'test_month':[[34]],\n","        'train_start_month':[[13]],                                # == 24 ==> less than a year of data, but avoids December 'outlier' of 2014\n","        'train_final_month':[[29]],                                # [29,32] #,30,32]\n","        'use_cartprod_fill':[[True]],                              # use cartesian fill, or not\n","        'use_categorical':[[True]],                                # relevant dataframe columns are changed to categorical dtype just before model fitting/creation\n","        'use_minmax_scaler':[[True]],                              # scale features linearly to use large range of np.int16 (NOTE: only use positive integer output)\n","        'use_robust_scaler':[[True]],                              # scale features using quantiles to reduce influence of outliers\n","        'validate_key':[[999]],                                    # 1 # ; if 999, val is all months after training, up to and including 33; otherwise val is this many months after train_month_end\n","        ###### last data manipulation parameter above ######\n","        ###### below are output results slots ########\n","        'best_iteration_':0,                                      \n","        'best_score_':0.0,\n","        'feature_names':[[[\"\"]]],\n","        'feature_importances_':[[[0.0]]],\n","        'model_params':[[{}]],\n","        'time_model_fit':datetime.datetime.utcfromtimestamp(2).strftime('%H:%M:%S'),\n","        'time_model_predict':datetime.datetime.utcfromtimestamp(1).strftime('%H:%M:%S'),\n","        'time_start':f'{strftime(\"%a %X %x\")}',\n","        'time_total_wall_clock':datetime.datetime.utcfromtimestamp(3).strftime('%H:%M:%S'),\n","        'tr_rmse':0.0,\n","        'tr_R2':0.0,\n","        'val_rmse':0.0,\n","        'val_R2':0.0\n","        })\n","\n","FEATURES[\"OUTPUTS\"] =                       ['best_iteration_','best_score_','feature_importances_','feature_names','model_params','time_model_fit',\n","                                                'time_model_predict','time_start','time_total_wall_clock','tr_R2','tr_rmse','val_R2','val_rmse']\n","FEATURES[\"PARAMS_LGBM_SETUP\"] =             ['boosting_type','metric','learning_rate','n_estimators','colsample_bytree','random_state','subsample_for_bin','num_leaves','max_depth','min_split_gain',\n","                                                'min_child_weight','min_child_samples','silent','importance_type','reg_alpha','reg_lambda','n_jobs','subsample','subsample_freq','objective']\n","FEATURES[\"PARAMS_LGBM_FIT\"] =               ['eval_metric','early_stopping_rounds','init_score','eval_init_score','verbose','feature_name','categorical_feature','callbacks']\n","FEATURES[\"PARAMS_DATA_MANIP\"] =             [e for e in ALL_RUN_PARAMETERS.columns if e not in set(FEATURES[\"OUTPUTS\"] + FEATURES[\"PARAMS_LGBM_SETUP\"] + FEATURES[\"PARAMS_LGBM_FIT\"])]\n","\n","outputs_dict = ALL_RUN_PARAMETERS[FEATURES[\"OUTPUTS\"]].iloc[0].to_dict()\n","params_lgbm_setup_dict = ALL_RUN_PARAMETERS[FEATURES[\"PARAMS_LGBM_SETUP\"]].iloc[0].to_dict()        # pprint.pprint(PARS.filter(like='', axis=1).iloc[0].to_dict())\n","params_lgbm_fit_dict = ALL_RUN_PARAMETERS[FEATURES[\"PARAMS_LGBM_FIT\"]].iloc[0].to_dict()\n","params_data_manip_dict = ALL_RUN_PARAMETERS[FEATURES[\"PARAMS_DATA_MANIP\"]].iloc[0].to_dict()        # print(params_data_manip_dict)\n","\n","\n","# Explode the dataframe so each row is one iteration of modeling parameters\n","ITERS = ALL_RUN_PARAMETERS.copy(deep=True)\n","for c in ITERS.columns:\n","    ITERS = ITERS.explode(c)\n","ITERS = ITERS.reset_index(drop=True)\n","N_TRAIN_MODELS = ITERS.shape[0]\n","\n","print(f'Shape of ITERS DataFrame: {ITERS.shape}')\n","print(f'N train models: {N_TRAIN_MODELS}')\n","print(f'N features: {N_FEATURES}')\n","\n","print(\"\\nLGBM Regressor Setup Parameters:\"); pprint.pprint(params_lgbm_setup_dict)\n","print(\"\\nLGBM Regressor Fit Parameters:\"); pprint.pprint(params_lgbm_fit_dict)\n","print(\"\\nData Manipulation Parameters:\"); pprint.pprint(params_data_manip_dict)                     # ,width=200,compact=True)\n","print(\"\\nOutput Results:\"); pprint.pprint(outputs_dict)\n","print(\"\\nFeature Column Info:\"); pprint.pprint(dict(FEATURES),width=200,compact=True)\n","print(f'\\n\\nEntire ALL_RUN_PARAMETERS DataFrame, as a dictionary:\\n{ALL_RUN_PARAMETERS.to_dict(orient=\"list\")}')    # simple format like dict\n","print(f'\\nEntire ITERS DataFrame, as a dictionary:\\n{ITERS.to_dict(orient=\"list\")}')                                # simple format like dict\n","print(f'\\nITERS DataFrame:\\n{ITERS}\\n')                                                                             # tabular format\n","\n","mu,vu,vt,va,t = get_memory_stats(\"Done Defining Iteration Parameters: \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of ITERS DataFrame: (2, 64)\n","N train models: 2\n","N features: 58\n","\n","LGBM Regressor Setup Parameters:\n","{'boosting_type': 'gbdt',\n"," 'colsample_bytree': [0.4],\n"," 'importance_type': ['split'],\n"," 'learning_rate': [0.05],\n"," 'max_depth': [-1],\n"," 'metric': ['rmse'],\n"," 'min_child_samples': [20],\n"," 'min_child_weight': [0.001],\n"," 'min_split_gain': [0.0],\n"," 'n_estimators': [200],\n"," 'n_jobs': -1,\n"," 'num_leaves': [31],\n"," 'objective': 'regression',\n"," 'random_state': [42],\n"," 'reg_alpha': [0.0],\n"," 'reg_lambda': [0.0],\n"," 'silent': [False],\n"," 'subsample': 1.0,\n"," 'subsample_for_bin': [200000, 800000],\n"," 'subsample_freq': 0}\n","\n","LGBM Regressor Fit Parameters:\n","{'callbacks': None,\n"," 'categorical_feature': ['auto'],\n"," 'early_stopping_rounds': [20],\n"," 'eval_init_score': None,\n"," 'eval_metric': ['rmse'],\n"," 'feature_name': ['auto'],\n"," 'init_score': None,\n"," 'verbose': [True]}\n","\n","Data Manipulation Parameters:\n","{'cartprod_fillna0': [True],\n"," 'cartprod_first_month': [13],\n"," 'cartprod_test_pairs': [False],\n"," 'clip_predict_H': [20],\n"," 'clip_predict_L': [0],\n"," 'clip_train_H': [20],\n"," 'clip_train_L': [0],\n"," 'eda_delete_item_cats': [[8, 10, 32, 59, 80, 81, 82]],\n"," 'eda_delete_shops': [[9, 20]],\n"," 'eda_scale_month': ['week_retail_weight'],\n"," 'feature_data_type': [<class 'numpy.int16'>],\n"," 'minmax_scaler_range': [(0, 16000)],\n"," 'model_name_base': 'v13_15b_ens',\n"," 'model_type': ['LGBM'],\n"," 'robust_scaler_quantiles': [(20, 80)],\n"," 'test_month': [34],\n"," 'train_final_month': [29],\n"," 'train_start_month': [13],\n"," 'use_cartprod_fill': [True],\n"," 'use_categorical': [True],\n"," 'use_minmax_scaler': [True],\n"," 'use_robust_scaler': [True],\n"," 'validate_key': [999]}\n","\n","Output Results:\n","{'best_iteration_': 0,\n"," 'best_score_': 0.0,\n"," 'feature_importances_': [[0.0]],\n"," 'feature_names': [['']],\n"," 'model_params': [{}],\n"," 'time_model_fit': '00:00:02',\n"," 'time_model_predict': '00:00:01',\n"," 'time_start': 'Mon 21:05:14 07/27/20',\n"," 'time_total_wall_clock': '00:00:03',\n"," 'tr_R2': 0.0,\n"," 'tr_rmse': 0.0,\n"," 'val_R2': 0.0,\n"," 'val_rmse': 0.0}\n","\n","Feature Column Info:\n","{'AGG_STATS': OrderedDict([('sales', ['sum', 'median', 'count']), ('revenue', ['sum'])]),\n"," 'ALL_FEATURES': ['month', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster', 'y_sales_L1', 'shop_id_x_item_id_sales_median_L1', 'shop_id_x_item_id_sales_count_L1',\n","                  'shop_id_x_item_id_revenue_sum_L1', 'shop_id_x_item_category_id_sales_sum_L1', 'shop_id_x_item_category_id_sales_median_L1', 'shop_id_x_item_category_id_sales_count_L1',\n","                  'shop_id_x_item_cluster_sales_sum_L1', 'shop_id_x_item_cluster_sales_median_L1', 'shop_id_sales_sum_L1', 'shop_id_sales_count_L1', 'item_id_sales_sum_L1', 'item_id_sales_median_L1',\n","                  'item_id_sales_count_L1', 'item_id_revenue_sum_L1', 'shop_group_revenue_sum_L1', 'item_category_id_sales_sum_L1', 'item_category_id_sales_count_L1',\n","                  'item_category_id_revenue_sum_L1', 'item_group_sales_sum_L1', 'item_group_revenue_sum_L1', 'item_cluster_sales_sum_L1', 'item_cluster_sales_count_L1', 'item_cluster_revenue_sum_L1',\n","                  'y_sales_L2', 'shop_id_x_item_id_sales_count_L2', 'shop_id_x_item_id_revenue_sum_L2', 'shop_id_x_item_category_id_sales_count_L2', 'shop_id_x_item_category_id_revenue_sum_L2',\n","                  'shop_id_sales_sum_L2', 'item_id_sales_sum_L2', 'item_id_sales_count_L2', 'item_id_revenue_sum_L2', 'item_category_id_sales_sum_L2', 'item_category_id_sales_count_L2',\n","                  'item_cluster_sales_sum_L2', 'item_cluster_sales_count_L2', 'item_cluster_revenue_sum_L2', 'y_sales_L3', 'shop_id_sales_sum_L3', 'item_id_sales_sum_L3', 'item_id_sales_count_L3',\n","                  'item_id_revenue_sum_L3', 'item_category_id_sales_sum_L3', 'item_category_id_sales_count_L3', 'item_cluster_sales_count_L3', 'item_id_sales_sum_L4', 'y_sales_L5', 'y_sales_L6',\n","                  'item_id_sales_sum_L7', 'y_sales_L8'],\n"," 'COLS_KEEP_BASE_TRAIN_TEST': ['month', 'price', 'sales', 'shop_id', 'item_id'],\n"," 'COLS_KEEP_DATE_SCALING': ['month', 'days_in_M', 'weekday_weight', 'retail_sales', 'week_retail_weight'],\n"," 'COLS_KEEP_ITEMS': ['item_id', 'item_group', 'item_cluster', 'item_category_id'],\n"," 'COLS_KEEP_SHOPS': ['shop_id', 'shop_group'],\n"," 'COLS_ORDER_STT': ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster'],\n"," 'LAGGED_FEATURE_GROUPS': [['shop_id', 'item_id'], ['shop_id', 'item_category_id'], ['shop_id', 'item_cluster'], 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster'],\n"," 'LAGS_MONTHS': [1, 2, 3, 4, 5, 6, 7, 8],\n"," 'LAG_FEATURES': OrderedDict([(1,\n","                               ['y_sales', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_sum',\n","                                'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median',\n","                                'shop_id_sales_sum', 'shop_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', 'shop_group_revenue_sum',\n","                                'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', 'item_group_sales_sum', 'item_group_revenue_sum',\n","                                'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']),\n","                              (2,\n","                               ['y_sales', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_revenue_sum',\n","                                'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count',\n","                                'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']),\n","                              (3,\n","                               ['y_sales', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count',\n","                                'item_cluster_sales_count']),\n","                              (4, ['item_id_sales_sum']), (5, ['y_sales']), (6, ['y_sales']), (7, ['item_id_sales_sum']), (8, ['y_sales'])]),\n"," 'OUTPUTS': ['best_iteration_', 'best_score_', 'feature_importances_', 'feature_names', 'model_params', 'time_model_fit', 'time_model_predict', 'time_start', 'time_total_wall_clock', 'tr_R2',\n","             'tr_rmse', 'val_R2', 'val_rmse'],\n"," 'PARAMS_DATA_MANIP': ['cartprod_fillna0', 'cartprod_first_month', 'cartprod_test_pairs', 'clip_train_H', 'clip_train_L', 'clip_predict_H', 'clip_predict_L', 'eda_delete_shops',\n","                       'eda_delete_item_cats', 'eda_scale_month', 'feature_data_type', 'minmax_scaler_range', 'model_name_base', 'model_type', 'robust_scaler_quantiles', 'test_month',\n","                       'train_start_month', 'train_final_month', 'use_cartprod_fill', 'use_categorical', 'use_minmax_scaler', 'use_robust_scaler', 'validate_key'],\n"," 'PARAMS_LGBM_FIT': ['eval_metric', 'early_stopping_rounds', 'init_score', 'eval_init_score', 'verbose', 'feature_name', 'categorical_feature', 'callbacks'],\n"," 'PARAMS_LGBM_SETUP': ['boosting_type', 'metric', 'learning_rate', 'n_estimators', 'colsample_bytree', 'random_state', 'subsample_for_bin', 'num_leaves', 'max_depth', 'min_split_gain',\n","                       'min_child_weight', 'min_child_samples', 'silent', 'importance_type', 'reg_alpha', 'reg_lambda', 'n_jobs', 'subsample', 'subsample_freq', 'objective']}\n","\n","\n","Entire ALL_RUN_PARAMETERS DataFrame, as a dictionary:\n","{'boosting_type': ['gbdt'], 'metric': [['rmse']], 'learning_rate': [[0.05]], 'n_estimators': [[200]], 'colsample_bytree': [[0.4]], 'random_state': [[42]], 'subsample_for_bin': [[200000, 800000]], 'num_leaves': [[31]], 'max_depth': [[-1]], 'min_split_gain': [[0.0]], 'min_child_weight': [[0.001]], 'min_child_samples': [[20]], 'silent': [[False]], 'importance_type': [['split']], 'reg_alpha': [[0.0]], 'reg_lambda': [[0.0]], 'n_jobs': [-1], 'subsample': [1.0], 'subsample_freq': [0], 'objective': ['regression'], 'early_stopping_rounds': [[20]], 'eval_metric': [['rmse']], 'init_score': [None], 'eval_init_score': [None], 'verbose': [[True]], 'feature_name': [['auto']], 'categorical_feature': [['auto']], 'callbacks': [None], 'cartprod_fillna0': [[True]], 'cartprod_first_month': [[13]], 'cartprod_test_pairs': [[False]], 'clip_train_H': [[20]], 'clip_train_L': [[0]], 'clip_predict_H': [[20]], 'clip_predict_L': [[0]], 'eda_delete_shops': [[[9, 20]]], 'eda_delete_item_cats': [[[8, 10, 32, 59, 80, 81, 82]]], 'eda_scale_month': [['week_retail_weight']], 'feature_data_type': [[<class 'numpy.int16'>]], 'minmax_scaler_range': [[(0, 16000)]], 'model_name_base': ['v13_15b_ens'], 'model_type': [['LGBM']], 'robust_scaler_quantiles': [[(20, 80)]], 'test_month': [[34]], 'train_start_month': [[13]], 'train_final_month': [[29]], 'use_cartprod_fill': [[True]], 'use_categorical': [[True]], 'use_minmax_scaler': [[True]], 'use_robust_scaler': [[True]], 'validate_key': [[999]], 'best_iteration_': [0], 'best_score_': [0.0], 'feature_names': [[['']]], 'feature_importances_': [[[0.0]]], 'model_params': [[{}]], 'time_model_fit': ['00:00:02'], 'time_model_predict': ['00:00:01'], 'time_start': ['Mon 21:05:14 07/27/20'], 'time_total_wall_clock': ['00:00:03'], 'tr_rmse': [0.0], 'tr_R2': [0.0], 'val_rmse': [0.0], 'val_R2': [0.0]}\n","\n","Entire ITERS DataFrame, as a dictionary:\n","{'boosting_type': ['gbdt', 'gbdt'], 'metric': ['rmse', 'rmse'], 'learning_rate': [0.05, 0.05], 'n_estimators': [200, 200], 'colsample_bytree': [0.4, 0.4], 'random_state': [42, 42], 'subsample_for_bin': [200000, 800000], 'num_leaves': [31, 31], 'max_depth': [-1, -1], 'min_split_gain': [0.0, 0.0], 'min_child_weight': [0.001, 0.001], 'min_child_samples': [20, 20], 'silent': [False, False], 'importance_type': ['split', 'split'], 'reg_alpha': [0.0, 0.0], 'reg_lambda': [0.0, 0.0], 'n_jobs': [-1, -1], 'subsample': [1.0, 1.0], 'subsample_freq': [0, 0], 'objective': ['regression', 'regression'], 'early_stopping_rounds': [20, 20], 'eval_metric': ['rmse', 'rmse'], 'init_score': [None, None], 'eval_init_score': [None, None], 'verbose': [True, True], 'feature_name': ['auto', 'auto'], 'categorical_feature': ['auto', 'auto'], 'callbacks': [None, None], 'cartprod_fillna0': [True, True], 'cartprod_first_month': [13, 13], 'cartprod_test_pairs': [False, False], 'clip_train_H': [20, 20], 'clip_train_L': [0, 0], 'clip_predict_H': [20, 20], 'clip_predict_L': [0, 0], 'eda_delete_shops': [[9, 20], [9, 20]], 'eda_delete_item_cats': [[8, 10, 32, 59, 80, 81, 82], [8, 10, 32, 59, 80, 81, 82]], 'eda_scale_month': ['week_retail_weight', 'week_retail_weight'], 'feature_data_type': [<class 'numpy.int16'>, <class 'numpy.int16'>], 'minmax_scaler_range': [(0, 16000), (0, 16000)], 'model_name_base': ['v13_15b_ens', 'v13_15b_ens'], 'model_type': ['LGBM', 'LGBM'], 'robust_scaler_quantiles': [(20, 80), (20, 80)], 'test_month': [34, 34], 'train_start_month': [13, 13], 'train_final_month': [29, 29], 'use_cartprod_fill': [True, True], 'use_categorical': [True, True], 'use_minmax_scaler': [True, True], 'use_robust_scaler': [True, True], 'validate_key': [999, 999], 'best_iteration_': [0, 0], 'best_score_': [0.0, 0.0], 'feature_names': [[''], ['']], 'feature_importances_': [[0.0], [0.0]], 'model_params': [{}, {}], 'time_model_fit': ['00:00:02', '00:00:02'], 'time_model_predict': ['00:00:01', '00:00:01'], 'time_start': ['Mon 21:05:14 07/27/20', 'Mon 21:05:14 07/27/20'], 'time_total_wall_clock': ['00:00:03', '00:00:03'], 'tr_rmse': [0.0, 0.0], 'tr_R2': [0.0, 0.0], 'val_rmse': [0.0, 0.0], 'val_R2': [0.0, 0.0]}\n","\n","ITERS DataFrame:\n","  boosting_type metric learning_rate n_estimators colsample_bytree random_state subsample_for_bin num_leaves max_depth min_split_gain min_child_weight min_child_samples silent importance_type reg_alpha reg_lambda  \\\n","0          gbdt   rmse         0.050          200            0.400           42            200000         31        -1              0            0.001                20  False           split         0          0   \n","1          gbdt   rmse         0.050          200            0.400           42            800000         31        -1              0            0.001                20  False           split         0          0   \n","\n","   n_jobs  subsample  subsample_freq   objective early_stopping_rounds eval_metric init_score eval_init_score verbose feature_name categorical_feature callbacks cartprod_fillna0 cartprod_first_month  ...  \\\n","0      -1          1               0  regression                    20        rmse       None            None    True         auto                auto      None             True                   13  ...   \n","1      -1          1               0  regression                    20        rmse       None            None    True         auto                auto      None             True                   13  ...   \n","\n","  clip_predict_L eda_delete_shops         eda_delete_item_cats     eda_scale_month      feature_data_type minmax_scaler_range model_name_base model_type robust_scaler_quantiles test_month train_start_month  \\\n","0              0          [9, 20]  [8, 10, 32, 59, 80, 81, 82]  week_retail_weight  <class 'numpy.int16'>          (0, 16000)     v13_15b_ens       LGBM                (20, 80)         34                13   \n","1              0          [9, 20]  [8, 10, 32, 59, 80, 81, 82]  week_retail_weight  <class 'numpy.int16'>          (0, 16000)     v13_15b_ens       LGBM                (20, 80)         34                13   \n","\n","  train_final_month use_cartprod_fill use_categorical use_minmax_scaler use_robust_scaler validate_key best_iteration_ best_score_ feature_names feature_importances_  model_params  time_model_fit time_model_predict  \\\n","0                29              True            True              True              True          999               0           0            []                [0.0]            {}        00:00:02           00:00:01   \n","1                29              True            True              True              True          999               0           0            []                [0.0]            {}        00:00:02           00:00:01   \n","\n","              time_start time_total_wall_clock tr_rmse tr_R2 val_rmse val_R2  \n","0  Mon 21:05:14 07/27/20              00:00:03       0     0        0      0  \n","1  Mon 21:05:14 07/27/20              00:00:03       0     0        0      0  \n","\n","[2 rows x 64 columns]\n","\n","Done Defining Iteration Parameters: Memory Use: 0.81 | 1.25 | 26.14 | 27.39 GB:  pid, vm_used, vm_available, vm_total.  Mon 21:05:14 07/27/20.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_Ldvqgyw0_Hw","colab_type":"text"},"source":["#**Data Preparation, Including Feature Merging and Feature Generation**"]},{"cell_type":"markdown","metadata":{"id":"Isean25B9FjA","colab_type":"text"},"source":["###**Helper Functions**"]},{"cell_type":"code","metadata":{"id":"tCWGzR4vl8TW","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595898387299,"user_tz":240,"elapsed":341,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"aad6510b-14d3-4677-9c41-c97c40f813e5"},"source":["# helper function to print out column datatypes and memory usage, using multiple columns so we don't have to scroll so much\n","def print_col_info(df,nrows=5):\n","    \"\"\"\n","    instead of the usual single column (plus index) series printout of dtypes and memory use in a dataframe,\n","    this function combines dtypes and memory use so they use the same index,\n","    and then prints out multiple columns of length \"nrows\", where each column is like: \"column_dtype \\t column_memory_use(MB) \\t column_name\"\n","        df = dataframe of interest\n","            col_dtypes = pd.Series, type obj, index = column_name, values = dtype  (e.g., from the command \"df.dtypes\")\n","            col_mem = pd.Series, type int64, index = column_name, values = column memory use (bytes) (e.g., from the command \"df.memory_usage(deep=True)\")\n","        nrows = int, tells how many rows of (type/mem/name) to print before moving to a new printout column for the next triplet (type/mem/name)\n","                if nrows == 0, print all triplets in just one column, with no \"wrapping\"\n","    finishes with a printout of total df memory usage\n","    \"\"\"\n","    col_mem = df.memory_usage(deep=True)\n","    col_mem = col_mem/1e6  #change to MB\n","    total_mem = col_mem.sum()\n","\n","    col_dtypes = pd.Series([df.index.dtype], index = ['Index'])  # df.memory_usage includes Index, but df.dtypes does not include Index, so we have to add it\n","    col_dtypes = pd.concat([col_dtypes,df.dtypes], axis=0)\n","\n","    col_info_df = pd.concat([col_dtypes, col_mem], axis=1).reset_index().rename(columns={'index':'Column Name', 0:'DType', 1:'MBytes'})\n","\n","\n","    if nrows == 0:\n","        print(col_info_df)\n","    else:\n","        col_info_df.MBytes = col_info_df.MBytes.apply(lambda x: str(f'{x:.1f}'))\n","        info_df_len = len(col_info_df)\n","        cnames = col_info_df.columns\n","        n_info_cols = len(cnames)\n","        between_cols = 6  # spaces separating the info-group columns (e.g., between \"ColName Dtype Mem\" and next column \"ColName Dtype Mem\")\n","\n","        # adjust number of rows such that we don't have nasty column with just one or a few rows\n","        stragglers = info_df_len % nrows\n","        n_print_cols = info_df_len // nrows\n","        if (stragglers > nrows/2):\n","            n_print_cols += 1\n","        elif (stragglers > 0):\n","            nrows = info_df_len // n_print_cols\n","            if info_df_len % n_print_cols > 0:\n","                nrows += 1\n","\n","        df_list = []\n","        for pc in range(n_print_cols):\n","            df_list.append(col_info_df.shift(periods = -nrows*pc))\n","        df_print = pd.concat(df_list, axis = 1)\n","        df_print = df_print.iloc[:nrows][:].fillna(\" \")\n","        col_headers = df_print.columns\n","        n_df_cols = len(col_headers)\n","        \n","        # find max string length in each column\n","        columnLengths = np.vectorize(len)\n","        maxColumnLengths = columnLengths(df_print.values.astype(str)).max(axis=0)\n","        col_widths = np.add(maxColumnLengths,3)\n","\n","        for r in range(nrows+1):\n","            if r==0:\n","                string_list = col_headers\n","            else:\n","                string_list = df_print.iloc[r-1][:]\n","            print_row = ''\n","            c_count = 0\n","            for c in range(n_df_cols):\n","                print_row = print_row + f'{str(string_list[c]):>{col_widths[c]}} '\n","                c_count += 1\n","                if c_count == n_info_cols:\n","                    c_count = 0\n","                    print_row += \" \" * between_cols  # extra space between columns of common data\n","\n","            print(print_row)\n","\n","    print(f'\\nDataFrame Shape: {df.shape}')\n","    print(f'DataFrame total memory usage: {total_mem:.0f} MB')\n","    mu,vu,vt,va,t = get_memory_stats(\"Done Printing Column Info: \")\n","    \n","print(f'Done: {strftime(\"%a %X %x\")}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done: Mon 21:06:25 07/27/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fz81jlrrjz55","colab_type":"text"},"source":["##**Initial data prep, formatting**"]},{"cell_type":"markdown","metadata":{"id":"W7uoRFv-uSC2","colab_type":"text"},"source":["###**Clean up the data, drop undesirable columns, merge shops and items info into train/test dataframe**"]},{"cell_type":"code","metadata":{"id":"seucVyaI90Mt","colab_type":"code","cellView":"both","colab":{}},"source":["# Remove columns with features that we don't use at this time\n","shops_purged = shops_enc[FEATURES[\"COLS_KEEP_SHOPS\"]].copy(deep=True) #.rename(columns = SHOPS_COLUMN_RENAME)\n","items_purged = items_enc[FEATURES[\"COLS_KEEP_ITEMS\"]].copy(deep=True) #.rename(columns = ITEMS_COLUMN_RENAME)\n","date_adj_purged = date_adjustments[FEATURES[\"COLS_KEEP_DATE_SCALING\"]].copy(deep=True)\n","stt = train_test_base[FEATURES[\"COLS_KEEP_BASE_TRAIN_TEST\"]].copy(deep=True)  # 'stt' will be the dataframe for (S)ales of (T)rain appended with (T)est\n","\n","# Merge shops and items into stt\n","stt = stt.merge(shops_purged, on='shop_id', how='left')\n","stt = stt.merge(items_purged, on='item_id', how='left')\n","\n","print(f'shops_purged dataframe shape: {shops_purged.shape}\\n{shops_purged.head(2)}\\n')\n","print(f'items_purged dataframe shape: {items_purged.shape}\\n{items_purged.head(2)}\\n')\n","print(f'date_adj_purged dataframe shape: {date_adj_purged.shape}\\n{date_adj_purged.head(2)}\\n')\n","print(f'stt dataframe shape after merge with shops and items data: {stt.shape}\\n{stt.head(2)}\\n')\n","\n","mu,vu,vt,va,t = get_memory_stats(\"Done Merging Data Sets: \")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1GXED3-jyQC7"},"source":["##**Clean STT DataFrame; Compute and Merge Statistics-Based Features on Grouped-by-Month training data**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Jhj7bbgCceh8"},"source":["###Create monthly_stt dataframe, grouping by month the (s)ales_(t)rain_(t)est dataframe"]},{"cell_type":"code","metadata":{"id":"AghbBFHC_ljc","colab_type":"code","cellView":"both","colab":{}},"source":["\n","# Cleanup STT DataFrame; delete unwanted shops & item_categories; scale by monthly date scaler\n","\n","def clean_stt(stt_, del_shops, del_item_cats, scale_month, date_df, int_cols, stt_cols):\n","    \"\"\"\n","    Delete unwanted shops and item categories; Scale monthly sales for days in month, etc\n","    Insert revenue feature; Adjust data types; Keep only desired columns, and set them in preferred order\n","    \"\"\"\n","    print(f'\\nInitial shape of stt_ DataFrame, before dropping unwanted shops and item categories: {stt_.shape}')\n","\n","    # drop undesirable shops and item categories\n","    if del_shops:\n","        stt_ = stt_.query('shop_id != @del_shops')\n","        print(f'Shape of stt_ after deleting shops {del_shops}: {stt_.shape}')\n","\n","    if del_item_cats:\n","        stt_ = stt_.query('item_category_id != @del_item_cats')\n","        print(f'Shape of stt_ after deleting item categories {del_item_cats}: {stt_.shape}\\n')\n","\n","    # scale by date_adjustments as desired\n","    if scale_month:\n","        stt_ = stt_.merge(date_df[['month',scale_month]], on='month', how='left')\n","        stt_.sales = stt_.sales * stt_[scale_month]\n","        stt_.drop(scale_month, axis=1, inplace=True) \n","\n","    # insert revenue feature; adjust data types and select only desired columns and column order for stt_ dataframe\n","    stt_['revenue'] = stt_.sales * stt_.price / 1000\n","    stt_[['sales','price','revenue']].astype(np.float32) #apply(pd.to_numeric, downcast= 'np.float32') # use float so fractional date_adj weight is more accurate, and can use price feature #reset index saves 25MB\n","    stt_[int_cols] = stt_[int_cols].astype('int16')\n","    stt_ = stt_[stt_cols]     # drop unnecessary columns and reorder as desired\n","    stt_ = stt_.reset_index(drop=True)\n","    print(f'\\nFinal shape of stt_ DataFrame: {stt_.shape}')\n","\n","    return stt_\n","\n","\n","# Create MONTHLY_STT DataFrame = STT grouped by month, with statistics\n","# Compute values in \"real time,\" then in a later code cell we will compute shifted versions\n","\n","def compute_stats(stt_=pd.DataFrame(), monthly_stt_=pd.DataFrame(), no_merge=True, group=['month','item_id'], \n","                  aggstats={'sales':['sum','median']}, aggcolnames=['item_id_sales_sum','item_id_sales_med']):\n","    \"\"\"\n","    function for computing statistics-based features, in an attempt to be flexible if\n","    we wish to add in extra statistics or extra group-by categories\n","    no_merge True during first runthrough of this function, to create the initial dataframe into which we will merge various statistics features\n","    \"\"\"\n","    grouped_df = stt_.groupby(group).agg(aggstats)\n","    grouped_df.columns = aggcolnames\n","    grouped_df.reset_index(inplace=True)\n","    if no_merge:                                # this creates the initial monthly-grouped dataframe into which we will merge all other grouped statistics\n","        monthly_stt_ = grouped_df.copy(deep=True)\n","    else:\n","        monthly_stt_ = monthly_stt_.merge(grouped_df, on = group, how = 'left')\n","    return monthly_stt_                         # original monthly_df merged with aggregated and suitably named stats columns created from ungrouped dataframe (stt)\n","\n","\n","\n","def create_monthly_stt(stt_, lag_feature_groups, cols_order_stt, agg_stats_params, clipL, clipH, int_cols, lag_stats_set_,use_robust_scaler,robust_scaler_quantiles,\n","                       use_minmax_scaler,minmax_scaler_range,feature_data_type):\n","    monthly_stt_ = pd.DataFrame()\n","    stats_ = OrderedDict()\n","    for i,feat in enumerate(lag_feature_groups):\n","        if type(feat) is list:\n","            f = \"_x_\".join(feat)\n","            gp = ['month'] + feat\n","        else:\n","            f = feat\n","            gp = ['month'] + [feat]\n","        agg_stats = OrderedDict()\n","        agg_names = []\n","        if i == 0:\n","            for pf in [e for e in cols_order_stt if e not in ['month','shop_id','item_id'] + list(agg_stats_params.keys()) ]:  # start monthly_stt creation with group(month,shop_id,item_id)\n","                agg_stats[pf] = ['first']\n","                agg_names.append(pf)\n","        for k,v in agg_stats_params.items():\n","            agg_stats[k] = v\n","            for val in v:\n","                agg_names.append(f + \"_\" + k + \"_\" + val)\n","        stats_[f] = {'group':gp, 'aggstats':agg_stats, 'aggnames':agg_names}\n","\n","    no_merge = True\n","    #print('Completed: ',end='')\n","    for k,v in stats_.items():\n","        monthly_stt_ = compute_stats(stt_, monthly_stt_, no_merge, v['group'], v['aggstats'], v['aggnames']).copy(deep=True)\n","        no_merge = False\n","        #print(f'{k}, ',end='')\n","    #print('\\nDone')\n","\n","    monthly_stt_ = monthly_stt_.rename(columns={'shop_id_x_item_id_sales_sum':'y_sales'})  # rename for convenience\n","    monthly_stt_.y_sales = monthly_stt_.y_sales.clip(clipL, clipH)\n","    monthly_stt_ = monthly_stt_[int_cols + lag_stats_set_]   # ensure proper order and inclusion of desired feature columns\n","\n","    # always do minmax scaling after robust scaling; and do inverse scaling with minmax first, then robust\n","    if use_robust_scaler:\n","        robust_scalers = {} # squeeze outliers into central distribution\n","        for aggcol in lag_stats_set_:\n","            robust_scalers[aggcol] = RobustScaler(with_centering=False,quantile_range=robust_scaler_quantiles)\n","            monthly_stt_[aggcol] = robust_scalers[aggcol].fit_transform(monthly_stt_[aggcol].to_numpy().reshape(-1, 1))\n","    if use_minmax_scaler:\n","        minmax_scalers = {}  # apply min-max scaler to make best use of np.int16 and memory usage\n","        for aggcol in lag_stats_set_:\n","            minmax_scalers[aggcol] = MinMaxScaler(feature_range=minmax_scaler_range)\n","            if (feature_data_type == np.int16) or (FEATURES[\"_FEATURE_DATA_TYPE\"] == np.uint16):\n","                monthly_stt_[aggcol] = monthly_stt_[aggcol].fillna(0)\n","            monthly_stt_[aggcol] = minmax_scalers[aggcol].fit_transform(monthly_stt_[aggcol].to_numpy().reshape(-1, 1))\n","\n","    monthly_stt_.sort_values(['month','shop_id','item_id'],inplace=True)\n","\n","    if feature_data_type in [np.int16, np.uint16]:\n","        monthly_stt_ = monthly_stt_.round()\n","    monthly_stt_ = monthly_stt_.reset_index(drop=True).astype(feature_data_type)\n","    print(f'\\nmonthly_stt_ fully grouped and merged: shape = {monthly_stt_.shape}, memory usage = {monthly_stt_.memory_usage(deep=True).sum()/1e6:.0f} MBytes')\n","\n","    return monthly_stt_, robust_scalers, minmax_scalers\n","\n","print(f'Done: {strftime(\"%a %X %x\")}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nLmjmqw1oihf"},"source":["##**Add Cartesian Product rows to the training data:**\n","Idea is to help the model by informing it that we explicitly have no information about certain relevant shop_item pairs in certain months."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b5sECVIITIlK"},"source":["###*Use numpy to create Cartesian Product:*\n","Each month in train data will have additional rows such that the Cartesian Product of all shops and items ALREADY PRESENT IN THAT MONTH will be included.</br>\n","\n","When we merge lagged features below, we will only forward-shift the shop-item pairs that are present in the later month.</br>\n","*(Might revisit later, if memory requirements not too big, can forward-shift all shop-item pairs.)*"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aWEbp9hBVIqd"},"source":["#####*If not adding Cartesian Product, or if fillna(0), can round various features to integers to save memory:*"]},{"cell_type":"code","metadata":{"id":"YW5giGxfrv5-","colab_type":"code","cellView":"both","colab":{}},"source":["\n","del stt\n","\n","tic = perf_counter()\n","maxlags = max(FEATURES[\"LAGS_MONTHS\"])\n","if FEATURES[\"_USE_CARTPROD_FILL\"]:\n","    # Create cartesian product so model has info to look at for every relevant shop-item-month combination in the months desired\n","    # add enough months of cartesian product that after time-LAGS, we end up with CartProds in months CARTPROD_FILL_MONTH_BEGIN through 33 (don't need to fill month 34)\n","    matrix = []\n","    for i in range(max(FEATURES[\"_CARTPROD_FIRST_MONTH\"]-maxlags,0),34):\n","        if FEATURES[\"_CARTPROD_TEST_PAIRS\"]:\n","            sales = monthly_stt.query('(month == @i)|(month == 34)')  # include all test shop-items in with cartesian product from that month\n","        else:\n","            sales = monthly_stt.query('month == @i')  # include only shop-items present in that month\n","        matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype=np.int16))    \n","\n","    df = pd.DataFrame(np.vstack(matrix), columns=['month','shop_id','item_id'])\n","\n","    # # merge in the rows from training set with month < CART_FILL_MO_START ? (and > maxlags )??\n","    # now merge in the rows for the test set, month 34\n","    df = df.append(monthly_stt[monthly_stt.month ==34][['month','shop_id','item_id']], ignore_index=True)\n","    df = df.reset_index(drop=True)\n","    print(f'Column Data Types: \\n{df.dtypes}\\ndf memory usage: {df.memory_usage(deep=True).sum()/1e6:.0f} MBytes')\n","    print(f'Number of months: {df.month.nunique():,d}\\nNumber of shops: {df.shop_id.nunique():,d}\\nNumber of items: {df.item_id.nunique():,d}\\nDataFrame length: {len(df):,d}\\n')\n","\n","    # merge the shops_purged dataframe: (can't just merge with monthly_stt, because cartesian product df has more shop-item pairs)\n","    df = df.merge(shops_purged, how='left', on='shop_id')\n","    # Next, merge the items_purged dataframe to be sure we cover all items in the cartesian product df\n","    df = df.merge(items_purged, how='left', on='item_id')\n","\n","    df.sort_values(['month','shop_id','item_id'],inplace=True)\n","    df = df.reset_index(drop=True)\n","    # Now we merge in the real-time block-month data and see if memory requirements aren't overloading Colab:\n","    if FEATURES[\"_CARTPROD_FILLNA0\"] or (FEATURES[\"_FEATURE_DATA_TYPE\"] == np.int16) or (FEATURES[\"_FEATURE_DATA_TYPE\"] == np.uint16):\n","        df = df.merge(monthly_stt, how='left',on=INTEGER_COLS).fillna(0).reset_index(drop=True) # can store as integers, saving memory, but issue with price fill values... need to kill price stats / use revenue\n","    else:\n","        df = df.merge(monthly_stt, how='left',on=INTEGER_COLS).reset_index(drop=True)   # leave cartesian product row unknowns as N/A (float, not integer)\n","\n","    print('Cartesian-product df:')\n","else:\n","    df = monthly_stt.copy(deep=True)\n","    print('df with no Cartesian-products:')\n","\n","toc = perf_counter()\n","print(f\"df creation time: {timedelta(seconds = toc - tic)}\")\n","\n","del monthly_stt\n","\n","df = df.astype(FEATURES[\"_FEATURE_DATA_TYPE\"]) #np.int16 #.apply(pd.to_numeric, downcast= np.float32)\n","df=df.reset_index(drop=True)\n","print_col_info(df,6)\n","print(f'\\ndf.head:\\n{df.head(2)}')\n","print(f'\\ndf.tail:\\n{df.tail(2)}\\n')\n","# display(df.describe())\n","\n","print(f'\\nDone: {strftime(\"%a %X %x\")}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4kjKndgDM8Wi","colab_type":"text"},"source":["##**Compute and Merge the Time-Lag Features into Train + Test data sets**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nVew2KThVzWw"},"source":["###*Merge lag stats and check dataframe memory requirements:*"]},{"cell_type":"code","metadata":{"id":"i4SxT9QYbkWK","colab_type":"code","cellView":"both","colab":{}},"source":["\n","%%time\n","# shift the stuff and merge into df\n","#   drop any rows in t-lag that don't have matching shop-item pair at time t\n","print(f'Unlagged DataFrame length: {len(df):,d}\\n')\n","df['y_target'] = df.y_sales.copy(deep=True)  # keep an unlagged version of shop_item sales per month as our training / test target value\n","if FEATURES[\"_CARTPROD_FILLNA0\"] or (FEATURES[\"_FEATURE_DATA_TYPE\"] == np.int16) or (FEATURES[\"_FEATURE_DATA_TYPE\"] == np.uint16):\n","    df.y_target = df.y_target.fillna(0)     #.clip(INTEGER_MULTIPLIER*FEATURES[\"_CLIP_TRAIN_L\"], INTEGER_MULTIPLIER*FEATURES[\"_CLIP_TRAIN_H\"])\n","lag_merge_on_cols = ['month','shop_id','item_id']\n","cols_to_shift = lag_merge_on_cols + LAG_STATS_SET\n","lag_cols_df = df[cols_to_shift].copy(deep=True)  \n","df.drop(LAG_STATS_SET, axis=1, inplace=True)  # reduce size of df in memory so merging below doesn't crash Colab\n","for lag in FEATURES[\"LAGS_MONTHS\"]:\n","    cols_to_shift = lag_merge_on_cols + FEATURES[\"LAG_FEATURES\"][lag]\n","    lag_df = lag_cols_df[cols_to_shift].copy(deep=True)  \n","    lag_df.eval('month = month + @lag', inplace=True)\n","    suffix = '_L'+str(lag)\n","    lagged_col_names = lag_merge_on_cols + [x + suffix for x in FEATURES[\"LAG_FEATURES\"][lag]]\n","    lag_df.columns = lagged_col_names\n","    print(f'Column names for lag = {lag}: {lag_df.columns}')\n","    lag_df = lag_df.astype(FEATURES[\"_FEATURE_DATA_TYPE\"])\n","    if FEATURES[\"_CARTPROD_FILLNA0\"] or (FEATURES[\"_FEATURE_DATA_TYPE\"] == np.int16) or (FEATURES[\"_FEATURE_DATA_TYPE\"] == np.uint16):\n","        df = df.merge(lag_df, on = lag_merge_on_cols, how = 'left').fillna(0)  # 'left' serves to discard rows from earlier month if there is no match with later month\n","        print('fillna(0) done\\n')\n","    else:\n","        df = df.merge(lag_df, on = lag_merge_on_cols, how = 'left')\n","    df = df.reset_index(drop=True)\n","    df = df.astype(FEATURES[\"_FEATURE_DATA_TYPE\"])\n","print(df.head(2))\n","print(df.dtypes)\n","print(f'Lagged DataFrame length: {len(df):,d}\\n')\n","\n","first_train_month = min(FEATURES[\"_TRAIN_START_MONTH\"])\n","df = df.query('month >= @first_train_month')      # remove early months that don't participate in model training\n","df = df.reset_index(drop=True)\n","df = df.astype(FEATURES[\"_FEATURE_DATA_TYPE\"])  #.apply(pd.to_numeric, downcast= 'float')\n","\n","if FEATURES[\"_USE_CATEGORICAL\"]:\n","    for cf in CATEGORICAL_COLS:\n","        df[cf] = df[cf].astype('category')\n","\n","print('lagged features df (after downcast and removal of unused early months):')\n","print_col_info(df,8)\n","print(f'\\ndf.head():\\n{df.head()}')\n","\n","print(f'\\nDone: {strftime(\"%a %X %x\")}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jtZ0iBdUCD-Z","colab_type":"text"},"source":["#**Modeling**\n","*   Train/Val/Test split\n","*   Model Fit & Validate\n","*   Test/Submission Results\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9TJk9bzeCjqF","colab_type":"text"},"source":["##**Train/Test split**"]},{"cell_type":"code","metadata":{"id":"9YCzNW7-oHsF","colab_type":"code","cellView":"both","colab":{}},"source":["\n","def TTSplit(data=df, params=pd.DataFrame(), iternum=1):\n","    \"\"\"\n","    data is entire dataframe with train, validation, and test rows, and all columns including target prediction at \"y_target\"\n","    params is dictionary of this particular model train/val split and model fitting/prediction parameters\n","    \"\"\"\n","    DataSets = {}\n","    if params.at[iternum,'_model_type'] == 'LGBM':\n","        \n","        train_start = params.at[iternum,\"_train_start_month\"]\n","        train_end   = params.at[iternum,\"_train_final_month\"]\n","        val_key     = params.at[iternum,\"_validate_key\"]\n","        test_month  = params.at[iternum,\"_test_month\"]\n","\n","        train   = data.query('(month >= @train_start) & (month <= @train_end)')\n","        y_train = train['y_target'].astype(np.float32)\n","        y_train = y_train.reset_index(drop=True)\n","        X_train = train.drop(['y_target'], axis=1)\n","        X_train = X_train.reset_index(drop=True)\n","        feature_names = X_train.columns\n","\n","        if val_key < 34:\n","            val = data.query('(month > (@train_end)) & (month <= (@train_end + @val_key)) & (month < @test_month)')\n","        else:\n","            val = data.query('((month > (@train_end)) & (month < @test_month)) | (month == (@test_month-1))')\n","        y_val = val['y_target'].astype(np.float32)\n","        y_val = y_val.reset_index(drop=True)\n","        X_val = val.drop(['y_target'], axis=1)\n","        X_val = X_val.reset_index(drop=True)\n","\n","        X_test = data.query('month == @test_month').drop(['y_target'], axis=1)\n","        X_test = X_test.reset_index(drop=True)\n","\n","        print('X_train:')\n","        print_col_info(X_train,8)\n","        #print(f'\\n{X_train.head(2)}\\n\\n')\n","        print('X_val:')\n","        print_col_info(X_val,8)\n","        #print(f'\\n{X_val.head(2)}\\n\\n')\n","        print('X_test:')\n","        print_col_info(X_test,8)\n","        #print(f'\\n{X_test.head(2)}\\n\\n')\n","        #data_types = X_train.dtypes\n","\n","        del [[data, train, val]]\n","\n","        DataSets['X_train'] = X_train\n","        DataSets['y_train'] = y_train\n","        DataSets['X_val'] = X_val\n","        DataSets['y_val'] = y_val\n","        DataSets['X_test'] = X_test\n","\n","    return DataSets, feature_names\n","\n","print(f'Done: {strftime(\"%a %X %x\")}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HprRItOZV8o6","colab_type":"text"},"source":["##**LightGBM - Lightweight Gradient-Boosted Decision Tree**\n","##**SK_HGBR - SKLearn Histogram Gradient Boosting Regressor**"]},{"cell_type":"code","metadata":{"id":"CyAnC6q2qG6w","colab_type":"code","cellView":"both","colab":{}},"source":["\n","def unscale(scaler,target):\n","    return scaler.inverse_transform(target.reshape(-1, 1)).squeeze()\n","\n","def GBDT_model(data={}, param_df=pd.DataFrame(), iternum=1):\n","    \"\"\"\n","    data dictionary includes X_train, X_val, X_test, y_train, y_val (keys = string version of values)\n","    param_df is dataframe of setup constants and also holds results\n","    iternum is the nth model being trained/fit/predicted/ensembled\n","    \"\"\"\n","    if param_df.at[iternum,\"_model_type\"] == 'LGBM':\n","        print('Starting training...')\n","        param_df.at[iternum,\"start_time\"] = f'{strftime(\"%a %X %x\")}'\n","        tic = perf_counter()\n","\n","        model_gbdt = lgb.LGBMRegressor(\n","            objective=param_df.at[iternum,'objective'],\n","            boosting_type=param_df.at[iternum,'boosting_type'],\n","            learning_rate=param_df.at[iternum,'learning_rate'],\n","            n_estimators=param_df.at[iternum,'n_estimators'],\n","            #metric=param_df.at[iternum,'metric'],\n","            subsample_for_bin=param_df.at[iternum,'subsample_for_bin'],\n","            num_leaves=param_df.at[iternum,'num_leaves'],\n","            max_depth=param_df.at[iternum,'max_depth'],\n","            min_split_gain=param_df.at[iternum,'min_split_gain'],\n","            min_child_weight=param_df.at[iternum,'min_child_weight'],\n","            min_child_samples=param_df.at[iternum,'min_child_samples'],\n","            colsample_bytree=param_df.at[iternum,'colsample_bytree'],\n","            random_state=param_df.at[iternum,'random_state'],\n","            silent=param_df.at[iternum,'silent'],\n","            importance_type=param_df.at[iternum,'importance_type'],\n","            reg_alpha=param_df.at[iternum,'reg_alpha'],\n","            reg_lambda=param_df.at[iternum,'reg_lambda'],\n","            n_jobs=param_df.at[iternum,'n_jobs'],\n","            subsample=param_df.at[iternum,'subsample'],\n","            subsample_freq=param_df.at[iternum,'subsample_freq']\n","            )\n","\n","        model_gbdt.fit(\n","            data['X_train'],                                # Input feature matrix (array-like or sparse matrix of shape = [n_samples, n_features])\n","            data['y_train'],                                # The target values (class labels in classification, real numbers in regression) (array-like of shape = [n_samples])\n","            eval_set=[(data['X_val'], data['y_val'])],      # can have multiple tuples of validation data inside this list\n","            eval_names=None,                                # Names of eval_set (list of strings or None, optional (default=None))\n","            eval_metric=param_df.at[iternum,'eval_metric'],\n","            early_stopping_rounds=param_df.at[iternum,'early_stopping_rounds'],\n","            init_score=param_df.at[iternum,'init_score'],\n","            eval_init_score=param_df.at[iternum,'eval_init_score'],\n","            verbose=param_df.at[iternum,'verbose'],\n","            feature_name=param_df.at[iternum,'feature_name'],\n","            categorical_feature=param_df.at[iternum,'categorical_feature'],\n","            callbacks=param_df.at[iternum,'callbacks']\n","             )\n","        toc = perf_counter()\n","        \n","\n","        param_df.at[iternum,\"best_iteration_\"]          = model_gbdt.best_iteration_\n","        param_df.at[iternum,\"best_score_\"]              = model_gbdt.best_score_['valid_0']['rmse']\n","        param_df.at[iternum,\"feature_importances_\"]     = model_gbdt.feature_importances_\n","        param_df.at[iternum,\"model_fit_time\"]           = datetime.datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","        print(f'Model LGBM fit time: {param_df.at[iternum,\"model_fit_time\"]}')\n","\n","\n","    \n","    print(\"Starting predictions...\")\n","    tic = perf_counter()\n","    y_pred_train =  model_gbdt.predict( data['X_train'], num_iteration=model_gbdt.best_iteration_ )\n","    y_pred_val =    model_gbdt.predict( data['X_val'],   num_iteration=model_gbdt.best_iteration_ )\n","    y_pred_test =   model_gbdt.predict( data['X_test'],  num_iteration=model_gbdt.best_iteration_ )\n","    y_train =       data['y_train'].to_numpy()\n","    y_val =         data['y_val'].to_numpy()\n","    # always do minmax scaling after robust scaling; and do inverse scaling with minmax first, then robust\n","    if param_df.at[iternum,'use_minmax_scaler']:\n","        y_pred_train =  unscale(minmax_scalers['y_sales'],  y_pred_train)\n","        y_pred_val =    unscale(minmax_scalers['y_sales'],  y_pred_val)\n","        y_pred_test =   unscale(minmax_scalers['y_sales'],  y_pred_test)\n","        y_train =       unscale(minmax_scalers['y_sales'],  y_train)\n","        y_val =         unscale(minmax_scalers['y_sales'],  y_val)\n","    if param_df.at[iternum,'use_robust_scaler']:\n","        y_pred_train =  unscale(robust_scalers['y_sales'],  y_pred_train)\n","        y_pred_val =    unscale(robust_scalers['y_sales'],  y_pred_val)\n","        y_pred_test =   unscale(robust_scalers['y_sales'],  y_pred_test)\n","        y_train =       unscale(robust_scalers['y_sales'],  y_train)\n","        y_val =         unscale(robust_scalers['y_sales'],  y_val)\n","    y_pred_train =  y_pred_train.clip(param_df.at[iternum,'sales_predict_clip_L'],param_df.at[iternum,'sales_predict_clip_H'])\n","    y_pred_val =    y_pred_val.clip(param_df.at[iternum,'sales_predict_clip_L'],param_df.at[iternum,'sales_predict_clip_H'])\n","    y_pred_test =   y_pred_test.clip(param_df.at[iternum,'sales_predict_clip_L'],param_df.at[iternum,'sales_predict_clip_H'])\n","    toc = perf_counter()\n","\n","    param_df.at[iternum,\"model_predict_time\"] = datetime.datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","    param_df.at[iternum,'tr_R2']    = sklearn.metrics.r2_score(y_train, y_pred_train)   \n","    param_df.at[iternum,'val_R2']   = sklearn.metrics.r2_score(y_val, y_pred_val)\n","    param_df.at[iternum,'tr_rmse']  = np.sqrt(sklearn.metrics.mean_squared_error(y_train, y_pred_train)) \n","    param_df.at[iternum,'val_rmse'] = np.sqrt(sklearn.metrics.mean_squared_error(y_val, y_pred_val))\n","    \n","    a    = sklearn.metrics.r2_score(y_train, y_pred_train)   \n","    b   = sklearn.metrics.r2_score(y_val, y_pred_val)\n","    c  = np.sqrt(sklearn.metrics.mean_squared_error(y_train, y_pred_train)) \n","    d = np.sqrt(sklearn.metrics.mean_squared_error(y_val, y_pred_val))\n","    \n","    print(f'Model LGBM fit time: {param_df.at[iternum,\"model_fit_time\"]}')\n","    print(f'Transform and Predict train/val/test time: {param_df.at[iternum,\"model_predict_time\"]}')\n","    print(f'R^2 train  = {param_df.at[iternum,\"tr_R2\"]:.4f}    R^2 val  = {param_df.at[iternum,\"val_R2\"]:.4f}')\n","    print(f'RMSE train = {param_df.at[iternum,\"tr_rmse\"]:.4f}    RMSE val = {param_df.at[iternum,\"val_rmse\"]:.4f}\\n')\n","\n","    \n","    print(f'R^2 train  = {a:.4f}    R^2 val  = {b:.4f}')\n","    print(f'RMSE train = {c:.4f}    RMSE val = {d:.4f}\\n')\n","\n","    return y_pred_test, param_df, model_gbdt.get_params()\n","\n","\n","print(f'Done: {strftime(\"%a %X %x\")}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8aPOnrqM9y6r"},"source":["##**Parameters**"]},{"cell_type":"code","metadata":{"id":"_qXTF3EFe7eD","colab_type":"code","cellView":"both","colab":{}},"source":["\n","def display_params():\n","    df_mem = df.memory_usage(deep=True)\n","    print(f'{strftime(\"%a %X %x\")};  df_size = {df_mem.sum()/(10**6):0.1f} MB, df_shape = {df.shape}; N train models: {N_TRAIN_MODELS}; N features: {N_FEATURES}')\n","    \n","    print(\"------\\n------------\")\n","    pprint.pprint(dict(FEATURES),width=200,compact=True)\n","    print(\"\\n\")\n","    pprint.pprint(ITERS,width=200,compact=True)\n","    print(\"\\n\")\n","\n","\n","    print(f'COLS_ORDER_STT = {FEATURES[\"COLS_ORDER_STT\"]}')\n","    print(f'LAGGED_FEATURE_GROUPS = {FEATURES[\"LAGGED_FEATURE_GROUPS\"]}')\n","    print(f'AGG_STATS = {FEATURES[\"AGG_STATS\"]};  _CLIP_TRAIN_L = {FEATURES[\"_CLIP_TRAIN_L\"]}, _CLIP_PREDICT_L = {FEATURES[\"_CLIP_PREDICT_L\"]}')\n","    print(f'EDA_DELETE_SHOPS = {FEATURES[\"_EDA_DELETE_SHOPS\"]}; EDA_DELETE_ITEM_CATS = {FEATURES[\"_EDA_DELETE_ITEM_CATS\"]}; EDA_SCALE_MONTH = {FEATURES[\"_EDA_SCALE_MONTH\"]}')\n","    print(f'LAGS = {FEATURES[\"LAGS_MONTHS\"]} (months)')\n","    for lag in FEATURES[\"LAGS_MONTHS\"]:\n","        print(f'COLUMNS_TO_LAG[{lag}] = {FEATURES[\"LAG_FEATURES\"][lag]}')\n","    print(f'USE_CARTPROD_FILL = {FEATURES[\"_USE_CARTPROD_FILL\"]}, CARTPROD_WITH_TEST_PAIRS = {FEATURES[\"_CARTPROD_TEST_PAIRS\"]}, CARTPROD_FILLNA_WITH_0 = {FEATURES[\"_CARTPROD_FILLNA0\"]}')\n","    print(f'USE_ROBUST_SCALER = {FEATURES[\"_USE_ROBUST_SCALER\"]}, ROBUST_SCALER_QUANTILES = {FEATURES[\"_ROBUST_SCALER_QUANTILES\"]}, USE_MINMAX_SCALER = {FEATURES[\"_USE_MINMAX_SCALER\"]}, ',end=\"\")\n","    print(f'MINMAX_SCALER_RANGE = {FEATURES[\"_MINMAX_SCALER_RANGE\"]}, FEATURE_DATA_TYPE = {FEATURES[\"_FEATURE_DATA_TYPE\"]}')\n","    print(f'CARTPROD_FILL_MONTH_BEGIN = {FEATURES[\"_CARTPROD_FIRST_MONTH\"]}, ',end='') \n","    print(f'TRAIN_START_MONTH = {FEATURES[\"_TRAIN_START_MONTH\"]},  TRAIN_FINAL_MONTH = {FEATURES[\"_TRAIN_FINAL_MONTH\"]}, VALIDATE_KEY = {FEATURES[\"_VALIDATE_KEY\"]}')\n","    print(f'LEARNING_RATE = {ITERS[\"learning_rate\"].unique()}, MAX_ITERATIONS = {ITERS[\"n_estimators\"].unique()}, EARLY_STOPPING = {ITERS[\"early_stopping_rounds\"].unique()}, ',end='')\n","    print(f'REGULARIZATION = {ITERS[\"colsample_bytree\"].unique()}, SEED_VALUES = {ITERS[\"random_state\"].unique()}')\n","    print(f'SUBSAMPLE_FOR_BIN = {ITERS[\"subsample_for_bin\"].unique()}')\n","    return\n","\n","print(f'{FEATURES[\"_MODEL_NAME\"]}  Model Type: {FEATURES[\"_MODEL_TYPE\"]}')\n","display_params()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FPzNWMRAyx6M"},"source":["##**K-Fold Training Splits; Ensemble Average; Save Intermediate Results**"]},{"cell_type":"code","metadata":{"id":"D2tNdYiiNa7x","colab_type":"code","cellView":"both","colab":{}},"source":["\n","%cd \"{GDRIVE_REPO_PATH}\"\n","\n","ensemble_y_pred_test = []\n","ensemble_df_columns = ['tr_rmse','val_rmse','trR2','valR2','lr','reg','max_iter','estop','bin_sample','start','end','val_key','seed','best_iter','best_val_rmse','model_t','predict_t','total_t']\n","ensemble_df_rows = []\n","\n","\n","for itercount in range(len(ITERS)):\n","\n","    \n","    fit_params_dict = ITERS[LGBM_FIT_PARAMS_COLS].iloc[itercount].to_dict(OrderedDict)\n","    s = pd.Series([1, 2, 3, 4]) = ITERS[model_params_cols].iloc[itercount]\n","model_params_dict i= s.to_dict()   or s.to_dict(OrderedDict)\n","{0: 1, 1: 2, 2: 3, 3: 4}\n","\n","stt = clean_stt(stt,i[\"_eda_delete_shops\"],i[\"_eda_delete_item_cats\"],i[\"_eda_scale_month\"],date_adj_purged,INTEGER_COLS,FEATURES[\"COLS_ORDER_STT\"])\n","print('stt DataFrame:'); print_col_info(stt,5); display(stt.head(2))\n","print(f'Done: {strftime(\"%a %X %x\")}')\n","\n","monthly_stt = create_monthly_stt(stt,FEATURES[\"LAGGED_FEATURE_GROUPS\"],FEATURES[\"COLS_ORDER_STT\"],FEATURES[\"AGG_STATS\"],i[\"_clip_train_L\"],i[\"_clip_train_H\"],INTEGER_COLS,LAG_STATS_SET,\n","                                 i[\"_use_robust_scaler\"],i[\"_robust_scaler_quantiles\"],i[\"_use_minmax_scaler\"],i[\"_minmax_scaler_range\"],i[\"_feature_data_type\"])\n","print('\\nmonthly_stt (after reset_index) and downcast:')\n","print_col_info(monthly_stt,6)\n","print('\\n')\n","print(monthly_stt.head())\n","print(monthly_stt.describe())\n","print(f'Done: {strftime(\"%a %X %x\")}')\n","\n","\n","    if not ITERS.at[itercount,\"_model_name_base\"]:\n","        ITERS.at[itercount,\"_model_name_base\"] = input(\"Enter the Base Model Name Substring for Output File Naming (like: 'v4mg_01' )\")\n","    filename_parameters = ITERS.at[itercount,\"_model_type\"] + ITERS.at[itercount,\"_model_name_base\"] + \"_params.csv\"\n","    filename_submission = ITERS.at[itercount,\"_model_type\"] + ITERS.at[itercount,\"_model_name_base\"] + '_submission.csv'\n","\n","    print(f'\\n\\nBelow: Model {itercount+1} of {len(ITERS)}: lr= {ITERS.at[itercount,\"learning_rate\"]}; Reg= {ITERS.at[itercount,\"colsample_bytree\"]}, ',end='')\n","    print(f'train_start = {ITERS.at[itercount,\"_train_start_month\"]}; train_end = {ITERS.at[itercount,\"_train_final_month\"]}; seed = {ITERS.at[itercount,\"random_state\"]}\\n')\n","    \n","    time0 = time.time()\n","    # CHANGE --> only redo this inside the loop if the months change\n","    DataSets, ITERS.at[itercount,\"feature_name_\"] = TTSplit(data=df, params=ITERS, iternum=itercount)\n","\n","    y_pred_test, results_dict, parameters_of_model = GBDT_model(DataSets, model_params_dict, fit_params_dict) #ITERS, itercount)\n","    time1 = time.time()\n","    \n","\n","    ITERS.at[itercount,\"time_total_wall_clock\"] = datetime.datetime.utcfromtimestamp(time1 - time0).strftime('%H:%M:%S')\n","    print(f'Total Iteration Execution Time = {ITERS.at[itercount,\"time_total_wall_clock\"]}')\n","\n","    # intermediate save after each model fit set of parameters, in case of crash or disconnect from Colab\n","    # Simple ensemble averaging\n","    ensemble_y_pred_test.append(y_pred_test)\n","    y_test_pred_avg = np.mean(ensemble_y_pred_test, axis=0)\n","    # Merge the test predictions with IDs from the original test dataset, and keep only columns \"ID\" and \"item_cnt_month\"\n","    y_submission = pd.DataFrame.from_dict({'item_cnt_month':y_test_pred_avg,'shop_id':DataSets['X_test'].shop_id,'item_id':DataSets['X_test'].item_id})\n","    y_submission = test.merge(y_submission, on=['shop_id','item_id'], how= 'left').reset_index(drop=True).drop(['shop_id','item_id'],axis=1)\n","    y_submission.to_csv(\"./models_and_predictions/\" + filename_submission, index=False)\n","\n","    ITERS.to_csv(\"./models_and_predictions/\" + filename_parameters, index=False)\n","\n","    ensemble_df_rows.append(ITERS[['tr_rmse','val_rmse','tr_R2','val_R2','learning_rate','colsample_bytree','n_estimators','early_stopping_rounds','subsample_for_bin','_train_start_month','_train_final_month',\n","                                          '_validate_key','random_state','best_iteration_','best_score_','time_model_fit','time_model_predict','time_total_wall_clock']].iloc[itercount].to_list())\n","    ensemble_scores = pd.DataFrame(ensemble_df_rows, columns = ensemble_df_columns)\n","\n","    print(f'\\nModel {itercount+1} of {len(ITERS)}: lr= {ITERS.at[itercount,\"learning_rate\"]}; Reg= {ITERS.at[itercount,\"colsample_bytree\"]}, ',end='')\n","    print(f'train_start = {ITERS.at[itercount,\"_train_start_month\"]}; train_end = {ITERS.at[itercount,\"_train_final_month\"]}; seed = {ITERS.at[itercount,\"random_state\"]}\\n')\n","    display(ensemble_scores)\n","    \n","    itercount += 1\n","\n","print(f'\\nDone: {strftime(\"%a %X %x\")}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lKGrJUG7f50F"},"source":["##**Feature Importances**"]},{"cell_type":"code","metadata":{"id":"LbszXTq0vhZ-","colab_type":"code","cellView":"both","colab":{}},"source":["# Plot feature importance - Results Visualization\n","itercount=0\n","if ITERS.at[itercount,'_model_type'] == 'LGBM':\n","    print_threshold = 25\n","    feature_importances_ = ITERS.at[itercount,'feature_importances_']\n","    feature_name_        = ITERS.at[itercount,'feature_name_']\n","    fi = pd.DataFrame(zip(feature_name_,feature_importances_),columns=['feature','value'])\n","    fi = fi.sort_values('value',ascending=False,ignore_index=True)\n","    fi['norm_value'] = round(100*fi.value / fi.value.max(),2)\n","    fi['lag'] = fi.feature.apply(lambda x: (x.split('L')[-1]) if len(x.split('L'))> 1 else 0)\n","    fi['feature_base'] = fi.feature.apply(lambda x: x.split('_L')[0])\n","    print(fi.iloc[list(range(0,8))+list(range(-7,0)),:]) #[[1,3,5,7,-7,-5]][:])\n","    # model_name_fi = ITERS.at[itercount,'_model_type']+ITERS.at[itercount,'_model_name_base'] + \"_feature_importance.csv\"\n","    # fi.to_csv(\"./models_and_predictions/\" + model_name_fi, index=False)\n","    # printout to assist with removing low-importance features for following runs\n","    if fi.norm_value.min() < print_threshold:\n","        fi_low = fi[fi.norm_value < print_threshold]\n","        fi_low = fi_low.sort_values(['lag','norm_value'])\n","        fi_low.norm_value = fi_low.norm_value.apply(lambda x: f'{round(x):d}')\n","        fi_low['lag_feature_importance'] = fi_low.apply(lambda x: f\"{f'L{x.lag} fi{x.norm_value}':{len(x.feature_base)}s}\",axis=1)\n","        print(fi_low.lag_feature_importance.to_list())\n","        print(fi_low.feature_base.to_list())\n","    # make importances relative to max importance\n","    feature_importances_ = 100.0 * (feature_importances_ / feature_importances_.max())\n","    sorted_idx = np.arange(feature_importances_.shape[0])\n","    pos = np.arange(sorted_idx.shape[0]) + .5\n","    plt.figure(figsize=(24,12)) \n","    plt.bar(pos, feature_importances_[sorted_idx], align='center')\n","    plt.xticks(pos, feature_name_[sorted_idx])\n","    plt.ylabel('Relative Importance')\n","    plt.title('Variable Importance')\n","    plt.tick_params(axis='x', which='major', labelsize = 13, labelrotation=90)\n","    plt.grid(True,which='major',axis='y')\n","    plt.tick_params(axis='y', which='major', grid_color='black',grid_alpha=0.7)\n","    # plt.savefig('LGBM_feature_importance_v1.4_mg.png')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"a4U1U6nZy8wd"},"source":["##**Document Results**"]},{"cell_type":"code","metadata":{"id":"RhUA6AyHvjf3","colab_type":"code","cellView":"both","colab":{}},"source":["# Printout for copy-paste version control\n","\n","print('\\n------------------------------------------\\n------------------------------------------')\n","print(f'{FEATURES[\"_MODEL_NAME\"]}  Model Type: {FEATURES[\"_MODEL_TYPE\"]}\\nCoursera: \\n------------------------------------------')\n","display_params()\n","print('------')\n","print(ensemble_scores)\n","print('------')\n","print(ensemble_scores.describe(percentiles=[], include=np.number))\n","print(f'------\\nHighest and Lowest Feature Importance for Final Model:\\n{fi.iloc[list(range(0,8))+list(range(-7,0)),:]}\\n------')\n","print(y_submission.head(8))\n","print('------------------------------------------\\n\\n')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GF6qWsBIgBUX"},"source":["##**Stop Execution / Ensemble**"]},{"cell_type":"code","metadata":{"id":"Hma9PfJGRCHk","colab_type":"code","cellView":"both","colab":{}},"source":["# Dummy cell to stop the execution so we don't run any of the random code below (if we select \"Run All\", e.g.)\n","stop_running_code_at_this_cell = yes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"c98tKblpRWya"},"source":["##**Record Results**"]},{"cell_type":"code","metadata":{"id":"mIERXSQWz95V","colab_type":"code","cellView":"both","colab":{}},"source":["# Results\n","'''\n","\n","Best Coursera score so far: 8/10 public and private LB scores are: 0.985186 and 0.979359 on 5/12 with Andreas' numbers\n","Best with this model: v7_ens21 8/10 public and private LB scores are: 0.974590 and 0.971219\n","\n","LGBMv10_15ens 8/10 public and private LB scores are: 0.984054 and 0.979126\n","* LGBMv10_13ens 8/10 public and private LB scores are: 0.976077 and 0.973442\n","* LGBMv10_11ens 8/10 public and private LB scores are: 0.977330 and 0.974200  same as v10_10ens, but removed all features with importance below 20%\n","*** LGBMv10_10ens 8/10 public and private LB scores are: 0.975422 and 0.971682\n","LGBMv10_09ens 8/10 public and private LB scores are: 0.984677 and 0.984238\n","LGBMv10_08ens 8/10 public and private LB scores are: 0.984238 and 0.982864\n","LGBMv10_07ens 8/10 public and private LB scores are: 0.985275 and 0.985093\n","LGBMv10_06ens 8/10 public and private LB scores are: 0.984912 and 0.983360\n","LGBMv10_v9_18noscaler 8/10 8/10 public and private LB scores are: 0.984643 and 0.985256\n","LGBMv10_v9_18 8/10 public and private LB scores are: 0.982740 and 0.983633  robust scaler used\n","LGBMv9_18ens 8/10 public and private LB scores are: 0.984137 and 0.984686\n","LGBMv9_09clip 8/10 public and private LB scores are: 0.984877 and 0.985790\n","LGBMv9_08clip 8/10 public and private LB scores are: 0.985158 and 0.986282\n","LGBMv9_04ens (less memory) 8/10 public and private LB scores are: 0.981707 and 0.985473\n","* LGBMv9_03ens 8/10 public and private LB scores are: 0.975438 and 0.973606\n","LGBMv8_v7_21B_ens 8/10 public and private LB scores are: 0.976147 and 0.972920\n","*** LGBMv6v7_bag06 8/10 public and private LB scores are: 0.974873 and 0.971385\n","* LGBMv6v7_bag05 8/10 public and private LB scores are: 0.975973 and 0.972537\n","**** v7_ens21 8/10 public and private LB scores are: 0.974590 and 0.971219\n","** v7_ens20 8/10 public and private LB scores are: 0.975499 and 0.971916\n","* v6_ens32 8/10 public and private LB scores are: 0.975826 and 0.972352\n","v6_10 8/10 public and private LB scores are: 0.984495 and 0.978631\n","v6_ens01 (avg v6 #17 through #31): 8/10 public and private LB scores are: 0.984457 and 0.978061\n","v6_ens33 8/10 public and private LB scores are: 0.980232 and 0.975554\n","v7_03 8/10 public and private LB scores are: 0.980832 and 0.975157\n","v7_ens07 8/10 public and private LB scores are: 0.980749 and 0.978082\n","\n","------------------------------------------\n","------------------------------------------\n","LGBMv10_15ens  Model Type: LGBM\n","------------------------------------------\n","Coursera: 8/10 public and private LB scores are: 0.984054 and 0.979126\n","Wed 19:10:11 07/15/20;  Size of df = 734.8 MB, Shape = (6226880, 59);  Size of X_train_np = 595.7 MB;  N Training Runs for this Model: 2\n","EDA_DELETE_SHOPS = [9, 20]; EDA_DELETE_ITEM_CATS = [8, 10, 32, 59, 80, 81, 82]; EDA_SCALE_MONTH = week_retail_weight; Lags(months) = [1, 2, 3, 4, 5, 6, 7, 8]\n","KEEP_STT_COLUMN_ORDER = ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']\n","STATS_FEATURES = [['shop_id', 'item_id'], ['shop_id', 'item_category_id'], ['shop_id', 'item_cluster'], 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster']\n","AGG_STATS = OrderedDict([('sales', ['sum', 'median', 'count']), ('revenue', ['sum'])]); _CLIP_TRAIN_L = 0; _CLIP_TRAIN_H = 20; _CLIP_PREDICT_L = 0; _CLIP_PREDICT_H = 20\n","COLUMNS_TO_LAG[1] = ['y_sales', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', 'shop_id_sales_sum', 'shop_id_sales_count', 'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', 'shop_group_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', 'item_group_sales_sum', 'item_group_revenue_sum', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","COLUMNS_TO_LAG[2] = ['y_sales', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_revenue_sum', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","COLUMNS_TO_LAG[3] = ['y_sales', 'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', 'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_cluster_sales_count']\n","COLUMNS_TO_LAG[4] = ['item_id_sales_sum']\n","COLUMNS_TO_LAG[5] = ['y_sales']\n","COLUMNS_TO_LAG[6] = ['y_sales']\n","COLUMNS_TO_LAG[7] = ['item_id_sales_sum']\n","COLUMNS_TO_LAG[8] = ['y_sales']\n","CARTPROD_FILL_MONTH_BEGIN = 13; TRAIN_MONTH_START = [13]; TRAIN_FINAL_MONTH = [29]; N_VAL_MONTHS = [False]; USE_CARTESIAN_FILL = True; CART_PROD_INCLUDES_TEST = False; CARTPROD_FILLNA_WITH_0 = True\n","USE_ROBUST_SCALER = True; ROBUST_SCALER_QUANTILES = (20, 80); USE_MINMAX_SCALER = True; MINMAX_RANGE = (0, 32000); DATA_TYPE = <class 'numpy.int16'>\n","LEARNING_RATE = [0.02, 0.005]; MAX_ITERATIONS = [8000]; EARLY_STOPPING = [200]; REGULARIZATION = [0.4]; SEED_VALUES = [42]\n","------\n","     lr   reg  max_iter  estop  start  end  n_val_mo  seed  trR2  valR2  tr_rmse  val_rmse  best_iter  best_val_rmse model_time predict_time total_time\n","0 0.020 0.400      8000    200     13   29     False    42 0.572  0.427    0.743     0.783          0      1,253.473   00:35:36     00:13:33   00:49:09\n","1 0.005 0.400      8000    200     13   29     False    42 0.549  0.427    0.763     0.784          0      1,254.303   01:24:53     00:35:09   02:00:02\n","------\n","         lr   reg  max_iter  estop  start  end  seed  trR2  valR2  tr_rmse  val_rmse  best_iter  best_val_rmse\n","count     2     2         2      2      2    2     2     2      2        2         2          2              2\n","mean  0.013 0.400      8000    200     13   29    42 0.561  0.427    0.753     0.784          0      1,253.888\n","std   0.011     0         0      0      0    0     0 0.017  0.001    0.014     0.000          0          0.587\n","min   0.005 0.400      8000    200     13   29    42 0.549  0.427    0.743     0.783          0      1,253.473\n","50%   0.013 0.400      8000    200     13   29    42 0.561  0.427    0.753     0.784          0      1,253.888\n","max   0.020 0.400      8000    200     13   29    42 0.572  0.427    0.763     0.784          0      1,254.303\n","------\n","Highest and Lowest Feature Importance for Final Model:\n","                             feature  value  norm_value lag                   feature_base\n","0             item_id_sales_count_L1   9378         100   1            item_id_sales_count\n","1               item_id_sales_sum_L1   9199      98.090   1              item_id_sales_sum\n","2                       item_cluster   8256      88.040   0                   item_cluster\n","3                            item_id   7862      83.830   0                        item_id\n","4                   item_category_id   7540      80.400   0               item_category_id\n","5             item_id_revenue_sum_L1   7261      77.430   1            item_id_revenue_sum\n","6                            shop_id   6958      74.190   0                        shop_id\n","7                         y_sales_L1   6754      72.020   1                        y_sales\n","51       item_cluster_sales_count_L2   2536      27.040   2       item_cluster_sales_count\n","52              item_id_sales_sum_L3   2535      27.030   3              item_id_sales_sum\n","53  shop_id_x_item_id_sales_count_L2   2293      24.450   2  shop_id_x_item_id_sales_count\n","54   item_category_id_sales_count_L3   2170      23.140   3   item_category_id_sales_count\n","55       item_cluster_sales_count_L3   2067      22.040   3       item_cluster_sales_count\n","56   item_category_id_sales_count_L2   1987      21.190   2   item_category_id_sales_count\n","57     item_category_id_sales_sum_L3   1879      20.040   3     item_category_id_sales_sum\n","------\n","   ID  item_cnt_month\n","0   0           0.781\n","1   1           0.079\n","2   2           1.174\n","3   3           0.247\n","4   4           0.875\n","5   5           0.697\n","6   6           0.514\n","7   7           0.095\n","------------------------------------------\n","------------------------------------------\n","------------------------------------------\n","\n","'''\n","nocode=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zPM4RLKcn3RE"},"source":["##**Averaging Several Stored Prediction/Submission File from Disk**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dzzImBPS3CsO","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"error","timestamp":1595671233442,"user_tz":240,"elapsed":295,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"036f8998-a1b3-4dde-9471-ffc906bf3375"},"source":["# average several submission files to get ensemble average\n","%cd \"{GDRIVE_REPO_PATH}\"\n","# source_dir = Path('models_and_predictions/bagging_LGBM')\n","# prediction_files = source_dir.iterdir()\n","source_dir = 'models_and_predictions/bagging_LGBM'\n","prediction_files = os.listdir(source_dir)\n","print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","\n","# filename to save ensemble average predictions for submission\n","ensemble_name = 'LGBMv6v7_bag06'\n","\n","print(f'filename {ensemble_name}')\n","# Loop to load the data files into appropriately-named pandas DataFrames, and save in np array for easy calc of ensemble average\n","preds = []\n","for f_name in prediction_files:\n","    filename = f_name.rsplit(\"/\")[-1]\n","    data_frame_name = filename.split(\".\")[0][:-11]\n","    path_name = os.path.join('models_and_predictions/bagging_LGBM/'+ filename)\n","    exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","    print(f'Data Frame: {data_frame_name}; n_rows = {len(eval(data_frame_name))}, n_cols = ')\n","    preds.append(eval(data_frame_name).item_cnt_month.to_numpy())\n","\n","# Simple ensemble averaging\n","pred_ens_avg = np.mean(preds, axis=0)\n","ensemble_submission = LGBMv6mg_17_.copy(deep=True)\n","ensemble_submission.item_cnt_month = pred_ens_avg\n","\n","ensemble_submission.to_csv(\"./models_and_predictions/\" + ensemble_name + '_submission.csv', index=False)\n","\n","display(ensemble_submission.head(8))\n","print(f'filename {ensemble_name} saved: {strftime(\"%a %X %x\")}')\n","print('Coursera:  ')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: '{GDRIVE_REPO_PATH}'\n","/content\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-b4beb690e6d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# prediction_files = source_dir.iterdir()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msource_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'models_and_predictions/bagging_LGBM'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprediction_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Files from Google Drive repo into Colab...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LF91nOlf3Km7"},"source":["#**Useful Code Snippets**"]},{"cell_type":"code","metadata":{"id":"-QoWKEW63ADk","colab_type":"code","cellView":"both","colab":{},"executionInfo":{"status":"ok","timestamp":1595937160483,"user_tz":240,"elapsed":388,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["\n","# FEATURES[\"_CARTPROD_FILLNA0\"]         = ITERS[\"_cartprod_fillna0\"]\n","# FEATURES[\"_CARTPROD_FIRST_MONTH\"]     = ITERS[\"_cartprod_first_month\"]\n","# FEATURES[\"_CARTPROD_TEST_PAIRS\"]      = ITERS[\"_cartprod_test_pairs\"]\n","# FEATURES[\"_CLIP_TRAIN_H\"]             = ITERS[\"_clip_train_H\"]            \n","# FEATURES[\"_CLIP_TRAIN_L\"]             = ITERS[\"_clip_train_L\"]                   \n","# FEATURES[\"_CLIP_PREDICT_H\"]           = ITERS[\"_clip_predict_H\"]          \n","# FEATURES[\"_CLIP_PREDICT_L\"]           = ITERS[\"_clip_predict_L\"]    \n","# FEATURES[\"_EDA_DELETE_SHOPS\"]         = ITERS[\"_eda_delete_shops\"]\n","# FEATURES[\"_EDA_DELETE_ITEM_CATS\"]     = ITERS[\"_eda_delete_item_cats\"]\n","# FEATURES[\"_EDA_SCALE_MONTH\"]          = ITERS[\"_eda_scale_month\"]\n","# FEATURES[\"_FEATURE_DATA_TYPE\"]        = ITERS[\"_feature_data_type\"]\n","# FEATURES[\"_MINMAX_SCALER_RANGE\"]      = ITERS[\"_minmax_scaler_range\"]\n","# FEATURES[\"_MODEL_NAME_BASE\"]          = ITERS[\"_model_name_base\"]\n","# FEATURES[\"_MODEL_TYPE\"]               = ITERS[\"_model_type\"]\n","# FEATURES[\"_ROBUST_SCALER_QUANTILES\"]  = ITERS[\"_robust_scaler_quantiles\"]     \n","# FEATURES[\"_TEST_MONTH\"]               = ITERS[\"_test_month\"]\n","# FEATURES[\"_TRAIN_START_MONTH\"]        = ITERS[\"_train_start_month\"]\n","# FEATURES[\"_TRAIN_FINAL_MONTH\"]        = ITERS[\"_train_final_month\"]\n","# FEATURES[\"_USE_CARTPROD_FILL\"]        = ITERS[\"_use_cartprod_fill\"]\n","# FEATURES[\"_USE_CATEGORICAL\"]          = ITERS[\"_use_categorical\"]         \n","# FEATURES[\"_USE_ROBUST_SCALER\"]        = ITERS[\"_use_robust_scaler\"]      \n","# FEATURES[\"_USE_MINMAX_SCALER\"]        = ITERS[\"_use_minmax_scaler\"]\n","# FEATURES[\"_VALIDATE_KEY\"]             = ITERS[\"_validate_key\"]\n","\n","# pprint.pprint(ITERS,width=200,compact=True)\n","# pprint.pprint(ALL_RUN_PARAMETERS,width=200,compact=True)\n","\n","# print(ITERS[['random_state','n_estimators','boosting_type','metric']].iloc[0].to_list())   # selecting only certain variables from a certain iteration line (0 in this case)\n","\n","\n","\n","# class Delete_Me:\n","#     def __init__(self, name, df):\n","#         self.name = name\n","#         self.df = df\n","#     def clear_memory(self,dataframe):\n","#         print(f'Removing {self.name}')\n","#         del dataframe\n","#         gc.collect()\n","#         #dataframe = pd.DataFrame(np.zeros((1,1),dtype=np.int8)) # not sure whether this line really helps\n","#         return True\n","\n","# def rm_df(rm_dict={'df':pd.DataFrame()}):\n","#     \"\"\"\n","#     try to save memory by deleting unneeded dataframes\n","#     input is a dictionary of dataframe string names as keys, and dataframes as values\n","#     \"\"\"\n","#     print(f'\\nPrior to df delete, Google Colab runtime is using {virtual_memory().used / 1e9:.1f} GB of {virtual_memory().total / 1e9:.1f} GB available RAM\\n')\n","#     for k,v in rm_dict.items():\n","#         rm_df_class = Delete_Me(k,v)\n","#         rm_df_class.clear_memory(v)\n","#         v = pd.DataFrame(np.zeros((1,1),dtype=np.int8))\n","#         # try: del v\n","#         # except: print(f'DataFrame {k} delete error.')\n","#     #gc.disable()\n","#     #gc.collect()\n","#     print(f'\\nAfter gc.collect(), Google Colab runtime is using {virtual_memory().used / 1e9:.1f} GB of {virtual_memory().total / 1e9:.1f} GB available RAM\\n')\n","#     return \n","\n","\n","nocode=True"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qG665uxzQ5J5"},"source":["###**Old code: LightGBM - Lightweight Gradient-Boosted Decision Tree**\n","###**Old code: SK_HGBR - SKLearn Histogram Gradient Boosting Regressor**"]},{"cell_type":"code","metadata":{"id":"_qUFDLNRQ_TC","colab_type":"code","cellView":"both","colab":{},"executionInfo":{"status":"ok","timestamp":1595936950394,"user_tz":240,"elapsed":380,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# model_gbdt = lgb.LGBMRegressor(\n","#     objective='regression',\n","#     boosting_type='gbdt',           # gbdt= Gradient Boosting Decision Tree; dart= Dropouts meet Multiple Additive Regression Trees; goss= Gradient-based One-Side Sampling; rf= Random Forest\n","#     learning_rate=params[\"lr\"],     # You can use callbacks parameter of fit method to shrink/adapt learning rate in training using reset_parameter callback\n","#     n_estimators=params[\"maxit\"],   # Number of boosted trees to fit = max_iterations\n","#     metric='rmse',\n","#     subsample_for_bin=200000,       # Number of samples for constructing bins\n","#     num_leaves=31,                  # Maximum tree leaves for base learners\n","#     max_depth=-1,                   # Maximum tree depth for base learners, <=0 means no limit\n","#     min_split_gain=0.0,             # Minimum loss reduction required to make a further partition on a leaf node of the tree\n","#     min_child_weight=0.001,         # Minimum sum of instance weight (hessian) needed in a child (leaf)\n","#     min_child_samples=20,           # Minimum number of data needed in a child (leaf)\n","#     colsample_bytree=params[\"reg\"], # dropout fraction of columns during fitting (max=1 = no dropout)\n","#     random_state=params[\"seed\"],    # seed value\n","#     silent=False,                   # whether to print info during fitting\n","#     importance_type='split',        # feature importance type: 'split'= N times feature is used in model; 'gain'= total gains of splits which use the feature\n","#     reg_alpha=0.0,                  # L1 regularization\n","#     reg_lambda=0.0,                 # L2 regularization\n","#     n_jobs=- 1,                     # N parallel threads to use on computer\n","#     subsample=1.0,                  # row fraction used for training: keep at 1 for time series data\n","#     subsample_freq=0                # keep at 0 for time series\n","#     )\n","\n","\n","# model_gbdt.fit( \n","#     data['X_train'],                        # Input feature matrix (array-like or sparse matrix of shape = [n_samples, n_features])\n","#     data['y_train'],                        # The target values (class labels in classification, real numbers in regression) (array-like of shape = [n_samples])\n","#     eval_set=[(data['X_val'], data['y_val'])],              # can have multiple tuples of validation data inside this list\n","#     eval_names=None,                        # Names of eval_set (list of strings or None, optional (default=None))\n","#     eval_metric='rmse',                     # Default: 'l2' (= mean squared error, 'mse') for LGBMRegressor; options include 'l2_root'='root_mean_squared_error'='rmse' and 'l1'='mean_absolute_error'='mae' + more\n","#     early_stopping_rounds=params[\"estop\"],  # Activates early stopping. The model will train until the validation score stops improving. Validation score needs to improve at least every early_stopping_rounds \n","#                                             #     to continue training. Requires at least one validation data and one metric. If theres more than one, will check all of them. But the training data is ignored anyway. \n","#                                             #     To check only the first metric, set the first_metric_only parameter to True in additional parameters **kwargs of the model constructor.\n","#     init_score=None,                        # Init score of train data\n","#     eval_init_score=None,                   # Init score of eval data (list of arrays or None, optional (default=None))\n","#     verbose=CONSTANTS[\"VERBOSITY\"] ,        # If True, metric on the eval set is printed at each boosting stage. If n=int, the metric on the eval set is printed at every nth boosting stage. Best and final also print.\n","#     feature_name='auto',                    # Feature names. If 'auto' and data is pandas DataFrame, data columns names are used. (list of strings or 'auto', optional (default='auto'))\n","#     categorical_feature='auto',             # Categorical features (list of strings or int, or 'auto', optional (default='auto')) If list of int, interpreted as indices. \n","#                                             # If list of strings, interpreted as feature names (need to specify feature_name as well). \n","#                                             # If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used. All values in categorical features should be less than int32 max value (2147483647). \n","#                                             # Large values could be memory-consuming. Consider using consecutive integers starting from zero. All negative values in categorical features are treated as missing values.\n","#     callbacks=None                          # List of callback functions that are applied at each iteration (list of callback functions or None, optional (default=None)) See Callbacks in Python API for more information.\n","#     )\n","\n","    # if mod_type == 'HGBR':\n","    #     # TTSplit should use TRAIN_FINAL = 33 (train on all data), and it will return also val=month33 for calculation at end (only)\n","    #     model_gbdt = HistGradientBoostingRegressor(\n","    #         learning_rate=LR, \n","    #         max_iter=maxiter, \n","    #         l2_regularization = reg,\n","    #         early_stopping=False, \n","    #         verbosity = verb,\n","    #         random_state=seed_val)\n","    \n","    #     tic = perf_counter()\n","    #     model_gbdt.fit(X_train_np, y_train)\n","    #     toc = perf_counter()\n","    #     model_fit_time = datetime.datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","    #     print(f\"model HGBR fit time: {model_fit_time}\")\n","    #     best_iter = maxiter\n","    #     best_val_rmse = 0\n","# model_params = {\n","        #         'objective':param_df.at[iternum,'objective'],\n","        #         'boosting_type':param_df.at[iternum,'boosting_type'],\n","        #         'learning_rate':param_df.at[iternum,'learning_rate'],\n","        #         'n_estimators':param_df.at[iternum,'n_estimators'],\n","        #         'metric':param_df.at[iternum,'metric'],\n","        #         'subsample_for_bin':param_df.at[iternum,'subsample_for_bin'],\n","        #         'num_leaves':param_df.at[iternum,'num_leaves'],\n","        #         'max_depth':param_df.at[iternum,'max_depth'],\n","        #         'min_split_gain':param_df.at[iternum,'min_split_gain'],\n","        #         'min_child_weight':param_df.at[iternum,'min_child_weight'],\n","        #         'min_child_samples':param_df.at[iternum,'min_child_samples'],\n","        #         'colsample_bytree':param_df.at[iternum,'colsample_bytree'],\n","        #         'random_state':param_df.at[iternum,'random_state'],\n","        #         'silent':param_df.at[iternum,'silent'],\n","        #         'importance_type':param_df.at[iternum,'importance_type'],\n","        #         'reg_alpha':param_df.at[iternum,'reg_alpha'],\n","        #         'reg_lambda':param_df.at[iternum,'reg_lambda'],\n","        #         'n_jobs':param_df.at[iternum,'n_jobs'],\n","        #         'subsample':param_df.at[iternum,'subsample'],\n","        #         'subsample_freq':param_df.at[iternum,'subsample_freq']\n","                # }\n","\n","        # fit_params = {\n","        #         'eval_metric':param_df.at[iternum,'eval_metric'],\n","        #         'early_stopping_rounds':param_df.at[iternum,'early_stopping_rounds'],\n","        #         'init_score':param_df.at[iternum,'init_score'],\n","        #         'eval_init_score':param_df.at[iternum,'eval_init_score'],\n","        #         'verbose':param_df.at[iternum,'verbose'],\n","        #         'feature_name':param_df.at[iternum,'feature_name'],\n","        #         'categorical_feature':param_df.at[iternum,'categorical_feature'],\n","        #         'callbacks':param_df.at[iternum,'callbacks']\n","        #         }\n","\n","        # param_df.at[iternum,\"feature_name_\"]            = model_gbdt.feature_name_\n","\n","\n","\n","\n","# # Parameters Dictionary stores everything for dumping to file later\n","# SPEC = OrderedDict()\n","# FEATURES[\"_MODEL_NAME\"] = 'LGBMv13_15ens'   # 'LGBMv10_11ens'  # Name of file model substring to save data submission to (= False if user to input it below)\n","# FEATURES[\"_MODEL_TYPE\"] = 'LGBM'  # 'HGBR'\n","# FEATURES[\"_TEST_MONTH\"] = 34\n","\n","# # Optional operations to delete irrelevant shops or item categories, and to scale sales by month length, etc.;  set to FALSE if no operation desired\n","# FEATURES[\"_EDA_DELETE_SHOPS\"]     = [9,20] #[0,1,8,9,11,13,17,20,23,27,29,30,32,33,40,43,51,54] #[8, 9, 13, 20, 23, 32, 33, 40] # [9,20] #  # False # these are online shops, untested shops, and early-termination + online shops\n","# FEATURES[\"_EDA_DELETE_ITEM_CATS\"] = [8, 10, 32, 59, 80, 81, 82]  #[1,4,8,10,13,14,17,18,32,39,46,48,50,51,52,53,59,66,68,80,81,82] #  #[8, 80, 81, 82]  # False # hokey categories, untested categories, really hokey categories\n","# FEATURES[\"_EDA_SCALE_MONTH\"]         = 'week_retail_weight'  # False # scale sales by days in month, number of each weekday, and Russian recession retail sales index\n","\n","# # columns to keep for this round of modeling (dropping some of the less important features to save memory):\n","# FEATURES[\"COLS_KEEP_ITEMS\"]             = ['item_id', 'item_group', 'item_cluster', 'item_category_id']  #, 'item_category4']\n","# FEATURES[\"COLS_KEEP_SHOPS\"]             = ['shop_id','shop_group']\n","# FEATURES[\"COLS_KEEP_DATE_SCALING\"]      = ['month', 'days_in_M', 'weekday_weight', 'retail_sales', 'week_retail_weight']\n","# FEATURES[\"COLS_KEEP_BASE_TRAIN_TEST\"]   = ['month', 'price', 'sales', 'shop_id', 'item_id']\n","\n","# # re-order columns for organized readability, for the (to be created) combined sales-train-test (stt) dataset\n","# FEATURES[\"COLS_ORDER_STT\"]        = ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster'] #,   'revenue','item_category4','shop_group'\n","# FEATURES[\"PROVIDED_INTEGER_FEATURES\"]        = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in {'sales','price','revenue'}]  \n","# FEATURES[\"FEATURES_MONTHLY_STT_START\"]       = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in {'month','sales','price','revenue','shop_id','item_id'}]  # these are categorical features that need to be merged onto test data set\n","# FEATURES[\"PROVIDED_CATEGORICAL_FEATURES\"]    = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in {'month','sales','price','revenue'}]\n","# FEATURES[\"_USE_CATEGORICAL\"]         = True  # pd dataframe columns \"PROVIDED_CATEGORICAL_FEATURES\" are changed to categorical dtype just before model fitting/creation\n","\n","# FEATURES[\"AGG_STATS\"] = OrderedDict()\n","# FEATURES[\"AGG_STATS\"][\"sales\"]     = ['sum', 'median', 'count']\n","# FEATURES[\"AGG_STATS\"][\"revenue\"]   = ['sum']  # revenue can handle fillna(0) cartesian product; price doesn't make sense with fillna(0), so don't use that at this time\n","# #FEATURES[\"AGG_STATS\"][\"price\"]     = ['median','std']\n","\n","# # aggregate statistics columns (initial computation shall be 'sales per month' prediction target for shop_id-item_id pair grouping)\n","# FEATURES[\"STATS_FEATURES\"] = [['shop_id', 'item_id'], ['shop_id', 'item_category_id'], ['shop_id', 'item_cluster']] + FEATURES[\"PROVIDED_CATEGORICAL_FEATURES\"]\n","\n","# FEATURES[\"LAGS_MONTHS\"] = [1,2,3,4,5,6,7,8]  # month lags to include in model \n","# FEATURES[\"LAG_FEATURES\"] = {}\n","# for i in FEATURES[\"LAGS_MONTHS\"]:\n","#     FEATURES[\"LAG_FEATURES\"][i] = ['y_sales', 'shop_id_x_item_category_id_sales_sum', 'item_id_sales_sum', 'item_cluster_sales_sum'] \n","# FEATURES[\"LAG_FEATURES\"][1] = ['y_sales', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', \n","#                      'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', \n","#                      'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', \n","#                      'shop_id_sales_sum', 'shop_id_sales_count', \n","#                      'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', \n","#                      'shop_group_revenue_sum', \n","#                      'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', \n","#                      'item_group_sales_sum', 'item_group_revenue_sum', \n","#                      'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","\n","# FEATURES[\"LAG_FEATURES\"][2] = ['y_sales', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', \n","#                      'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_revenue_sum', \n","#                      'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_count', \n","#                      'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', \n","#                      'item_category_id_sales_sum', 'item_category_id_sales_count', \n","#                      'item_group_sales_sum', \n","#                      'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","\n","# FEATURES[\"LAG_FEATURES\"][3] = ['y_sales', 'shop_id_x_item_id_sales_count', \n","#                      'shop_id_x_item_category_id_sales_sum', \n","#                      'shop_id_sales_sum', \n","#                      'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', \n","#                      'item_category_id_sales_sum', 'item_category_id_sales_count', \n","#                      'item_cluster_sales_sum', 'item_cluster_sales_count']\n","\n","# # keep at least the highest importance feature for each lag, but remove all others with < 20% importance (month 13-32 training)\n","# FEATURES[\"LAG_FEATURES\"][2] = [e for e in FEATURES[\"LAG_FEATURES\"][2] if e not in {'item_group_sales_sum','shop_id_x_item_category_id_sales_sum','shop_id_x_item_cluster_sales_sum','shop_id_x_item_cluster_sales_count'}]\n","# FEATURES[\"LAG_FEATURES\"][3] = [e for e in FEATURES[\"LAG_FEATURES\"][3] if e not in {'item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum','shop_id_x_item_id_sales_count'}]\n","# FEATURES[\"LAG_FEATURES\"][4] = [e for e in FEATURES[\"LAG_FEATURES\"][4] if e not in {'shop_id_x_item_category_id_sales_sum','y_sales','item_cluster_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][5] = [e for e in FEATURES[\"LAG_FEATURES\"][5] if e not in {'item_cluster_sales_sum','item_id_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][6] = [e for e in FEATURES[\"LAG_FEATURES\"][6] if e not in {'item_id_sales_sum','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][7] = [e for e in FEATURES[\"LAG_FEATURES\"][7] if e not in {'y_sales','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][8] = [e for e in FEATURES[\"LAG_FEATURES\"][8] if e not in {'item_id_sales_sum','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","\n","# # LAG_STATS_SET is SET of all aggregate statistics columns for all lags (allows us to shed the other stats, keeping memory requirements low)\n","# LAG_STATS_SET = FEATURES[\"LAG_FEATURES\"][1]\n","# for l in FEATURES[\"LAGS_MONTHS\"][1:]:\n","#     LAG_STATS_SET = LAG_STATS_SET + [x for x in FEATURES[\"LAG_FEATURES\"][l] if x not in LAG_STATS_SET]\n","# FEATURES[\"STT_MONTHLY_COLS\"] = FEATURES[\"PROVIDED_INTEGER_FEATURES\"] + LAG_STATS_SET\n","\n","# # Define various constants that drive the attributes of the various features\n","# FEATURES[\"_CLIP_TRAIN_H\"]   = 20          # this clips sales after doing monthly groupings (monthly_stt dataframe) will also clip item_cnt_month predictions to 20 after the model runs\n","# FEATURES[\"_CLIP_TRAIN_L\"]   = 0                   \n","# FEATURES[\"_CLIP_PREDICT_H\"] = 20          # this clips the final result before submission to coursera\n","# FEATURES[\"_CLIP_PREDICT_L\"] = 0    \n","\n","# FEATURES[\"_USE_ROBUST_SCALER\"]         = True        # scale features to reduce influence of outliers\n","# FEATURES[\"_ROBUST_SCALER_QUANTILES\"]   = (20,80)\n","# FEATURES[\"_USE_MINMAX_SCALER\"]         = True        # scale features to use large range of np.int16\n","# FEATURES[\"_MINMAX_SCALER_RANGE\"]       = (0,16000)   # int16 = (0,32700); uint16 = (0,65500)  --> keep this range positive for best results with LGBM; keep range smaller for faster LGBM fitting\n","# FEATURES[\"_FEATURE_DATA_TYPE\"]         = np.int16    # np.float32 #np.int16   np.uint16          # if fill n/a = 0, can adjust feature values to be integer values and save memory (not finding that int can store np.NAN)\n","# FEATURES[\"_USE_CARTPROD_FILL\"]         = True        # use cartesian fill, or not\n","# FEATURES[\"_CARTPROD_TEST_PAIRS\"]  = False       # include all shop-item pairings from test month as well as the in-month pairings\n","# FEATURES[\"_CARTPROD_FILLNA0\"]    = True        # fill n/a cartesian additions with zeros (not good for price-based stats, however)\n","# FEATURES[\"_CARTPROD_FIRST_MONTH\"] = 13          # month number + max lag to start adding Cartesian product rows (i.e., maxlag=6mo and CARTPROD_FILL_MONTH_BEGIN=10 will cartesian fill from 4 to 33)\n","# FEATURES[\"TRAIN_MONTH_START\"]         = [13]        # == 24 ==> less than a year of data, but avoids December 'outlier' of 2014\n","# FEATURES[\"TRAIN_MONTH_END\"]           = [29]        # [29,32] #,30,32]\n","# FEATURES[\"N_VAL_MONTHS\"]              = [False]     #1 # ; if false, val is all months after training, up to and including 33; otherwise val is this many months after train_month_end\n","\n","# # Define hyperparameters for modeling\n","# FEATURES[\"LEARNING_RATE\"]       = [0.05]  # default = 0.1\n","# FEATURES[\"MAX_ITERATIONS\"]      = [200] # default = 100\n","# FEATURES[\"EARLY_STOPPING\"]      = [20]\n","# FEATURES[\"REGULARIZATION\"]      = [0.4] # default = 1 for LGBM, 0 for HGBR (these models use inverse forms of regularization)\n","# FEATURES[\"VERBOSITY\"]           = True #4 four is to print every 4th iteration; True is every iteration; False is no print except best and last\n","# FEATURES[\"SEED_VALUES\"]         = [42]\n","\n","# FEATURES[\"N_TRAIN_MODELS\"] = (len(FEATURES[\"SEED_VALUES\"])*len(FEATURES[\"N_VAL_MONTHS\"])*len(FEATURES[\"TRAIN_MONTH_END\"])*len(FEATURES[\"TRAIN_MONTH_START\"])*\n","#                          len(FEATURES[\"EARLY_STOPPING\"])*len(FEATURES[\"MAX_ITERATIONS\"])*len(FEATURES[\"REGULARIZATION\"])*len(FEATURES[\"LEARNING_RATE\"]) )\n","\n","\n","# print(f'Done: {strftime(\"%a %X %x\")}')\n","\n","\n","\n","\n","\n","\n","\n","\n","# def unscale(scaler,target):\n","#     return scaler.inverse_transform(target.reshape(-1, 1)).squeeze()\n","\n","# def GBDT_model(data=df, CONSTANTS=SPEC, params=OrderedDict()):\n","#     \"\"\"\n","#     data is entire dataframe with train, validation, and test rows, and all columns including target prediction at \"y_target\"\n","#     constants is dictionary of setup constants\n","#     params is dictionary of this particular model train/val split and model fitting/prediction parameters\n","#     \"\"\"\n","#     results = OrderedDict()\n","#     if CONSTANTS[\"_MODEL_TYPE\"] == 'LGBM':\n","        \n","#         train_start = params[\"train_start_mo\"]\n","#         train_end   = params[\"train_final_mo\"]\n","#         val_months  = params[\"val_mo\"]\n","#         test_month  = CONSTANTS[\"TEST_MONTH\"]\n","\n","#         train   = data.query('(month >= @train_start) & (month <= @train_end)')\n","#         y_train = train['y_target'].astype(np.float32)\n","#         y_train = y_train.reset_index(drop=True)\n","#         X_train = train.drop(['y_target'], axis=1)\n","#         X_train = X_train.reset_index(drop=True)\n","#         feature_names = X_train.columns\n","\n","#         if val_months:\n","#             val = data.query('(month > (@train_end)) & (month <= (@train_end + @val_months)) & (month < @test_month)')\n","#         else:\n","#             val = data.query('((month > (@train_end)) & (month < @test_month)) | (month == (@test_month-1))')\n","#         y_val = val['y_target'].astype(np.float32)\n","#         y_val = y_val.reset_index(drop=True)\n","#         X_val = val.drop(['y_target'], axis=1)\n","#         X_val = X_val.reset_index(drop=True)\n","\n","#         X_test = data.query('month == @test_month').drop(['y_target'], axis=1)\n","#         X_test = X_test.reset_index(drop=True)\n","\n","#         print('X_train:')\n","#         print_col_info(X_train,8)\n","#         print(f'\\n{X_train.head(2)}\\n\\n')\n","#         print('X_val:')\n","#         print_col_info(X_val,8)\n","#         print(f'\\n{X_val.head(2)}\\n\\n')\n","#         print('X_test:')\n","#         print_col_info(X_test,8)\n","#         print(f'\\n{X_test.head(2)}\\n\\n')\n","#         data_types = X_train.dtypes\n","\n","#         del [[data, train, val]]\n","\n","#         print('Starting training...')\n","#         model_gbdt = lgb.LGBMRegressor(\n","#             objective='regression',\n","#             boosting_type='gbdt',           # gbdt= Gradient Boosting Decision Tree; dart= Dropouts meet Multiple Additive Regression Trees; goss= Gradient-based One-Side Sampling; rf= Random Forest\n","#             learning_rate=params[\"lr\"],     # You can use callbacks parameter of fit method to shrink/adapt learning rate in training using reset_parameter callback\n","#             n_estimators=params[\"maxit\"],   # Number of boosted trees to fit = max_iterations\n","#             metric='rmse',\n","#             subsample_for_bin=200000,       # Number of samples for constructing bins\n","#             num_leaves=31,                  # Maximum tree leaves for base learners\n","#             max_depth=-1,                   # Maximum tree depth for base learners, <=0 means no limit\n","#             min_split_gain=0.0,             # Minimum loss reduction required to make a further partition on a leaf node of the tree\n","#             min_child_weight=0.001,         # Minimum sum of instance weight (hessian) needed in a child (leaf)\n","#             min_child_samples=20,           # Minimum number of data needed in a child (leaf)\n","#             colsample_bytree=params[\"reg\"], # dropout fraction of columns during fitting (max=1 = no dropout)\n","#             random_state=params[\"seed\"],    # seed value\n","#             silent=False,                   # whether to print info during fitting\n","#             importance_type='split',        # feature importance type: 'split'= N times feature is used in model; 'gain'= total gains of splits which use the feature\n","#             reg_alpha=0.0,                  # L1 regularization\n","#             reg_lambda=0.0,                 # L2 regularization\n","#             n_jobs=- 1,                     # N parallel threads to use on computer\n","#             subsample=1.0,                  # row fraction used for training: keep at 1 for time series data\n","#             subsample_freq=0,               # keep at 0 for time series\n","#             )\n","\n","#         tic = perf_counter()\n","#         model_gbdt.fit( \n","#             X_train,                                # Input feature matrix (array-like or sparse matrix of shape = [n_samples, n_features])\n","#             y_train,                                # The target values (class labels in classification, real numbers in regression) (array-like of shape = [n_samples])\n","#             eval_set=[(X_val, y_val)],              # can have multiple tuples of validation data inside this list\n","#             eval_names=None,                        # Names of eval_set (list of strings or None, optional (default=None))\n","#             eval_metric='rmse',                     # Default: 'l2' (= mean squared error, 'mse') for LGBMRegressor; options include 'l2_root'='root_mean_squared_error'='rmse' and 'l1'='mean_absolute_error'='mae' + more\n","#             early_stopping_rounds=params[\"estop\"],  # Activates early stopping. The model will train until the validation score stops improving. Validation score needs to improve at least every early_stopping_rounds \n","#                                                     #     to continue training. Requires at least one validation data and one metric. If theres more than one, will check all of them. But the training data is ignored anyway. \n","#                                                     #     To check only the first metric, set the first_metric_only parameter to True in additional parameters **kwargs of the model constructor.\n","#             init_score=None,                        # Init score of train data\n","#             eval_init_score=None,                   # Init score of eval data (list of arrays or None, optional (default=None))\n","#             verbose=CONSTANTS[\"VERBOSITY\"] ,        # If True, metric on the eval set is printed at each boosting stage. If n=int, the metric on the eval set is printed at every nth boosting stage. Best and final also print.\n","#             feature_name='auto',                    # Feature names. If 'auto' and data is pandas DataFrame, data columns names are used. (list of strings or 'auto', optional (default='auto'))\n","#             categorical_feature='auto',             # Categorical features (list of strings or int, or 'auto', optional (default='auto')) If list of int, interpreted as indices. \n","#                                                     # If list of strings, interpreted as feature names (need to specify feature_name as well). \n","#                                                     # If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used. All values in categorical features should be less than int32 max value (2147483647). \n","#                                                     # Large values could be memory-consuming. Consider using consecutive integers starting from zero. All negative values in categorical features are treated as missing values.\n","#             callbacks=None                          # List of callback functions that are applied at each iteration (list of callback functions or None, optional (default=None)) See Callbacks in Python API for more information.\n","#             )\n","\n","#         toc = perf_counter()\n","#         results[\"model_fit_time\"] = datetime.datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","#         print(f'Model LGBM fit time: {results[\"model_fit_time\"]}')\n","#         results[\"best_iter\"] = model_gbdt.best_iteration_\n","#         results[\"best_val_rmse\"] = 0 #best_score\n","\n","\n","#     # if mod_type == 'HGBR':\n","#     #     # TTSplit should use TRAIN_FINAL = 33 (train on all data), and it will return also val=month33 for calculation at end (only)\n","#     #     model_gbdt = HistGradientBoostingRegressor(\n","#     #         learning_rate=LR, \n","#     #         max_iter=maxiter, \n","#     #         l2_regularization = reg,\n","#     #         early_stopping=False, \n","#     #         verbosity = verb,\n","#     #         random_state=seed_val)\n","    \n","#     #     tic = perf_counter()\n","#     #     model_gbdt.fit(X_train_np, y_train)\n","#     #     toc = perf_counter()\n","#     #     model_fit_time = datetime.datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","#     #     print(f\"model HGBR fit time: {model_fit_time}\")\n","#     #     best_iter = maxiter\n","#     #     best_val_rmse = 0\n","        \n","#     print(\"Starting predictions...\")\n","#     tic = perf_counter()\n","#     y_pred_train =  model_gbdt.predict( X_train, num_iteration=model_gbdt.best_iteration_ )\n","#     y_pred_val =    model_gbdt.predict( X_val,   num_iteration=model_gbdt.best_iteration_ )\n","#     y_pred_test =   model_gbdt.predict( X_test,  num_iteration=model_gbdt.best_iteration_ )\n","#     y_train =       y_train.to_numpy()\n","#     y_val =         y_val.to_numpy()\n","#     # always do minmax scaling after robust scaling; and do inverse scaling with minmax first, then robust\n","#     if CONSTANTS[\"_USE_MINMAX_SCALER\"]:\n","#         y_pred_train =  unscale(minmax_scalers['y_sales'],  y_pred_train)\n","#         y_pred_val =    unscale(minmax_scalers['y_sales'],  y_pred_val)\n","#         y_pred_test =   unscale(minmax_scalers['y_sales'],  y_pred_test)\n","#         y_train =       unscale(minmax_scalers['y_sales'],  y_train)\n","#         y_val =         unscale(minmax_scalers['y_sales'],  y_val)\n","#     if CONSTANTS[\"_USE_ROBUST_SCALER\"]:\n","#         y_pred_train =  unscale(robust_scalers['y_sales'],  y_pred_train)\n","#         y_pred_val =    unscale(robust_scalers['y_sales'],  y_pred_val)\n","#         y_pred_test =   unscale(robust_scalers['y_sales'],  y_pred_test)\n","#         y_train =       unscale(robust_scalers['y_sales'],  y_train)\n","#         y_val =         unscale(robust_scalers['y_sales'],  y_val)\n","#     y_pred_train =  y_pred_train.clip(CONSTANTS[\"_CLIP_PREDICT_L\"], CONSTANTS[\"_CLIP_PREDICT_H\"])\n","#     y_pred_val =    y_pred_val.clip(  CONSTANTS[\"_CLIP_PREDICT_L\"], CONSTANTS[\"_CLIP_PREDICT_H\"])\n","#     y_pred_test =   y_pred_test.clip( CONSTANTS[\"_CLIP_PREDICT_L\"], CONSTANTS[\"_CLIP_PREDICT_H\"]) \n","#     toc = perf_counter()\n","#     results[\"predict_time\"] = datetime.datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","#     print(f'Transform and Predict train/val/test time: {results[\"predict_time\"]}')\n","\n","#     results[\"train_r2\"],   results[\"val_r2\"]    = sk_r2(y_train, y_pred_train),            sk_r2(y_val, y_pred_val)\n","#     results[\"train_rmse\"], results[\"val_rmse\"]  = np.sqrt(sk_mse(y_train, y_pred_train)),  np.sqrt(sk_mse(y_val, y_pred_val))\n","#     print(f'R^2 train  = {results[\"train_r2\"]:.4f}      R^2 val  = {results[\"val_r2\"]:.4f}')\n","#     print(f'RMSE train = {results[\"train_rmse\"]:.4f}    RMSE val = {results[\"val_rmse\"]:.4f}\\n')\n","\n","#     return model_gbdt, model_gbdt.get_params(), X_test, y_pred_test, feature_names, data_types, results\n","\n","# print(f'Done: {strftime(\"%a %X %x\")}')\n","\n","\n","\n","\n","\n","# ensemble_feature_names = []\n","# ensemble_y_pred_test = []\n","# ensemble_df_columns = ['lr', 'reg', 'max_iter', 'estop', 'start', 'end', 'n_val_mo', 'seed', 'trR2', 'valR2', 'tr_rmse', 'val_rmse', 'best_iter', 'best_val_rmse', 'model_time','predict_time','total_time']\n","# ensemble_df_rows = []\n","# model_params = OrderedDict()\n","# itercount = 0\n","# for lr in FEATURES[\"LEARNING_RATE\"]:\n","#     for reg in FEATURES[\"REGULARIZATION\"]:\n","#         for maxit in FEATURES[\"MAX_ITERATIONS\"]:\n","#             for estop in FEATURES[\"EARLY_STOPPING\"]:\n","#                 for train_start_mo in FEATURES[\"TRAIN_MONTH_START\"]:\n","#                     for train_final_mo in FEATURES[\"TRAIN_MONTH_END\"]:\n","#                         for val_mo in FEATURES[\"N_VAL_MONTHS\"]:\n","#                             for seed in FEATURES[\"SEED_VALUES\"]:\n","#                                 itercount += 1\n","#                                 print(f'\\n\\nBelow: Model {itercount} of {FEATURES[\"N_TRAIN_MODELS\"]}: LR = {lr}; LFF = {reg}, train_start = {train_start_mo}; train_end = {train_final_mo}; seed = {seed}\\n')\n","#                                 time0 = time.time()\n","#                                 model_params[\"lr\"] = lr\n","#                                 model_params['reg'] = reg\n","#                                 model_params['maxit'] = maxit\n","#                                 model_params['estop'] = estop\n","#                                 model_params['train_start_mo'] = train_start_mo\n","#                                 model_params['train_final_mo'] = train_final_mo\n","#                                 model_params['val_mo'] = val_mo\n","#                                 model_params['seed'] = seed\n","#                                 ##model_fit, y_pred_test, train_r2, val_r2, train_rmse, val_rmse, best_iter, best_val_rmse, model_fit_time, predict_time = \n","#                                 model_fit, model_params, X_test, y_pred_test, feature_names, data_types, results = GBDT_model(df, SPEC, model_params)\n","#                                 time2 = time.time(); model_time = datetime.datetime.utcfromtimestamp(time2 - time0).strftime('%H:%M:%S')\n","\n","#                                 ensemble_feature_names.append(feature_names)\n","#                                 ensemble_y_pred_test.append(y_pred_test)\n","#                                 ##ensemble_df_rows.append([lr,reg,maxit,estop,train_start_mo,train_final_mo,val_mo,seed,train_r2,val_r2,train_rmse,val_rmse,best_iter,best_val_rmse,model_fit_time,predict_time,model_time])\n","\n","#                                 # intermediate save after each model fit set of parameters, in case of crash or disconnect from Colab\n","#                                 # Simple ensemble averaging\n","#                                 y_test_pred_avg = np.mean(ensemble_y_pred_test, axis=0)\n","#                                 # Merge the test predictions with IDs from the original test dataset, and keep only columns \"ID\" and \"item_cnt_month\"\n","#                                 y_submission = pd.DataFrame.from_dict({'item_cnt_month':y_test_pred_avg,'shop_id':X_test.shop_id,'item_id':X_test.item_id})\n","#                                 y_submission = test.merge(y_submission, on=['shop_id','item_id'], how= 'left').reset_index(drop=True).drop(['shop_id','item_id'],axis=1)\n","#                                 y_submission.to_csv(\"./models_and_predictions/\" + FEATURES[\"_MODEL_NAME\"] + '_submission.csv', index=False)\n","#                                 ##ensemble_scores = pd.DataFrame(ensemble_df_rows, columns = ensemble_df_columns)\n","#                                 ##ensemble_scores.to_csv(\"./models_and_predictions/\" + model_name_ens, index=False)\n","#                                 time3 = time.time(); iteration_time = datetime.datetime.utcfromtimestamp(time3 - time0).strftime('%H:%M:%S')\n","#                                 #print(f'TTSplit Execution Time = {ttsplit_time};  \n","#                                 print(f'Model fit/predict Execution Time = {model_time};  Total Iteration Execution Time = {iteration_time}')\n","#                                 print(f'Below: Model {itercount} of {FEATURES[\"N_TRAIN_MODELS\"]}: LR = {lr}; LFF = {reg}, train_start = {train_start_mo}; train_end = {train_final_mo}; seed = {seed}\\n')\n","# print(model_params)\n","# print(feature_names)\n","# print(data_types)\n","# print(results)\n","# #display(ensemble_scores)\n","\n","# print(f'\\nDone: {strftime(\"%a %X %x\")}\\n')\n","\n","nocode=True"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x4YcaYylQ0d8"},"source":["###**Multiprocessing to Reduce pandas Memory Usage**"]},{"cell_type":"code","metadata":{"id":"fVXBcQQXQR9T","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595936942249,"user_tz":240,"elapsed":268,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["\n","# 16.6.2.9. Process Pools   https://docs.python.org/2/library/multiprocessing.html\n","# One can create a pool of processes which will carry out tasks submitted to it with the Pool class.\n","# class multiprocessing.Pool([processes[, initializer[, initargs[, maxtasksperchild]]]])\n","# processes is the number of worker processes to use. If processes is None then the number returned by cpu_count() is used. If initializer is not None then each worker process will call initializer(*initargs) when it starts.\n","# Note that the methods of the pool object should only be called by the process which created the pool.\n","# New in version 2.7: maxtasksperchild is the number of tasks a worker process can complete before it will exit and be replaced with a fresh worker process, to enable unused resources to be freed. \n","# The default maxtasksperchild is None, which means worker processes will live as long as the pool.\n","\n","# https://stackoverflow.com/questions/39100971/how-do-i-release-memory-used-by-a-pandas-dataframe/39101287#39101287\n","# There is one thing that always works, however, because it is done at the OS, not language, level.\n","# Suppose you have a function that creates an intermediate huge DataFrame, and returns a smaller result (which might also be a DataFrame):\n","# >     def huge_intermediate_calc(something):\n","# >         ...\n","# >         huge_df = pd.DataFrame(...)\n","# >         ...\n","# >         return some_aggregate\n","# Then if you do something like:\n","# >     import multiprocessing\n","# >     result = multiprocessing.Pool(1).map(huge_intermediate_calc, [something_])[0]\n","# Then the function is executed at a different process. When that process completes, the OS retakes all the resources it used. \n","# Maybe it help someone, when creating the Pool try to use maxtasksperchild = 1\n","# In an iPython environment (like jupyter notebook), you need to .close() and .join() or .terminate() the pool to get rid of the spawned process. \n","# The easiest way of doing that since Python 3.3 is to use the context management protocol: \n","# >     with multiprocessing.Pool(1) as pool: \n","# >         result = pool.map(huge_intermediate_calc, [something])\n","# Another option is for the subprocess to write the dataframe to disk using something like parquet. \n","#    It may be faster than moving a big pickled dataframe back to the parent. It will be in the disk cache so it is fast. \n","# If you stick to numeric numpy arrays, those are freed, but boxed object types are not.\n","# When modifying your dataframe, prefer inplace=True, so you don't create copies.\n","\n","#  Use PIPE to concatenate functions in a multiprocess pool, keeping workers / dataframes all within the process so it kills them when the process terminates\n","# The Pipe() function returns a pair of connection objects connected by a pipe which by default is duplex (two-way). For example:\n","# from multiprocessing import Process, Pipe\n","# def f(conn):\n","#     conn.send([42, None, 'hello'])\n","#     conn.close()\n","# if __name__ == '__main__':\n","#     parent_conn, child_conn = Pipe()\n","#     p = Process(target=f, args=(child_conn,))\n","#     p.start()\n","#     print parent_conn.recv()   # prints \"[42, None, 'hello']\"\n","#     p.join()\n","# parent,child = multiprocessing.Pipe()\n","\n","# with multiprocessing.Pool(None,maxtasksperchild = 1) as pool:  # None = use all available processors, could use Pool(1, maxtasksperchild = 1)\n","#     result = pool.map(load_dfs, [args])[0]\n","# pool.close()  # pool.terminate()\n","# pool.join()   \n","\n","# gc.collect()\n","#print(parent.recv())\n","#child.close()\n","#print(f'Result = {result[:5]}') #.head(2))\n","#print(f'Result = {result[:5]}') #.head(2))\n","#print(result.head(2))\n","#result = 1.0\n","#print(f'Result = {result}') #.head(2))\n","# del result\n","# gc.collect()\n","# result = pd.DataFrame()  # doesn't seem to help immediately, nor does \"del df\" or put df in list and \"del list\" or \"del list[0]\" or \"del list[0] --> del list\", all followed by gc.collect()\n","# pool.close(); pool.join();  mu,vu,vt,va,t = get_memory_stats(\"4) After checking active children: \")  # pool.terminate()\n","# gc.collect(); mu,vu,vt,va,t = get_memory_stats(\"5) After gc.collect(): \")\n","# %reset Out   # flush the output cache (no obvious improvement with IPynb)\n","# gc.collect(); mu,vu,vt,va,t = get_memory_stats(\"6) After Reset Cache and gc.collect(): \")\n","\n","                # \"readonly/final_project_data/test.csv.gz\",\n","                # \"data_output/train_test_base.csv.gz\"]\n","\n","# dummy test program to verify OK to run with multiprocessing (yes; it releases memory when parent process is done)\n","# def print_tail(df=pd.DataFrame()):\n","#     print(df.tail(2))\n","#     return df\n","#             exec(data_frame_name + ' = print_tail(' + data_frame_name + ')')\n","#         df = print_tail(eval(data_frame_name)) ;  arg_dict['q'].put(df)  #(df.tail(1))\n","#         arg_dict['q'].put(eval(data_frame_name)) # very slow to try passing full dfs by queue\n","# q = multiprocessing.Queue(len(data_files))  # optional argument: length of queue\n","# proc = multiprocessing.Process(target=load_dfs, args=(args_dict,))\n","# proc.start()\n","# proc.join()  # waits for queue to be filled/flushed\n","# mu,vu,vt,va,t = get_memory_stats(\"C) After Close and Join Pool: \")\n","# print(q.get().head(1))\n","# print(items_enc.head(2))\n","# print(q.get().head(1))\n","# print(shops_enc.head(2))\n","# print(q.get().head(1))\n","# print(date_adjustments.head(2))\n","# print(q.get().head(1))\n","# print(test.head(2))\n","# print(q.get().head(1))\n","# print(train_test_base.head(2))\n","\n","nocode=True"],"execution_count":9,"outputs":[]}]}