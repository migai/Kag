{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_cleaning_and_eda_feature_merging_v2_mg.ipynb","provenance":[{"file_id":"1vmc6H9xvSFVlHGzP2lBXBcCcNdPGlMxL","timestamp":1589119317913},{"file_id":"14t_SkT4SYL-JAcrbJIJ60cbNgCrJlvIN","timestamp":1589069755220},{"file_id":"1mFtJLElc2hyopq6yrPAoi0zQVUegsAP5","timestamp":1589018631041},{"file_id":"1y04qp_hoyBnsJQwkX67pk4iZsKGQIqNy","timestamp":1588805435380},{"file_id":"1b_K0QD9U6dofQ7VtTAtzUrqbKJMdj64l","timestamp":1588785261238},{"file_id":"1gcbeu-d1GUUzznZwTzfqYYbaD6cJ7EQ4","timestamp":1588238522691},{"file_id":"1pSGNRDJGzdeI69bw1zWefzPifBq-rv9H","timestamp":1588151557805},{"file_id":"1hq-ivO1BBtc5IC5xd-JdH8HNAQ81bkRf","timestamp":1587386702728},{"file_id":"1I7DWo2B7q7g9Ne2khD11YGTq_gD2FoaT","timestamp":1587321559573},{"file_id":"1fyZv-jgb8twsCBQwPxjgk_XSYA6dOa2t","timestamp":1587303588700},{"file_id":"1iKsplqpLQQZqdr3Trflk7TapksgkQXjX","timestamp":1587145642564},{"file_id":"https://github.com/migai/Kag/blob/master/Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587076517706}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn","colab_type":"text"},"source":["#**Quick processing of data sets for input to model**\n","\n","**Data Cleaning, Feature Generation**\n","\n","Andreas Theodoulou and Michael Gaidis (June, 2020)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"auU4VWkzZvTM"},"source":["#**Data Ouput from This Notebook**\n","\n","##**1. A lightly-cleaned version of sales_train data set, merged with the test data set** (distinguishable as month = 34)\n","* *train_test_base*\n","\n","##**2. Data sets to merge with the aforementioned data set, and also important to merge with Cartesian-Product rows that we insert into the training data.**\n","* *shops_features*\n","* *items_features*\n","* *date_adjustments*\n","\n","###The intent is for the user to adapt these data sets as desired, in the IPynb notebook focused on modeling.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m-64o-4yXF4P"},"source":["#**0.1 Mount Google Drive (Local File Storage/Repo For Colab)**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XeWResomXF4j","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1592676660616,"user_tz":240,"elapsed":27003,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"d35fbc32-376a-4a6c-d1cf-3ec43b9d2cea"},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, then paste it into the input box and hit 'enter' to complete mounting of the drive\n","from google.colab import drive  \n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zRTE5ZTlXF4w"},"source":["#**0.2 Configure Environment and Load Data Files**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sQ5m2qfIXF4x","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592676661099,"user_tz":240,"elapsed":27454,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"d9a1fddc-84a6-45f3-b3c1-884f472981c4"},"source":["# python libraries/modules used throughout this notebook (with some holdovers from other, similar notebooks)\n","# pandas data(database) storage, EDA, and manipulation\n","import pandas as pd\n","### pandas formatting\n","### Here's what I find works well for this particular IPynb, when using a FHD laptop monitor with a full-screen browser window containing my IPynb notebook:\n","pd.set_option(\"display.max_rows\",120)     # Override pandas choice of how many rows to show, so, for example, we can see the full 84-row item_category dataframe instead of the first few rows, then ...., then the last few rows\n","pd.set_option(\"display.max_columns\",26)   # Similar to row code above, we can show more columns than default  \n","pd.set_option(\"display.width\", 230)       # Tune this to our monitor window size to avoid horiz scroll bars in output windows (but, the drawback is that we will get output text wrapping)\n","pd.set_option(\"max_colwidth\", None)       # This is done, for example, so we can see full item name and not '...' in the middle\n","# Try to convince pandas to print without decimal places if a number is actually an integer (helps keep column width down, and highlights data types), or with precision = 3 decimals if a float\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)\n","\n","# Pandas additional enhancements\n","pd.set_option('compute.use_bottleneck', False)  # speed up operation when using NaNs\n","pd.set_option('compute.use_numexpr', False)     # speed up boolean operations, large dataframes; DataFrame.query() and pandas.eval() will evaluate the subexpressions that can be evaluated by numexpr\n","\n","\n","# data visualization\n","import matplotlib.pyplot as plt\n","# ipynb magic command to allow interactive matplotlib graphics in ipynb notebook\n","%matplotlib inline  \n","\n","# computations\n","import numpy as np\n","\n","# file operations\n","import os\n","from urllib.parse import urlunparse\n","from pathlib import Path\n","import feather   # this is 3x to 8x faster than pd.read_csv and pd.to_hdf, but file size is 2x hdf and 10x csv.gz\n","import pickle\n","\n","# misc. python enhancements\n","import re\n","import string\n","from itertools import product\n","from collections import OrderedDict\n","import time\n","import datetime\n","from time import sleep, localtime, strftime, tzset, strptime\n","os.environ['TZ'] = 'EST+05EDT,M4.1.0,M10.5.0'   # allows user to simply print a formatted version of the local date and time; helps keep track of what cells were run, and when\n","tzset()\n","\n","# ML packages\n","import sklearn\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["done: Sat 14:11:03 06/20/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s5CBLhjQXF42","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592676666164,"user_tz":240,"elapsed":32495,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"17c67606-d37a-4bfd-abed-9c6ee011946b"},"source":["train_test_base_save = False #True  # set to false if you plan to read in from previously-created csv.gz file\n","\n","GDRIVE_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\"\n","\n","data_files = [  #\"readonly/final_project_data/shops.csv\",\n","                #\"data_output/shops_transl.csv\",\n","                \"data_output/shops_augmented.csv\",\n","                \"data_output/shops_new.csv\",\n","               \n","                #\"readonly/final_project_data/items.csv\",\n","                #\"data_output/items_transl.csv\",\n","                \"data_output/items_augmented.csv\",\n","                \"data_output/items_new.csv\",\n","                \"data_output/items_clustered_22170b.csv.gz\",\n","               \n","                #\"readonly/final_project_data/item_categories.csv\",\n","                #\"data_output/item_categories_transl.csv\",\n","                \"data_output/item_categories_augmented.csv\",\n","                #\"readonly/en_50k.csv\",\n","               \n","                \"readonly/final_project_data/sales_train.csv.gz\",\n","                #\"data_output/sales_train_cleaned.csv.gz\",\n","               \n","                #\"readonly/final_project_data/sample_submission.csv.gz\",\n","                \"readonly/final_project_data/test.csv.gz\"\n","                ]\n","\n","\n","# Dict of helper code files, to be loaded and imported {filepath : import_as}\n","code_files = {}  # not used at this time; example dict = {\"helper_code/kaggle_utils_at_mg.py\" : \"kag_utils\"}\n","\n","\n","# GitHub file location info\n","git_hub_url = \"https://raw.githubusercontent.com/migai\"\n","repo_name = 'Kag'\n","branch_name = 'master'\n","base_url = os.path.join(git_hub_url, repo_name, branch_name)\n","\n","if data_files:\n","    print('\\n\\ncsv files source directory: ', end='')\n","    %cd \"{GDRIVE_REPO_PATH}\"\n","\n","    print(\"\\nLoading csv Files from Google Drive repo into Colab...\\n\")\n","\n","    # Loop to load the data files into appropriately-named pandas DataFrames\n","    for path_name in data_files:\n","        filename = path_name.rsplit(\"/\")[-1]\n","        data_frame_name = filename.split(\".\")[0]\n","        exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","        # if data_frame_name == 'sales_train':\n","        #     sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","        print(f'DataFrame {data_frame_name}, shape = {eval(data_frame_name).shape} :')\n","        print(eval(data_frame_name).head(2))\n","        print(\"\\n\")\n","else: \n","    %cd \"{GDRIVE_REPO_PATH}\"\n","    \n","print(f'\\nDataFrame Loading Complete: {strftime(\"%a %X %x\")}\\n')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\n","\n","csv files source directory: /content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","\n","Loading csv Files from Google Drive repo into Colab...\n","\n","DataFrame shops_augmented, shape = (60, 17) :\n","                       shop_name  shop_id                       en_shop_name  shop_city_population  shop_tested shop_type  shop_type_enc shop_city  shop_city_enc shop_federal_district  shop_federal_district_enc s_type_broad  \\\n","0  !Якутск Орджоникидзе, 56 фран        0  ! Yakutsk Ordzhonikidze, 56 Franc                235600        False      Shop             20   Yakutsk             54               Eastern                         16         Shop   \n","1  !Якутск ТЦ \"Центральный\" фран        1       ! Yakutsk TC \"Central\" Franc                235600        False      Mall             50   Yakutsk             54               Eastern                         16         Mall   \n","\n","   s_type_broad_enc fd_popdens  fd_popdens_enc        fd_gdp  fd_gdp_enc  \n","0                10     Remote               5  Intermediate          43  \n","1                60     Remote               5  Intermediate          43  \n","\n","\n","DataFrame shops_new, shape = (60, 14) :\n","   shop_id  shop_tested shop_type  shop_type_enc shop_city  shop_city_enc shop_federal_district  shop_federal_district_enc s_type_broad  s_type_broad_enc fd_popdens  fd_popdens_enc        fd_gdp  fd_gdp_enc\n","0        0        False      Shop             20   Yakutsk             54               Eastern                         16         Shop                10     Remote               5  Intermediate          43\n","1        1        False      Mall             50   Yakutsk             54               Eastern                         16         Mall                60     Remote               5  Intermediate          43\n","\n","\n","DataFrame items_augmented, shape = (22170, 5) :\n","   item_id                                                         item_name  item_tested  item_category_id                                                   orig_eng_name_transl\n","0        0                                         power in glamor plast dvd        False                40                                           ! POWER IN glamor (PLAST.) D\n","1        1  abbyy finereader 12 professional edition full pc digital version        False                76  ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]\n","\n","\n","DataFrame items_new, shape = (22170, 8) :\n","   item_id  item_tested  item_category_id  cluster_code item_category3  item_category3_enc item_category4  item_category4_enc\n","0        0        False                40           920         Movies                   7         Movies                   3\n","1        1        False                76          2600       Software                  10             PC                   5\n","\n","\n","DataFrame items_clustered_22170b, shape = (22170, 6) :\n","   item_id                                                                                         item_name  item_category_id  item_tested  item_cluster  n_items_in_cluster\n","0        0                                                               movie dvd power in glamor plast dvd                40        False           100                 111\n","1        1  program home and office digital abbyy finereader 12 professional edition full pc digital version                76        False           105                  19\n","\n","\n","DataFrame item_categories_augmented, shape = (84, 8) :\n","        item_category_name  item_category_id                 en_cat_name item_category1 item_category2 item_category3 item_category4  item_cat_tested\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones          Audio             PC    Accessories             PC             True\n","1         Аксессуары - PS2                 1           Accessories - PS2    Accessories    PlayStation    Accessories    PlayStation            False\n","\n","\n","DataFrame sales_train, shape = (2935849, 6) :\n","         date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0  02.01.2013               0       59    22154         999             1\n","1  03.01.2013               0       25     2552         899             1\n","\n","\n","DataFrame test, shape = (214200, 3) :\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","\n","\n","\n","DataFrame Loading Complete: Sat 14:11:08 06/20/20\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aJI8Lzq7CZlt","colab_type":"text"},"source":["#**1. *train_test_base***\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eZWaNltRcV3_"},"source":["\n","***sales_train*** dataset outliers: \n","\n","* Clip these rows:\n","```\n","Shop 24, sales of item 20949 --> clip to 200\n","Shop 25, sales of item 20949 --> clip to roughly 200/day\n","```\n","* Delete these rows (use this reverse order for deleting if using .iloc): </br>\n","```\n","[2909818, 2909401, 2326930, 2257299, 1163158, 484683]\n","```\n","\n","***sales_train*** dataset shop overlap \n","```\n","* Combine shop 11 into shop 10  (id == 11 --> set id = 10)\n","* Combine shop  0 into shop 57  (id ==  0 --> set id = 57)\n","* Combine shop  1 into shop 58  (id ==  1 --> set id = 58)\n","```\n","\n","***sales_train*** dataset late-opening shop\n","```\n","* Multiply shop 36 sales by 31/15 to account for it being open only for the last 15 days of training.\n","The user can later scale again as desired to conform to the 30-day test month.\n","```\n","\n","***sales_train merge with test***\n","```\n","* Append the test shop-item pairs to the sales_train data set, so merging and feature generation\n","have the option of including these rows as well.\n","```\n","\n","***Additional time-based features***\n","```\n","Replace the \"date\" and \"date_block_num\" columns with:\n","    1. 'day'    = integer value of day number, \n","                starting at day = 0 for the first training set transaction, and incrementing by \"calendar\" day number \n","                (not by \"transaction\" day number).\n","                Thus, 'day' may not include all possible integers from start to finish.  \n","                It only assigns integer values (based on the calendar) to days when there are transactions in the \n","                input dataframe --> if the input dataframe has no transactions on a particular day, that day's \n","                \"calendar\" integer value will not be present in the column.\n","    2. 'week'   = integer value of week number, \n","                however, unlike 'day', the 'week' number is aligned not to start at the first training set transaction, \n","                but rather so that there is a full 'week' of 7 days that ends on Oct. 31, 2015 (the final day of training data).  \n","                If using the full sales_train data set, this results in week = 0 having only 5 days in it. \n","                The final week of October, 2015 is assigned 'week' number = 147.  \n","                Arbitrarily assigning test to \"Nov. 1, 2015\" results in test week = 148\n","    3. 'month'  = renamed from \"date_block_num\" of original data set (no changes).  \n","                Integer values from 0 to 33 represent the months starting at day0.  Test month == 34 is Nov. 2015.\n","    4. 'qtr'    = quarter = integer number of 3-month chunks of time, aligned with the end of October, 2015.  \n","                day0 is included in 'qtr' = 0, but 'qtr'=0 only contains 1 month (Jan 2013) of data due to the alignment.\n","                The months of August, Sept, Oct 2015 form 'qtr' = 11.  \"qtr\" in this sense is just 3-month chunks... \n","                it is not the traditional Q1,Q2,Q3,Q4 beginning Jan 1, but instead is more like date_block_num in that it \n","                is monotonically increasing integers, incremented every 3 months.\n","    5. 'season' = integer number of 3-month chunks of time, reset each year (allowed values = 0,1,2,3)... \n","                not quite the same as spring-summer-winter-fall, or Q1,Q2,Q3,Q4, but instead shifted to \n","                better capture seasonal spending trends aligned in particular with high December spending\n","                2 = Dec 1 to Feb 28 (biggest spending season), 3 = Mar 1 to May 31, \n","                0 = June 1 to Aug 30 (lowest spending season), 1 = Sept 1 to Nov 30\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"61sfwjHEjk5R","colab_type":"text"},"source":["**3,150,043 rows**, corresponding to the original rows ***with outliers removed or clipped***</br>\n","and ***with identical shops merged together*** \n","</br>\n","\n","**9 columns:**\n"," * month (int8, ordinal, 0 to 34) = date_block_num\n"," * day (int16, ordinal, 0 to 1033) \n"," * week (int8, ordinal, 0 to 148)\n"," * qtr (int8, ordinal, 0 to 12)\n"," * season (int8, categorical, 0 to 3) \n"," * shop_id (int8, categorical, 2 to 10 and 12 to 59)\n"," * item_id (int16, categorical, 0 to 22,169)\n"," * price (float32, continuous, max is near 60,000) = item_price\n"," * sales (int16, continuous, range is roughly -20 to 1000) = item_cnt_day\n","</br>\n","\n","```\n","train_test_base dataframe creation started: Sat 14:11:09 06/20/20\n","\n","Shape of original sales_train data set = (2935849, 6)\n","Rows being clipped:\n","  1,501,160 sales clipped to 200\n","  1,708,207 sales clipped to 200\n","  2,296,209 sales clipped to 100\n","  2,341,308 sales clipped to 100\n","Rows being deleted:\n","  2909818\n","  2909401\n","  2326930\n","  2257299\n","  1163158\n","  484683\n","Shape of sales_train_cleaned after 6 outlier rows were removed: (2935843, 6)\n","Shape of sales_train_cleaned after merging shops as in {0: 57, 1: 58, 11: 10}: (2935843, 6)\n","Shops being scaled:\n","  shop 36 scaled by 2.07\n","Shape of traintest after appending test to sales_train_cleaned: (3150043, 6)\n","Shape of traintest after creating time-based feature columns: (3150043, 9)\n","traintest DataFrame creation done: Sat 14:13:28 06/20/20\n","\n","/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","train_test_base.csv.gz file stored on google drive in data_output directory\n","train_test_base file save done: Sat 14:14:02 06/20/20\n","\n","Example: display(train_test_base[train_test_base.week == 102].tail(2))\n","\n","           day    week    qtr    season    month    price    sales  shop_id  item_id\n","2257039\t718\t  102\t  8\t    1\t    23      399\t    1\t59\t21970\n","2257040\t718\t  102\t  8\t    1\t    23\t  499\t    1\t59\t22060\n","\n","train_test_base done: Sat 14:14:02 06/20/20\n","```"]},{"cell_type":"markdown","metadata":{"id":"iiCw8FkBFStA","colab_type":"text"},"source":["##**1.1 Merge data sets and create day, week, quarter, and season feature columns**"]},{"cell_type":"code","metadata":{"id":"ZdhfLlXwxhRe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"status":"ok","timestamp":1592676666365,"user_tz":240,"elapsed":32669,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"71e07b14-9fe7-44c4-c475-3edbe21c780d"},"source":["# determine which rows I need to clip for shops 24 and 25\n","stclip = sales_train.query('((shop_id == 24) and (item_cnt_day > 100)) or ((shop_id == 25) and (item_cnt_day > 100))')\n","stclip"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>date_block_num</th>\n","      <th>shop_id</th>\n","      <th>item_id</th>\n","      <th>item_price</th>\n","      <th>item_cnt_day</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>862929</th>\n","      <td>17.09.2013</td>\n","      <td>8</td>\n","      <td>25</td>\n","      <td>3732</td>\n","      <td>2,545.135</td>\n","      <td>264</td>\n","    </tr>\n","    <tr>\n","      <th>862945</th>\n","      <td>17.09.2013</td>\n","      <td>8</td>\n","      <td>25</td>\n","      <td>3734</td>\n","      <td>2,548.455</td>\n","      <td>110</td>\n","    </tr>\n","    <tr>\n","      <th>868495</th>\n","      <td>05.09.2013</td>\n","      <td>8</td>\n","      <td>25</td>\n","      <td>2808</td>\n","      <td>999</td>\n","      <td>133</td>\n","    </tr>\n","    <tr>\n","      <th>1501160</th>\n","      <td>15.03.2014</td>\n","      <td>14</td>\n","      <td>24</td>\n","      <td>20949</td>\n","      <td>5</td>\n","      <td>405</td>\n","    </tr>\n","    <tr>\n","      <th>1708207</th>\n","      <td>28.06.2014</td>\n","      <td>17</td>\n","      <td>25</td>\n","      <td>20949</td>\n","      <td>5</td>\n","      <td>501</td>\n","    </tr>\n","    <tr>\n","      <th>2176634</th>\n","      <td>18.11.2014</td>\n","      <td>22</td>\n","      <td>25</td>\n","      <td>3733</td>\n","      <td>3,070.565</td>\n","      <td>147</td>\n","    </tr>\n","    <tr>\n","      <th>2296209</th>\n","      <td>30.12.2014</td>\n","      <td>23</td>\n","      <td>25</td>\n","      <td>20949</td>\n","      <td>5</td>\n","      <td>205</td>\n","    </tr>\n","    <tr>\n","      <th>2341308</th>\n","      <td>17.01.2015</td>\n","      <td>24</td>\n","      <td>25</td>\n","      <td>20949</td>\n","      <td>5</td>\n","      <td>222</td>\n","    </tr>\n","    <tr>\n","      <th>2567454</th>\n","      <td>14.04.2015</td>\n","      <td>27</td>\n","      <td>25</td>\n","      <td>3731</td>\n","      <td>1,941.995</td>\n","      <td>207</td>\n","    </tr>\n","    <tr>\n","      <th>2615667</th>\n","      <td>19.05.2015</td>\n","      <td>28</td>\n","      <td>25</td>\n","      <td>10209</td>\n","      <td>1,490.892</td>\n","      <td>148</td>\n","    </tr>\n","    <tr>\n","      <th>2615680</th>\n","      <td>19.05.2015</td>\n","      <td>28</td>\n","      <td>25</td>\n","      <td>10210</td>\n","      <td>3,496.761</td>\n","      <td>134</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","862929   17.09.2013               8       25     3732   2,545.135           264\n","862945   17.09.2013               8       25     3734   2,548.455           110\n","868495   05.09.2013               8       25     2808         999           133\n","1501160  15.03.2014              14       24    20949           5           405\n","1708207  28.06.2014              17       25    20949           5           501\n","2176634  18.11.2014              22       25     3733   3,070.565           147\n","2296209  30.12.2014              23       25    20949           5           205\n","2341308  17.01.2015              24       25    20949           5           222\n","2567454  14.04.2015              27       25     3731   1,941.995           207\n","2615667  19.05.2015              28       25    10209   1,490.892           148\n","2615680  19.05.2015              28       25    10210   3,496.761           134"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"YY7-Idw86ST2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1592676666369,"user_tz":240,"elapsed":32647,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"35fac221-0b7c-404d-f033-d51830116acb"},"source":["def clean_merge_augment(day0 = datetime.datetime(2013,1,1),\n","                        clip_rows = {1501160: 200, 1708207: 200, 2296209: 100, 2341308: 100},\n","                        delete_rows = [2909818, 2909401, 2326930, 2257299, 1163158, 484683],\n","                        merge_shops = {0: 57, 1: 58, 11: 10},\n","                        scale_shops = {36: 31/15},\n","                        dropout_repair = {},\n","                        delete_shops = []):\n","    \"\"\"\n","    Parameters:\n","    day0 = datetime.datetime object representing the day you wish to use as your reference when creating time-based features\n","    clip_rows = not quite as bad as erroneous outliers, but sales are so unlike other days that clipping should help\n","    delete_rows = list of integer row numbers that you wish to delete from the sales_train data set, e.g., from outliers/erroneous rows\n","    merge_shops = dictionary of integer shop_id key:value pairs where shop(=key) is merged into shop(=value)\n","    scale_shops = artificially adjust sales amounts for shops that are only open for partial amounts of months\n","    dropout_repair = optional, fill in the dropouts where shops are apparently erroneously missing sales (I am pushing this for now, as it looks to be of only marginal use, and not easy to do in a robust way)\n","    delete_shops = optional, can delete shops if you think they are not of value to training (I am pushing this to the modeling IPynb for easier iteration)\n","\n","    Global Variables: this function assumes you have the following pandas dataframes available globally:\n","    1) unaltered sales_train\n","    2) unaltered test\n","\n","    This function does the following:\n","    1) clips moderate outlier rows\n","    2) cleans (deletes) severe outlier rows from the training set that appear to be erroneous or irrelevant entries\n","    3) merges 3 shops into other shops where it appears that the sales_train set simply has different names for the \n","        same shop at different time periods (shop 0 absorbed by 57; shop 1 absorbed by 58, shop 11 absorbed by 10)\n","    4) optionally delete shops entirely from the sales_train data set (e.g., for irrelevant shops)\n","    5) append the test set rows to the sales_train rows, using a date of November 1, 2015 for test\n","    6) adjust the 'date' column on the merged dataset to be in datetime format, so it looks like a string of format: 'YYYY-M-D'\n","\n","    Then, creates and inserts new time-based feature columns as follows:\n","    Given a dataframe with a 'date' column containing strings like '2015-10-30', create new time-series columns:\n","    1. 'day'    = integer value of day number, starting at day = 0 for parameter day0, and incrementing by calendar day number (not by transaction day number)... \n","                    Thus, 'day' may not include all possible integers from start to finish.  It only assigns integer values (based on the calendar) to days when \n","                    there are transactions in the input dataframe --> if the input dataframe has no transactions on a particular day, that day's 'calendar' integer \n","                    value will not be present in the column (will be = 0)\n","    2. 'week'   = integer value of week number, with week = 0 at time= parameter day0.  However, unlike 'day', the 'week' number is aligned not to start at day0, but rather\n","                    so that there is a full 'week' of 7 days that ends on Oct. 31, 2015 (the final day of training data).  This results in week = 0 having only 5 days in it.\n","                    n.b., the final week of October, 2015 is assigned 'week' number = 147.  Artifically assigning test to Nov. 1, 2015 results in test week = 148\n","    3. 'month'  = renamed from \"date_block_num\" of original data set (no changes).  Integer values from 0 to 33 represent the months starting at day0.  Test month=34 is Nov. 2015.\n","    4. 'qtr'    = quarter = integer number of 3-month chunks of time, aligned with the end of October, 2015.  day0 is included in 'qtr' = 0, but 'qtr'=0 only contains 1 month (Jan 2013) of data due to the alignment\n","                    The months of August, Sept, Oct 2015 form 'qtr' = 11.  \"qtr\" in this sense is just 3-month chunks... it is not the traditional Q1,Q2,Q3,Q4 beginning Jan 1, but instead is more like\n","                    date_block_num in that it is monotonically increasing integers, incremented every 3 months such that #11 ends at the end of our training data\n","    5. 'season' = integer number of 3-month chunks of time, reset each year (allowed values = 0,1,2,3)... not quite the same as spring-summer-winter-fall, or Q1,Q2,Q3,Q4, but instead shifted to \n","                    better capture seasonal spending trends aligned in particular with high December spending\n","                    2 = Dec 1 to Feb 28 (biggest spending season), 3 = Mar 1 to May 31, 0 = June 1 to Aug 30 (lowest spending season), 1 = Sept 1 to Nov 30\n","\n","    Finally, drop the date column from the dataframe, and sort the dataframe by ['day','shop_id','item_id']  (original dataframe seems to be sorted by month, but unsorted within each month)\n","\n","    returns: the cleaned/dated/feature-augmented DataFrame\n","    \"\"\"\n","\n","    print(f'Shape of original sales_train data set = {sales_train.shape}')\n","\n","    # clip moderate outliers (first make a DataFrame copy so we can reuse sales_train later, if we need to)\n","    sales_train_cleaned = sales_train.copy(deep=True)\n","    if clip_rows:\n","        print('Rows being clipped:')\n","        for k,v in clip_rows.items(): \n","            sales_train_cleaned.at[k,'item_cnt_day'] = v\n","            print(f'  {k:,d} sales clipped to {v}')\n","\n","    # remove outlier rows from training set \n","    print('Rows being deleted:')\n","    for i in sorted(delete_rows, reverse=True):   # delete the rows in reverse order to be sure we don't run into issues with indexing\n","        print(f'  {i}')\n","        sales_train_cleaned.drop(sales_train_cleaned.index[i],inplace=True)\n","    print(f'Shape of sales_train_cleaned after {len(delete_rows)} outlier rows were removed: {sales_train_cleaned.shape}')\n","    \n","    # Merge the 3 shops we are nearly certain must correctly fit into the other shops' dropout regions:\n","    sales_train_cleaned.shop_id = sales_train_cleaned.shop_id.replace(merge_shops)\n","    print(f'Shape of sales_train_cleaned after merging shops as in {merge_shops}: {sales_train_cleaned.shape}')\n","\n","    # scale shops if desired\n","    if scale_shops:\n","        print('Shops being scaled:')\n","        for k,v in scale_shops.items(): \n","            sales_train_cleaned.item_cnt_day = sales_train_cleaned.apply(lambda row: row.item_cnt_day * v if row.shop_id == k else row.item_cnt_day, axis = 1)\n","            print(f'  shop {k} scaled by {v:.2f}')\n","\n","    # Remove irrelevant shops entirely from the sales_train_cleaned DataFrame:\n","    if delete_shops:\n","        sales_train_cleaned = sales_train_cleaned.query('shop_id != @delete_shops')\n","        print(f'Shape of sales_train_cleaned after deleting shops {delete_shops}: {sales_train_cleaned.shape}')\n","\n","    # sales_train_cleaned = sales_train_cleaned[sales_train_cleaned.shop_id != 9]\n","    # sales_train_cleaned = sales_train_cleaned[sales_train_cleaned.shop_id != 13]\n","    # print(f'Shape of sales_train_cleaned after removal of shops: {sales_train_cleaned.shape})\n","    # print(f'{sales_train_cleaned.shop_id.nunique()} shops remaining in sales_train_cleaned DataFrame: {sorted(sales_train_cleaned.shop_id.unique())})\n","\n","    sales_train_cleaned = sales_train_cleaned.astype({'date_block_num':np.int8,'shop_id':np.int8,'item_id':np.int16,\n","                                                    'item_price':np.float32,'item_cnt_day':np.int16}).reset_index(drop=True)\n","\n","    # merge dataframes so we optionally include test elements in our EDA and feature generation\n","    test_prep = test.copy(deep=True)\n","    test_prep['date_block_num'] = 34\n","    test_prep['date'] = '1.11.2015' #pd.Timestamp(year=2015, month=11, day=1)\n","    traintest = sales_train_cleaned.append(test_prep).fillna(0)\n","\n","    traintest = traintest[['date', 'date_block_num', 'item_price', 'item_cnt_day', 'shop_id', 'item_id']]\n","    traintest.columns = ['date', 'month', 'price', 'sales', 'shop_id', 'item_id']\n","    print(f'Shape of traintest after appending test to sales_train_cleaned: {traintest.shape}')\n","        \n","    # Add in the time-based feature columns\n","    traintest.date =  pd.to_datetime(traintest.date, dayfirst=True, infer_datetime_format=True)\n","    traintest.insert(1,'day', traintest.date.apply(lambda x: (x - day0).days))\n","    traintest.insert(2,'week', (traintest.day+2) // 7 )             # add the 2 days so we have end of a week coinciding with end of training data Oct. 31, 2015\n","    traintest.insert(3,'qtr', (traintest.month + 2) // 3 )          # add the 2 months so we have end of a quarter aligning with end of training data Oct. 31, 2015\n","    traintest.insert(4,'season', (traintest.month + 2) % 4 ) \n","    traintest.drop('date',axis=1,inplace=True)\n","    traintest = traintest.sort_values(['day','shop_id','item_id']).reset_index(drop=True)  # note that the train dataset is sorted by month, but nothing obvious within the month; we sort it here for consistent results in calculations below\n","    print(f'Shape of traintest after creating time-based feature columns: {traintest.shape}')\n","    print(f'traintest DataFrame creation done: {strftime(\"%a %X %x\")}\\n')\n","    return traintest\n","\n","print(f'\\nDone: {strftime(\"%a %X %x\")}\\n')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\n","Done: Sat 14:11:09 06/20/20\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j83GHWtrz_Gy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":587},"executionInfo":{"status":"ok","timestamp":1592676839891,"user_tz":240,"elapsed":206134,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"468f9df1-f188-4cc5-ddd9-3a26cdeb767f"},"source":["if train_test_base_save:\n","    print(f'train_test_base dataframe creation started: {strftime(\"%a %X %x\")}\\n')\n","    train_test_base = clean_merge_augment()\n","\n","    %cd \"{GDRIVE_REPO_PATH}\"\n","    # can save as csv.gz for < 100 MB storage and sync with GitHub\n","    compression_opts = dict(method='gzip',\n","                            archive_name='train_test_base.csv')  \n","    train_test_base.to_csv('data_output/train_test_base.csv.gz', index=False, compression=compression_opts)\n","    print(\"train_test_base.csv.gz file stored on google drive in data_output directory\")\n","    print(f'train_test_base file save done: {strftime(\"%a %X %x\")}')\n","\n","display(train_test_base[train_test_base.week == 102].tail(2))\n","\n","print(f'\\ntrain_test_base done: {strftime(\"%a %X %x\")}')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["train_test_base dataframe creation started: Sat 14:11:09 06/20/20\n","\n","Shape of original sales_train data set = (2935849, 6)\n","Rows being clipped:\n","  1,501,160 sales clipped to 200\n","  1,708,207 sales clipped to 200\n","  2,296,209 sales clipped to 100\n","  2,341,308 sales clipped to 100\n","Rows being deleted:\n","  2909818\n","  2909401\n","  2326930\n","  2257299\n","  1163158\n","  484683\n","Shape of sales_train_cleaned after 6 outlier rows were removed: (2935843, 6)\n","Shape of sales_train_cleaned after merging shops as in {0: 57, 1: 58, 11: 10}: (2935843, 6)\n","Shops being scaled:\n","  shop 36 scaled by 2.07\n","Shape of traintest after appending test to sales_train_cleaned: (3150043, 6)\n","Shape of traintest after creating time-based feature columns: (3150043, 9)\n","traintest DataFrame creation done: Sat 14:13:28 06/20/20\n","\n","/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","train_test_base.csv.gz file stored on google drive in data_output directory\n","train_test_base file save done: Sat 14:14:02 06/20/20\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>day</th>\n","      <th>week</th>\n","      <th>qtr</th>\n","      <th>season</th>\n","      <th>month</th>\n","      <th>price</th>\n","      <th>sales</th>\n","      <th>shop_id</th>\n","      <th>item_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2257039</th>\n","      <td>718</td>\n","      <td>102</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>399</td>\n","      <td>1</td>\n","      <td>59</td>\n","      <td>21970</td>\n","    </tr>\n","    <tr>\n","      <th>2257040</th>\n","      <td>718</td>\n","      <td>102</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>499</td>\n","      <td>1</td>\n","      <td>59</td>\n","      <td>22060</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         day  week  qtr  season  month  price  sales  shop_id  item_id\n","2257039  718   102    8       1     23    399      1       59    21970\n","2257040  718   102    8       1     23    499      1       59    22060"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train_test_base done: Sat 14:14:02 06/20/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P99yjzAasJva"},"source":["#1. Create ***shops_new*** data file </br>\n","**60 rows**, corresponding to the 60 original shops</br>\n","**14 columns:**\n"," * shop_id  (categorical; 0-59, int8, from original data set)\n"," * shop_tested (categorical; bool, indicating if the shop is in *test* set)\n"," * shop_type (categorical; string object: online, small shop, mall, SEC, Mega)\n"," * shop_type_enc (categorical; int8, ordinal/weighted encoding based on number of rows present in *test*, scaled to cover roughly the same range as shop_id values (0-59))\n"," * shop_city (categorical; string)\n"," * shop_city_enc (categorical; int8, encoding weighted like shop_type)\n"," * shop_federal_district (categorical; string object)\n"," * shop_federal_district_enc (categorical; int8, encoding weighted like shop_type)\n"," * s_type_broad (categorical; string object: like shop_type, but fewer categories by merging together \"Mall\",\"Mega\",\"SEC\")\n"," * s_type_broad_enc (categorical; int8, ordinal encoding roughly weighted by shop size, 0-60 scale)\n"," * fd_popdens (categorical; string object: 4 categories named by population density in the shop's federal district)\n"," * fd_popdens_enc (categorical; int8; ordinal encoding weight based on population density)\n"," * fd_gdp (categorical; string object: 3 categories named by gdp per person)\n"," * fd_gdp_enc (categorical; int8; ordinal encoding weight based on gdp/person)"]},{"cell_type":"code","metadata":{"id":"l1oWEs1gf6hm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1589233034232,"user_tz":240,"elapsed":43820,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"08302230-0e85-4f4c-e41f-359903744539"},"source":["# each of the shops in the test set has 5100 rows in the test set, but not all shops are present in the test set\n","# encode categories in the new dataset such that there is more weight given to the category values with more presence in the test set\n","shops_new = shops_augmented[['shop_id','shop_category','shop_city','shop_federal_district','shop_tested']].rename(columns={'shop_category':'shop_type'})\n","shops_new['test_rows'] = shops_new.shop_tested.apply(lambda x: 5100 if x else 0)\n","types = defaultdict.fromkeys(shops_new.shop_type.unique(),0)\n","cities = defaultdict.fromkeys(shops_new.shop_city.unique(),0)\n","feddists = defaultdict.fromkeys(shops_new.shop_federal_district.unique(),0)\n","for i in range(len(shops_new)):  # most of the weighting used to order these categories comes from the number of test rows; number of train rows helps to break ties\n","    train_items_sold = int(round(sales_train[sales_train.shop_id == i].item_cnt_day.sum()/1e3))\n","    types[shops_new.at[i,'shop_type']] += shops_new.at[i,'test_rows']*50 + train_items_sold\n","    cities[shops_new.at[i,'shop_city']] += shops_new.at[i,'test_rows']*50 + train_items_sold\n","    feddists[shops_new.at[i,'shop_federal_district']] += shops_new.at[i,'test_rows']*50 + train_items_sold\n","\n","enc_types = defaultdict(np.int8)\n","enc_cities = defaultdict(np.int8)\n","enc_feddists = defaultdict(np.int8)\n","for feat in [[types,enc_types],[cities,enc_cities],[feddists,enc_feddists]]:\n","    enc = 0\n","    for cat in sorted(feat[0], key=feat[0].get): #, reverse=True):\n","        feat[1][cat] = enc * int(round((60 / len(feat[0]))))  # scale encoding to be similar to 0-60 range of shop_id\n","        enc+=1\n","\n","shops_new['shop_type_enc'] = shops_new.shop_type.apply(lambda x: enc_types[x])\n","shops_new['shop_city_enc'] = shops_new.shop_city.apply(lambda x: enc_cities[x])\n","shops_new['shop_federal_district_enc'] = shops_new.shop_federal_district.apply(lambda x: enc_feddists[x])\n","\n","\n","# # From Wikipedia (2010, 2014, and 2017 numbers)\n","\n","# Federal district\tPopulation density(per km2)\tGDP per capita (2017)\n","# Central\t          59                          $11423\n","# Northwestern        8                           $10088\n","# Southern            33                          $5592\n","# North Caucasian     55                          $3262\n","# Volga               29                          $6388\n","# Ural                7                           $14819\n","# Siberian            4                           $6887\n","# Far Eastern         1                           $10767\n","# popdensity categories = remote, intermediate, populous; encode values as: (1+4+7+8)/4 = 5, (29+33)/2 = 31, (55+59)/2 = 57, online = overall avg = 196/8 = 24.5 --> 25\n","# gdp categories = bottom(3262), low((5592+6388+6887)/3)=6289), intermediate((10767+10088+11423)/3) = 10759), high(14819) --> use \"high\" for online; \n","#      divide all by 250 to scale closer to shop_id encoding range --> 13, 25, 43, 60\n","\n","type2cats_encs = {'featurename':'s_type_broad','enc_name':'s_type_broad_enc','Shop': ['Shop',10], 'Mall': ['Mall',60], 'Mega': ['Mall',60], 'SEC': ['Mall',60], 'Itinerant': ['Online',35], 'Online': ['Online',35]}\n","popdensitycats_encs = {'featurename':'fd_popdens','enc_name':'fd_popdens_enc','Eastern': ['Remote',5], 'South': ['Intermediate',31], 'Central': ['Populous',57], 'Northwestern': ['Remote',5], \n","                  'None': ['Online',25], 'Volga': ['Intermediate',31], 'Siberian': ['Remote',5], 'Ural': ['Remote',5]}\n","gdpcats_encs = {'featurename':'fd_gdp','enc_name':'fd_gdp_enc','Eastern': ['Intermediate',43], 'South': ['Low',25], 'Central': ['Intermediate',43], 'Northwestern': ['Intermediate',43], \n","                  'None': ['High',60], 'Volga': ['Low',25], 'Siberian': ['Low',25], 'Ural': ['High',60]}\n","\n","for feat in [type2cats_encs]:\n","    shops_new[feat['featurename']] = shops_new.shop_type.apply(lambda x: feat[x][0])\n","    shops_new[feat['enc_name']] = shops_new.shop_type.apply(lambda x: feat[x][1])\n","\n","for feat in [popdensitycats_encs,gdpcats_encs]:\n","    shops_new[feat['featurename']] = shops_new.shop_federal_district.apply(lambda x: feat[x][0])\n","    shops_new[feat['enc_name']] = shops_new.shop_federal_district.apply(lambda x: feat[x][1])\n","\n","shops_new.drop('test_rows',axis=1,inplace=True)\n","shops_new = shops_new[['shop_id','shop_tested','shop_type','shop_type_enc','shop_city','shop_city_enc','shop_federal_district',\n","                       'shop_federal_district_enc','s_type_broad','s_type_broad_enc','fd_popdens','fd_popdens_enc','fd_gdp','fd_gdp_enc']]\n","\n","shops_new = shops_new.astype({'shop_id':np.int8,'shop_tested':'bool','shop_type':'str','shop_type_enc':np.int8,'shop_city':'str',\n","                              'shop_city_enc':np.int8,'shop_federal_district':'str','shop_federal_district_enc':np.int8,\n","                              's_type_broad':'str','s_type_broad_enc':np.int8,'fd_popdens':'str','fd_popdens_enc':np.int8,'fd_gdp':'str','fd_gdp_enc':np.int8})\n","print('\\n',shops_new.dtypes)\n","print('\\n',shops_new)\n","\n","# shops_new.to_csv(\"data_output/shops_new.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," shop_id                        int8\n","shop_tested                    bool\n","shop_type                    object\n","shop_type_enc                  int8\n","shop_city                    object\n","shop_city_enc                  int8\n","shop_federal_district        object\n","shop_federal_district_enc      int8\n","s_type_broad                 object\n","s_type_broad_enc               int8\n","fd_popdens                   object\n","fd_popdens_enc                 int8\n","fd_gdp                       object\n","fd_gdp_enc                     int8\n","dtype: object\n","\n","     shop_id  shop_tested  shop_type  shop_type_enc        shop_city  shop_city_enc shop_federal_district  shop_federal_district_enc s_type_broad  s_type_broad_enc    fd_popdens  fd_popdens_enc        fd_gdp  fd_gdp_enc\n","0         0        False       Shop             20          Yakutsk             54               Eastern                         16         Shop                10        Remote               5  Intermediate          43\n","1         1        False       Mall             50          Yakutsk             54               Eastern                         16         Mall                60        Remote               5  Intermediate          43\n","2         2         True       Mega             30           Adygea             10                 South                         32         Mall                60  Intermediate              31           Low          25\n","3         3         True       Mall             50       Balashikha              8               Central                         56         Mall                60      Populous              57  Intermediate          43\n","4         4         True       Mall             50        Volgograd             14                 South                         32         Mall                60  Intermediate              31           Low          25\n","5         5         True        SEC             40          Vologda             12          Northwestern                          8         Mall                60        Remote               5  Intermediate          43\n","6         6         True       Shop             20         Voronezh             52               Central                         56         Shop                10      Populous              57  Intermediate          43\n","7         7         True        SEC             40         Voronezh             52               Central                         56         Mall                60      Populous              57  Intermediate          43\n","8         8        False       Mall             50         Voronezh             52               Central                         56         Mall                60      Populous              57  Intermediate          43\n","9         9        False  Itinerant              0             None             50                  None                          0       Online                35        Online              25          High          60\n","10       10         True       Shop             20        Zhukovsky              6               Central                         56         Shop                10      Populous              57  Intermediate          43\n","11       11        False       Shop             20        Zhukovsky              6               Central                         56         Shop                10      Populous              57  Intermediate          43\n","12       12         True     Online             10             None             50                  None                          0       Online                35        Online              25          High          60\n","13       13        False       Mall             50            Kazan             22                 Volga                         48         Mall                60  Intermediate              31           Low          25\n","14       14         True       Mall             50            Kazan             22                 Volga                         48         Mall                60  Intermediate              31           Low          25\n","15       15         True        SEC             40           Kaluga             26               Central                         56         Mall                60      Populous              57  Intermediate          43\n","16       16         True       Mall             50          Kolomna             20               Central                         56         Mall                60      Populous              57  Intermediate          43\n","17       17        False       Mall             50      Krasnoyarsk             34              Siberian                         40         Mall                60        Remote               5           Low          25\n","18       18         True       Mall             50      Krasnoyarsk             34              Siberian                         40         Mall                60        Remote               5           Low          25\n","19       19         True       Mall             50            Kursk             28               Central                         56         Mall                60      Populous              57  Intermediate          43\n","20       20        False     Online             10             None             50                  None                          0       Online                35        Online              25          High          60\n","21       21         True       Mall             50           Moscow             56               Central                         56         Mall                60      Populous              57  Intermediate          43\n","22       22         True       Shop             20           Moscow             56               Central                         56         Shop                10      Populous              57  Intermediate          43\n","23       23        False       Mall             50           Moscow             56               Central                         56         Mall                60      Populous              57  Intermediate          43\n","24       24         True       Mall             50           Moscow             56               Central                         56         Mall                60      Populous              57  Intermediate          43\n","25       25         True        SEC             40           Moscow             56               Central                         56         Mall                60      Populous              57  Intermediate          43\n","26       26         True       Mall             50           Moscow             56               Central                         56         Mall                60      Populous              57  Intermediate          43\n","27       27        False       Mega             30           Moscow             56               Central                         56         Mall                60      Populous              57  Intermediate          43\n","28       28         True       Mega             30           Moscow             56               Central                         56         Mall                60      Populous              57  Intermediate          43\n","29       29        False       Mall             50           Moscow             56               Central                         56         Mall                60      Populous              57  Intermediate          43\n","30       30        False        SEC             40        Mytishchi              0               Central                         56         Mall                60      Populous              57  Intermediate          43\n","31       31         True        SEC             40           Moscow             56               Central                         56         Mall                60      Populous              57  Intermediate          43\n","32       32        False        SEC             40           Moscow             56               Central                         56         Mall                60      Populous              57  Intermediate          43\n","33       33        False        SEC             40        Mytishchi              0               Central                         56         Mall                60      Populous              57  Intermediate          43\n","34       34         True        SEC             40  Nizhny Novgorod             42                 Volga                         48         Mall                60  Intermediate              31           Low          25\n","35       35         True        SEC             40  Nizhny Novgorod             42                 Volga                         48         Mall                60  Intermediate              31           Low          25\n","36       36         True        SEC             40      Novosibirsk             38              Siberian                         40         Mall                60        Remote               5           Low          25\n","37       37         True       Mega             30      Novosibirsk             38              Siberian                         40         Mall                60        Remote               5           Low          25\n","38       38         True       Mega             30             Omsk             18              Siberian                         40         Mall                60        Remote               5           Low          25\n","39       39         True        SEC             40    Rostov-on-Don             40                 South                         32         Mall                60  Intermediate              31           Low          25\n","40       40        False     Online             10    Rostov-on-Don             40                 South                         32       Online                35  Intermediate              31           Low          25\n","41       41         True       Mega             30    Rostov-on-Don             40                 South                         32         Mall                60  Intermediate              31           Low          25\n","42       42         True       Mall             50   St. Petersburg             36          Northwestern                          8         Mall                60        Remote               5  Intermediate          43\n","43       43        False        SEC             40   St. Petersburg             36          Northwestern                          8         Mall                60        Remote               5  Intermediate          43\n","44       44         True       Mall             50           Samara             44                 Volga                         48         Mall                60  Intermediate              31           Low          25\n","45       45         True       Mall             50           Samara             44                 Volga                         48         Mall                60  Intermediate              31           Low          25\n","46       46         True        SEC             40   Sergiyev Posad             32               Central                         56         Mall                60      Populous              57  Intermediate          43\n","47       47         True        SEC             40           Surgut             24                  Ural                         24         Mall                60        Remote               5          High          60\n","48       48         True        SEC             40            Tomsk              4              Siberian                         40         Mall                60        Remote               5           Low          25\n","49       49         True        SEC             40           Tyumen             48                  Ural                         24         Mall                60        Remote               5          High          60\n","50       50         True       Mall             50           Tyumen             48                  Ural                         24         Mall                60        Remote               5          High          60\n","51       51        False       Mall             50           Tyumen             48                  Ural                         24         Mall                60        Remote               5          High          60\n","52       52         True       Mall             50              Ufa             46                 Volga                         48         Mall                60  Intermediate              31           Low          25\n","53       53         True       Mall             50              Ufa             46                 Volga                         48         Mall                60  Intermediate              31           Low          25\n","54       54        False       Mega             30           Khimki              2               Central                         56         Mall                60      Populous              57  Intermediate          43\n","55       55         True     Online             10             None             50                  None                          0       Online                35        Online              25          High          60\n","56       56         True        SEC             40          Chekhov             30               Central                         56         Mall                60      Populous              57  Intermediate          43\n","57       57         True       Shop             20          Yakutsk             54               Eastern                         16         Shop                10        Remote               5  Intermediate          43\n","58       58         True       Mall             50          Yakutsk             54               Eastern                         16         Mall                60        Remote               5  Intermediate          43\n","59       59         True        SEC             40        Yaroslavl             16               Central                         56         Mall                60      Populous              57  Intermediate          43\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CZW9d7dfcjDY"},"source":["#2. Create ***items_new*** data file </br>\n","**22,170 rows**, corresponding to the 22,170 original items</br>\n","**7 columns:**\n"," * item_id  (categorical; range(22170), int16, from original data set)\n"," * item_tested (categorical; bool, indicating if item is in *test* set)\n"," * item_category_id (categorical; range(84), int8, from original data set)\n"," * cluster_code (categorical; int32, weighted encoding based on similarity of item names in a given cluster)\n"," * item_category3 (categorical; string)\n"," * item_category3_enc (categorical; int8, random nominal encoding done by pandas)\n"," * item_category4 (categorical; string)\n"," * item_category4_enc (categorical; int8, random nominal encoding done by pandas)\n"]},{"cell_type":"code","metadata":{"id":"ag88eVurf50h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":561},"executionInfo":{"status":"ok","timestamp":1589233034232,"user_tz":240,"elapsed":43811,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"13699295-f739-4e06-fe3d-5b5573dd6dd9"},"source":["\n","item_categories_augmented[\"item_category3\"] = item_categories_augmented[\"item_category3\"].astype('category')\n","item_categories_augmented[\"item_category3_enc\"] = item_categories_augmented['item_category3'].cat.codes\n","item_categories_augmented[\"item_category4\"] = item_categories_augmented[\"item_category4\"].astype('category')\n","item_categories_augmented[\"item_category4_enc\"] = item_categories_augmented['item_category4'].cat.codes\n","\n","items_new = items_clustered_22170[['item_id','item_tested','item_category_id','cluster_code']].copy(deep=True)\n","\n","items_new = items_new.merge(item_categories_augmented[['item_category_id','item_category3','item_category3_enc','item_category4','item_category4_enc']],how='left',on='item_category_id')\n","# print(len(items_new))\n","# print(items_new.head())\n","# print(items_new['item_category3_enc'].unique())\n","# print(items_new['item_category4_enc'].unique())\n","items_new = items_new.astype({'item_id':np.int16,'item_tested':'bool','item_category_id':np.int8,'cluster_code':np.int32,\n","                              'item_category3':'str','item_category3_enc':np.int8, 'item_category4':'str','item_category4_enc':np.int8}) \n","\n","print('\\n',items_new.dtypes)\n","print('\\n',items_new.head(20))\n","\n","# items_new.to_csv(\"data_output/items_new.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," item_id                int16\n","item_tested             bool\n","item_category_id        int8\n","cluster_code           int32\n","item_category3        object\n","item_category3_enc      int8\n","item_category4        object\n","item_category4_enc      int8\n","dtype: object\n","\n","     item_id  item_tested  item_category_id  cluster_code item_category3  item_category3_enc item_category4  item_category4_enc\n","0         0        False                40           920         Movies                   7         Movies                   3\n","1         1        False                76          2600       Software                  10             PC                   5\n","2         2        False                40           802         Movies                   7         Movies                   3\n","3         3        False                40           330         Movies                   7         Movies                   3\n","4         4        False                40          1686         Movies                   7         Movies                   3\n","5         5        False                40           806         Movies                   7         Movies                   3\n","6         6        False                40          2371         Movies                   7         Movies                   3\n","7         7        False                40          2371         Movies                   7         Movies                   3\n","8         8        False                40            40         Movies                   7         Movies                   3\n","9         9        False                40           806         Movies                   7         Movies                   3\n","10       10        False                40           499         Movies                   7         Movies                   3\n","11       11        False                40          3027         Movies                   7         Movies                   3\n","12       12        False                55          6042          Music                   8          Music                   4\n","13       13        False                40            40         Movies                   7         Movies                   3\n","14       14        False                40            40         Movies                   7         Movies                   3\n","15       15        False                40          1372         Movies                   7         Movies                   3\n","16       16        False                40            40         Movies                   7         Movies                   3\n","17       17        False                40            40         Movies                   7         Movies                   3\n","18       18        False                40            40         Movies                   7         Movies                   3\n","19       19        False                40          3278         Movies                   7         Movies                   3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rQebgs8xcrrN"},"source":["#3. Create ***sales_train_cleaned*** data file \n","Remove outliers, shops 9 and 13, and merge shops 0,1,11 (and downcast)\n","\n","</br>\n","\n","**2,935,849 - 6 = 2,935,843 rows**, corresponding to the original rows ***with outliers removed*** (6 outlier rows deleted)=\n","\n","* Delete these rows (use this order for deleting if using .iloc): [2909818,2909401,2326930,2257299,1163158,484683]\n","\n","</br>\n","\n","**2,914,268 rows**, after removal and merging of the shops:\n","\n","* Combine shop 11 into shop 10 (i.e., wherever you see shop_id == 11, set it to shop_id = 10), so *sales_train* no longer contains any shop 11.\n","* Combine shop 0 into shop 57 (id == 0 --> set id = 57)\n","* Combine shop 1 into shop 58 (id == 1 --> set id = 58)\n","* Delete all *sales_train* rows where shop_id == 9\n","* Delete all *sales_train* rows where shop_id == 13\n","\n","</br>\n","\n","**6 columns:**\n"," * date (pd.datetime format)\n"," * date_block_num (int8) \n"," * shop_id (categorical; int8, 55 unique values, inside the range 2 to 59, having removed shops 9 and 13, and merged shops 0,1,11) \n"," * item_id (categorical; int16, range 0 to 21,699)\n"," * item_price (float32; max is near 60,000, roughly 4500 items 0 < $p <= 0.5, so don't round off to integer values)  \n"," * item_cnt_day (int16, range is roughly 0 to 1000)\n"]},{"cell_type":"code","metadata":{"id":"X3fRh7lXf2CW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":561},"executionInfo":{"status":"ok","timestamp":1589239040386,"user_tz":240,"elapsed":44152,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"9009b345-1b92-42d1-8a1e-07687b38d752"},"source":["sales_train_cleaned = sales_train.copy(deep=True)\n","print(len(sales_train_cleaned))\n","for i in [2909818,2909401,2326930,2257299,1163158,484683]:\n","    print(sales_train_cleaned[sales_train_cleaned.index == i])\n","    sales_train_cleaned.drop(sales_train_cleaned.index[i],inplace=True)\n","print(len(sales_train_cleaned))\n","\n","print(sales_train_cleaned.shop_id.nunique())\n","\n","sales_train_cleaned = sales_train_cleaned[sales_train_cleaned.shop_id != 9]\n","print(len(sales_train_cleaned))\n","sales_train_cleaned = sales_train_cleaned[sales_train_cleaned.shop_id != 13]\n","print(len(sales_train_cleaned))\n","\n","sales_train_cleaned['shop_id'] = sales_train_cleaned.shop_id.apply(lambda x: 57 if x == 0 else x)\n","sales_train_cleaned['shop_id'] = sales_train_cleaned.shop_id.apply(lambda x: 58 if x == 1 else x)\n","sales_train_cleaned['shop_id'] = sales_train_cleaned.shop_id.apply(lambda x: 10 if x == 11 else x)\n","\n","\n","sales_train_cleaned = sales_train_cleaned.astype({'date_block_num':np.int8,'shop_id':np.int8,'item_id':np.int16,\n","                              'item_price':np.float32,'item_cnt_day':np.int16}) \n","\n","print('\\n',sales_train_cleaned.dtypes)\n","print('\\n',sales_train_cleaned.head())\n","\n","compression_opts = dict(method='gzip',\n","                        archive_name='sales_train_cleaned.csv')  \n","sales_train_cleaned.to_csv('data_output/sales_train_cleaned.csv.gz', index=False, compression=compression_opts)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2935849\n","              date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","2909818 2015-10-28              33       12    11373       0.909          2169\n","              date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","2909401 2015-10-14              33       12    20949           4           500\n","              date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","2326930 2015-01-15              24       12    20949           4          1000\n","              date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","2257299 2014-12-19              23       12    20949           4           500\n","              date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","1163158 2013-12-13              11       12     6066      307980             1\n","             date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","484683 2013-05-15               4       32     2973          -1             1\n","2935843\n","60\n","2932092\n","2914268\n","\n"," date              datetime64[ns]\n","date_block_num              int8\n","shop_id                     int8\n","item_id                    int16\n","item_price               float32\n","item_cnt_day               int16\n","dtype: object\n","\n","         date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0 2013-01-02               0       59    22154         999             1\n","1 2013-01-03               0       25     2552         899             1\n","2 2013-01-05               0       25     2552         899            -1\n","3 2013-01-06               0       25     2554   1,709.050             1\n","4 2013-01-15               0       25     2555        1099             1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zLBcrK2cddkC"},"source":["#4. Create ***sales_train_cln_mrg*** and ***test_mrg*** data files </br>\n","Remove outliers, adjust shops, merge with several encoded features from *shops_new* and *items_new*\n","\n","</br>\n","\n","##Merge overview: (how = \"left\")\n","left = *sales_train_cleaned* dataframe and/or *test* dataframe\n","\n","right = *shops_new* on 'shop_id'\n","\n","right = *items_new* on 'item_id'\n","\n","***only merge select columns, to keep sales_train_cln_mrg filesize manageable***\n","\n","</br>\n","\n","###***sales_train_cleaned*** dataframe:\n","\n","**2,914,268 rows** (down from original 2,935,849 by removing outliers, shop 9, shop 13)\n","\n","**6 columns:**\n","\n","| dtype: | datetime64[ns] | int8 | int8 | int16 | float32 | int16 |\n","| :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n","| row | date | date_block_num | shop_id | item_id | item_price | item_cnt_day |\n","| 0 | 2013-01-02 | 0 | 59 | 22154 | 999 | 1 |\n","| 1 | 2013-01-03 | 0 | 25 | 2552  | 899 | 1 |\n","| 2 | 2013-01-05 | 0 | 25 | 2552  | 899 | -1 |\n","| 3 | 2013-01-06 | 0 | 25 | 2554  | 1709 | 1 |\n","| 4 | 2013-01-15 | 0 | 25 | 2555  | 1099 | 1 |\n","\n","</br>\n","\n","###***shops_new*** dataframe:\n","**60 rows**, corresponding to the 60 original shops</br>\n","**14 columns** -- we will merge the bold columns below, on = 'shop_id'\n"," * shop_id  (categorical; 0-59, int8, from original data set)\n"," * ***shop_tested*** (categorical; bool, indicating if the shop is in *test* set)\n"," * shop_type (categorical; string object: online, small shop, mall, SEC, Mega)\n"," * ***shop_type_enc*** (categorical; int8, ordinal/weighted encoding based on number of rows present in *test*, scaled to cover roughly the same range as shop_id values (0-59))\n"," * shop_city (categorical; string)\n"," * ***shop_city_enc*** (categorical; int8, encoding weighted like shop_type)\n"," * shop_federal_district (categorical; string object)\n"," * ***shop_federal_district_enc*** (categorical; int8, encoding weighted like shop_type)\n"," * s_type_broad (categorical; string object: like shop_type, but fewer categories by merging together \"Mall\",\"Mega\",\"SEC\")\n"," * ***s_type_broad_enc*** (categorical; int8, ordinal encoding roughly weighted by shop size, 0-60 scale)\n"," * fd_popdens (categorical; string object: 4 categories named by population density in the shop's federal district)\n"," * ***fd_popdens_enc*** (categorical; int8; ordinal encoding weight based on population density)\n"," * fd_gdp (categorical; string object: 3 categories named by gdp per person)\n"," * ***fd_gdp_enc*** (categorical; int8; ordinal encoding weight based on gdp/person)\n","\n","</br>\n","\n","###***items_new*** dataframe:\n","\n","**22,170 rows**, corresponding to the 22,170 original items</br>\n","**7 columns** -- we will merge the 5 bold columns below, on = 'item_id'\n"," * item_id  (categorical; 0 - 22169, int16, from original data set)\n"," * ***item_tested*** (categorical; bool, indicating if item is in *test* set)\n"," * ***item_category_id*** (categorical; 0-83, int8, from original data set)\n"," * ***cluster_code*** (categorical; int32, weighted encoding based on similarity of item names in a given cluster)\n"," * item_category3 (categorical; string)\n"," * ***item_category3_enc*** (categorical; int8, random nominal encoding done by pandas)\n"," * item_category4 (categorical; string)\n"," * ***item_category4_enc*** (categorical; int8, random nominal encoding done by pandas)\n"]},{"cell_type":"code","metadata":{"id":"xMIdkCXlf3yf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1589238211445,"user_tz":240,"elapsed":1145,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"5c29d1d7-9fd4-4bc8-bf47-1c06fdb827a9"},"source":["print(f'Number of rows in sales_train: {len(sales_train)}')\n","print(f'Number of rows in sales_train_cleaned: {len(sales_train_cleaned)}')\n","print(f'Number of columns in sales_train_cleaned: {len(sales_train_cleaned.columns)}')\n","print(f'Column datatypes for sales_train_cleaned:\\n{sales_train_cleaned.dtypes}')\n","print(f'\\nFirst 2 rows of sales_train_cleaned:\\n{sales_train_cleaned.head(2)}')\n","print('\\n')\n","print(f'Number of rows in test: {len(test)}')\n","print(f'Number of columns in test: {len(test.columns)}')\n","print(f'Column datatypes for test:\\n{test.dtypes}')\n","print(f'\\nFirst 2 rows of test:\\n{test.head(2)}')\n","print('\\n')\n","\n","# Merge shop category encodings\n","sales_train_cln_mrg = sales_train_cleaned.merge(shops_new[['shop_id','shop_tested','shop_type_enc','shop_city_enc','shop_federal_district_enc',\n","                                                           's_type_broad_enc','fd_popdens_enc','fd_gdp_enc']], how='left', on='shop_id')\n","test_mrg = test.merge(shops_new[['shop_id','shop_tested','shop_type_enc','shop_city_enc','shop_federal_district_enc',\n","                                                           's_type_broad_enc','fd_popdens_enc','fd_gdp_enc']], how='left', on='shop_id')\n","\n","# Merge item category encodings\n","sales_train_cln_mrg = sales_train_cln_mrg.merge(items_new[['item_id','item_tested','item_category_id','cluster_code',\n","                                                           'item_category3_enc','item_category4_enc']], how='left', on='item_id')\n","test_mrg = test_mrg.merge(items_new[['item_id','item_tested','item_category_id','cluster_code',\n","                                                           'item_category3_enc','item_category4_enc']], how='left', on='item_id')\n","\n","# Reduce size of test_mrg columns from int64 to 32 or 16 or 8\n","test_mrg = test_mrg.astype({'ID':np.int32,'shop_id':np.int8,'item_id':np.int16}) \n","\n","\n","\n","print(f'Number of rows in sales_train_cln_mrg: {len(sales_train_cln_mrg)}')\n","print(f'Number of columns in sales_train_cln_mrg: {len(sales_train_cln_mrg.columns)}')\n","print(f'Column datatypes for sales_train_cln_mrg:\\n{sales_train_cln_mrg.dtypes}')\n","print(f'\\nFirst 2 rows of sales_train_cln_mrg:\\n{sales_train_cln_mrg.head(2)}')\n","print('\\n')\n","print(f'Number of rows in test_mrg: {len(test_mrg)}')\n","print(f'Number of columns in test_mrg: {len(test_mrg.columns)}')\n","print(f'Column datatypes for test_mrg:\\n{test_mrg.dtypes}')\n","print(f'\\nFirst 2 rows of test_mrg:\\n{test_mrg.head(2)}')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of rows in sales_train: 2935849\n","Number of rows in sales_train_cleaned: 2914268\n","Number of columns in sales_train_cleaned: 6\n","Column datatypes for sales_train_cleaned:\n","date              datetime64[ns]\n","date_block_num              int8\n","shop_id                     int8\n","item_id                    int16\n","item_price               float32\n","item_cnt_day               int16\n","dtype: object\n","\n","First 2 rows of sales_train_cleaned:\n","        date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0 2013-01-02               0       59    22154         999             1\n","1 2013-01-03               0       25     2552         899             1\n","\n","\n","Number of rows in test: 214200\n","Number of columns in test: 3\n","Column datatypes for test:\n","ID         int64\n","shop_id    int64\n","item_id    int64\n","dtype: object\n","\n","First 2 rows of test:\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","\n","\n","Number of rows in sales_train_cln_mrg: 2914268\n","Number of columns in sales_train_cln_mrg: 18\n","Column datatypes for sales_train_cln_mrg:\n","date                         datetime64[ns]\n","date_block_num                         int8\n","shop_id                                int8\n","item_id                               int16\n","item_price                          float32\n","item_cnt_day                          int16\n","shop_tested                            bool\n","shop_type_enc                          int8\n","shop_city_enc                          int8\n","shop_federal_district_enc              int8\n","s_type_broad_enc                       int8\n","fd_popdens_enc                         int8\n","fd_gdp_enc                             int8\n","item_tested                            bool\n","item_category_id                       int8\n","cluster_code                          int32\n","item_category3_enc                     int8\n","item_category4_enc                     int8\n","dtype: object\n","\n","First 2 rows of sales_train_cln_mrg:\n","        date  date_block_num  shop_id  item_id  item_price  item_cnt_day  shop_tested  shop_type_enc  shop_city_enc  shop_federal_district_enc  s_type_broad_enc  fd_popdens_enc  fd_gdp_enc  item_tested  item_category_id  cluster_code  item_category3_enc  item_category4_enc\n","0 2013-01-02               0       59    22154         999             1         True             40             16                         56                60              57          43         True                37          1296                   7                   3\n","1 2013-01-03               0       25     2552         899             1         True             40             56                         56                60              57          43        False                58           892                   8                   4\n","\n","\n","Number of rows in test_mrg: 214200\n","Number of columns in test_mrg: 15\n","Column datatypes for test_mrg:\n","ID                           int32\n","shop_id                       int8\n","item_id                      int16\n","shop_tested                   bool\n","shop_type_enc                 int8\n","shop_city_enc                 int8\n","shop_federal_district_enc     int8\n","s_type_broad_enc              int8\n","fd_popdens_enc                int8\n","fd_gdp_enc                    int8\n","item_tested                   bool\n","item_category_id              int8\n","cluster_code                 int32\n","item_category3_enc            int8\n","item_category4_enc            int8\n","dtype: object\n","\n","First 2 rows of test_mrg:\n","   ID  shop_id  item_id  shop_tested  shop_type_enc  shop_city_enc  shop_federal_district_enc  s_type_broad_enc  fd_popdens_enc  fd_gdp_enc  item_tested  item_category_id  cluster_code  item_category3_enc  item_category4_enc\n","0   0        5     5037         True             40             12                          8                60               5          43         True                19          2556                   4                   6\n","1   1        5     5320         True             40             12                          8                60               5          43         True                55          1372                   8                   4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2FbAtxeZqrJV","colab_type":"code","colab":{}},"source":["# save the gzipped sales_train_cln_mrg and test_mrg dataframes to GoogleDrive (& GitHub)\n","\n","compression_opts = dict(method='gzip',\n","                        archive_name='sales_train_cln_mrg.csv')  \n","sales_train_cln_mrg.to_csv('data_output/sales_train_cln_mrg.csv.gz', index=False, compression=compression_opts)\n","\n","compression_opts = dict(method='gzip',\n","                        archive_name='test_mrg.csv')  \n","test_mrg.to_csv('data_output/test_mrg.csv.gz', index=False, compression=compression_opts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJNOoyg62duU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1589236453635,"user_tz":240,"elapsed":862,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"b27ac218-ed97-46db-a945-fae5729fab8e"},"source":["print(sales_train_cln_mrg['item_id'].nunique(), sales_train_cln_mrg['shop_id'].nunique())\n","print(len(items_new))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["21671 55\n","22170\n","22170\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OqkTcDmN4Vs7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1589237186502,"user_tz":240,"elapsed":855,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"c8a7df5a-08cf-4d25-cb3d-165e31f1aee1"},"source":["print(sales_train_cln_mrg.item_price.max(), sales_train_cln_mrg.item_price.min())\n","print(sales_train_cln_mrg.item_cnt_day.max(), sales_train_cln_mrg.item_cnt_day.min())\n","print(sales_train_cln_mrg.cluster_code.max(), sales_train_cln_mrg.cluster_code.min())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["59200 0\n","669 -22\n","34420 19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rLYmKDfX6vwi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1589237948783,"user_tz":240,"elapsed":2531,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"2a89392d-8df8-4817-a6c5-c84fd849f152"},"source":["print(len(sales_train_cln_mrg[sales_train_cln_mrg.item_price ==0]))\n","print(len(sales_train[sales_train.item_price < 1]))\n","print(len(sales_train[sales_train.item_price <= 0.5]))\n","st9_0 = sales_train[sales_train.shop_id == 9]\n","print(len(st9_0[st9_0.item_price < 1]))\n","st13_0 = sales_train[sales_train.shop_id == 13]\n","print(len(st13_0[st13_0.item_price < 1]))\n","st_rnd = sales_train.item_price.apply(lambda x: int(round(x)))\n","print(len(st_rnd[st_rnd == 0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4163\n","4658\n","4164\n","0\n","0\n","4163\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1s_nDstaJEki"},"source":["#**Data Ouput from This Notebook**\n","\n","##**1.  *shops_new*** </br>\n","**60 rows**, corresponding to the 60 original shops</br>\n","**14 columns:**\n"," * shop_id (categorical; 0-59, int8, from original data set)\n"," * shop_tested (categorical; bool, indicating if the shop is in test set)\n"," * shop_type (categorical; string object: online, small shop, mall, SEC, Mega)\n"," * shop_type_enc (categorical; int8, ordinal/weighted encoding based on number of rows present in test, scaled to cover roughly the same range as shop_id values (0-59))\n"," * shop_city (categorical; string)\n"," * shop_city_enc (categorical; int8, encoding weighted like shop_type)\n"," * shop_federal_district (categorical; string object)\n"," * shop_federal_district_enc (categorical; int8, encoding weighted like shop_type)\n"," * s_type_broad (categorical; string object: like shop_type, but fewer categories by merging together \"Mall\",\"Mega\",\"SEC\")\n"," * s_type_broad_enc (categorical; int8, ordinal encoding roughly weighted by shop size, 0-60 scale)\n"," * fd_popdens (categorical; string object: 4 categories named by population density in the shop's federal district)\n"," * fd_popdens_enc (categorical; int8; ordinal encoding weight based on population density)\n"," * fd_gdp (categorical; string object: 3 categories named by gdp per person)\n"," * fd_gdp_enc (categorical; int8; ordinal encoding weight based on gdp/person)\n","\n","</br>\n","\n","##**2.  *items_new*** </br>\n","**22,170 rows**, corresponding to the 22,170 original items</br>\n","**7 columns:**\n"," * item_id  (categorical; 0 - 22169, int16, from original data set)\n"," * item_tested (categorical; bool, indicating if item is in *test* set)\n"," * item_category_id (categorical; 0 - 83, int8, from original data set)\n"," * cluster_code (categorical; int32, weighted encoding based on similarity of item names in a given cluster)\n"," * item_category3 (categorical; string)\n"," * item_category3_enc (categorical; int8, random nominal encoding done by pandas)\n"," * item_category4 (categorical; string)\n"," * item_category4_enc (categorical; int8, random nominal encoding done by pandas)\n","\n","</br>\n","\n","##**3.  *sales_train_cleaned*** - remove outliers and adjust shops</br>\n","**2,914,268 rows**, corresponding to the original rows ***with outliers removed*** (6 outlier rows deleted)</br>\n","and ***with shops 9 and 13 rows removed*** \n","</br>\n","\n","**6 columns:**\n"," * date (pd.datetime format)\n"," * date_block_num (int8) \n"," * shop_id (categorical; int8, range **2** to 59, and no entries for 9, 11, 13 either, having removed shops 9 and 13, and merged shops 0,1,11)\n"," * item_id (categorical; int16, range 0 to 22,169 (but only 21,671 present in *sales_train_cleaned*))\n"," * item_price (float32; downcast, max is near 60,000)  \n"," * item_cnt_day (int16, range is roughly 0 to 1000)\n","\n","</br>\n","\n","</br>\n","\n","---\n","---\n","\n","##**4a. *sales_train_cln_mrg:***\n","* remove outliers, \n","* adjust shops (remove 9,13, and merge away 0,1,11)\n","* merge with encoded features from *shops_new* and *items_new* dataframes</br>\n","\n","**2,914,268 rows,** corresponding the above *sales_train_cleaned* data set </br>\n","**18 columns,** corresponding to the 6 from *sales_train_cleaned* plus 12 (encoded) categorical columns coming from *shops_new* and *items_new*</br>\n","\n","Column descriptions for sales_train_cln_mrg:</br>\n","\n","| Column Name | DType | Description |\n","| ----------: | :---: | :--------- |\n","| date | datetime64 | ordinal day, month, year of transaction in that row |\n","| date_block_num | int8 | ordinal-encoded month # from start of train data |\n","| shop_id | int8 | categorical range(60) original shop_id values, minus 0,1,9,11,13 |\n","| item_id | int16 | categorical range(22170) original item_id values, 21671 present in *sales_train_cln_mrg* |\n","| item_price | float32 | continuous variable, downcast from float64; price is in range (0 to 59200] |\n","| item_cnt_day | int16 | continuous variable, items sold during the day of the sales_train row; range = [-22 to 669] |\n","| shop_tested | bool | True if shop id is in the test set |\n","| shop_type_enc | int8 | Categorical feature indicating small shop / mall / SEC / online... |\n","| shop_city_enc | int8 | Categorical feature indicating which city hosts the shop |\n","| shop_federal_district_enc | int8 | Categorical feature indicating which federal district the shop is in |\n","| s_type_broad_enc | int8 | Categorical feature like shop_type_enc, but merging together mall/Mega/SEC so fewer categories |\n","| fd_popdens_enc | int8 | Categorical feature indicating population density of the federal district the shop is in |\n","| fd_gdp_enc | int8 | Categorical feature indicating gdp/person for the federal district the shop is in |\n","| item_tested | bool | True if item id is in the test set |\n","| item_category_id | int8 | Original category codes for the items (0 to 83) |\n","| cluster_code | int32 | Categorical grouping of items by name similarity; encoding weighted </br>by avg. strength of the group coupling (19 to 34420; roughly 2000 groups)\n","| item_category3_enc | int8 | reduction of original 84 categories, grouping primarily by item type |\n","| item_category4_enc | int8 | reduction of original 84 categories, grouping primarily by item brand |\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sgAY5jIb-htX","colab_type":"text"},"source":["**Code in this ipynb does the following:**\n","\n","***sales_train*** dataset outliers: \n","\n","* Delete these rows (use this order for deleting if using .iloc): [2909818,2909401,2326930,2257299,1163158,484683]\n","\n","\n","***sales_train*** dataset shop overlap / untested shops: \n","\n","* Combine shop 11 into shop 10 (i.e., wherever you see shop_id == 11, set it to shop_id = 10), so *sales_train* no longer contains any shop 11.\n","* Combine shop 0 into shop 57 (id == 0 --> set id = 57)\n","* Combine shop 1 into shop 58 (id == 1 --> set id = 58)\n","* Delete all *sales_train* rows where shop_id == 9\n","* Delete all *sales_train* rows where shop_id == 13\n","\n","***shops_new*** dataset features:\n","\n","* continue using ***shop_id*** as a categorical feature for training/test; suggest encoding it by weighting with the number of rows present in the ***test set***\n","* add ***shop_type*** as a categorical feature (also recommend encoding these categories by weighting with the number of rows present in ***test*** set)\n","* also see if ***shop_federal_district*** has significant feature importance (also encode based on relative presence in ***test*** set), depending on your time/computing resources.  I will also add ***shop_city*** as another potential feature if you have time/computing resources to try it.\n","* I'm adding 3 other potential features (2 columns: one for text description, and one for integer encoding):  \n","  * s_type_broad (like shop_type, but fewer categories)\n","  * fd_popdens (4-category grouping based on shop federal district population density)\n","  * fd_gdp (3-category grouping based on gdp per person in the shop's federal district)\n","\n","***items_new*** dataset features:\n","\n","* delete ***item_name*** column (just a waste of space for training purposes)\n","* merge with ***item_categories*** dataset to add ***item_category3*** and ***item_category4*** as feature columns.  Encoding randomly done by pandas.\n","* merge with ***items_clustered_22170*** dataset to add ***item_cluster_code*** as a pre-encoded categorical feature.\n","* suggest using ***item_id***, ***item_cluster_code***, ***item_category3_enc***, ***item_category_id***, and ***item_category4_enc*** as features in the model, in that order of importance (depending on your time/computing resources).  \n","\n","</br>\n","\n","</br>\n","\n","**Some Thoughts On The Cleaning and Use of Features Above:**\n","\n","1.  **Category overlap** in the new features of the items dataset:</br>\n","If one or more of these 4 category/cluster groupings shows little feature importance upon training, then drop it from the model inputs.  If one or two of the 4 groupings are substantially higher importance upon training, then suggest you do not use the other 2 or 3 category groupings as features at all. (There is *some* correlation between the categories of two different grouping methods, but I would be concerned that there are also anti-correlations between the different feature variants, and that it is not straightforward for the model to resolve conflicting category information.  I'm guessing that the best results would come from using one or two of the \"broad\" category groupings (cat3 or cat4) and one (and only one) of the \"finer\" category groupings (the original 84-category feature, or the weighted 2000ish-category cluster_code feature)\n","\n","2.  **Shops inclusion and feature priority**:</br>\n","* I suspect *shop_federal_district* will not have a great importance in the training, but I would be interested to see if you find the same conclusion after training and looking at relative feature importance.  Although I'm also including *shop_city* as a possible feature to use, I don't recommend this unless you have plenty of time/computing resources.  If you can, give it a try, and see if it actually does have significant feature importance.\n","* I am dropping shops 9 and 13 from the training set due to their odd behavior and because they are not in the test set.  I don't think there will be worse model performance because of this (I suspect actually it could be significantly better), but if time permits, it would be worth double-checking this hypothesis by training the model with these two shops still in the *sales_train* data set, and see if you see any model improvement.\n","* Similarly, the merging / removal of shops 0, 1, and 11 is being done on a hunch that the similarity of these 3 untested shops with their (tested) merge partners is so strong that the merging to create more data for training tested shops will be beneficial.  The issue is that I have not done a deep dive into the aforementioned similarity.  There is a possibility that this merging of shops could distort the training behavior of time lag, item_id, or item_category if the shop 'similiarity' is not strong enough.  For example,  the 3 shop merges could create a substantially different sales distribution of items/categories/time for the 3 shops being tested. A shallow exploration suggests it's not an issue, but I'm not 100% confident, and we should consider poentially running the model training without merging these shops together.  (The good news is I don't think the 3 merges will have much effect, if any, on any of the other shops.)\n","\n"]},{"cell_type":"code","metadata":{"id":"rzNtOz8qhtQ5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":326},"executionInfo":{"status":"ok","timestamp":1589286964779,"user_tz":240,"elapsed":8512,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"b9832ec8-5995-4b7d-8fa0-f92f01ca76cb"},"source":["df_name_dict = {'shops_features':shops_features,'items_features':items_features,'train_test_base':train_test_base}\n","\n","for k,v in df_name_dict.items():\n","    print(f'{k}: n_rows = {len(v)}, n_cols = {len(v.columns)}\\n{v.columns}\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["shops_new: n_rows = 60, n_cols = 14\n","Index(['shop_id', 'shop_tested', 'shop_type', 'shop_type_enc', 'shop_city', 'shop_city_enc', 'shop_federal_district', 'shop_federal_district_enc', 's_type_broad', 's_type_broad_enc', 'fd_popdens', 'fd_popdens_enc', 'fd_gdp', 'fd_gdp_enc'], dtype='object')\n","\n","items_new: n_rows = 22170, n_cols = 8\n","Index(['item_id', 'item_tested', 'item_category_id', 'cluster_code', 'item_category3', 'item_category3_enc', 'item_category4', 'item_category4_enc'], dtype='object')\n","\n","sales_train_cleaned: n_rows = 2914268, n_cols = 6\n","Index(['date', 'date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day'], dtype='object')\n","\n","sales_train_cln_mrg: n_rows = 2914268, n_cols = 18\n","Index(['date', 'date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day', 'shop_tested', 'shop_type_enc', 'shop_city_enc', 'shop_federal_district_enc', 's_type_broad_enc', 'fd_popdens_enc', 'fd_gdp_enc', 'item_tested', 'item_category_id', 'cluster_code', 'item_category3_enc',\n","       'item_category4_enc'],\n","      dtype='object')\n","\n","test_mrg: n_rows = 214200, n_cols = 15\n","Index(['ID', 'shop_id', 'item_id', 'shop_tested', 'shop_type_enc', 'shop_city_enc', 'shop_federal_district_enc', 's_type_broad_enc', 'fd_popdens_enc', 'fd_gdp_enc', 'item_tested', 'item_category_id', 'cluster_code', 'item_category3_enc', 'item_category4_enc'], dtype='object')\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DON1f1BI7E7F","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}