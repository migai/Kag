{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_cleaning_and_eda_feature_merging_v2_mg.ipynb","provenance":[{"file_id":"1vmc6H9xvSFVlHGzP2lBXBcCcNdPGlMxL","timestamp":1589119317913},{"file_id":"14t_SkT4SYL-JAcrbJIJ60cbNgCrJlvIN","timestamp":1589069755220},{"file_id":"1mFtJLElc2hyopq6yrPAoi0zQVUegsAP5","timestamp":1589018631041},{"file_id":"1y04qp_hoyBnsJQwkX67pk4iZsKGQIqNy","timestamp":1588805435380},{"file_id":"1b_K0QD9U6dofQ7VtTAtzUrqbKJMdj64l","timestamp":1588785261238},{"file_id":"1gcbeu-d1GUUzznZwTzfqYYbaD6cJ7EQ4","timestamp":1588238522691},{"file_id":"1pSGNRDJGzdeI69bw1zWefzPifBq-rv9H","timestamp":1588151557805},{"file_id":"1hq-ivO1BBtc5IC5xd-JdH8HNAQ81bkRf","timestamp":1587386702728},{"file_id":"1I7DWo2B7q7g9Ne2khD11YGTq_gD2FoaT","timestamp":1587321559573},{"file_id":"1fyZv-jgb8twsCBQwPxjgk_XSYA6dOa2t","timestamp":1587303588700},{"file_id":"1iKsplqpLQQZqdr3Trflk7TapksgkQXjX","timestamp":1587145642564},{"file_id":"https://github.com/migai/Kag/blob/master/Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587076517706}],"collapsed_sections":["aJI8Lzq7CZlt","P99yjzAasJva","CZW9d7dfcjDY"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn","colab_type":"text"},"source":["#**Quick processing of data sets for input to model**\n","\n","**Data Cleaning, Feature Generation**\n","\n","Andreas Theodoulou and Michael Gaidis (June, 2020)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"auU4VWkzZvTM"},"source":["#**Data Ouput from This Notebook**\n","\n","##**1. A lightly-cleaned version of sales_train data set, merged with the test data set** (distinguishable as month = 34)\n","* *train_test_base*\n","\n","##**2. Data sets to merge with the aforementioned data set, and also important to merge with Cartesian-Product rows that we insert into the training data.**\n","* *shops_enc*\n","* *items_enc*\n","* *date_adjustments*\n","\n","###The intent is for the user to adapt these data sets as desired, in the IPynb notebook focused on modeling.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m-64o-4yXF4P"},"source":["#**0.1 Mount Google Drive (Local File Storage/Repo For Colab)**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XeWResomXF4j","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1593425251639,"user_tz":240,"elapsed":25186,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"3b713273-a1e1-4029-f02e-f3c8dfe2b2c1"},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, then paste it into the input box and hit 'enter' to complete mounting of the drive\n","from google.colab import drive  \n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zRTE5ZTlXF4w"},"source":["#**0.2 Configure Environment and Load Data Files**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sQ5m2qfIXF4x","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593425252118,"user_tz":240,"elapsed":25641,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"40bec33f-5a19-4b1b-a5d1-751c08144db6"},"source":["# python libraries/modules used throughout this notebook (with some holdovers from other, similar notebooks)\n","# pandas data(database) storage, EDA, and manipulation\n","import pandas as pd\n","### pandas formatting\n","### Here's what I find works well for this particular IPynb, when using a FHD laptop monitor with a full-screen browser window containing my IPynb notebook:\n","pd.set_option(\"display.max_rows\",120)     # Override pandas choice of how many rows to show, so, for example, we can see the full 84-row item_category dataframe instead of the first few rows, then ...., then the last few rows\n","pd.set_option(\"display.max_columns\",26)   # Similar to row code above, we can show more columns than default  \n","pd.set_option(\"display.width\", 230)       # Tune this to our monitor window size to avoid horiz scroll bars in output windows (but, the drawback is that we will get output text wrapping)\n","pd.set_option(\"max_colwidth\", None)       # This is done, for example, so we can see full item name and not '...' in the middle\n","# Try to convince pandas to print without decimal places if a number is actually an integer (helps keep column width down, and highlights data types), or with precision = 3 decimals if a float\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)\n","\n","# Pandas additional enhancements\n","pd.set_option('compute.use_bottleneck', False)  # speed up operation when using NaNs\n","pd.set_option('compute.use_numexpr', False)     # speed up boolean operations, large dataframes; DataFrame.query() and pandas.eval() will evaluate the subexpressions that can be evaluated by numexpr\n","\n","# computations\n","import numpy as np\n","\n","# file operations\n","import os\n","from urllib.parse import urlunparse\n","from pathlib import Path\n","\n","# misc. python enhancements\n","from collections import OrderedDict\n","import time\n","import datetime\n","from time import sleep, localtime, strftime, tzset, strptime\n","os.environ['TZ'] = 'EST+05EDT,M4.1.0,M10.5.0'   # allows user to simply print a formatted version of the local date and time; helps keep track of what cells were run, and when\n","tzset()\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["done: Mon 06:07:35 06/29/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s5CBLhjQXF42","colab":{"base_uri":"https://localhost:8080/","height":850},"executionInfo":{"status":"ok","timestamp":1593433283778,"user_tz":240,"elapsed":2496,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"d90b81ee-eed6-4681-9d56-793cdf754361"},"source":["train_test_base_save = False #True  # set to false if you plan to read in from previously-created csv.gz file\n","\n","GDRIVE_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\"\n","\n","data_files = [  \"data_output/shops_augmented.csv\",\n","                \"data_output/items_clustered_22170b.csv.gz\",\n","                \"data_output/item_categories_augmented.csv\",\n","                \"data_output/days_by_month.csv\",\n","                \"readonly/final_project_data/sales_train.csv.gz\",\n","                \"readonly/final_project_data/test.csv.gz\"\n","                ]\n","\n","\n","# Dict of helper code files, to be loaded and imported {filepath : import_as}\n","code_files = {}  # not used at this time; example dict = {\"helper_code/kaggle_utils_at_mg.py\" : \"kag_utils\"}\n","\n","\n","# GitHub file location info\n","git_hub_url = \"https://raw.githubusercontent.com/migai\"\n","repo_name = 'Kag'\n","branch_name = 'master'\n","base_url = os.path.join(git_hub_url, repo_name, branch_name)\n","\n","if data_files:\n","    print('\\n\\ncsv files source directory: ', end='')\n","    %cd \"{GDRIVE_REPO_PATH}\"\n","\n","    print(\"\\nLoading csv Files from Google Drive repo into Colab...\\n\")\n","\n","    # Loop to load the data files into appropriately-named pandas DataFrames\n","    for path_name in data_files:\n","        filename = path_name.rsplit(\"/\")[-1]\n","        data_frame_name = filename.split(\".\")[0]\n","        exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","        # if data_frame_name == 'sales_train':\n","        #     sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","        print(f'DataFrame {data_frame_name}, shape = {eval(data_frame_name).shape} :')\n","        print(eval(data_frame_name).head(2))\n","        print(\"\\n\")\n","else: \n","    %cd \"{GDRIVE_REPO_PATH}\"\n","    \n","print(f'\\nDataFrame Loading Complete: {strftime(\"%a %X %x\")}\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","csv files source directory: /content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","\n","Loading csv Files from Google Drive repo into Colab...\n","\n","DataFrame shops_augmented, shape = (60, 18) :\n","                       shop_name  shop_id shop_group shop_federal_district  shop_federal_district_enc  shop_tested                       en_shop_name  shop_city_population shop_type  shop_type_enc shop_city  shop_city_enc  \\\n","0  !Якутск Орджоникидзе, 56 фран        0          N               Eastern                         16        False  ! Yakutsk Ordzhonikidze, 56 Franc                235600      Shop             20   Yakutsk             54   \n","1  !Якутск ТЦ \"Центральный\" фран        1          N               Eastern                         16        False       ! Yakutsk TC \"Central\" Franc                235600      Mall             50   Yakutsk             54   \n","\n","  s_type_broad  s_type_broad_enc fd_popdens  fd_popdens_enc        fd_gdp  fd_gdp_enc  \n","0         Shop                10     Remote               5  Intermediate          43  \n","1         Mall                60     Remote               5  Intermediate          43  \n","\n","\n","DataFrame items_clustered_22170b, shape = (22170, 6) :\n","   item_id                                                                                         item_name  item_category_id  item_tested  item_cluster  n_items_in_cluster\n","0        0                                                               movie dvd power in glamor plast dvd                40        False           100                 111\n","1        1  program home and office digital abbyy finereader 12 professional edition full pc digital version                76        False           105                  19\n","\n","\n","DataFrame item_categories_augmented, shape = (84, 9) :\n","        item_category_name  item_category_id                 en_cat_name item_group item_category1 item_category2 item_category3 item_category4  item_cat_tested\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones         AA          Audio             PC    Accessories             PC             True\n","1         Аксессуары - PS2                 1           Accessories - PS2          I    Accessories    PlayStation    Accessories    PlayStation            False\n","\n","\n","DataFrame days_by_month, shape = (35, 15) :\n","   month  year  season  MoY  days_in_M  Suns_in_M  Mons_in_M  Tues_in_M  Weds_in_M  Thus_in_M  Fris_in_M  Sats_in_M  weekday_weight  retail_sales  week_retail_weight\n","0      0  2013       2    1         31          4          4          5          5          5          4          4           0.979         1.052               1.030\n","1      1  2013       3    2         28          4          4          4          4          4          4          4           1.069         1.072               1.146\n","\n","\n","DataFrame sales_train, shape = (2935849, 6) :\n","         date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0  02.01.2013               0       59    22154         999             1\n","1  03.01.2013               0       25     2552         899             1\n","\n","\n","DataFrame test, shape = (214200, 3) :\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","\n","\n","\n","DataFrame Loading Complete: Mon 08:21:27 06/29/20\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aJI8Lzq7CZlt","colab_type":"text"},"source":["#**1. *train_test_base***\n","**3,150,043 rows**, corresponding to the original rows ***with outliers removed or clipped***</br>\n","and ***with identical shops merged together*** \n","</br>\n","\n","**9 columns:**\n"," * month (int8, ordinal, 0 to 34) = date_block_num\n"," * day (int16, ordinal, 0 to 1033) \n"," * week (int8, ordinal, 0 to 148)\n"," * qtr (int8, ordinal, 0 to 12)\n"," * season (int8, categorical, 0 to 3) \n"," * shop_id (int8, categorical, 2 to 10 and 12 to 59)\n"," * item_id (int16, categorical, 0 to 22,169)\n"," * price (float32, continuous, max is near 60,000) = item_price\n"," * sales (int16, continuous, range is roughly -20 to 1000) = item_cnt_day\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eZWaNltRcV3_"},"source":["\n","***sales_train*** dataset outliers: \n","\n","* Clip these rows:\n","```\n","Shop 24, sales of item 20949 --> clip to 200\n","Shop 25, sales of item 20949 --> clip to roughly 200/day\n","```\n","* Delete these rows (use this reverse order for deleting if using .iloc): </br>\n","```\n","[2909818, 2909401, 2326930, 2257299, 1163158, 484683]\n","```\n","\n","***sales_train*** dataset shop overlap \n","```\n","* Combine shop 11 into shop 10  (id == 11 --> set id = 10)\n","* Combine shop  0 into shop 57  (id ==  0 --> set id = 57)\n","* Combine shop  1 into shop 58  (id ==  1 --> set id = 58)\n","```\n","\n","***sales_train*** dataset late-opening shop\n","```\n","* Multiply shop 36 sales by 31/15 to account for it being open only for the last 15 days of training.\n","The user can later scale again as desired to conform to the 30-day test month.\n","```\n","\n","***sales_train merge with test***\n","```\n","* Append the test shop-item pairs to the sales_train data set, so merging and feature generation\n","have the option of including these rows as well.\n","```\n","\n","***Additional time-based features***\n","```\n","Replace the \"date\" and \"date_block_num\" columns with:\n","    1. 'day'    = integer value of day number, \n","                starting at day = 0 for the first training set transaction, and incrementing by \"calendar\" day number \n","                (not by \"transaction\" day number).\n","                Thus, 'day' may not include all possible integers from start to finish.  \n","                It only assigns integer values (based on the calendar) to days when there are transactions in the \n","                input dataframe --> if the input dataframe has no transactions on a particular day, that day's \n","                \"calendar\" integer value will not be present in the column.\n","    2. 'week'   = integer value of week number, \n","                however, unlike 'day', the 'week' number is aligned not to start at the first training set transaction, \n","                but rather so that there is a full 'week' of 7 days that ends on Oct. 31, 2015 (the final day of training data).  \n","                If using the full sales_train data set, this results in week = 0 having only 5 days in it. \n","                The final week of October, 2015 is assigned 'week' number = 147.  \n","                Arbitrarily assigning test to \"Nov. 1, 2015\" results in test week = 148\n","    3. 'month'  = renamed from \"date_block_num\" of original data set (no changes).  \n","                Integer values from 0 to 33 represent the months starting at day0.  Test month == 34 is Nov. 2015.\n","    4. 'qtr'    = quarter = integer number of 3-month chunks of time, aligned with the end of October, 2015.  \n","                day0 is included in 'qtr' = 0, but 'qtr'=0 only contains 1 month (Jan 2013) of data due to the alignment.\n","                The months of August, Sept, Oct 2015 form 'qtr' = 11.  \"qtr\" in this sense is just 3-month chunks... \n","                it is not the traditional Q1,Q2,Q3,Q4 beginning Jan 1, but instead is more like date_block_num in that it \n","                is monotonically increasing integers, incremented every 3 months.\n","    5. 'season' = integer number of 3-month chunks of time, reset each year (allowed values = 0,1,2,3)... \n","                not quite the same as spring-summer-winter-fall, or Q1,Q2,Q3,Q4, but instead shifted to \n","                better capture seasonal spending trends aligned in particular with high December spending\n","                2 = Dec 1 to Feb 28 (biggest spending season), 3 = Mar 1 to May 31, \n","                0 = June 1 to Aug 30 (lowest spending season), 1 = Sept 1 to Nov 30\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"61sfwjHEjk5R","colab_type":"text"},"source":["##Code Output:\n","```\n","train_test_base dataframe creation started: Sat 14:11:09 06/20/20\n","\n","Shape of original sales_train data set = (2935849, 6)\n","Rows being clipped:\n","  1,501,160 sales clipped to 200\n","  1,708,207 sales clipped to 200\n","  2,296,209 sales clipped to 100\n","  2,341,308 sales clipped to 100\n","Rows being deleted:\n","  2909818\n","  2909401\n","  2326930\n","  2257299\n","  1163158\n","  484683\n","Shape of sales_train_cleaned after 6 outlier rows were removed: (2935843, 6)\n","Shape of sales_train_cleaned after merging shops as in {0: 57, 1: 58, 11: 10}: (2935843, 6)\n","Shops being scaled:\n","  shop 36 scaled by 2.07\n","Shape of traintest after appending test to sales_train_cleaned: (3150043, 6)\n","Shape of traintest after creating time-based feature columns: (3150043, 9)\n","traintest DataFrame creation done: Sat 14:13:28 06/20/20\n","\n","/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","train_test_base.csv.gz file stored on google drive in data_output directory\n","train_test_base file save done: Sat 14:14:02 06/20/20\n","\n","Example: display(train_test_base[train_test_base.week == 102].tail(2))\n","\n","           day    week    qtr    season    month    price    sales  shop_id  item_id\n","2257039\t718\t  102\t  8\t    1\t    23      399\t    1\t59\t21970\n","2257040\t718\t  102\t  8\t    1\t    23\t  499\t    1\t59\t22060\n","\n","train_test_base done: Sat 14:14:02 06/20/20\n","```"]},{"cell_type":"markdown","metadata":{"id":"iiCw8FkBFStA","colab_type":"text"},"source":["##**1.1 Merge data sets and create day, week, quarter, and season feature columns**"]},{"cell_type":"code","metadata":{"id":"ZdhfLlXwxhRe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"status":"ok","timestamp":1592676666365,"user_tz":240,"elapsed":32669,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"71e07b14-9fe7-44c4-c475-3edbe21c780d"},"source":["# determine which rows I need to clip for shops 24 and 25\n","stclip = sales_train.query('((shop_id == 24) and (item_cnt_day > 100)) or ((shop_id == 25) and (item_cnt_day > 100))')\n","stclip"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>date_block_num</th>\n","      <th>shop_id</th>\n","      <th>item_id</th>\n","      <th>item_price</th>\n","      <th>item_cnt_day</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>862929</th>\n","      <td>17.09.2013</td>\n","      <td>8</td>\n","      <td>25</td>\n","      <td>3732</td>\n","      <td>2,545.135</td>\n","      <td>264</td>\n","    </tr>\n","    <tr>\n","      <th>862945</th>\n","      <td>17.09.2013</td>\n","      <td>8</td>\n","      <td>25</td>\n","      <td>3734</td>\n","      <td>2,548.455</td>\n","      <td>110</td>\n","    </tr>\n","    <tr>\n","      <th>868495</th>\n","      <td>05.09.2013</td>\n","      <td>8</td>\n","      <td>25</td>\n","      <td>2808</td>\n","      <td>999</td>\n","      <td>133</td>\n","    </tr>\n","    <tr>\n","      <th>1501160</th>\n","      <td>15.03.2014</td>\n","      <td>14</td>\n","      <td>24</td>\n","      <td>20949</td>\n","      <td>5</td>\n","      <td>405</td>\n","    </tr>\n","    <tr>\n","      <th>1708207</th>\n","      <td>28.06.2014</td>\n","      <td>17</td>\n","      <td>25</td>\n","      <td>20949</td>\n","      <td>5</td>\n","      <td>501</td>\n","    </tr>\n","    <tr>\n","      <th>2176634</th>\n","      <td>18.11.2014</td>\n","      <td>22</td>\n","      <td>25</td>\n","      <td>3733</td>\n","      <td>3,070.565</td>\n","      <td>147</td>\n","    </tr>\n","    <tr>\n","      <th>2296209</th>\n","      <td>30.12.2014</td>\n","      <td>23</td>\n","      <td>25</td>\n","      <td>20949</td>\n","      <td>5</td>\n","      <td>205</td>\n","    </tr>\n","    <tr>\n","      <th>2341308</th>\n","      <td>17.01.2015</td>\n","      <td>24</td>\n","      <td>25</td>\n","      <td>20949</td>\n","      <td>5</td>\n","      <td>222</td>\n","    </tr>\n","    <tr>\n","      <th>2567454</th>\n","      <td>14.04.2015</td>\n","      <td>27</td>\n","      <td>25</td>\n","      <td>3731</td>\n","      <td>1,941.995</td>\n","      <td>207</td>\n","    </tr>\n","    <tr>\n","      <th>2615667</th>\n","      <td>19.05.2015</td>\n","      <td>28</td>\n","      <td>25</td>\n","      <td>10209</td>\n","      <td>1,490.892</td>\n","      <td>148</td>\n","    </tr>\n","    <tr>\n","      <th>2615680</th>\n","      <td>19.05.2015</td>\n","      <td>28</td>\n","      <td>25</td>\n","      <td>10210</td>\n","      <td>3,496.761</td>\n","      <td>134</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","862929   17.09.2013               8       25     3732   2,545.135           264\n","862945   17.09.2013               8       25     3734   2,548.455           110\n","868495   05.09.2013               8       25     2808         999           133\n","1501160  15.03.2014              14       24    20949           5           405\n","1708207  28.06.2014              17       25    20949           5           501\n","2176634  18.11.2014              22       25     3733   3,070.565           147\n","2296209  30.12.2014              23       25    20949           5           205\n","2341308  17.01.2015              24       25    20949           5           222\n","2567454  14.04.2015              27       25     3731   1,941.995           207\n","2615667  19.05.2015              28       25    10209   1,490.892           148\n","2615680  19.05.2015              28       25    10210   3,496.761           134"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"YY7-Idw86ST2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1592676666369,"user_tz":240,"elapsed":32647,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"35fac221-0b7c-404d-f033-d51830116acb"},"source":["def clean_merge_augment(day0 = datetime.datetime(2013,1,1),\n","                        clip_rows = {1501160: 200, 1708207: 200, 2296209: 100, 2341308: 100},\n","                        delete_rows = [2909818, 2909401, 2326930, 2257299, 1163158, 484683],\n","                        merge_shops = {0: 57, 1: 58, 11: 10},\n","                        scale_shops = {36: 31/15},\n","                        dropout_repair = {},\n","                        delete_shops = []):\n","    \"\"\"\n","    Parameters:\n","    day0 = datetime.datetime object representing the day you wish to use as your reference when creating time-based features\n","    clip_rows = not quite as bad as erroneous outliers, but sales are so unlike other days that clipping should help\n","    delete_rows = list of integer row numbers that you wish to delete from the sales_train data set, e.g., from outliers/erroneous rows\n","    merge_shops = dictionary of integer shop_id key:value pairs where shop(=key) is merged into shop(=value)\n","    scale_shops = artificially adjust sales amounts for shops that are only open for partial amounts of months\n","    dropout_repair = optional, fill in the dropouts where shops are apparently erroneously missing sales (I am pushing this for now, as it looks to be of only marginal use, and not easy to do in a robust way)\n","    delete_shops = optional, can delete shops if you think they are not of value to training (I am pushing this to the modeling IPynb for easier iteration)\n","\n","    Global Variables: this function assumes you have the following pandas dataframes available globally:\n","    1) unaltered sales_train\n","    2) unaltered test\n","\n","    This function does the following:\n","    1) clips moderate outlier rows\n","    2) cleans (deletes) severe outlier rows from the training set that appear to be erroneous or irrelevant entries\n","    3) merges 3 shops into other shops where it appears that the sales_train set simply has different names for the \n","        same shop at different time periods (shop 0 absorbed by 57; shop 1 absorbed by 58, shop 11 absorbed by 10)\n","    4) optionally delete shops entirely from the sales_train data set (e.g., for irrelevant shops)\n","    5) append the test set rows to the sales_train rows, using a date of November 1, 2015 for test\n","    6) adjust the 'date' column on the merged dataset to be in datetime format, so it looks like a string of format: 'YYYY-M-D'\n","\n","    Then, creates and inserts new time-based feature columns as follows:\n","    Given a dataframe with a 'date' column containing strings like '2015-10-30', create new time-series columns:\n","    1. 'day'    = integer value of day number, starting at day = 0 for parameter day0, and incrementing by calendar day number (not by transaction day number)... \n","                    Thus, 'day' may not include all possible integers from start to finish.  It only assigns integer values (based on the calendar) to days when \n","                    there are transactions in the input dataframe --> if the input dataframe has no transactions on a particular day, that day's 'calendar' integer \n","                    value will not be present in the column (will be = 0)\n","    2. 'week'   = integer value of week number, with week = 0 at time= parameter day0.  However, unlike 'day', the 'week' number is aligned not to start at day0, but rather\n","                    so that there is a full 'week' of 7 days that ends on Oct. 31, 2015 (the final day of training data).  This results in week = 0 having only 5 days in it.\n","                    n.b., the final week of October, 2015 is assigned 'week' number = 147.  Artifically assigning test to Nov. 1, 2015 results in test week = 148\n","    3. 'month'  = renamed from \"date_block_num\" of original data set (no changes).  Integer values from 0 to 33 represent the months starting at day0.  Test month=34 is Nov. 2015.\n","    4. 'qtr'    = quarter = integer number of 3-month chunks of time, aligned with the end of October, 2015.  day0 is included in 'qtr' = 0, but 'qtr'=0 only contains 1 month (Jan 2013) of data due to the alignment\n","                    The months of August, Sept, Oct 2015 form 'qtr' = 11.  \"qtr\" in this sense is just 3-month chunks... it is not the traditional Q1,Q2,Q3,Q4 beginning Jan 1, but instead is more like\n","                    date_block_num in that it is monotonically increasing integers, incremented every 3 months such that #11 ends at the end of our training data\n","    5. 'season' = integer number of 3-month chunks of time, reset each year (allowed values = 0,1,2,3)... not quite the same as spring-summer-winter-fall, or Q1,Q2,Q3,Q4, but instead shifted to \n","                    better capture seasonal spending trends aligned in particular with high December spending\n","                    2 = Dec 1 to Feb 28 (biggest spending season), 3 = Mar 1 to May 31, 0 = June 1 to Aug 30 (lowest spending season), 1 = Sept 1 to Nov 30\n","\n","    Finally, drop the date column from the dataframe, and sort the dataframe by ['day','shop_id','item_id']  (original dataframe seems to be sorted by month, but unsorted within each month)\n","\n","    returns: the cleaned/dated/feature-augmented DataFrame\n","    \"\"\"\n","\n","    print(f'Shape of original sales_train data set = {sales_train.shape}')\n","\n","    # clip moderate outliers (first make a DataFrame copy so we can reuse sales_train later, if we need to)\n","    sales_train_cleaned = sales_train.copy(deep=True)\n","    if clip_rows:\n","        print('Rows being clipped:')\n","        for k,v in clip_rows.items(): \n","            sales_train_cleaned.at[k,'item_cnt_day'] = v\n","            print(f'  {k:,d} sales clipped to {v}')\n","\n","    # remove outlier rows from training set \n","    print('Rows being deleted:')\n","    for i in sorted(delete_rows, reverse=True):   # delete the rows in reverse order to be sure we don't run into issues with indexing\n","        print(f'  {i}')\n","        sales_train_cleaned.drop(sales_train_cleaned.index[i],inplace=True)\n","    print(f'Shape of sales_train_cleaned after {len(delete_rows)} outlier rows were removed: {sales_train_cleaned.shape}')\n","    \n","    # Merge the 3 shops we are nearly certain must correctly fit into the other shops' dropout regions:\n","    sales_train_cleaned.shop_id = sales_train_cleaned.shop_id.replace(merge_shops)\n","    print(f'Shape of sales_train_cleaned after merging shops as in {merge_shops}: {sales_train_cleaned.shape}')\n","\n","    # scale shops if desired\n","    if scale_shops:\n","        print('Shops being scaled:')\n","        for k,v in scale_shops.items(): \n","            sales_train_cleaned.item_cnt_day = sales_train_cleaned.apply(lambda row: row.item_cnt_day * v if row.shop_id == k else row.item_cnt_day, axis = 1)\n","            print(f'  shop {k} scaled by {v:.2f}')\n","\n","    # Remove irrelevant shops entirely from the sales_train_cleaned DataFrame:\n","    if delete_shops:\n","        sales_train_cleaned = sales_train_cleaned.query('shop_id != @delete_shops')\n","        print(f'Shape of sales_train_cleaned after deleting shops {delete_shops}: {sales_train_cleaned.shape}')\n","\n","    # sales_train_cleaned = sales_train_cleaned[sales_train_cleaned.shop_id != 9]\n","    # sales_train_cleaned = sales_train_cleaned[sales_train_cleaned.shop_id != 13]\n","    # print(f'Shape of sales_train_cleaned after removal of shops: {sales_train_cleaned.shape})\n","    # print(f'{sales_train_cleaned.shop_id.nunique()} shops remaining in sales_train_cleaned DataFrame: {sorted(sales_train_cleaned.shop_id.unique())})\n","\n","    sales_train_cleaned = sales_train_cleaned.astype({'date_block_num':np.int8,'shop_id':np.int8,'item_id':np.int16,\n","                                                    'item_price':np.float32,'item_cnt_day':np.int16}).reset_index(drop=True)\n","\n","    # merge dataframes so we optionally include test elements in our EDA and feature generation\n","    test_prep = test.copy(deep=True)\n","    test_prep['date_block_num'] = 34\n","    test_prep['date'] = '1.11.2015' #pd.Timestamp(year=2015, month=11, day=1)\n","    traintest = sales_train_cleaned.append(test_prep).fillna(0)\n","\n","    traintest = traintest[['date', 'date_block_num', 'item_price', 'item_cnt_day', 'shop_id', 'item_id']]\n","    traintest.columns = ['date', 'month', 'price', 'sales', 'shop_id', 'item_id']\n","    print(f'Shape of traintest after appending test to sales_train_cleaned: {traintest.shape}')\n","        \n","    # Add in the time-based feature columns\n","    traintest.date =  pd.to_datetime(traintest.date, dayfirst=True, infer_datetime_format=True)\n","    traintest.insert(1,'day', traintest.date.apply(lambda x: (x - day0).days))\n","    traintest.insert(2,'week', (traintest.day+2) // 7 )             # add the 2 days so we have end of a week coinciding with end of training data Oct. 31, 2015\n","    traintest.insert(3,'qtr', (traintest.month + 2) // 3 )          # add the 2 months so we have end of a quarter aligning with end of training data Oct. 31, 2015\n","    traintest.insert(4,'season', (traintest.month + 2) % 4 ) \n","    traintest.drop('date',axis=1,inplace=True)\n","    # note that the train dataset is sorted by month, but nothing obvious within the month; we sort it here for consistent results in calculations below\n","    traintest = traintest.sort_values(['day','shop_id','item_id']).reset_index(drop=True)  \n","    print(f'Shape of traintest after creating time-based feature columns: {traintest.shape}')\n","    print(f'traintest DataFrame creation done: {strftime(\"%a %X %x\")}\\n')\n","    return traintest\n","\n","print(f'\\nDone: {strftime(\"%a %X %x\")}\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Done: Sat 14:11:09 06/20/20\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j83GHWtrz_Gy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":587},"executionInfo":{"status":"ok","timestamp":1592676839891,"user_tz":240,"elapsed":206134,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"468f9df1-f188-4cc5-ddd9-3a26cdeb767f"},"source":["if train_test_base_save:\n","    print(f'train_test_base dataframe creation started: {strftime(\"%a %X %x\")}\\n')\n","    train_test_base = clean_merge_augment()\n","\n","    %cd \"{GDRIVE_REPO_PATH}\"\n","    # can save as csv.gz for < 100 MB storage and sync with GitHub\n","    compression_opts = dict(method='gzip',\n","                            archive_name='train_test_base.csv')  \n","    train_test_base.to_csv('data_output/train_test_base.csv.gz', index=False, compression=compression_opts)\n","    print(\"train_test_base.csv.gz file stored on google drive in data_output directory\")\n","    print(f'train_test_base file save done: {strftime(\"%a %X %x\")}')\n","\n","display(train_test_base[train_test_base.week == 102].tail(2))\n","\n","print(f'\\ntrain_test_base done: {strftime(\"%a %X %x\")}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train_test_base dataframe creation started: Sat 14:11:09 06/20/20\n","\n","Shape of original sales_train data set = (2935849, 6)\n","Rows being clipped:\n","  1,501,160 sales clipped to 200\n","  1,708,207 sales clipped to 200\n","  2,296,209 sales clipped to 100\n","  2,341,308 sales clipped to 100\n","Rows being deleted:\n","  2909818\n","  2909401\n","  2326930\n","  2257299\n","  1163158\n","  484683\n","Shape of sales_train_cleaned after 6 outlier rows were removed: (2935843, 6)\n","Shape of sales_train_cleaned after merging shops as in {0: 57, 1: 58, 11: 10}: (2935843, 6)\n","Shops being scaled:\n","  shop 36 scaled by 2.07\n","Shape of traintest after appending test to sales_train_cleaned: (3150043, 6)\n","Shape of traintest after creating time-based feature columns: (3150043, 9)\n","traintest DataFrame creation done: Sat 14:13:28 06/20/20\n","\n","/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","train_test_base.csv.gz file stored on google drive in data_output directory\n","train_test_base file save done: Sat 14:14:02 06/20/20\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>day</th>\n","      <th>week</th>\n","      <th>qtr</th>\n","      <th>season</th>\n","      <th>month</th>\n","      <th>price</th>\n","      <th>sales</th>\n","      <th>shop_id</th>\n","      <th>item_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2257039</th>\n","      <td>718</td>\n","      <td>102</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>399</td>\n","      <td>1</td>\n","      <td>59</td>\n","      <td>21970</td>\n","    </tr>\n","    <tr>\n","      <th>2257040</th>\n","      <td>718</td>\n","      <td>102</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>499</td>\n","      <td>1</td>\n","      <td>59</td>\n","      <td>22060</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         day  week  qtr  season  month  price  sales  shop_id  item_id\n","2257039  718   102    8       1     23    399      1       59    21970\n","2257040  718   102    8       1     23    499      1       59    22060"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","train_test_base done: Sat 14:14:02 06/20/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P99yjzAasJva"},"source":["#**2. *shops_enc*** </br>\n","**60 rows**, corresponding to the 60 original shops</br>\n","**9 columns:**\n"," 1. shop_id  (categorical; int8, from original data set)\n"," 2. shop_tested (categorical; int8 0/1, indicating if the shop is in *test* set)\n"," 3. shop_group (categorical; tighter-grouped clusters were assigned letters first, so tend to be closer to A than to Z)\n"," 4. shop_type (categorical; online, small shop, mall, SEC, Mega)\n"," 5. s_type_broad (categorical; like shop_type, but fewer categories by merging together \"Mall\",\"Mega\",\"SEC\")\n"," 6. shop_federal_district (categorical)\n"," 7. fd_popdens (categorical; 4 categories named by population density in the shop's fed district)\n"," 8. fd_gdp (categorical; 3 categories named by gdp per person)\n"," 9. shop_city (categorical)"]},{"cell_type":"code","metadata":{"id":"g9G4EXQ8lcK2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593425922932,"user_tz":240,"elapsed":449,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"37368cf1-ea86-4f07-f370-9bf73e8abd82"},"source":["print(shops_augmented.columns)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['shop_name', 'shop_id', 'shop_group', 'shop_federal_district', 'shop_federal_district_enc', 'shop_tested', 'en_shop_name', 'shop_city_population', 'shop_type', 'shop_type_enc', 'shop_city', 'shop_city_enc', 's_type_broad',\n","       's_type_broad_enc', 'fd_popdens', 'fd_popdens_enc', 'fd_gdp', 'fd_gdp_enc'],\n","      dtype='object')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QtY2XXrck9OB","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1593431038459,"user_tz":240,"elapsed":449,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"239bfcbe-9281-423a-e92b-4780347eb14a"},"source":["shops_enc = shops_augmented[['shop_id', 'shop_tested', 'shop_group', 'shop_type', 's_type_broad', 'shop_federal_district', 'fd_popdens', 'fd_gdp', 'shop_city']].copy(deep=True)\n","shops_enc.shop_tested = shops_enc.shop_tested.astype(np.int8)\n","for c in ['shop_group', 'shop_type', 's_type_broad', 'shop_federal_district', 'fd_popdens', 'fd_gdp', 'shop_city']:\n","    shops_enc[c] = shops_enc[c].astype('category')\n","    shops_enc[c] = shops_enc[c].cat.codes\n","    print(f'Column {c} number of unique category codes: {shops_enc[c].nunique()}')\n","print('\\n')\n","display(shops_enc.head())\n","\n","shops_enc.to_csv(\"data_output/shops_enc.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Column shop_group number of unique category codes: 14\n","Column shop_type number of unique category codes: 6\n","Column s_type_broad number of unique category codes: 3\n","Column shop_federal_district number of unique category codes: 8\n","Column fd_popdens number of unique category codes: 4\n","Column fd_gdp number of unique category codes: 3\n","Column shop_city number of unique category codes: 29\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>shop_id</th>\n","      <th>shop_tested</th>\n","      <th>shop_group</th>\n","      <th>shop_type</th>\n","      <th>s_type_broad</th>\n","      <th>shop_federal_district</th>\n","      <th>fd_popdens</th>\n","      <th>fd_gdp</th>\n","      <th>shop_city</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>23</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   shop_id  shop_tested  shop_group  shop_type  s_type_broad  shop_federal_district  fd_popdens  fd_gdp  shop_city\n","0        0            0           7          5             2                      1           3       1         26\n","1        1            0           7          1             0                      1           3       1         26\n","2        2            1           9          2             0                      5           0       2          0\n","3        3            1           6          1             0                      0           2       1          1\n","4        4            1           9          1             0                      5           0       2         23"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CZW9d7dfcjDY"},"source":["#**3. *items_enc*** </br>\n","**22,170 rows**, corresponding to the 22,170 original items</br>\n","**10 columns:**\n"," 1. item_id  (categorical; int8, from original data set)\n"," 2. item_tested (categorical; int8 0/1, indicating if the item is in *test* set)\n"," 3. item_cluster (categorical; int; grouping from item sales correlations)\n"," ---\n"," 1. item_category_id  (categorical; int8, from original data set)\n"," 2. item_cat_tested (categorical; int8 0/1, indicating if the item category is in *test* set)\n"," 3. item_group (categorical; int8)\n"," 4. item_category1 (categorical; int8)\n"," 5. item_category2 (categorical; int8)\n"," 6. item_category3 (categorical; int8)\n"," 7. item_category4 (categorical; int8)\n"," "]},{"cell_type":"code","metadata":{"id":"ag88eVurf50h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"ok","timestamp":1593431050193,"user_tz":240,"elapsed":500,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"4945e28c-eb78-49c8-d969-e250c5cf2208"},"source":["items_enc = items_clustered_22170b[['item_id','item_tested','item_cluster','item_category_id']].copy(deep=True)\n","items_enc = items_enc.merge(item_categories_augmented[['item_category_id','item_cat_tested','item_group','item_category1','item_category2','item_category3','item_category4']],how='left',on='item_category_id')\n","\n","items_enc.item_tested = items_enc.item_tested.astype(np.int8)\n","items_enc.item_cat_tested = items_enc.item_cat_tested.astype(np.int8)\n","for c in ['item_group','item_category1','item_category2','item_category3','item_category4']:\n","    items_enc[c] = items_enc[c].astype('category')\n","    items_enc[c] = items_enc[c].cat.codes\n","    print(f'Column {c} number of unique category codes: {items_enc[c].nunique()}')\n","print('\\n')\n","display(items_enc.head())\n","\n","items_enc.to_csv(\"data_output/items_enc.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Column item_group number of unique category codes: 27\n","Column item_category1 number of unique category codes: 13\n","Column item_category2 number of unique category codes: 10\n","Column item_category3 number of unique category codes: 12\n","Column item_category4 number of unique category codes: 8\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>item_tested</th>\n","      <th>item_cluster</th>\n","      <th>item_category_id</th>\n","      <th>item_cat_tested</th>\n","      <th>item_group</th>\n","      <th>item_category1</th>\n","      <th>item_category2</th>\n","      <th>item_category3</th>\n","      <th>item_category4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>100</td>\n","      <td>40</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>105</td>\n","      <td>76</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>11</td>\n","      <td>6</td>\n","      <td>10</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>100</td>\n","      <td>40</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>110</td>\n","      <td>40</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>116</td>\n","      <td>40</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id  item_tested  item_cluster  item_category_id  item_cat_tested  item_group  item_category1  item_category2  item_category3  item_category4\n","0        0            0           100                40                1           6               8               3               7               3\n","1        1            0           105                76                1           6              11               6              10               5\n","2        2            0           100                40                1           6               8               3               7               3\n","3        3            0           110                40                1           6               8               3               7               3\n","4        4            0           116                40                1           6               8               3               7               3"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VCH1Xso68dWv"},"source":["#**4. *date_adjustments*** </br>\n","**35 rows**, corresponding to the 34 training months + 1 test month</br>\n","**8 columns:**\n"," 1. month  (numerical; int8 0-35, from original data set = date_block_num)\n"," 2. year (numerical; int16, 2013-2015)\n"," 3. season (categorical; int8 0-3, 3-month chunks aligned with seasons & shopping trends, repeating each year)\n"," 4. MoY  (categorical; int8 1-12, month of the year)\n"," 5. days_in_M (numerical; int8 28-31, number of days in that row's month)\n"," 6. weekday_weight (numerical, float, scaling for weekly shopping trends)\n"," 7. retail_sales (numerical, float, scaling for Russian economy)\n"," 8. week_retail_weight (numerical, float, scaling for days_in_M, weekday_weight, and retail_sales combined)\n"," \n","\n","* to normalize sales per month by number of days in month (28-31), multiply by ( 30 / (column \"days_in_M\"))\n","* to normalize sales per month by number of days in month, number of each weekday (Sun, Mon, Tues...) in month (mean sales over all 34 train months), multiply by column \"weekday_weight\"\n","* to normalize sales per month by recorded retail sales per month numbers for Russia, multiply by column \"retail_sales\"\n","* to normalize sales per month by number of days in month (28-31), number of each weekday (mean sales over all 34 train months), and retail sales numbers for Russia, multiply by column \"week_retail_weight\"\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M2tINZaK8dW2","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1593433304739,"user_tz":240,"elapsed":401,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"a9b349e5-dba6-4f53-c9e7-ce1cb41a4e03"},"source":["date_adjustments = days_by_month[['month','year','season','MoY','days_in_M','weekday_weight','retail_sales','week_retail_weight']].copy(deep=True)\n","display(date_adjustments.head())\n","\n","date_adjustments.to_csv(\"data_output/date_adjustments.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>month</th>\n","      <th>year</th>\n","      <th>season</th>\n","      <th>MoY</th>\n","      <th>days_in_M</th>\n","      <th>weekday_weight</th>\n","      <th>retail_sales</th>\n","      <th>week_retail_weight</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2013</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>31</td>\n","      <td>0.979</td>\n","      <td>1.052</td>\n","      <td>1.030</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2013</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>28</td>\n","      <td>1.069</td>\n","      <td>1.072</td>\n","      <td>1.146</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2013</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>31</td>\n","      <td>0.946</td>\n","      <td>0.989</td>\n","      <td>0.936</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2013</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>30</td>\n","      <td>1.010</td>\n","      <td>0.989</td>\n","      <td>0.999</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>2013</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>31</td>\n","      <td>0.973</td>\n","      <td>0.966</td>\n","      <td>0.940</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   month  year  season  MoY  days_in_M  weekday_weight  retail_sales  week_retail_weight\n","0      0  2013       2    1         31           0.979         1.052               1.030\n","1      1  2013       3    2         28           1.069         1.072               1.146\n","2      2  2013       0    3         31           0.946         0.989               0.936\n","3      3  2013       1    4         30           1.010         0.989               0.999\n","4      4  2013       2    5         31           0.973         0.966               0.940"]},"metadata":{"tags":[]}}]}]}