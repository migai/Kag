{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"items_nlp_cats_shops_clusters_v5_mg.ipynb","provenance":[{"file_id":"1XyM72BkhI503rOTsWAWIDyYyR-Gi1HWH","timestamp":1590658110496},{"file_id":"14t_SkT4SYL-JAcrbJIJ60cbNgCrJlvIN","timestamp":1589069755220},{"file_id":"1mFtJLElc2hyopq6yrPAoi0zQVUegsAP5","timestamp":1589018631041},{"file_id":"1y04qp_hoyBnsJQwkX67pk4iZsKGQIqNy","timestamp":1588805435380},{"file_id":"1b_K0QD9U6dofQ7VtTAtzUrqbKJMdj64l","timestamp":1588785261238},{"file_id":"1gcbeu-d1GUUzznZwTzfqYYbaD6cJ7EQ4","timestamp":1588238522691},{"file_id":"1pSGNRDJGzdeI69bw1zWefzPifBq-rv9H","timestamp":1588151557805},{"file_id":"1hq-ivO1BBtc5IC5xd-JdH8HNAQ81bkRf","timestamp":1587386702728},{"file_id":"1I7DWo2B7q7g9Ne2khD11YGTq_gD2FoaT","timestamp":1587321559573},{"file_id":"1fyZv-jgb8twsCBQwPxjgk_XSYA6dOa2t","timestamp":1587303588700},{"file_id":"1iKsplqpLQQZqdr3Trflk7TapksgkQXjX","timestamp":1587145642564},{"file_id":"https://github.com/migai/Kag/blob/master/Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587076517706}],"collapsed_sections":["PmD9Z6DNYEZK"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn","colab_type":"text"},"source":["#**Investigation of *items* database and correlations between items**\n","\n","**EDA, NLP, Feature Generation**\n","\n","Andreas Theodoulou and Michael Gaidis (May, 2020)"]},{"cell_type":"code","metadata":{"id":"kKNGyP8QYBGY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zoa3T6b2YDar","colab_type":"text"},"source":["##**summary of Version 3 additions, May 25, 2020:**"]},{"cell_type":"markdown","metadata":{"id":"DcZ3MkTtYbTD","colab_type":"text"},"source":["1. Try to reduce the number of clusters to something closer to 250 instead of 2000+... (doing this after reading that decision trees don't like to have to many categories within a single feature)"]},{"cell_type":"markdown","metadata":{"id":"PmD9Z6DNYEZK","colab_type":"text"},"source":["##**summary of Version 2, May 9, 2020:**"]},{"cell_type":"markdown","metadata":{"id":"L6ErXnphuc2t","colab_type":"text"},"source":["I refined the \"delimiter\" characters, and did a bit more cleaning on some stuff I noticed with \"blu-ray\" vs. \"bluray\" vs. \"bd\"...\n","--> generated a new set of unique n-grams for n=1 to highest n, and filtered to include only where there are at least 2 item names containing that n-gram\n","\n","--> created \"word vector\" representations of the item names in items dataset, including roughly 4000 elements (all of the delimited n-grams mentioned above)\n","\n","--> for each of the 21700 items, they were encoded as word vectors, and then I used dot-product to identify which items were similar to others.  In the encoding of the word vectors, I did some \"weighting\" such that I didn't just have a word vector that was all 0's except for 1's in locations representing the particular n-grams that are found in the item name string (English translation).  I used 2 types of weighting: \n","1. like TF-IDF, I counted the number of occurrences of each of the roughly 4000 n-grams in the set of all item names, and I binned the n-grams to more heavily weight n-grams that have less representation in the item names. (For example, \"klompferstietnitz\" is more valuable than \"dvd\", because so many item names contain \"dvd\".  So, the former 1-gram gets a larger integer inserted into its location in the word vector.)  I used binning rather than strict TF-IDF \"continuous\" weighting because I believe there is a cutoff at which a term no longer holds much weight at all.\n","2. longer n-grams get heavier weight as well... if two item names have the same 10-word string (10-gram), it is much more relevant than if they have the same 1-word string (1-gram).\n","\n","--> after running the dot products between items in the 21700 x 4000 size matrix containing word vectors for each item, I end up with a 21700 x 21700 matrix containing integers = dot products of the word vectors i,j for cell at i,j coming from item i and item j.\n","\n","--> then, I found a nifty jit-accelerated function (reference below) to pick out the top K largest dot-product values for a given item.  The function gives me the top K items and their dot-product values with the item of interest.  I run this function on all 21700 items, and pick (at first the top 3, but now...) the top 10 highest dot-product values for a given item.\n","\n","**To date** I have only then taken the 5100 items that we know are in the test set (I was afraid of overwhelming the computing power or memory allocation in Colab).  \n","--> so, now I have a dataset with 5100 rows (each test item), and columns for the top-10 matching item ids as per the dot product, and for the actual top-10 dot products also.\n","\n","--> I \"explode\" (unravel) the list of 10 matching items and dot product values so now I have 51000 rows and columns indicating item id of interest (one of the 5100 in the test set), the top-10 matching item ids, and a column for the dot product values between the two.\n","\n","**Now we need to create features from this**\n","\n","**First, look for clusters of tightly-matched item-item pairs**\n","\n","I decided to use the networkX package to map these item pairs into an undirected graph with nodes = item ids, and edges weighted by the value of the dot product.  Then, I can utilize some of the pre-made algorithms that can automatically identify strongly-clustered groups.\n","\n","Before feeding the 51000 row matrix into the graph, I applied a threshold so only dot products above a certain value would be allowed as nodes/edges in the graph.  (This is to filter out \"matches\" where both items have some common term like \"dvd\" but nothing else.  But, as some of the item names are short an nondescriptive, even this \"dvd\" match can place the item-item pair in the top 10.  Of course, you could have 1000 items like this that match the subject item with the same dot-product value, but the aforementioned algorithm just picks the first 10.)\n","\n","Ok, so it goes into the graph, and I apply a \"community\" algorithm that takes into account the weighted edge values (preferentially grouping together items that have higher dot-product values).\n","I didn't find a way to get much control over how many clusters are identified by the algorithm.  It seems to go up roughly linearly in the number of nodes(items) in the graph.  Anyhow, I get about 1400 clusters for my 5100 input items.  These clusters contain item_ids both in and not in the test set, as the graph was made with 51000 edges (minus about 10000 from thresholding) that used top-10 matches with the 5100 test items.  These top-10 matches may or may not be in the test set.\n","\n","These 1400 clusters have anywhere from 2 items (one edge) to perhaps 100 items.  I computed an average dot product value between all elements in a cluster, and used that to estimate the overall \"strength\" of clustering.  This provides a natural way to do category encoding.  I simply use the integer average of cluster dot-products as the \"cluster category code\".  (One minor complication is that some clusters have identical averages... I gave a small boost to the clusters with greater number of elements, so n=2, avg=300 might get a category value of 300, whereas n=5 avg=300 might get category value = 320)\n","\n","I assign all items in a given cluster the same \"cluster category code\".\n","\n","Any items that do not belong to a cluster (either because they didn't make the top-10 list for any of the test item matches, or because the thresholding eliminated them from inclusion in the graph) were assigned a \"cluster category code\" equal to their original item_category_code.  The cluster category code is a minimum of 2x or 3x larger than the largest original item category code (83), and the cluster category code can be quite a bit larger ... 100x or more, for the strongest-matching clusters.\n","\n","</br>\n","\n","**This was all done with the v1.1 items EDA ipynb on GitHub, and the dataset containing the \"cluster category codes\" is saved as csv.gz in the data_output directory.  You can just load in that dataset and use the cluster category code column (alongside the item_id column) as a feature in the model.  It shouldn't need further category encoding.**\n","\n","I'm now working on v2.0 of this items EDA (this file), to remove unnecessary code stragglers, and I hope to try a graph/clustering with all 21700 items rather than just the 5100 test items.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ruw_WyRxhqpx"},"source":["#0. Configure Environment\n","**NOT OPTIONAL**"]},{"cell_type":"code","metadata":{"id":"sTVAxnMnenrB","colab_type":"code","outputId":"5359bc7c-1fc1-4899-8662-9deb927621d6","executionInfo":{"status":"ok","timestamp":1590673556546,"user_tz":240,"elapsed":1914,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# General python libraries/modules used throughout the notebook\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import MultipleLocator, FormatStrFormatter, AutoMinorLocator\n","import numpy as np\n","from scipy import sparse\n","import seaborn as sns\n","from numba import jit, prange\n","import networkx as nx\n","from networkx.algorithms import community, cluster\n","\n","import os\n","import feather   # this is 3x to 8x faster than pd.read_csv and pd.to_hdf, but file size is 2x hdf and 10x csv.gz\n","import pickle\n","import string\n","from itertools import product\n","import re\n","from collections import OrderedDict\n","import json\n","import time\n","import datetime\n","from time import sleep, localtime, strftime, tzset, strptime\n","os.environ['TZ'] = 'EST+05EDT,M4.1.0,M10.5.0'\n","tzset()\n","\n","# Magics\n","%matplotlib inline\n","\n","\n","# NLP packages\n","import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer \n","lemmatizer = WordNetLemmatizer() \n","\n","# # ML packages\n","# from sklearn.linear_model import LinearRegression\n","\n","# !pip install catboost\n","# from catboost import CatBoostRegressor \n","\n","# %tensorflow_version 2.x\n","# import tensorflow as tf\n","# import keras as K\n","\n","# # List of the modules we need to version-track for reference\n","modules = ['pandas','matplotlib','numpy','scipy','numba','seaborn','sklearn','tensorflow','keras','catboost','pip','nltk','networkx']\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","done: Thu 09:45:55 05/28/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NoOf7oi8Oe-Q","colab_type":"code","outputId":"94d4761c-15fd-494a-a5c2-3200eccdd535","executionInfo":{"status":"ok","timestamp":1590673556547,"user_tz":240,"elapsed":1899,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Notebook formatting\n","# Adjust as per your preferences.  I'm using a FHD monitor with a full-screen browser window containing my IPynb notebook\n","\n","# format pandas output so we can see all the columns we care about (instead of \"col1  col2  ........ col8 col9\", we will see \"col1 col2 col3 col4 col5 col6 col7 col8 col9\" if it fits inside display.width parameter)\n","pd.set_option(\"display.max_columns\",30)  \n","pd.set_option(\"display.max_rows\",100)     # Override pandas choice of how many rows to show, so, for example, we can see the full 84-row item_category dataframe instead of the first few rows, then ...., then the last few rows\n","pd.set_option(\"display.width\", 250)       # Similar to the above for showing more rows than pandas defaults to, we can show more columns than default, if we tune this to our monitor window size\n","pd.set_option(\"max_colwidth\", None)\n","\n","#pd.set_option(\"display.precision\", 3)  # Nah, this is helpful, but below is even better\n","#Try to convince pandas to print without decimal places if a number is actually an integer (helps keep column width down, and highlights data types)\n","pd.options.display.float_format = lambda x : '{:.0f}'.format(x) if round(x,0) == x else '{:,.3f}'.format(x)\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["done: Thu 09:45:55 05/28/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hPrPvh7sorJd","colab_type":"text"},"source":["#0.99999) Mount Google Drive (Local File Storage/Repo)"]},{"cell_type":"code","metadata":{"id":"CUIE1PVjSAmg","colab_type":"code","outputId":"172513f1-5bc4-4258-9a76-e72b745c67bd","executionInfo":{"status":"ok","timestamp":1590673556547,"user_tz":240,"elapsed":1879,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, then paste it into the input box and hit 'enter' to complete mounting of the drive\n","from google.colab import drive  \n","drive.mount('/content/drive')\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","done: Thu 09:45:55 05/28/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"miCS3XqUhDXz"},"source":["#1. Load Data Files\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kjWzoXizEa5O","colab_type":"text"},"source":["##1.1) Enter Data File Names and Paths\n","\n","**NOT Optional**"]},{"cell_type":"code","metadata":{"id":"p9vsd3EynZLO","colab_type":"code","outputId":"3ac2c12d-ca5d-4ed6-f5f9-d1aee7e82e6d","executionInfo":{"status":"ok","timestamp":1590673556548,"user_tz":240,"elapsed":1865,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#  FYI, data is coming from a public repo on GitHub at github.com/migai/Kag\n","# List of the data files (path relative to GitHub master), to be loaded into pandas DataFrames\n","data_files = [  #\"readonly/final_project_data/shops.csv\",\n","                #\"readonly/final_project_data/sample_submission.csv.gz\",\n","                #\"data_output/shops_transl.csv\",\n","                \"data_output/items_transl.csv\",\n","                #\"readonly/final_project_data/item_categories.csv\",\n","                #\"data_output/items_clustered_22170.csv.gz\",\n","                #\"readonly/en_50k.csv\",\n","                #\"data_output/item_categories_transl.csv\",\n","                \"data_output/shops_augmented.csv\",\n","                #\"readonly/final_project_data/items.csv\",\n","                \"data_output/sales_train_cleaned.csv.gz\",\n","                \"data_output/item_categories_augmented.csv\",\n","                \"data_output/items_new.csv\",\n","                \"data_output/shops_new.csv\",\n","                \"readonly/final_project_data/sales_train.csv.gz\",\n","                \"readonly/final_project_data/test.csv.gz\"\n","              ]\n","\n","\n","# Dict of helper code files, to be loaded and imported {filepath : import_as}\n","code_files = {}  # not used at this time; example dict = {\"helper_code/kaggle_utils_at_mg.py\" : \"kag_utils\"}\n","\n","\n","# GitHub file location info\n","git_hub_url = \"https://raw.githubusercontent.com/migai\"\n","repo_name = 'Kag'\n","branch_name = 'master'\n","base_url = os.path.join(git_hub_url, repo_name, branch_name)\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["done: Thu 09:45:55 05/28/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T0EI1VfQq-WW"},"source":["##1.2) Load Data Files"]},{"cell_type":"code","metadata":{"id":"dy5i7jl00oX-","colab_type":"code","outputId":"f7d15b13-4307-44a8-eac5-cc46338ce48d","executionInfo":{"status":"ok","timestamp":1590673562776,"user_tz":240,"elapsed":8083,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":952}},"source":["%%time\n","# 4.5sec with csv and csv.gz files (including conversion to datetime)\n","# 3.6sec with csv and csv.gz files (no datetime conversion)\n","\n","'''\n","############################################################\n","############################################################\n","'''\n","# Replace this path with the path on *your* Google Drive where the repo master branch is stored\n","#   (on GitHub, the remote repo is located at github.com/migai/Kag --> below is my cloned repo location)\n","GDRIVE_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\"\n","OUT_OF_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks\"   # place > 100MB files here, because they won't sync with GitHub\n","'''\n","############################################################\n","############################################################\n","'''\n","\n","# do feather files manually for now\n","%cd \"{OUT_OF_REPO_PATH}\"\n","testtrain_mrg_ftr = True\n","testtrain_mrg = pd.read_feather('testtrain_mrg.ftr', columns=None, use_threads=True);\n","\n","\n","%cd \"{GDRIVE_REPO_PATH}\"\n","\n","print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","\n","# Loop to load the data files into appropriately-named pandas DataFrames\n","for path_name in data_files:\n","    filename = path_name.rsplit(\"/\")[-1]\n","    data_frame_name = filename.split(\".\")[0]\n","    exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","    # if data_frame_name == 'sales_train':\n","    #     sales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')\n","    print(\"Data Frame: \" + data_frame_name)\n","    print(eval(data_frame_name).head(2))\n","    print(\"\\n\")\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks\n","/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\n","Loading Files from Google Drive repo into Colab...\n","\n","Data Frame: items_transl\n","                                                              item_name  item_id  item_category_id                                                           en_item_name\n","0                             ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0                40                                           ! POWER IN glamor (PLAST.) D\n","1  !ABBYY FineReader 12 Professional Edition Full [PC, Цифровая версия]        1                76  ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]\n","\n","\n","Data Frame: shops_augmented\n","                       shop_name  shop_id                       en_shop_name shop_city shop_category shop_federal_district  shop_city_population  shop_tested\n","0  !Якутск Орджоникидзе, 56 фран        0  ! Yakutsk Ordzhonikidze, 56 Franc   Yakutsk          Shop               Eastern                235600        False\n","1  !Якутск ТЦ \"Центральный\" фран        1       ! Yakutsk TC \"Central\" Franc   Yakutsk          Mall               Eastern                235600        False\n","\n","\n","Data Frame: sales_train_cleaned\n","         date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0  2013-01-02               0       59    22154         999             1\n","1  2013-01-03               0       25     2552         899             1\n","\n","\n","Data Frame: item_categories_augmented\n","        item_category_name  item_category_id                 en_cat_name item_category1 item_category2 item_category3 item_category4  item_cat_tested\n","0  PC - Гарнитуры/Наушники                 0  PC - Headsets / Headphones          Audio             PC    Accessories             PC             True\n","1         Аксессуары - PS2                 1           Accessories - PS2    Accessories    PlayStation    Accessories    PlayStation            False\n","\n","\n","Data Frame: items_new\n","   item_id  item_tested  item_category_id  cluster_code item_category3  item_category3_enc item_category4  item_category4_enc\n","0        0        False                40           920         Movies                   7         Movies                   3\n","1        1        False                76          2600       Software                  10             PC                   5\n","\n","\n","Data Frame: shops_new\n","   shop_id  shop_tested shop_type  shop_type_enc shop_city  shop_city_enc shop_federal_district  shop_federal_district_enc s_type_broad  s_type_broad_enc fd_popdens  fd_popdens_enc        fd_gdp  fd_gdp_enc\n","0        0        False      Shop             20   Yakutsk             54               Eastern                         16         Shop                10     Remote               5  Intermediate          43\n","1        1        False      Mall             50   Yakutsk             54               Eastern                         16         Mall                60     Remote               5  Intermediate          43\n","\n","\n","Data Frame: sales_train\n","         date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n","0  02.01.2013               0       59    22154         999             1\n","1  03.01.2013               0       25     2552         899             1\n","\n","\n","Data Frame: test\n","   ID  shop_id  item_id\n","0   0        5     5037\n","1   1        5     5320\n","\n","\n","done: Thu 09:46:01 05/28/20\n","CPU times: user 4.58 s, sys: 1 s, total: 5.58 s\n","Wall time: 6.16 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P99yjzAasJva"},"source":["#2. Explore Data (EDA), Clean Data, and Generate Features"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b-VRzaGK6Gup"},"source":["#2.x) ***items*** and ***item_categories*** Datasets: \n","EDA, Cleaning, Correlations, and Feature Generation\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"YY7-Idw86ST2","colab_type":"code","outputId":"7a399127-e9f2-43a9-8d89-5234938f4d89","executionInfo":{"status":"ok","timestamp":1590673562777,"user_tz":240,"elapsed":8082,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["if not testtrain_mrg_ftr:  # we already have this loaded from feather file\n","    # merge dataframes so we can do closer analysis of item dependence on shop and categories\n","    test_prep = test.copy(deep=True)\n","    test_prep['date_block_num'] = 34\n","    test_prep['date'] = '2015-11-30' #pd.Timestamp(year=2015, month=11, day=30)\n","    sales_traintest_clean_mrg = sales_train_cleaned.append(test_prep).fillna(0)\n","    testtrain_mrg = sales_traintest_clean_mrg.merge(items_new[['item_id','item_category_id','item_tested']],on='item_id',how='left').reset_index(drop=True)\n","    testtrain_mrg = testtrain_mrg.merge(items_transl[['item_id','en_item_name']],on='item_id',how='left').reset_index(drop=True)\n","    testtrain_mrg = testtrain_mrg.merge(item_categories_augmented[['item_category_id','en_cat_name','item_cat_tested','item_category3','item_category4']],on='item_category_id',how='left').reset_index(drop=True)\n","    testtrain_mrg = testtrain_mrg.merge(shops_augmented[['shop_id', 'en_shop_name', 'shop_city', 'shop_federal_district',  'shop_city_population',  'shop_tested']], on='shop_id',how='left').reset_index(drop=True)\n","    testtrain_mrg = testtrain_mrg.merge(shops_new[['shop_id', 'shop_type', 'fd_popdens',  'fd_gdp']], on='shop_id',how='left').reset_index(drop=True)\n","    testtrain_mrg = testtrain_mrg[['date', 'date_block_num', 'item_price', 'item_cnt_day', 'shop_id', 'item_id', 'en_item_name', 'item_tested', 'item_category_id', 'en_cat_name', 'item_cat_tested',\n","                                'item_category3', 'item_category4', 'en_shop_name', 'shop_type','shop_tested', 'shop_federal_district', 'fd_popdens', 'fd_gdp', 'shop_city', 'shop_city_population']]\n","    testtrain_mrg.columns = ['date', 'month', 'price', 'sales', 'shop_id', 'item_id', 'item_name', 'it_test', 'item_category_id', 'item_category_name', 'it_cat_test', 'item_cat3', 'item_cat4', \n","                            'shop_name', 'sh_cat', 'sh_test', 'district', 'fd_popdens', 'fd_gdp', 'city', 'population']\n","    testtrain_mrg.date = pd.to_datetime(testtrain_mrg.date, format='%Y-%m-%d')\n","\n","    # optional save file as feather type (big file; don't store inside repo) and/or csv.gz type (inside repo)\n","    # %cd \"{OUT_OF_REPO_PATH}\"\n","    # testtrain_mrg.to_feather('testtrain_mrg.ftr')\n","    # %cd \"{GDRIVE_REPO_PATH}\"\n","    # # alternative, or, in addition, can save as csv.gz for < 100 MB storage and sync with GitHub\n","    # compression_opts = dict(method='gzip',\n","    #                         archive_name='testtrain_mrg.csv')  \n","    # testtrain_mrg.to_csv('data_output/testtrain_mrg.csv.gz', index=False, compression=compression_opts)\n","\n","print(f'done: {strftime(\"%a %X %x\")}\\n')\n","testtrain_mrg.tail()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["done: Thu 09:46:01 05/28/20\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>month</th>\n","      <th>price</th>\n","      <th>sales</th>\n","      <th>shop_id</th>\n","      <th>item_id</th>\n","      <th>item_name</th>\n","      <th>it_test</th>\n","      <th>item_category_id</th>\n","      <th>item_category_name</th>\n","      <th>it_cat_test</th>\n","      <th>item_cat3</th>\n","      <th>item_cat4</th>\n","      <th>shop_name</th>\n","      <th>sh_cat</th>\n","      <th>sh_test</th>\n","      <th>district</th>\n","      <th>fd_popdens</th>\n","      <th>fd_gdp</th>\n","      <th>city</th>\n","      <th>population</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3128463</th>\n","      <td>2015-11-30</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>18454</td>\n","      <td>Sat. Union 55</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Samara mall of \"Parkhouse\"</td>\n","      <td>Mall</td>\n","      <td>True</td>\n","      <td>Volga</td>\n","      <td>Intermediate</td>\n","      <td>Low</td>\n","      <td>Samara</td>\n","      <td>1134730</td>\n","    </tr>\n","    <tr>\n","      <th>3128464</th>\n","      <td>2015-11-30</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>16188</td>\n","      <td>Board Game Nano Curling</td>\n","      <td>True</td>\n","      <td>64</td>\n","      <td>Gifts - Board Games</td>\n","      <td>True</td>\n","      <td>Gifts</td>\n","      <td>Gifts</td>\n","      <td>Samara mall of \"Parkhouse\"</td>\n","      <td>Mall</td>\n","      <td>True</td>\n","      <td>Volga</td>\n","      <td>Intermediate</td>\n","      <td>Low</td>\n","      <td>Samara</td>\n","      <td>1134730</td>\n","    </tr>\n","    <tr>\n","      <th>3128465</th>\n","      <td>2015-11-30</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>15757</td>\n","      <td>Novikov Aleksandr New Collection</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Samara mall of \"Parkhouse\"</td>\n","      <td>Mall</td>\n","      <td>True</td>\n","      <td>Volga</td>\n","      <td>Intermediate</td>\n","      <td>Low</td>\n","      <td>Samara</td>\n","      <td>1134730</td>\n","    </tr>\n","    <tr>\n","      <th>3128466</th>\n","      <td>2015-11-30</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>19648</td>\n","      <td>Terem - TEREMOK sb.m / f (Region)</td>\n","      <td>True</td>\n","      <td>40</td>\n","      <td>Movie - DVD</td>\n","      <td>True</td>\n","      <td>Movies</td>\n","      <td>Movies</td>\n","      <td>Samara mall of \"Parkhouse\"</td>\n","      <td>Mall</td>\n","      <td>True</td>\n","      <td>Volga</td>\n","      <td>Intermediate</td>\n","      <td>Low</td>\n","      <td>Samara</td>\n","      <td>1134730</td>\n","    </tr>\n","    <tr>\n","      <th>3128467</th>\n","      <td>2015-11-30</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>969</td>\n","      <td>3 DAYS TO KILL (BD)</td>\n","      <td>True</td>\n","      <td>37</td>\n","      <td>Movie - Blu-Ray</td>\n","      <td>True</td>\n","      <td>Movies</td>\n","      <td>Movies</td>\n","      <td>Samara mall of \"Parkhouse\"</td>\n","      <td>Mall</td>\n","      <td>True</td>\n","      <td>Volga</td>\n","      <td>Intermediate</td>\n","      <td>Low</td>\n","      <td>Samara</td>\n","      <td>1134730</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              date  month  price  sales  shop_id  item_id                          item_name  it_test  item_category_id              item_category_name  it_cat_test item_cat3 item_cat4                   shop_name sh_cat  sh_test district  \\\n","3128463 2015-11-30     34      0      0       45    18454                      Sat. Union 55     True                55  Music - CD of local production         True     Music     Music  Samara mall of \"Parkhouse\"   Mall     True    Volga   \n","3128464 2015-11-30     34      0      0       45    16188            Board Game Nano Curling     True                64             Gifts - Board Games         True     Gifts     Gifts  Samara mall of \"Parkhouse\"   Mall     True    Volga   \n","3128465 2015-11-30     34      0      0       45    15757   Novikov Aleksandr New Collection     True                55  Music - CD of local production         True     Music     Music  Samara mall of \"Parkhouse\"   Mall     True    Volga   \n","3128466 2015-11-30     34      0      0       45    19648  Terem - TEREMOK sb.m / f (Region)     True                40                     Movie - DVD         True    Movies    Movies  Samara mall of \"Parkhouse\"   Mall     True    Volga   \n","3128467 2015-11-30     34      0      0       45      969                3 DAYS TO KILL (BD)     True                37                 Movie - Blu-Ray         True    Movies    Movies  Samara mall of \"Parkhouse\"   Mall     True    Volga   \n","\n","           fd_popdens fd_gdp    city  population  \n","3128463  Intermediate    Low  Samara     1134730  \n","3128464  Intermediate    Low  Samara     1134730  \n","3128465  Intermediate    Low  Samara     1134730  \n","3128466  Intermediate    Low  Samara     1134730  \n","3128467  Intermediate    Low  Samara     1134730  "]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"-Ngfu6YMn8dQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":479},"outputId":"87c13555-1d4b-482d-d6ae-86ab8da3e890","executionInfo":{"status":"ok","timestamp":1590673579259,"user_tz":240,"elapsed":24554,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["# look at sales by week or quarter as well as sales by month\n","\n","# # make a new dataframe that only includes Sept 2014\n","# trans2014sept = transactions.loc[:][(transactions['date'] >= pd.Timestamp(year=2014, month=9, day=1)) \n","#            & (transactions['date'] < pd.Timestamp(year=2014, month=10, day=1))]\n","tt = testtrain_mrg.copy(deep=True)\n","tt.insert(1,'day',0)\n","tt.day = tt.date.apply(lambda x: (datetime.datetime(x.year,x.month,x.day) - datetime.datetime(2013,1,1)).days )\n","tt.insert(2,'week',0)\n","tt.week = tt.day // 7\n","tt.insert(4,'quarter',0)  # 3 month chunks, with final one being months 31,32,33\n","tt.quarter = (tt.month + 2) // 3\n","\n","print(f'done: {strftime(\"%a %X %x\")}\\n')\n","tt.tail()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["done: Thu 09:46:18 05/28/20\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>day</th>\n","      <th>week</th>\n","      <th>month</th>\n","      <th>quarter</th>\n","      <th>price</th>\n","      <th>sales</th>\n","      <th>shop_id</th>\n","      <th>item_id</th>\n","      <th>item_name</th>\n","      <th>it_test</th>\n","      <th>item_category_id</th>\n","      <th>item_category_name</th>\n","      <th>it_cat_test</th>\n","      <th>item_cat3</th>\n","      <th>item_cat4</th>\n","      <th>shop_name</th>\n","      <th>sh_cat</th>\n","      <th>sh_test</th>\n","      <th>district</th>\n","      <th>fd_popdens</th>\n","      <th>fd_gdp</th>\n","      <th>city</th>\n","      <th>population</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3128463</th>\n","      <td>2015-11-30</td>\n","      <td>1063</td>\n","      <td>151</td>\n","      <td>34</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>18454</td>\n","      <td>Sat. Union 55</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Samara mall of \"Parkhouse\"</td>\n","      <td>Mall</td>\n","      <td>True</td>\n","      <td>Volga</td>\n","      <td>Intermediate</td>\n","      <td>Low</td>\n","      <td>Samara</td>\n","      <td>1134730</td>\n","    </tr>\n","    <tr>\n","      <th>3128464</th>\n","      <td>2015-11-30</td>\n","      <td>1063</td>\n","      <td>151</td>\n","      <td>34</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>16188</td>\n","      <td>Board Game Nano Curling</td>\n","      <td>True</td>\n","      <td>64</td>\n","      <td>Gifts - Board Games</td>\n","      <td>True</td>\n","      <td>Gifts</td>\n","      <td>Gifts</td>\n","      <td>Samara mall of \"Parkhouse\"</td>\n","      <td>Mall</td>\n","      <td>True</td>\n","      <td>Volga</td>\n","      <td>Intermediate</td>\n","      <td>Low</td>\n","      <td>Samara</td>\n","      <td>1134730</td>\n","    </tr>\n","    <tr>\n","      <th>3128465</th>\n","      <td>2015-11-30</td>\n","      <td>1063</td>\n","      <td>151</td>\n","      <td>34</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>15757</td>\n","      <td>Novikov Aleksandr New Collection</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Samara mall of \"Parkhouse\"</td>\n","      <td>Mall</td>\n","      <td>True</td>\n","      <td>Volga</td>\n","      <td>Intermediate</td>\n","      <td>Low</td>\n","      <td>Samara</td>\n","      <td>1134730</td>\n","    </tr>\n","    <tr>\n","      <th>3128466</th>\n","      <td>2015-11-30</td>\n","      <td>1063</td>\n","      <td>151</td>\n","      <td>34</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>19648</td>\n","      <td>Terem - TEREMOK sb.m / f (Region)</td>\n","      <td>True</td>\n","      <td>40</td>\n","      <td>Movie - DVD</td>\n","      <td>True</td>\n","      <td>Movies</td>\n","      <td>Movies</td>\n","      <td>Samara mall of \"Parkhouse\"</td>\n","      <td>Mall</td>\n","      <td>True</td>\n","      <td>Volga</td>\n","      <td>Intermediate</td>\n","      <td>Low</td>\n","      <td>Samara</td>\n","      <td>1134730</td>\n","    </tr>\n","    <tr>\n","      <th>3128467</th>\n","      <td>2015-11-30</td>\n","      <td>1063</td>\n","      <td>151</td>\n","      <td>34</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>969</td>\n","      <td>3 DAYS TO KILL (BD)</td>\n","      <td>True</td>\n","      <td>37</td>\n","      <td>Movie - Blu-Ray</td>\n","      <td>True</td>\n","      <td>Movies</td>\n","      <td>Movies</td>\n","      <td>Samara mall of \"Parkhouse\"</td>\n","      <td>Mall</td>\n","      <td>True</td>\n","      <td>Volga</td>\n","      <td>Intermediate</td>\n","      <td>Low</td>\n","      <td>Samara</td>\n","      <td>1134730</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              date   day  week  month  quarter  price  sales  shop_id  item_id                          item_name  it_test  item_category_id              item_category_name  it_cat_test item_cat3 item_cat4                   shop_name sh_cat  \\\n","3128463 2015-11-30  1063   151     34       12      0      0       45    18454                      Sat. Union 55     True                55  Music - CD of local production         True     Music     Music  Samara mall of \"Parkhouse\"   Mall   \n","3128464 2015-11-30  1063   151     34       12      0      0       45    16188            Board Game Nano Curling     True                64             Gifts - Board Games         True     Gifts     Gifts  Samara mall of \"Parkhouse\"   Mall   \n","3128465 2015-11-30  1063   151     34       12      0      0       45    15757   Novikov Aleksandr New Collection     True                55  Music - CD of local production         True     Music     Music  Samara mall of \"Parkhouse\"   Mall   \n","3128466 2015-11-30  1063   151     34       12      0      0       45    19648  Terem - TEREMOK sb.m / f (Region)     True                40                     Movie - DVD         True    Movies    Movies  Samara mall of \"Parkhouse\"   Mall   \n","3128467 2015-11-30  1063   151     34       12      0      0       45      969                3 DAYS TO KILL (BD)     True                37                 Movie - Blu-Ray         True    Movies    Movies  Samara mall of \"Parkhouse\"   Mall   \n","\n","         sh_test district    fd_popdens fd_gdp    city  population  \n","3128463     True    Volga  Intermediate    Low  Samara     1134730  \n","3128464     True    Volga  Intermediate    Low  Samara     1134730  \n","3128465     True    Volga  Intermediate    Low  Samara     1134730  \n","3128466     True    Volga  Intermediate    Low  Samara     1134730  \n","3128467     True    Volga  Intermediate    Low  Samara     1134730  "]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"E8gJ3b-6SV7n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"954e7bcf-79ed-47a3-8e60-e1c62480e3e8","executionInfo":{"status":"ok","timestamp":1590673579260,"user_tz":240,"elapsed":24541,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["tt.head(25)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>day</th>\n","      <th>week</th>\n","      <th>month</th>\n","      <th>quarter</th>\n","      <th>price</th>\n","      <th>sales</th>\n","      <th>shop_id</th>\n","      <th>item_id</th>\n","      <th>item_name</th>\n","      <th>it_test</th>\n","      <th>item_category_id</th>\n","      <th>item_category_name</th>\n","      <th>it_cat_test</th>\n","      <th>item_cat3</th>\n","      <th>item_cat4</th>\n","      <th>shop_name</th>\n","      <th>sh_cat</th>\n","      <th>sh_test</th>\n","      <th>district</th>\n","      <th>fd_popdens</th>\n","      <th>fd_gdp</th>\n","      <th>city</th>\n","      <th>population</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2013-01-02</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>999</td>\n","      <td>1</td>\n","      <td>59</td>\n","      <td>22154</td>\n","      <td>Scene 2012 (BD)</td>\n","      <td>True</td>\n","      <td>37</td>\n","      <td>Movie - Blu-Ray</td>\n","      <td>True</td>\n","      <td>Movies</td>\n","      <td>Movies</td>\n","      <td>Yaroslavl shopping center \"Altair\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Yaroslavl</td>\n","      <td>606730</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2013-01-03</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>899</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2552</td>\n","      <td>DEEP PURPLE The House Of Blue Light LP</td>\n","      <td>False</td>\n","      <td>58</td>\n","      <td>Music - Vinyl</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2013-01-05</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>899</td>\n","      <td>-1</td>\n","      <td>25</td>\n","      <td>2552</td>\n","      <td>DEEP PURPLE The House Of Blue Light LP</td>\n","      <td>False</td>\n","      <td>58</td>\n","      <td>Music - Vinyl</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2013-01-06</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1,709.050</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2554</td>\n","      <td>DEEP PURPLE Who Do You Think We Are LP</td>\n","      <td>False</td>\n","      <td>58</td>\n","      <td>Music - Vinyl</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2013-01-15</td>\n","      <td>14</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1099</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2555</td>\n","      <td>DEEP PURPLE 30 Very Best Of 2CD (Businesses).</td>\n","      <td>False</td>\n","      <td>56</td>\n","      <td>Music - CD production firm</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2013-01-10</td>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>349</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2564</td>\n","      <td>DEEP PURPLE Perihelion: Live In Concert DVD (Cyrus).</td>\n","      <td>False</td>\n","      <td>59</td>\n","      <td>Music - Music video</td>\n","      <td>False</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2013-01-02</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>549</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2565</td>\n","      <td>DEEP PURPLE Stormbringer (firms).</td>\n","      <td>False</td>\n","      <td>56</td>\n","      <td>Music - CD production firm</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2013-01-04</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>239</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2572</td>\n","      <td>DEFTONES Koi No Yokan</td>\n","      <td>False</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2013-01-11</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>299</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2572</td>\n","      <td>DEFTONES Koi No Yokan</td>\n","      <td>False</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2013-01-03</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>299</td>\n","      <td>3</td>\n","      <td>25</td>\n","      <td>2573</td>\n","      <td>DEL REY LANA Born To Die</td>\n","      <td>False</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2013-01-03</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>399</td>\n","      <td>2</td>\n","      <td>25</td>\n","      <td>2574</td>\n","      <td>DEL REY LANA Born To Die The Paradise Edition 2CD</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2013-01-05</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>399</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2574</td>\n","      <td>DEL REY LANA Born To Die The Paradise Edition 2CD</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2013-01-07</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>399</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2574</td>\n","      <td>DEL REY LANA Born To Die The Paradise Edition 2CD</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2013-01-08</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>399</td>\n","      <td>2</td>\n","      <td>25</td>\n","      <td>2574</td>\n","      <td>DEL REY LANA Born To Die The Paradise Edition 2CD</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2013-01-10</td>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>399</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2574</td>\n","      <td>DEL REY LANA Born To Die The Paradise Edition 2CD</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2013-01-11</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>399</td>\n","      <td>2</td>\n","      <td>25</td>\n","      <td>2574</td>\n","      <td>DEL REY LANA Born To Die The Paradise Edition 2CD</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2013-01-13</td>\n","      <td>12</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>399</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2574</td>\n","      <td>DEL REY LANA Born To Die The Paradise Edition 2CD</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2013-01-16</td>\n","      <td>15</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>399</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2574</td>\n","      <td>DEL REY LANA Born To Die The Paradise Edition 2CD</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2013-01-26</td>\n","      <td>25</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>399</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2574</td>\n","      <td>DEL REY LANA Born To Die The Paradise Edition 2CD</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2013-01-27</td>\n","      <td>26</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>399</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2574</td>\n","      <td>DEL REY LANA Born To Die The Paradise Edition 2CD</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2013-01-09</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>279</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2593</td>\n","      <td>DEPECHE MODE Music For The Masses</td>\n","      <td>False</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>2013-01-16</td>\n","      <td>15</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>299</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2604</td>\n","      <td>DEPECHE MODE Sounds Of The Universe</td>\n","      <td>False</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2013-01-27</td>\n","      <td>26</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>299</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2604</td>\n","      <td>DEPECHE MODE Sounds Of The Universe</td>\n","      <td>False</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>2013-01-27</td>\n","      <td>26</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>279</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2607</td>\n","      <td>DEPECHE MODE The Best Of 1</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2013-01-29</td>\n","      <td>28</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>279</td>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>2607</td>\n","      <td>DEPECHE MODE The Best Of 1</td>\n","      <td>True</td>\n","      <td>55</td>\n","      <td>Music - CD of local production</td>\n","      <td>True</td>\n","      <td>Music</td>\n","      <td>Music</td>\n","      <td>Moscow SEC \"Atrium\"</td>\n","      <td>SEC</td>\n","      <td>True</td>\n","      <td>Central</td>\n","      <td>Populous</td>\n","      <td>Intermediate</td>\n","      <td>Moscow</td>\n","      <td>10381222</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         date  day  week  month  quarter     price  sales  shop_id  item_id                                             item_name  it_test  item_category_id              item_category_name  it_cat_test item_cat3 item_cat4  \\\n","0  2013-01-02    1     0      0        0       999      1       59    22154                                       Scene 2012 (BD)     True                37                 Movie - Blu-Ray         True    Movies    Movies   \n","1  2013-01-03    2     0      0        0       899      1       25     2552                DEEP PURPLE The House Of Blue Light LP    False                58                   Music - Vinyl         True     Music     Music   \n","2  2013-01-05    4     0      0        0       899     -1       25     2552                DEEP PURPLE The House Of Blue Light LP    False                58                   Music - Vinyl         True     Music     Music   \n","3  2013-01-06    5     0      0        0 1,709.050      1       25     2554                DEEP PURPLE Who Do You Think We Are LP    False                58                   Music - Vinyl         True     Music     Music   \n","4  2013-01-15   14     2      0        0      1099      1       25     2555         DEEP PURPLE 30 Very Best Of 2CD (Businesses).    False                56      Music - CD production firm         True     Music     Music   \n","5  2013-01-10    9     1      0        0       349      1       25     2564  DEEP PURPLE Perihelion: Live In Concert DVD (Cyrus).    False                59             Music - Music video        False     Music     Music   \n","6  2013-01-02    1     0      0        0       549      1       25     2565                     DEEP PURPLE Stormbringer (firms).    False                56      Music - CD production firm         True     Music     Music   \n","7  2013-01-04    3     0      0        0       239      1       25     2572                                 DEFTONES Koi No Yokan    False                55  Music - CD of local production         True     Music     Music   \n","8  2013-01-11   10     1      0        0       299      1       25     2572                                 DEFTONES Koi No Yokan    False                55  Music - CD of local production         True     Music     Music   \n","9  2013-01-03    2     0      0        0       299      3       25     2573                              DEL REY LANA Born To Die    False                55  Music - CD of local production         True     Music     Music   \n","10 2013-01-03    2     0      0        0       399      2       25     2574     DEL REY LANA Born To Die The Paradise Edition 2CD     True                55  Music - CD of local production         True     Music     Music   \n","11 2013-01-05    4     0      0        0       399      1       25     2574     DEL REY LANA Born To Die The Paradise Edition 2CD     True                55  Music - CD of local production         True     Music     Music   \n","12 2013-01-07    6     0      0        0       399      1       25     2574     DEL REY LANA Born To Die The Paradise Edition 2CD     True                55  Music - CD of local production         True     Music     Music   \n","13 2013-01-08    7     1      0        0       399      2       25     2574     DEL REY LANA Born To Die The Paradise Edition 2CD     True                55  Music - CD of local production         True     Music     Music   \n","14 2013-01-10    9     1      0        0       399      1       25     2574     DEL REY LANA Born To Die The Paradise Edition 2CD     True                55  Music - CD of local production         True     Music     Music   \n","15 2013-01-11   10     1      0        0       399      2       25     2574     DEL REY LANA Born To Die The Paradise Edition 2CD     True                55  Music - CD of local production         True     Music     Music   \n","16 2013-01-13   12     1      0        0       399      1       25     2574     DEL REY LANA Born To Die The Paradise Edition 2CD     True                55  Music - CD of local production         True     Music     Music   \n","17 2013-01-16   15     2      0        0       399      1       25     2574     DEL REY LANA Born To Die The Paradise Edition 2CD     True                55  Music - CD of local production         True     Music     Music   \n","18 2013-01-26   25     3      0        0       399      1       25     2574     DEL REY LANA Born To Die The Paradise Edition 2CD     True                55  Music - CD of local production         True     Music     Music   \n","19 2013-01-27   26     3      0        0       399      1       25     2574     DEL REY LANA Born To Die The Paradise Edition 2CD     True                55  Music - CD of local production         True     Music     Music   \n","20 2013-01-09    8     1      0        0       279      1       25     2593                     DEPECHE MODE Music For The Masses    False                55  Music - CD of local production         True     Music     Music   \n","21 2013-01-16   15     2      0        0       299      1       25     2604                   DEPECHE MODE Sounds Of The Universe    False                55  Music - CD of local production         True     Music     Music   \n","22 2013-01-27   26     3      0        0       299      1       25     2604                   DEPECHE MODE Sounds Of The Universe    False                55  Music - CD of local production         True     Music     Music   \n","23 2013-01-27   26     3      0        0       279      1       25     2607                            DEPECHE MODE The Best Of 1     True                55  Music - CD of local production         True     Music     Music   \n","24 2013-01-29   28     4      0        0       279      1       25     2607                            DEPECHE MODE The Best Of 1     True                55  Music - CD of local production         True     Music     Music   \n","\n","                             shop_name sh_cat  sh_test district fd_popdens        fd_gdp       city  population  \n","0   Yaroslavl shopping center \"Altair\"    SEC     True  Central   Populous  Intermediate  Yaroslavl      606730  \n","1                  Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","2                  Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","3                  Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","4                  Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","5                  Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","6                  Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","7                  Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","8                  Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","9                  Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","10                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","11                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","12                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","13                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","14                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","15                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","16                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","17                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","18                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","19                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","20                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","21                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","22                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","23                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  \n","24                 Moscow SEC \"Atrium\"    SEC     True  Central   Populous  Intermediate     Moscow    10381222  "]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"8vsjC_Q1JKCA","colab_type":"code","outputId":"a46166c4-f88b-4fc2-d881-fb8e059cded6","executionInfo":{"status":"ok","timestamp":1590599987506,"user_tz":240,"elapsed":374,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# To Do:\n","'''\n","re.findall isn't consistent... sometimes gives a list of an array of tuples (many of which are empty string, but a matching string will be in any one of the tuple positions... and, possibly more than one??), so x=[(\"\",\"\",\" dvd\",\"\")] and x[0] gives the tuple, and x[0][0]=''\n","sometimes gives a list of a single string, so x[0] = 'abbyy' and x[0][0]='a'\n","--> maybe add an extra df column for ngrams to append, and merge it with the delim_item_strings after all 'cleaning' is done\n","\n","explicitly concatenate the item category name as a n-gram string, with n the same for all categories (pad as needed); don't add it to string before separating\n","\n","focus on any of the 84 categories that have the most items or the most spread in behavior... split apart the ones with odd behavior, or the ones that are sold by  a subset of shops consistently\n","(original train set: item_cat_id.counts()\n","(original train set: item_cat_id - item_cnt_day.value_counts()\n","(group by item_cat_id (agg: counts); then do shop_id.counts()\n","(group by month and item_cat_id; look at sum of sales by item_cat_id for last xx months, and characterize the various groups with min/max/std over past n months, for several n)\n","--> look after stripping the version numbers, etc. off the games and software, and create new groups based on only that??\n","--> can also try combining similar of the 84 categories (e.g., all playstations, or all xboxes, or all tickets/cards/...) and see if we have more consistent performance within a category\n","--> can also look at top 50 clusters created by NLP, and see how correlated their sales are, within a cluster, vs. uncorrelated outside a cluster\n","\n","###\n","maybe create a combination category column like grouping certain shop-item pairs, or shop-item_category pairs, or shop_cat-item_cat pairing\n","\n","####\n","look at most common n-grams for the cleaned/non-delimited item name\n","(start with n=15 and work backwards to n=1)\n","clean/replace as much as possible without overly distorting the item name\n","then:\n","for each n:\n","--> split clean string into consecutive n-grams and put them in df columns, where the # columns in df = largest number of n-grams for any of the item names (search to find longest name, and calculate how many columns are needed/used)\n","--> in each column, do count_values... perhaps do a combined unique() to get all the n-grams, then put into an array or series containing the n-gram string and the sum of value_counts over each column\n","--> sort by frequency, and choose the ? top 100 and ? bottom 100 (for sum>1)\n","--> convert some of the desired n-grams into n+x grams to reflect the relative importance?\n","'''\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["done: Wed 13:19:47 05/27/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xr0FDeno_EUQ"},"source":["#2.5) ***items*** Dataset: EDA, Cleaning, Correlations, and Feature Generation\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FX9bOxkfr55N","colab_type":"text"},"source":["###2.5.1) Initial data exploration and Russian -> English translation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s0J5l5H98Xsh"},"source":["####Thoughts regarding items dataframe\n","Let's first look at how many training examples we have to work with..."]},{"cell_type":"markdown","metadata":{"id":"1lg7NbEchkuM","colab_type":"text"},"source":["Many of the items have similar names, but slightly different punctuation, or only very slightly different version numbers or types.  (e.g., 'Call of Duty III' vs. 'Call of Duty III DVD')\n","\n","One can expect that these two items would have similar sales in general, and by grouping them into a single feature category, we can eliminate some of the overfitting that might come as a result of the relatively small ratio of (training set shop-item-date combinations = 2935849)/(total number of unique items = 22170).  (This is an average of about 132 rows in the sales_train data for each shop-item-date combination that we are using to train our model.  Our task is to produce a monthly estimate of sales (for November 2015), so it is relevant to consider training our model based on how many sales in a month vs. how many sales in the entire training set.  Given that the sales_train dataset covers the time period from January 2013 to October 2015 (34 months), we have on average fewer than 4 shop-item combinations in our training set for a given item in any given month.  Furthermore, as we are trying to predict for a particular month (*November* 2015), it is relevant to consider how many rows in our training set occur in the month of November.  The sales_train dataset contains data for two 'November' months out of the total 34 months of data.  Another simple calculation gives us an estimate that our training set contains on average 0.23 shop-item combinations per item for November months.\n","\n","To summarize:\n","\n","*  *sales_train* contains 34 months of data, including 2935849 shop-item-date combinations\n","*  *items* contains 22170 \"unique\" item_id values\n","\n","In the *sales_train* data, we therefore have:\n","*  on average, 132 rows with a given shop-item pair for a given item_id\n","*  on average, 4 rows with a given shop-item pair for a given item_id in a given month\n","*  on average, 0.23 rows with a given shop-item pair for a given item_id in all months named 'November'\n","\n","If we wish to improve our model predictions for the following month of November, it behooves us to use monthly grouping of sales, or, even better, November grouping of sales.  This smooths out day-to-day variations in sales for a better monthly prediction.  However, the sparse number of available rows in the *sales_train* data will contribute to inaccuracy in our model training and predictions.\n","\n","Imagine if we could reduce the number of item_id values from 22170 to perhaps half that or even less.  Given that the number of rows for training (per item, on a monthly or a November basis) is so small, then such a reduction in the number of item_id values would have a big impact.  (The same is true for creating features to supplement \"shop_id\" so as to group and reduce the individuality of each shop - and thus effectively create, on average, more rows of training data for each shop-item pair."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZfgOx-0_KXg"},"source":["####Translate and Ruminate\n","We will start by translating the Russian text in the dataframe, and add our ruminations on possible new features we can generate.\n","\n","The dataframe *items_transl* (equivalent to *items* plus a column for English translation) is saved as a .csv file so we do not have to repeat the translation process the next time we open a Google Colab runtime."]},{"cell_type":"code","metadata":{"id":"FhHSfXNxsKxQ","colab_type":"code","outputId":"51a61eb1-5d15-461f-8941-35cf0f64a649","executionInfo":{"status":"ok","timestamp":1590575208368,"user_tz":240,"elapsed":52891,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["print(items_transl.info())\n","print(\"\\n\")\n","print(items_transl.tail(10))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 22170 entries, 0 to 22169\n","Data columns (total 4 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   item_name         22170 non-null  object\n"," 1   item_id           22170 non-null  int64 \n"," 2   item_category_id  22170 non-null  int64 \n"," 3   en_item_name      22170 non-null  object\n","dtypes: int64(2), object(2)\n","memory usage: 692.9+ KB\n","None\n","\n","\n","                                                   item_name  item_id  item_category_id                                           en_item_name\n","22160                             ЯРМАРКА ТЩЕСЛАВИЯ (Регион)    22160                40                                   Vanity Fair (Region)\n","22161                       ЯРОСЛАВ. ТЫСЯЧУ ЛЕТ НАЗАД э (BD)    22161                37                YAROSLAV. Thousands of years ago e (BD)\n","22162                                                 ЯРОСТЬ    22162                40                                                   FURY\n","22163                                       ЯРОСТЬ ( регион)    22163                40                                          FURY (region)\n","22164                                            ЯРОСТЬ (BD)    22164                37                                              FURY (BD)\n","22165                 Ядерный титбит 2 [PC, Цифровая версия]    22165                31                 Nuclear titbit 2 [PC, Digital Version]\n","22166        Язык запросов 1С:Предприятия  [Цифровая версия]    22166                54     Language 1C queries: Enterprises [Digital Version]\n","22167  Язык запросов 1С:Предприятия 8 (+CD). Хрусталева Е.Ю.    22167                49  1C query language: Enterprise 8 (+ CD). Khrustalev EY\n","22168                                    Яйцо для Little Inu    22168                62                                     Egg for Little Inu\n","22169                          Яйцо дракона (Игра престолов)    22169                69                           Dragon egg (Game of Thrones)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9oSMeRVd7dvZ"},"source":["###2.5.2) **NLP for feature generation from items data**\n","Automate the search for commonality among items, and create new categorical feature to prevent overfitting from close similarity between many item names"]},{"cell_type":"markdown","metadata":{"id":"CwfXcHsMJ0Yg","colab_type":"text"},"source":["####**Delimited Groups of Words**\n","\n","Investigating \"special\" delimited word groups (like this) or [here] or /hobbitville/ that are present in item names, and may be particularly important in creating n>1 n-grams for uniquely identifying items so that we can tell if two items are the same or nearly the same"]},{"cell_type":"markdown","metadata":{"id":"V7QY11R1QmlN","colab_type":"text"},"source":["#####Some details on the approach, and code for helper functions to clean and separate the text:"]},{"cell_type":"code","metadata":{"id":"jac_TColdsMf","colab_type":"code","colab":{}},"source":["# explanation of regex string I'm using to parse the item_name\n","'''\n","\n","^\\s+|\\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*|\\s+$\n","\n","gm\n","1st Alternative ^\\s+\n","^ asserts position at start of a line\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","2nd Alternative \\s*[,\\\"\\/\\(\\)\\[\\]]+\\s*\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","Match a single character present in the list below [,\\\"\\/\\(\\)\\[\\]]+\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n",", matches the character , literally (case sensitive)\n","\\\" matches the character \" literally (case sensitive)\n","\\/ matches the character / literally (case sensitive)\n","\\( matches the character ( literally (case sensitive)\n","\\) matches the character ) literally (case sensitive)\n","\\[ matches the character [ literally (case sensitive)\n","\\] matches the character ] literally (case sensitive)\n","\\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","* Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n","\n","3rd Alternative \\s+$\n","\\s+ matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n","+ Quantifier — Matches between one and unlimited times, as many times as possible, giving back as needed (greedy)\n","$ asserts position at the end of a line\n","\n","Global pattern flags\n","g modifier: global. All matches (don't return after first match)\n","m modifier: multi line. Causes ^ and $ to match the begin/end of each line (not only begin/end of string)\n","'''\n","print(f'done: {strftime(\"%a %X %x\")}')  # prevent Jupyter from printing triple-quoted comments"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rsc0yYJkiRBY","colab_type":"code","colab":{}},"source":["# This cell contains no code to run; it is simply a record of some inspections that were done on the items database\n","\n","# before removing undesirable characters / punctuation from the item name,\n","#   let's see if we can find n-grams or useful describers or common abbreviations by looking between the nasty characters\n","# first, let's see what characters are present in the en_item_name column\n","'''\n","nasty_symbols = re.compile('[^0-9a-zA-Z ]')\n","nasties = set()\n","for i in range(len(items_transl)):\n","  n = nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  nasties = nasties.union(set(n))\n","print(nasties)\n","{'[', '\\u200b', 'ñ', '(', ')', '.', 'à', '`', 'ó', '®', 'Á', \n","'\\\\', 'è', '&', '-', ':', 'ë', '_', 'û', '»', '=', '+', ']', ',', \n","'«', 'ú', \"'\", 'ö', '#', 'ä', ';', 'ü', '\"', 'ô', '/', '№', 'é', \n","'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","'''\n","# From the above set of nasty characters, it looks like slashes, single quotes, double quotes, parentheses, and square brackets might enclose relevant n-grams\n","# Let's pull everything from en_item_name that is inside ' ', \" \", (), or [] and see how many unique values we get, and if they are n-grams or abbreviations, for example\n","# It also seems that many of the item names end in a single character \"D\" for example, which should be converted to DVD\n","\n","# ignore the :&+' stuff for now...\n","# Let's set up columns for ()[]-grams, for last string in the name, and for first string in name, and for text that precedes \":\", and for text that surrounds \"&\" or \"+\"\n","#   but first, we will strip out every nasty character except ()[]:&+'\"/ and replace the nasties with spaces, then eliminating double spaces\n","\n","'''\n","# sanity check:\n","really_nasty_symbols = re.compile('[^0-9a-zA-Z \\(\\)\\[\\]:&+\\'\"/]')\n","really_nasties = set()\n","for i in range(len(items_transl)):\n","  rn = really_nasty_symbols.findall(items_transl.at[i,'en_item_name'])\n","  really_nasties = really_nasties.union(set(rn))\n","print(really_nasties)\n","{'\\u200b', 'ñ', '.', 'à', '`', 'ó', '®', 'Á', '\\\\', 'è', '-', 'ë', '_', 'û', '»', '=', ',', '«', 'ú', 'ö', '#', 'ä', ';', 'ü', 'ô', '№', 'é', 'í', '!', '°', 'å', '*', 'ĭ', 'ð', '?', 'â'}\n","OK, looks good\n","'''\n","print(f'done: {strftime(\"%a %X %x\")}')  # prevent Jupyter from printing triple-quoted comments"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fb3tLqLtcey","colab_type":"code","colab":{}},"source":["#  Start by defining stopwords and delimiters and punctuation that we wish to remove\n","#  Then, create a couple of functions to use for text cleaning, and for extracting delimited text n-grams\n","\n","# stopwords to remove from item names (these are only a bit better than arbitrary selections from large stopwords lists -- may be worth adjusting them)\n","stop_words = \"a,the,an,only,more,are,any,on,your,just,it,its,has,with,for,by,from\".split(\",\")\n","\n","# pre-compile regex strings to use for fast symbol removal or delimiting\n","nasty_symbols_re = re.compile(r'[^0-9a-zA-Z ]')  # remove all punctuation\n","really_nasty_symbols_re = re.compile(r'[^0-9a-zA-Z ,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@]')  # remove nasties, but leave behind the delimiters\n","delimiters_re = re.compile(r'[,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@\\u00AB\\u00BB~<>]')  # unicodes are << and >> thingies\n","# special symbols indicating a delimiter --> a space at start or end of item name will be removed at split time, along with ,;/()[]:\"-@~<<>><>\n","delim_pattern_re = re.compile(r'^\\s+|\\s*[,;\\\"\\/\\(\\)\\[\\]\\:\\-\\@\\u00AB\\u00BB~<>]+\\s*|\\s+$') \n","multiple_whitespace_re = re.compile(r'[ ]{2,}')\n","\n","# pre-compile some specific regex strings to deal with inconsistencies in item names (more of this will be done later, after delimiting)\n","cleanup_text = {}\n","cleanup_text['preorder'] = re.compile(r'pre.?order')\n","cleanup_text[' dvd'] = re.compile(r'\\s+d$')  #several item names end in \"d\" -- which actually seems to indicate dvd (because the items I see are in category 40: Movies-DVD)... standardize so d --> dvd\n","cleanup_text['digital version'] = re.compile(r'digital in$') # several items seem to end in \"digital in\"... maybe in = internet?, but looking at nearby items/categories, 'digital version' looks standard\n","cleanup_text['bluray dvd'] = re.compile(r'\\bbd\\b|\\bblu\\s+ray\\b|\\bblu\\-ray\\b|\\bblueray\\b|\\bblue\\s+ray\\b|\\bblue\\-ray\\b')\n","cleanup_text['007 : james bond : skyfall'] = re.compile(r'\\bskyfall\\b|\\bskayfoll\\b')\n","cleanup_text[' and '] = re.compile(r'[\\&\\+]')\n","cleanup_text[' xbox'] = re.compile(r'\\bx[^0-9a-zA-Z ]box')  # anything like \"x box\" or \"x-box\" or \"x%box\" gets converted to a standard \"xbox\"\n","cleanup_text[' ps'] = re.compile(r'\\bp[^0-9a-zA-Z ]s')      # attempt to do the same with \"p-s4\" --> \"ps4\"\n","\n","def maid_service(text):\n","    \"\"\"\n","    Compact routine to implement multiple regex substitutions using the above 'cleanup_text' dictionary\n","    \"\"\"\n","    text = text.lower()\n","    for repl_text, pattern in cleanup_text.items():\n","        text = pattern.sub(repl_text, text)\n","    #r = re.compile(r'\\bskayfoll\\b')   # can add 'quickie' items here if you don't want to add to above dictionary, or if you want to perform something other than re.sub\n","    #text = r.sub('skyfall',text)  \n","    return text\n","\n","def text_total_clean(text):\n","    \"\"\"\n","    Gives a punctuation-free, cleaned, lemmatized version of the original English translation\n","    inputs: (text): the original en_item_name single-string, uncleaned, translated version of the Russian item name\n","    returns: single-string text, made lowercase, stripped of \"really_nasties\" and multiple spaces, and every word lemmatized\n","    \"\"\"\n","    text = maid_service(text)\n","    text = delimiters_re.sub(\" \", text)  # replace all delimiters with a space; other nasties get simply deleted\n","    text = nasty_symbols_re.sub(\"\", text)  # delete anything other than letters, numbers, and spaces\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = text.strip() # remove whitespace around string\n","    # lemmatize each word\n","    text = \" \".join([lemmatizer.lemmatize(w) for w in text.split(\" \") if w not in stop_words])\n","    return text\n","\n","def text_clean_delimited(text):\n","    \"\"\"\n","    Gives a punctuation-free, cleaned version of the original English translation, \n","        but the function returns a list of strings instead of a single string,\n","        with each element in the list corresponding to text that was separated from neighboring\n","        text with one of the above-defined 'delimiter' characters\n","        (so, rather than analyzing the full item name for n-grams, we define an item's important\n","        n-grams as being separated by such delimiters.  It greatly reduces the number of n-grams we need to analyze)\n","    inputs: (text): the original en_item_name single-string, uncleaned, translated version of the Russian item name\n","    returns: en_item_name made lowercase, stripped of \"really_nasties\" and multiple spaces, \n","        in a list of strings that had been separated by one of the above 'delimiters',\n","        and, with every word in every string lemmatized \n","    \"\"\"\n","    text = maid_service(text)\n","    text = really_nasty_symbols_re.sub(\"\", text)  # just delete the nasty symbols\n","    text = multiple_whitespace_re.sub(\" \", text)  # replace multiple spaces with a single space\n","    text = delim_pattern_re.split(text)           # split item_name at all delimiters, irrespective of number of spaces before or after the string or delimiter\n","    text = [x.strip() for x in text if x != \"\"]           # remove empty strings \"\" from the list of split items in text, and remove whitespace outside text n-gram\n","    # lemmatize each word\n","    lemtext = []\n","    for ngram in text:\n","        lemtext.append(\" \".join([lemmatizer.lemmatize(w) for w in ngram.split(\" \") if w not in stop_words]))\n","    return lemtext\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvzvPqmCQ2ZM","colab_type":"text"},"source":["#####Add 'delimited' and 'cleaned' data columns; shorten the titles of other columns so dataframe fits better on the screen"]},{"cell_type":"code","metadata":{"id":"Z35pqOYCtyZ7","colab_type":"code","outputId":"d43ec343-4476-4b48-ba27-1b809f556ca9","executionInfo":{"status":"ok","timestamp":1590588668907,"user_tz":240,"elapsed":5130,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":646}},"source":["items_delimited = items_transl.copy(deep=True)\n","# delete the wide \"item_name\" column so we can read more of the data table width-wise\n","items_delimited = items_delimited.drop(\"item_name\", axis=1).rename(columns = {'en_item_name':'item_name','item_category_id':'i_cat_id'})\n","items_in_test_set = test.item_id.unique()\n","items_delimited[\"i_tested\"] = False\n","for i in items_in_test_set:\n","  items_delimited.at[i,\"i_tested\"] = True\n","\n","\n","# add item_category name with delimiter to the item_name, as this will be useful info for grouping similar items (remove delimiting punctuation from cat names first, so it stays as one chunk of text)\n","items_delimited['item_name'] = items_delimited.apply(lambda x: text_total_clean(item_categories_augmented.at[x.i_cat_id,'en_cat_name']) + \" : \" + x.item_name, axis=1)\n","\n","# add a column of simply cleaned text without any undesired punctuation or delimiters\n","items_delimited['clean_item_name'] = items_delimited['item_name'].apply(text_total_clean)\n","\n","# now add a column of lists of delimited (cleaned) text\n","items_delimited['delim_name_list'] = items_delimited['item_name'].apply(text_clean_delimited)\n","\n","# remove duplicate entries and single-character 1-grams to assist with operations to come later in this notebook\n","alphnum = list(string.ascii_lowercase) + list('1234567890')  # get rid of all length=1 1-grams\n","def remove_dupes_singles(gramlist):\n","    unwanted = set(alphnum)\n","    dupe_gramset = unwanted\n","    return [x for x in gramlist if x not in dupe_gramset and not dupe_gramset.add(x)]\n","items_delimited.delim_name_list = items_delimited.delim_name_list.apply(lambda x: remove_dupes_singles(x) )\n","\n","\n","# have a look at what we got with our delimited text globs\n","def maxgram(gramlist):\n","    maxg = 0\n","    for g in gramlist:\n","        maxg = max(maxg,len(g.split()))\n","    return maxg\n","items_delimited['d_len'] = items_delimited.delim_name_list.apply(lambda x: len(x))\n","items_delimited['d_maxgram'] = items_delimited.delim_name_list.apply(maxgram)\n","\n","#items_delimited.to_csv(\"data_output/items_delimited.csv\", index=False)\n","\n","print(f'done: {strftime(\"%a %X %x\")}')\n","print(\"\\n\")\n","print(items_delimited.describe())\n","print(\"\\n\")\n","print(items_delimited.iloc[31][:])\n","print(\"\\n\")\n","items_delimited.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["done: Wed 10:11:08 05/27/20\n","\n","\n","         item_id  i_cat_id  d_len  d_maxgram\n","count      22170     22170  22170      22170\n","mean  11,084.500    46.291  3.347      4.496\n","std    6,400.072    15.941  1.336      1.984\n","min            0         0      1          2\n","25%    5,542.250        37      2          3\n","50%   11,084.500        40      3          4\n","75%   16,626.750        58      4          5\n","max        22169        83     13         17\n","\n","\n","item_id                                                                              31\n","i_cat_id                                                                             37\n","item_name                           movie bluray dvd : 007: COORDINATES \"SKAYFOLL» (BD)\n","i_tested                                                                           True\n","clean_item_name       movie bluray dvd 007 coordinate 007 james bond skyfall bluray dvd\n","delim_name_list    [movie bluray dvd, 007, coordinate, james bond, skyfall, bluray dvd]\n","d_len                                                                                 6\n","d_maxgram                                                                             3\n","Name: 31, dtype: object\n","\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>i_cat_id</th>\n","      <th>item_name</th>\n","      <th>i_tested</th>\n","      <th>clean_item_name</th>\n","      <th>delim_name_list</th>\n","      <th>d_len</th>\n","      <th>d_maxgram</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>movie dvd : ! POWER IN glamor (PLAST.) D</td>\n","      <td>False</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>[movie dvd, power in glamor, plast, dvd]</td>\n","      <td>4</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>program home and office digital : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]</td>\n","      <td>False</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version]</td>\n","      <td>4</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>40</td>\n","      <td>movie dvd : *** In the glory (UNV) D</td>\n","      <td>False</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>[movie dvd, in glory, unv, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>40</td>\n","      <td>movie dvd : *** BLUE WAVE (Univ) D</td>\n","      <td>False</td>\n","      <td>movie dvd blue wave univ dvd</td>\n","      <td>[movie dvd, blue wave, univ, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>40</td>\n","      <td>movie dvd : *** BOX (GLASS) D</td>\n","      <td>False</td>\n","      <td>movie dvd box glass dvd</td>\n","      <td>[movie dvd, box, glass, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id  i_cat_id                                                                                                item_name  i_tested                                                                                   clean_item_name  \\\n","0        0        40                                                                 movie dvd : ! POWER IN glamor (PLAST.) D     False                                                               movie dvd power in glamor plast dvd   \n","1        1        76  program home and office digital : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]     False  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2        2        40                                                                     movie dvd : *** In the glory (UNV) D     False                                                                        movie dvd in glory unv dvd   \n","3        3        40                                                                       movie dvd : *** BLUE WAVE (Univ) D     False                                                                      movie dvd blue wave univ dvd   \n","4        4        40                                                                            movie dvd : *** BOX (GLASS) D     False                                                                           movie dvd box glass dvd   \n","\n","                                                                                         delim_name_list  d_len  d_maxgram  \n","0                                                               [movie dvd, power in glamor, plast, dvd]      4          3  \n","1  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version]      4          6  \n","2                                                                        [movie dvd, in glory, unv, dvd]      4          2  \n","3                                                                      [movie dvd, blue wave, univ, dvd]      4          2  \n","4                                                                           [movie dvd, box, glass, dvd]      4          2  "]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"qc0D5gkeuk2i","colab_type":"code","outputId":"563ef891-63b8-4dee-bb05-878fd33004bc","executionInfo":{"status":"ok","timestamp":1590588672289,"user_tz":240,"elapsed":510,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# do some more text manipulation to help ensure items are properly grouped\n","#   also, expand the breadth of n-gram matches to ignore things like version number, in an effort to reduce the final number of clusters that are generated\n","#   (looking for perhaps 200 clusters instead of 2000+ that we get without this extra treatment (see 'items_nlp_clusters_v3...ipynb' ))\n","\n","highlight_roots = OrderedDict()\n","cleanup_sub = OrderedDict()\n","cleanup_final = OrderedDict()\n","#cleanup_complete_replace = OrderedDict()\n","\n","\n","# for some matches, I want to only make a new entry in the list to standardize games to root values (e.g., \"assasin creed special ops\" = \"assasin creed part 2\")\n","#     The new list element will be a 5-gram, for example, to give it substantial weight when grouping items\n","#     The original list of delimited text will remain the same, so as to catch matches like \"assasin creed special ops dvd bluray english version\"\n","\n","# use replacement text = '' if you want the create operation to use the match as a base string (possibly adding to it with fill_strings until n_gram size is reached)\n","games1 = \"adventure of tintin|advanced warfare|army of two|assasin creed|angry bird|batman|battlefield|behind enemy line|black ops|borderland|call of duty|chaggington funny train\"\n","games2 = \"child of light|dark soul|dead space|disney infinity|dragon age|elder scroll|far cry|final fantasy|game of throne|god of war|grand theft auto|harry potter|james bond|(lord of ring|hobbit)\"\n","games3 = \"mario|masha and bear|max payne|medal of honor|men in black|metal gear solid|mickey mouse|might and magic|modern warfare|mortal kombat|nba|need speed|nhl|ninja storm|pirate of car\\w*\\b\"\n","games4 = \"plant v zombie|pro evolution|resident evil|secret of unicorn|shadow of mordor|sherlock holmes|sid meiers civilization|skylander|sniper elite|star war|stick of truth|street fighter\"\n","games5 = \"tiger wood|tom clancy(s)?|tomb raider|transformer|walking dead|warhammer|watch dog|witcher|world of warcraft\"\n","popular_games = re.compile(rf'\\b({games1}|{games2}|{games3}|{games4}|{games5})\\b')\n","highlight_roots['compress game names to root values'] =     {'optype':['create'], 'reg_pattern':popular_games, \n","                                                                'replacement_text':'', 'final_gram_n':5, \n","                                                                'fill_strings':['game','computer','electronic','multirelease']}\n","\n","lego = re.compile(r'\\blego\\b')\n","#lego = re.compile(rf'\\b(lego.*({games1}|{games2}|{games3}|{games4}|{games5})?|({games1}|{games2}|{games3}|{games4}|{games5}).*lego)\\b')\n","highlight_roots['lego products'] =                          {'optype':['create'], 'reg_pattern':lego,\n","                                                                'replacement_text':'lego brand lego style game'}\n","\n","popular_companies = re.compile(rf'\\b(1c|abbyy)\\b')\n","highlight_roots['highlight product origins'] =              {'optype':['create'], 'reg_pattern':popular_companies, \n","                                                                'replacement_text':'', 'final_gram_n':4, \n","                                                                'fill_strings':['educational','software','learning']}\n","\n","# for some of the matches, I just want to do an inplace substitution\n","fix_accessory_game = re.compile(r'\\baccessory game\\b')\n","cleanup_sub['game accessory'] =         {'optype':['sub'], 'replacement_text':'game accessory', 'reg_pattern':fix_accessory_game}\n","\n","biz = re.compile(r'\\b(firm|enterprise|company|corporation|shop|store|outlet)\\b')\n","cleanup_sub['standardize biz'] =        {'optype':['sub'], 'replacement_text':'business', 'reg_pattern':biz}\n","\n","digit = re.compile(r'\\b(digital|download|online)(\\s?(version|edition|set|box set))?\\b')\n","cleanup_sub['special edition'] =        {'optype':['sub'], 'replacement_text':'online digital version', 'reg_pattern':digit}\n","\n","special = re.compile(r'\\b(collector|premier|platinum|special|suite)(.*(version|edition|set|box set|suite))?\\b')\n","cleanup_sub['special edition'] =        {'optype':['sub'], 'replacement_text':'special version', 'reg_pattern':special}\n","\n","std = re.compile(r'\\b(standard|std)(\\s?(edition|version|set|box set))?\\b')\n","cleanup_sub['standard edition'] =       {'optype':['sub'], 'replacement_text':'standard version', 'reg_pattern':std}\n","\n","russia = re.compile(r'\\b(russian|ru)(\\s?(edition|version|set|box set|documentation|instruction|language|format|subtitle|feature))?\\b')\n","cleanup_sub['russian version'] =        {'optype':['sub'], 'replacement_text':'russian language version', 'reg_pattern':russia}\n","\n","engl = re.compile(r'\\b(english|en|eng|engl)(\\s?(edition|version|set|box set|documentation|instruction|language|format|subtitle|feature))?\\b')\n","cleanup_sub['english version'] =        {'optype':['sub'], 'replacement_text':'english language version', 'reg_pattern':engl}\n","\n","\n","# for other matches, I want to create a new n-gram and insert it into the list, and also do an inplace substitution\n","#   substitution text is made longer or shorter, depending on rough importance to matching (longer matching n-grams get more weight)\n","yo = re.compile(r'\\b(yo|yoyo|yo yo)\\b')\n","cleanup_final['yo yo yo'] =         {'optype':['sub','create'], 'replacement_text':'yo yo toy game fun', 'reg_pattern':yo}\n","\n","music = re.compile(r'\\b(cd mirex|mirex cd|cd mirex cd|vinyl|cd.*production firm|cd.*local production|mp3)(\\s?(cd mirex|mirex cd|cd mirex cd|vinyl|cd.*production firm|cd.*local production|mp3))?\\b')\n","cleanup_final['music media'] =      {'optype':['sub','create'], 'replacement_text':'music media', 'reg_pattern': music}\n","\n","dvdclean = re.compile(r'\\b(\\d\\s?)?(disc\\s?)?(\\d\\s?)?dvd\\b')\n","cleanup_final['dvd'] =             {'optype':['sub','create'], 'replacement_text':'dvd', 'reg_pattern':dvdclean}\n","\n","brdvd = re.compile(r'\\b(4k\\s?)?(\\d\\s?)?(bluray\\s?)?(\\d\\s?)?(dvd\\s?)?(and\\s?)?(\\d\\s?)?(disc\\s?)?(4k\\s?)?(\\d\\s?)?bluray(\\s?and)?(\\s?4k)?(\\s?(\\d\\s?)?dvd)?(\\s?4k)?(\\s?and)?(\\s?(\\d\\s?)?dvd)?\\b|\\b2bd\\b')\n","cleanup_final['bluray dvd'] =      {'optype':['sub','create'], 'replacement_text':'bluray dvd', 'reg_pattern':brdvd}\n","\n","br3d=re.compile(r'\\b(\\d\\s?)?(disc)?\\s?(\\d\\s?)?(dvd)?\\s?(and)?\\s?(3d\\s?(\\d\\s?)?(dvd)?\\s?(\\d\\s?)?bluray\\s?(\\d\\s?)?(dvd)?|(\\d\\s?)?bluray\\s?(\\d\\s?)?(dvd)?\\s?3d)\\s?(\\d\\s?)?(bluray dvd)?\\s?(3d)?\\s?(and)?\\s?(\\d\\s?)?(dvd)?\\s?(3d)?\\b')\n","cleanup_final['3d bluray dvd'] =   {'optype':['sub','create'], 'replacement_text':'3d bluray dvd', 'reg_pattern':br3d}\n","\n","macregx = re.compile(r'\\b(support\\s?)?(mac|ipad|macbook|powerbook|imac|apple)(\\s?support)?\\b')\n","cleanup_final['pc'] =              {'optype':['create'], 'replacement_text':'mac computing platform product', 'reg_pattern':macregx}\n","\n","pcregx = re.compile(r'\\b(support\\s?)?(pc|windows|microsoft windows)(\\s?support)?\\b')\n","cleanup_final['pc'] =              {'optype':['create'], 'replacement_text':'pc computing platform product', 'reg_pattern':pcregx}\n","\n","playsta = re.compile(r'\\b(support\\s?)?p(sp|\\s?s|\\s?s?\\s?(move|2|3|4|pro|vita|vita 1000))\\b')\n","cleanup_final['sony playstn'] =    {'optype':['create'], 'replacement_text':'sony playstation gaming platform', 'reg_pattern':playsta}\n","\n","xbox = re.compile(r'\\bx?\\s?box\\s?(one|360|live)(.*(kinect|knect))?\\b')\n","cleanup_final['microsoft xbox'] =  {'optype':['create'], 'replacement_text':'microsoft xbox gaming platform','reg_pattern':xbox}\n","\n","kinect = re.compile(r'\\b(support)?\\s?m?\\s?s?\\s?(kinect|knect)\\b')\n","cleanup_final['microsoft knect'] = {'optype':['create'], 'replacement_text':'microsoft xbox gaming platform', 'reg_pattern':kinect}\n","\n","msoffice = re.compile(r'\\b(microsoft office|ms office|m office|office mac|office home|office professional|home and office|office student|office enterprise)\\b')\n","cleanup_final['ms office'] =       {'optype':['create'], 'replacement_text':'microsoft office productivity software', 'reg_pattern':msoffice}\n","\n","educate = re.compile(r'\\b(education|educational|development|course|school|history|lesson|accounting|b8)\\b')\n","cleanup_final['educational dev'] = {'optype':['create'], 'replacement_text':'educational development training lessons', 'reg_pattern':educate}\n","\n","paycard = re.compile(r'\\b(payment|card|ticket|debit)(\\s?(card|ticket|debit))?\\b')\n","cleanup_final['payment card'] =    {'optype':['create'], 'replacement_text':'payment card ticket', 'reg_pattern':paycard}\n","\n","licenses = re.compile(r'\\b(subscription|renewal|1 year|extension|license)(.*(subscription|renewal|1 year|extension|license))?(.*(subscription|renewal|1 year|extension|license))?\\b')\n","cleanup_final['licenses'] =        {'optype':['create'], 'replacement_text':'license renewal subscription extension', 'reg_pattern':licenses}\n","\n","download = re.compile(r'\\b(online|digital|download|access|without disc|without disk|epay)(.*(online|digital|download|access|without disc|without disk|epay|version|edition))?\\b')\n","cleanup_final['downloads'] =       {'optype':['create'], 'replacement_text':'online download version', 'reg_pattern':download}\n","\n","ship = re.compile(r'\\b(delivery|deliver|postage|mail|send|ship|shipment)(.*(delivery|deliver|postage|mail|send|ship|shipment))?\\b')\n","cleanup_final['shipping'] =        {'optype':['create'], 'replacement_text':'shipping delivery postage', 'reg_pattern':ship}\n","\n","virus = re.compile(r'\\b(kaspersky|panda|drweb|eset nod32|security|antivirus|virus)(.*(kaspersky|panda|drweb|eset nod32|security|antivirus|virus|software))?\\b')\n","cleanup_final['antivirus'] =       {'optype':['create'], 'replacement_text':'antivirus defender internet security software', 'reg_pattern':virus}\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["done: Wed 10:11:12 05/27/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RzArCadlmhe4","colab_type":"code","outputId":"e62ccd8c-5109-48a4-f063-d55ceae394cd","executionInfo":{"status":"ok","timestamp":1590588674519,"user_tz":240,"elapsed":411,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# here are the routines to implement the above pattern-matching instructions, and modify the delim_items_list column of the dataframe\n","\n","def expand_gram(gram,final_gram_n,fill_strings):\n","    for f in range(final_gram_n - len(gram.split())):\n","        gram = gram + \" \" + fill_strings[f]\n","    return gram\n","\n","def cleanup_service(gramlist=[\"word1 this is a 6 gram\", \"word1\", \"two gram\", \"three gram string\"], \n","                    pattern_dict=OrderedDict({'replace 0007 with 007':{'optype':['sub'],'reg_pattern':re.compile(r'\\b0007\\b'), 'replacement_text':'007', \n","                                                                       'final_gram_n':4, 'fill_strings':['game','computer','electronic']},\n","                                             'replace skayfall with skyfall':{'optype':['sub','create'],'reg_pattern':re.compile(r'\\bskayfall\\b'), 'replacement_text':'skyfall'}})):\n","    \"\"\"\n","    for text modification in the items_delimited dataframe, in an effort to help standardize terms to better highlight similarities between items,\n","    and to help group items a bit more broadly in some cases, so we create fewer clusters with the following graph/network analysis.\n","\n","    gramlist = list of delimited n-grams provided typically from a single cell from 'items_delimited' DF, at a single row, in column = 'delim_name_list'\n","    pattern_dict = ordered dictionary of lists where operations are done in the order created by user (e.g., clean up \"dvd\" variants before cleaning up \"bluray dvd\" variants, so the latter becomes simpler in regex)\n","        keys = representative text, describing what is being done (somewhat irrelevant to this function)\n","        values = dict{  \n","                    optype = list of strings indicating if one wants to do one or more of the following 4 types of operation on the gramlist\n","                            'sub': (sub)stitute regex matches, searching each element in the gramlist,  (len(gramlist) remains the same, but each string in gramlist may shrink or grow or remain unchanged)\n","                            'create': (create) new \"standardized\" list elements if a match is found within the gramlist (so len(gramlist) grows by 1 for each match); original gramlist strings remain unchanged\n","                            'complete_replace': wherever you have matching elements in the gramlist, replace the entire gramlist element with the pattern_dict key (len(gramlist) remains the same, but n in each n-gram may change)\n","                    reg_pattern = regex patterns to find/substitute/create/replace, \n","                    replacement_text = the text to put in place of the reg_pattern match, or to use when creating a new gramlist list element\n","                    final_gram_n = integer; desired final n-gram length (if desired) \n","                    fill_strings= list(padding strings used to append on to the shorter regex matches to make final string = n grams in length, in order from most important to least)... must be long enough!\n","                    }\n","    \"\"\"\n","    print_counter = 0\n","    previous_gramlist = gramlist.copy()\n","    for key_text, op_details in pattern_dict.items():\n","        optype = op_details['optype']\n","        do_sub = 'sub' in optype\n","        do_create = 'create' in optype\n","        do_replace = 'complete_replace' in optype\n","        do_regfind = do_create or do_replace\n","        reg_pattern = op_details['reg_pattern']\n","        replacement_text = op_details['replacement_text']\n","        gram_set_n = False  # don't try to expand the n-gram to an (n+x)-gram unless the information is provided\n","        if 'final_gram_n' in op_details.keys():\n","            gram_set_n = True\n","            final_gram_n = op_details['final_gram_n']\n","        if 'fill_strings' in op_details.keys():\n","            fill_strings = op_details['fill_strings']\n","        else:\n","            gram_set_n = False\n","\n","        updated_gramlist = previous_gramlist.copy()\n","\n","        if do_sub:   # do substitutions first (cleanup), then do create, then do full replace\n","            for idx, gram in enumerate(previous_gramlist):\n","                updated_gramlist[idx] = reg_pattern.sub(replacement_text, gram)\n","            previous_gramlist = updated_gramlist.copy()\n","\n","        if do_regfind:\n","            for idx, gram in enumerate(previous_gramlist):\n","                # if (key_text == 'highlight product origins'):\n","                #     pfind = reg_pattern.findall(previous_gramlist[idx])\n","                #     #ffind = reg_pattern.find(previous_gramlist[idx])\n","                #     sfind = reg_pattern.search(previous_gramlist[idx])\n","                #     if 'abbyy' in pfind:\n","                #         print(f'abby gram: {previous_gramlist[idx]}')\n","                #         print(f'abbyy 1 findall: {pfind}')\n","                #         #print(f'abbyy 1 find: {ffind}')\n","                #         print(f'abbyy 1 search: {sfind}')\n","                    # for adx,aptn in enumerate(pfind):\n","                    #     if 'abbyy' in aptn:\n","                    #         print(f'abbyy 2: position {adx}, element {aptn}, full find = {pfind}')\n","                    #     if 'abbyy' in aptn[0]:\n","                    #         print(f'abbyy 3: position {adx}, element {aptn}[0], full find = {pfind}')\n","                find_list = [x[0] for x in reg_pattern.findall(previous_gramlist[idx])]\n","                if find_list:  # proceed only if we have found some matches\n","                    # if print_counter < 25:\n","                    #     pfind = reg_pattern.findall(previous_gramlist[idx])\n","                    #     mfind = reg_pattern.match(previous_gramlist[idx])\n","                    #     sfind = reg_pattern.search(previous_gramlist[idx])\n","\n","                    #     print(f'{key_text} search gram_list: {previous_gramlist}')\n","\n","                    #     print(f'  findall in \"{previous_gramlist[idx]}\": {pfind}')\n","                    #     print(f'  match   in \"{previous_gramlist[idx]}\": {mfind}')\n","                    #     print(f'  search  in \"{previous_gramlist[idx]}\": {sfind}\\n')\n","                    #     print_counter += 1\n","                    if do_create:  # do creations before full replacements\n","                        new_grams = []\n","                        for nmatch, match_str in enumerate(find_list):\n","                            if match_str:  # make sure it's not an empty list that was found as one of the matching groups\n","                                if replacement_text:\n","                                    new_grams.append(replacement_text)\n","                                elif gram_set_n:\n","                                    new_grams.append(expand_gram(match_str, final_gram_n, fill_strings))\n","                                else:\n","                                    new_grams.append(match_str)\n","                        updated_gramlist += new_grams\n","\n","                \n","                    # if do_replace:\n","                    #     print('You should not be in replace; not employed at this time')\n","                    #     modlist[idx] = replacement_text\n","\n","        previous_gramlist = updated_gramlist.copy()\n","    return updated_gramlist\n","\n","print(f'done: {strftime(\"%a %X %x\")}')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["done: Wed 10:11:14 05/27/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u7w-nOQNQuqO","colab_type":"code","outputId":"56ff48a6-a183-4b4d-bd0d-dfb3fc8809bd","executionInfo":{"status":"ok","timestamp":1590588681816,"user_tz":240,"elapsed":4238,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["# Test it on a few rows  highlight_roots, cleanup_sub, cleanup_sub_create\n","# dlist = items_delimited.at[36,'delim_name_list'].copy()\n","#\n","# for i in range(16000,16030):\n","#     dlist = items_delimited.at[i,'delim_name_list'].copy()\n","#     print(dlist)\n","#     for clean_dict in [highlight_roots,cleanup_sub,cleanup_final]:\n","#         dlist = cleanup_service(dlist,clean_dict)\n","#     print(dlist)\n","\n","\n","for clean_dict in [highlight_roots,cleanup_sub,cleanup_final]: #[cleanup_games,cleanup_sub,cleanup_sub_create]:\n","    items_delimited.delim_name_list = items_delimited.delim_name_list.apply(lambda x: cleanup_service(x,clean_dict))\n","\n","print(f'done: {strftime(\"%a %X %x\")}\\n')\n","items_delimited.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["done: Wed 10:11:21 05/27/20\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>i_cat_id</th>\n","      <th>item_name</th>\n","      <th>i_tested</th>\n","      <th>clean_item_name</th>\n","      <th>delim_name_list</th>\n","      <th>d_len</th>\n","      <th>d_maxgram</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>movie dvd : ! POWER IN glamor (PLAST.) D</td>\n","      <td>False</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>[movie dvd, power in glamor, plast, dvd]</td>\n","      <td>4</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>program home and office digital : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]</td>\n","      <td>False</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, a educational software learning, microsoft office productivity software, educational development training lessons, online download version, online download version]</td>\n","      <td>4</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>40</td>\n","      <td>movie dvd : *** In the glory (UNV) D</td>\n","      <td>False</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>[movie dvd, in glory, unv, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>40</td>\n","      <td>movie dvd : *** BLUE WAVE (Univ) D</td>\n","      <td>False</td>\n","      <td>movie dvd blue wave univ dvd</td>\n","      <td>[movie dvd, blue wave, univ, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>40</td>\n","      <td>movie dvd : *** BOX (GLASS) D</td>\n","      <td>False</td>\n","      <td>movie dvd box glass dvd</td>\n","      <td>[movie dvd, box, glass, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id  i_cat_id                                                                                                item_name  i_tested                                                                                   clean_item_name  \\\n","0        0        40                                                                 movie dvd : ! POWER IN glamor (PLAST.) D     False                                                               movie dvd power in glamor plast dvd   \n","1        1        76  program home and office digital : ! ABBYY FineReader 12 Professional Edition Full [PC, Digital Version]     False  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2        2        40                                                                     movie dvd : *** In the glory (UNV) D     False                                                                        movie dvd in glory unv dvd   \n","3        3        40                                                                       movie dvd : *** BLUE WAVE (Univ) D     False                                                                      movie dvd blue wave univ dvd   \n","4        4        40                                                                            movie dvd : *** BOX (GLASS) D     False                                                                           movie dvd box glass dvd   \n","\n","                                                                                                                                                                                                                                                              delim_name_list  \\\n","0                                                                                                                                                                                                                                    [movie dvd, power in glamor, plast, dvd]   \n","1  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, a educational software learning, microsoft office productivity software, educational development training lessons, online download version, online download version]   \n","2                                                                                                                                                                                                                                             [movie dvd, in glory, unv, dvd]   \n","3                                                                                                                                                                                                                                           [movie dvd, blue wave, univ, dvd]   \n","4                                                                                                                                                                                                                                                [movie dvd, box, glass, dvd]   \n","\n","   d_len  d_maxgram  \n","0      4          3  \n","1      4          6  \n","2      4          2  \n","3      4          2  \n","4      4          2  "]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"code","metadata":{"id":"o3JB31HqQRlS","colab_type":"code","outputId":"e376fc1c-0ca9-4427-8a78-d33eceb4c5b8","executionInfo":{"status":"ok","timestamp":1590588696659,"user_tz":240,"elapsed":291,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Let's remove duplicate entries and unwanted stuff\n","alphnum = list(string.ascii_lowercase) + list('1234567890')  # get rid of all length=1 1-grams\n","def remove_dupes(gramlist):\n","    unwanted = set(['and','weighed in','given y'] + alphnum)\n","    gramset = unwanted\n","    return [x for x in gramlist if x not in gramset and not gramset.add(x)]\n","\n","items_delimited.delim_name_list = items_delimited.delim_name_list.apply(lambda x: remove_dupes(x) )\n","\n","items_delimited['d_len'] = items_delimited.delim_name_list.apply(lambda x: len(x))\n","items_delimited['d_maxgram'] = items_delimited.delim_name_list.apply(maxgram)\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["done: Wed 10:11:36 05/27/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IUA58xQZ72NC","colab_type":"code","outputId":"cea336f6-cebd-4a36-f3d2-02ff9e29dc7b","executionInfo":{"status":"ok","timestamp":1590588698327,"user_tz":240,"elapsed":339,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":459}},"source":["# make item df easier to read for the following stuff\n","items_clean_delimited = items_delimited.copy(deep=True).drop(\"item_name\", axis=1).rename(columns = {'clean_item_name':'item_name'})\n","\n","print(f'done: {strftime(\"%a %X %x\")}')\n","print(\"\\n\")\n","print(items_clean_delimited.describe())\n","print(\"\\n\")\n","items_clean_delimited.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["done: Wed 10:11:37 05/27/20\n","\n","\n","         item_id  i_cat_id  d_len  d_maxgram\n","count      22170     22170  22170      22170\n","mean  11,084.500    46.291  3.778      4.577\n","std    6,400.072    15.941  1.836      2.018\n","min            0         0      1          2\n","25%    5,542.250        37      2          3\n","50%   11,084.500        40      3          4\n","75%   16,626.750        58      5          5\n","max        22169        83     14         18\n","\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>i_cat_id</th>\n","      <th>i_tested</th>\n","      <th>item_name</th>\n","      <th>delim_name_list</th>\n","      <th>d_len</th>\n","      <th>d_maxgram</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>[movie dvd, power in glamor, plast, dvd]</td>\n","      <td>4</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>False</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, a educational software learning, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>8</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>[movie dvd, in glory, unv, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>movie dvd blue wave univ dvd</td>\n","      <td>[movie dvd, blue wave, univ, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>movie dvd box glass dvd</td>\n","      <td>[movie dvd, box, glass, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id  i_cat_id  i_tested                                                                                         item_name  \\\n","0        0        40     False                                                               movie dvd power in glamor plast dvd   \n","1        1        76     False  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2        2        40     False                                                                        movie dvd in glory unv dvd   \n","3        3        40     False                                                                      movie dvd blue wave univ dvd   \n","4        4        40     False                                                                           movie dvd box glass dvd   \n","\n","                                                                                                                                                                                                                                     delim_name_list  \\\n","0                                                                                                                                                                                                           [movie dvd, power in glamor, plast, dvd]   \n","1  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, a educational software learning, microsoft office productivity software, educational development training lessons, online download version]   \n","2                                                                                                                                                                                                                    [movie dvd, in glory, unv, dvd]   \n","3                                                                                                                                                                                                                  [movie dvd, blue wave, univ, dvd]   \n","4                                                                                                                                                                                                                       [movie dvd, box, glass, dvd]   \n","\n","   d_len  d_maxgram  \n","0      4          3  \n","1      8          6  \n","2      4          2  \n","3      4          2  \n","4      4          2  "]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"USoJANp3U9Ei","outputId":"66183039-d235-4a8e-c571-cb8dd710cec9","executionInfo":{"status":"ok","timestamp":1590589474376,"user_tz":240,"elapsed":615,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":629}},"source":["#%%time\n","# Inspect a single n, gathered from all possible delimited n-grams (4.64sec to run this cell without GPU, 4.01sec with GPU)\n","n_in_ngram = 4    # look at, e.g. length-4 (4-grams) strings of words\n","print_top_f = 10  # printout the top xx ngram strings, sorted by frequency of occurrence in the data\n","\n","total_dupe_grams = 0\n","item_ngram = items_clean_delimited.copy(deep=True)\n","item_ngram['delim_ngrams'] = item_ngram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == n_in_ngram])\n","\n","item_ngram = item_ngram.explode('delim_ngrams').reset_index(drop=True) # < 0.2sec this method (CPU)\n","\n","freq_grams = item_ngram.delim_ngrams.value_counts()\n","grams_dupe = len(freq_grams[freq_grams > 1])\n","print(f'done: {strftime(\"%a %X %x\")}')\n","print('\\n')\n","print(f'Number of unique delimited {n_in_ngram}-grams: {len(freq_grams)}')\n","print(f'Number of unique delimited {n_in_ngram}-grams that are duplicated at least once: {grams_dupe}\\n')\n","print(f'Top {print_top_f:d} {n_in_ngram:d}-grams by frequency of appearance in item names:')\n","print(freq_grams[:print_top_f])\n","print('\\n')\n","item_ngram.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of unique delimited 4-grams: 3463\n","Number of unique delimited 4-grams that are duplicated at least once: 376\n","\n","Top 10 4-grams by frequency of appearance in item names:\n","educational development training lessons    1736\n","1 educational software learning             1042\n","microsoft xbox gaming platform               772\n","game pc standard version                     756\n","microsoft office productivity software       619\n","music cd production business                 397\n","gift gadget robot sport                      295\n","program home and office                      277\n","game pc additional publication               240\n","license renewal subscription extension       237\n","Name: delim_ngrams, dtype: int64\n","\n","\n","done: Wed 10:24:33 05/27/20\n","\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>i_cat_id</th>\n","      <th>i_tested</th>\n","      <th>item_name</th>\n","      <th>delim_name_list</th>\n","      <th>d_len</th>\n","      <th>d_maxgram</th>\n","      <th>delim_ngrams</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>[movie dvd, power in glamor, plast, dvd]</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>False</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, a educational software learning, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>8</td>\n","      <td>6</td>\n","      <td>a educational software learning</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>False</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, a educational software learning, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>8</td>\n","      <td>6</td>\n","      <td>microsoft office productivity software</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>False</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>[program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, a educational software learning, microsoft office productivity software, educational development training lessons, online download version]</td>\n","      <td>8</td>\n","      <td>6</td>\n","      <td>educational development training lessons</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>40</td>\n","      <td>False</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>[movie dvd, in glory, unv, dvd]</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id  i_cat_id  i_tested                                                                                         item_name  \\\n","0        0        40     False                                                               movie dvd power in glamor plast dvd   \n","1        1        76     False  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","2        1        76     False  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","3        1        76     False  program home and office digital abbyy finereader 12 professional edition full pc digital version   \n","4        2        40     False                                                                        movie dvd in glory unv dvd   \n","\n","                                                                                                                                                                                                                                     delim_name_list  \\\n","0                                                                                                                                                                                                           [movie dvd, power in glamor, plast, dvd]   \n","1  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, a educational software learning, microsoft office productivity software, educational development training lessons, online download version]   \n","2  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, a educational software learning, microsoft office productivity software, educational development training lessons, online download version]   \n","3  [program home and office digital, abbyy finereader 12 professional edition full, pc, digital version, a educational software learning, microsoft office productivity software, educational development training lessons, online download version]   \n","4                                                                                                                                                                                                                    [movie dvd, in glory, unv, dvd]   \n","\n","   d_len  d_maxgram                              delim_ngrams  \n","0      4          3                                       NaN  \n","1      8          6           a educational software learning  \n","2      8          6    microsoft office productivity software  \n","3      8          6  educational development training lessons  \n","4      4          2                                       NaN  "]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Uvreedy4YRVd"},"source":["#####Gather all info for duplicated n-grams in our delimited set"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"732651ac-13c4-44f8-da52-70fc84edcf03","executionInfo":{"status":"ok","timestamp":1590589257752,"user_tz":240,"elapsed":3384,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"id":"7H2VxLddVqlJ","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%%time \n","# Should take < 4sec on CPU\n","\n","# Get all of the delimited n-grams that are duplicated at least once in item names\n","#  range of sizes of delimited phrases (number of 'words'):\n","\n","min_gram = 1\n","max_gram = items_delimited.d_maxgram.max()\n","\n","total_dupe_grams = 0\n","gram_freqs = {}   # dict will hold elements that are pd.Series with index = phrase, value = number of repeats in items database item names\n","for n in range(min_gram,max_gram+1):\n","    item_ngram = items_clean_delimited.copy(deep=True)\n","    item_ngram['delim_ngrams'] = item_ngram.delim_name_list.apply(lambda x: [a for a in x if len(a.split()) == n])\n","\n","    item_ngram = item_ngram.explode('delim_ngrams').reset_index(drop=True)  \n","\n","    # grams = item_ngram.delim_ngrams.apply(pd.Series,1).stack()  #1min 23sec cpu\n","    # grams.index = grams.index.droplevel(-1)\n","    # grams.name = 'delim_ngrams'\n","    # del item_ngram['delim_ngrams']\n","    # item_ngram = item_ngram.join(grams)\n","\n","    freq_grams = item_ngram.delim_ngrams.value_counts()\n","    print(f'Number of unique delimited {n}-grams: {len(freq_grams)}')\n","    grams_dupe = len(freq_grams[freq_grams > 1])\n","    print(f'Number of unique delimited {n}-grams that are duplicated at least once: {grams_dupe}\\n')\n","    if grams_dupe > 0:\n","        gram_freqs[n] = freq_grams[freq_grams > 1].copy(deep=True)\n","        total_dupe_grams += grams_dupe\n","print(f'\\nTotal number of unique, delimited, duplicated n-grams for all n: {total_dupe_grams}')\n","\n","print(f'done: {strftime(\"%a %X %x\")}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of unique delimited 1-grams: 2819\n","Number of unique delimited 1-grams that are duplicated at least once: 1171\n","\n","Number of unique delimited 2-grams: 4236\n","Number of unique delimited 2-grams that are duplicated at least once: 1190\n","\n","Number of unique delimited 3-grams: 3870\n","Number of unique delimited 3-grams that are duplicated at least once: 752\n","\n","Number of unique delimited 4-grams: 3463\n","Number of unique delimited 4-grams that are duplicated at least once: 376\n","\n","Number of unique delimited 5-grams: 2843\n","Number of unique delimited 5-grams that are duplicated at least once: 288\n","\n","Number of unique delimited 6-grams: 1934\n","Number of unique delimited 6-grams that are duplicated at least once: 143\n","\n","Number of unique delimited 7-grams: 1258\n","Number of unique delimited 7-grams that are duplicated at least once: 68\n","\n","Number of unique delimited 8-grams: 829\n","Number of unique delimited 8-grams that are duplicated at least once: 31\n","\n","Number of unique delimited 9-grams: 522\n","Number of unique delimited 9-grams that are duplicated at least once: 18\n","\n","Number of unique delimited 10-grams: 264\n","Number of unique delimited 10-grams that are duplicated at least once: 10\n","\n","Number of unique delimited 11-grams: 150\n","Number of unique delimited 11-grams that are duplicated at least once: 5\n","\n","Number of unique delimited 12-grams: 75\n","Number of unique delimited 12-grams that are duplicated at least once: 5\n","\n","Number of unique delimited 13-grams: 34\n","Number of unique delimited 13-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 14-grams: 18\n","Number of unique delimited 14-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 15-grams: 9\n","Number of unique delimited 15-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 16-grams: 2\n","Number of unique delimited 16-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 17-grams: 2\n","Number of unique delimited 17-grams that are duplicated at least once: 0\n","\n","Number of unique delimited 18-grams: 1\n","Number of unique delimited 18-grams that are duplicated at least once: 0\n","\n","\n","Total number of unique, delimited, duplicated n-grams for all n: 4057\n","CPU times: user 2.91 s, sys: 15.7 ms, total: 2.92 s\n","Wall time: 2.92 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hnkJJylUh55B","colab_type":"code","outputId":"448179df-560f-4eeb-eba3-c3f94c28fcef","executionInfo":{"status":"ok","timestamp":1590575304791,"user_tz":240,"elapsed":149185,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["'''\n","May 25: try adjusting code to incude ngrams in range 1 and up, but reduce weight for n-grams that contain many common words\n","'''\n","start_n = 0\n","finish_n = 10\n","# first, inspect data to see what are the common n-grams of little value in determining cluster coupling\n","df_busy_grams=pd.DataFrame({'n3_names':gram_freqs[3].index[start_n:finish_n], 'n3_counts':gram_freqs[3].values[start_n:finish_n],\n","                 'n4_names':gram_freqs[4].index[start_n:finish_n], 'n4_counts':gram_freqs[4].values[start_n:finish_n],\n","                 'n5_names':gram_freqs[5].index[start_n:finish_n], 'n5_counts':gram_freqs[5].values[start_n:finish_n]\n","                 })\n","print(df_busy_grams)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["                   n3_names  n3_counts                                  n4_names  n4_counts                                         n5_names  n5_counts\n","0   online download version       2097  educational development training lessons       1736            music music media production business        397\n","1          movie bluray dvd       1787           1 educational software learning       1042                  program home and office digital        333\n","2  russian language version       1688            microsoft xbox gaming platform        772    antivirus defender internet security software        243\n","3         music music media       1217                  game pc standard version        756                xbox 360 russian language version        151\n","4           game pc digital       1125    microsoft office productivity software        619     batman game computer electronic multirelease        136\n","5             game xbox 360        501                   gift gadget robot sport        295                       call of duty game computer        135\n","6       payment card ticket        371                   program home and office        277  warhammer game computer electronic multirelease        115\n","7             gift soft toy        366            game pc additional publication        240              angry bird game computer electronic         97\n","8  english language version        346    license renewal subscription extension        237                star war game computer electronic         93\n","9    cinema special version        332                  gift souvenir weighed in        228                       lego brand lego style game         91\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sw9OnsLGv5P0","colab_type":"code","outputId":"161a35c9-0b17-46a5-a695-3f62f3e797f9","executionInfo":{"status":"ok","timestamp":1590575304957,"user_tz":240,"elapsed":149343,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# format data for feeding into word vector creator\n","\n","count_bins = [0, 2, 4, 8, 16, 32, 128, 1024, 32768]\n","idf_weights = [8,7,6,5,4,3,2,1]  # more weight for ngrams with lower counts\n","\n","notfirst = False\n","for n,s in gram_freqs.items():\n","    a=len(s)\n","    n_array = np.ones(a,dtype=np.int32)*n\n","    gram_count = s.values.astype(np.int32)\n","    gram_string0 = s.index.to_numpy(dtype='str')\n","    gram_string = [re.compile(r'\\b' + gs + r'\\b') for gs in gram_string0]  # I'm not looking for partial words; n-grams must match at word boundaries\n","    weight_bin = pd.cut(s,count_bins,labels=idf_weights,retbins=False).astype(np.int32)\n","\n","    if notfirst:\n","        n_arrays = np.concatenate((n_arrays,n_array))\n","        gram_counts = np.concatenate((gram_counts,gram_count))\n","        gram_strings = np.concatenate((gram_strings,gram_string))\n","        weight_bins = np.concatenate((weight_bins,weight_bin))\n","    else:\n","        n_arrays = n_array\n","        gram_counts = gram_count\n","        gram_strings = gram_string\n","        weight_bins = weight_bin\n","        notfirst = True\n","\n","print(n_arrays[:5],gram_counts[:5],gram_strings[:5],weight_bins[:5])\n","print(len(n_arrays),len(gram_counts),len(gram_strings),len(weight_bins))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1 1 1 1 1] [2742 1835 1360  893  755] [re.compile('\\\\bpc\\\\b') re.compile('\\\\bregion\\\\b')\n"," re.compile('\\\\bjewel\\\\b') re.compile('\\\\b1c\\\\b')\n"," re.compile('\\\\bbusiness\\\\b')] [1 1 1 2 2]\n","4056 4056 4056 4056\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EdrBr4rN7wr8","colab_type":"code","colab":{}},"source":["# use np matrix storage to speed this up... the following code cell takes about 3 min using np, vs. 8 min with pandas dataframe calculations\n","#   also, reducing np matrix to hold only ngrams of size 3 or greater (5/25/20) takes 48 sec on CPU\n","def make_word_vecs(item_names, ngram_re_patterns, ngram_ns, ngram_weights):\n","    \"\"\"Output is word vectors for input containing item names (english transl)\"\"\"\n","\n","    # create np zeros array of size (number of items, word vector length)\n","    n_items = len(item_names)\n","    wv_len = len(ngram_ns)\n","    item_vec_array = np.zeros((n_items, wv_len), dtype = np.int32)\n","\n","    for g in range(wv_len):\n","        gram_pattern = ngram_re_patterns[g] \n","        gram_len = ngram_ns[g]\n","        gram_weight = ngram_weights[g]\n","        for i in range(n_items):\n","            if gram_pattern.search(item_names[i]):\n","                item_vec_array[i,g] = 2 * gram_len * gram_weight  # use weighting function 2 * (n= length of ngram) * (idf weight from binning above)\n","    return item_vec_array\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrNVLGs1nLmR","colab_type":"code","outputId":"59a5a736-92cf-48c1-c140-db08a03cd16b","executionInfo":{"status":"ok","timestamp":1590575465760,"user_tz":240,"elapsed":310127,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","item_word_vectors = make_word_vecs(items_clean_delimited.loc[:,'item_name'].to_numpy(dtype='str'), gram_strings,n_arrays,weight_bins)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 2min 40s, sys: 153 ms, total: 2min 40s\n","Wall time: 2min 40s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_p1QepqQp1kv","colab_type":"code","colab":{}},"source":["# # intermediate point: can save word vectors here for the 22170 items\n","#np.savez_compressed('data_output/item_word_vectorsCompressed.npz', arrayname = item_word_vectors)\n","# # ...\n","# iwv = np.load(\"data_output/item_word_vectors.npz\")\n","# item_word_vectors = iwv['arrayname']\n","# print(item_word_vectors.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_B2ZZPQigJmh"},"source":["#####Use scipy sparse matrices instead of pandas... faster, and less memory use"]},{"cell_type":"code","metadata":{"id":"oWtui0wwF1Yp","colab_type":"code","colab":{}},"source":["item_vec_matrix = sparse.csr_matrix(item_word_vectors)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQEF0onOUtxz","colab_type":"code","outputId":"f21af694-81a5-4eec-8766-2b8a47d95e4b","executionInfo":{"status":"ok","timestamp":1590575468037,"user_tz":240,"elapsed":312381,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","# <2sec for 21,700 items x 4000+ ngrams; output is a csr matrix of type int64\n","dots = item_vec_matrix.dot(item_vec_matrix.transpose()) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 1.26 s, sys: 431 ms, total: 1.69 s\n","Wall time: 1.69 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yFPX5PouV_aw","colab_type":"code","colab":{}},"source":["# wicked fast way to get top K # of items by dot product value (i.e., closest K items to the item of interest)\n","# https://stackoverflow.com/questions/31790819/scipy-sparse-csr-matrix-how-to-get-top-ten-values-and-indices\n","# also, great reference for speeding up python here: https://colab.research.google.com/drive/1nMDtWcVZCT9q1VWen5rXL8ZHVlxn2KnL\n","\n","@jit(cache=True)\n","def row_topk_csr(data, indices, indptr, K):\n","    \"\"\"Take a sparse scipy csr matrix, and for each column, find the K largest \n","    values in that column (like argmax or argsort[:K]).  Return the row indices \n","    and associated values for each column as two separate np arrays of \n","    length = number of columns in sparse matrix.  Inputs are data/indices/indptr\n","    of csr matrix, and integer K.  Call function like this:\n","    rows, vals = row_topk_csr(csr_name.data, csr_name.indices, csr_name.indptr, K)\n","    Use jit by importing jit and prange from numba, and decorating with\n","    @jit(cache=True) immediately before this function definition\n","    (adopted from https://stackoverflow.com/users/3924566/deepak-saini ) \"\"\"\n","\n","    m = indptr.shape[0] - 1\n","    max_indices = np.zeros((m, K), dtype=indices.dtype)\n","    max_values = np.zeros((m, K), dtype=data.dtype)\n","    # for i in prange(m):\n","    #     top_inds = np.argsort(data[indptr[i] : indptr[i + 1]])[::-1][:K]\n","    #     max_indices[i] = indices[indptr[i] : indptr[i + 1]][top_inds]\n","    #     max_values[i] = data[indptr[i] : indptr[i + 1]][top_inds]\n","    for i in prange(m):\n","        top_inds = np.arange(22190-K,22190)\n","        tops = np.argsort(data[indptr[i] : indptr[i + 1]])[::-1][:K]\n","        top_inds[:len(tops)] = tops\n","        #print(i,top_inds)\n","        max_indices[i] = indices[indptr[i] : indptr[i + 1]][top_inds]\n","        max_values[i] = data[indptr[i] : indptr[i + 1]][top_inds]\n","\n","    return max_indices, max_values\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3D9OVzjXDlGm","colab_type":"code","outputId":"da1942d8-e972-493a-a148-fd07cbcd70aa","executionInfo":{"status":"ok","timestamp":1590575474758,"user_tz":240,"elapsed":319085,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["%%time\n","dots.setdiag(0)\n","print(dots.indptr.shape)\n","kval = 20\n","closest_indices, highest_values = row_topk_csr(dots.data, dots.indices, dots.indptr, K=kval)  # Changed K from 10 to 2 on 5/25/20 to 3 on 5/26 to 15 with fakedots"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n"],"name":"stderr"},{"output_type":"stream","text":["(22171,)\n","CPU times: user 6.15 s, sys: 458 ms, total: 6.61 s\n","Wall time: 6.66 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-xjDRQcM9odF","colab_type":"code","outputId":"a976beb7-87ea-4003-d628-2e80c57b349d","executionInfo":{"status":"ok","timestamp":1590575474759,"user_tz":240,"elapsed":319077,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["#print(closest_indices.shape)\n","print(closest_indices[:5,:7]) #[:10,:])\n","print(highest_values[8000:8008,:7])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 9920  9922 16973 21346 21661  9932 21667]\n"," [ 1155  1154  1156  1152  1153  1157  1182]\n"," [17212 16518 16519 16521 16616 16691 16692]\n"," [19630  9633 20027 10463 10462  9029 12427]\n"," [ 9172  2716  9449  9450  9451  7732  7845]]\n","[[ 656  480  480  480  480  480  480]\n"," [ 864  864  864  864  864  864  656]\n"," [ 224  224  224  224  224  224  224]\n"," [1024 1024 1024 1024 1024  224  224]\n"," [ 152  152  152  152  152  152  152]\n"," [2448  928  864  864  864  864  864]\n"," [2448  736  736  528  480  480  480]\n"," [ 772  720  708  708  708  672  672]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OfOzk_xWnF7q","colab_type":"code","outputId":"d727ba60-efeb-4e72-c88e-05898a106b7d","executionInfo":{"status":"ok","timestamp":1590575476990,"user_tz":240,"elapsed":321298,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":258}},"source":["similar_items = pd.DataFrame({'item_id':range(22170)}) #,'close_item_idx':closest_indices,'close_item_dot':highest_values})\n","similar_items['close_item_idx'] = [closest_indices[x][:kval] for x in range(22170)]\n","similar_items['close_item_dot'] = [highest_values[x][:kval] for x in range(22170)]\n","similar_items = similar_items.merge(items_clean_delimited[['item_id','i_tested','i_cat_id']], on='item_id')\n","similar_items['close_item_cat'] = similar_items.close_item_idx.apply(lambda x: [items.at[i,'item_category_id'] for i in x])\n","print(similar_items.head())\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   item_id                                                                                                                                close_item_idx                                                                                               close_item_dot  i_tested  i_cat_id  \\\n","0        0        [9920, 9922, 16973, 21346, 21661, 9932, 21667, 12449, 20043, 21420, 15819, 16616, 13950, 10811, 8125, 10290, 14864, 17831, 8635, 8631]         [288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288]     False        40   \n","1        1                      [1155, 1154, 1156, 1152, 1153, 1157, 1182, 1177, 1174, 1172, 1184, 1170, 1181, 5730, 3873, 3876, 3878, 3877, 3875, 3874]  [10564, 1832, 1832, 1768, 1284, 1048, 984, 984, 984, 984, 984, 984, 964, 948, 932, 932, 928, 928, 928, 928]     False        76   \n","2        2  [17212, 16518, 16519, 16521, 16616, 16691, 16692, 16973, 17125, 17251, 18732, 17265, 17352, 17518, 17831, 17910, 18530, 18531, 18532, 16513]         [288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288]     False        40   \n","3        3                [19630, 9633, 20027, 10463, 10462, 9029, 12427, 13115, 2486, 4662, 5639, 5638, 5603, 5600, 5582, 5163, 4668, 4661, 4569, 4660]         [132, 132, 132, 132, 116, 116, 116, 116, 116, 116, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]     False        40   \n","4        4         [9172, 2716, 9449, 9450, 9451, 7732, 7845, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17732, 17738, 17739, 17740, 17741, 17742]         [816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816, 816]     False        40   \n","\n","                                                                     close_item_cat  \n","0  [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]  \n","1  [75, 76, 76, 76, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 29, 28, 23, 19, 23, 19]  \n","2  [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]  \n","3       [40, 40, 40, 40, 37, 37, 55, 43, 56, 59, 2, 2, 5, 5, 5, 67, 55, 58, 55, 58]  \n","4  [43, 28, 43, 43, 43, 23, 28, 43, 43, 43, 43, 43, 43, 43, 28, 75, 75, 75, 75, 75]  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OWuI-nw27XFH","colab_type":"code","outputId":"bdb2b68f-14f3-4d44-9034-96d9f7e16016","executionInfo":{"status":"ok","timestamp":1590575478064,"user_tz":240,"elapsed":322362,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["# create a graph with nodes = item ids in test set, and edge weights = dot product values\n","\n","# we will use the \"community\" algorithms to determine useful groupings of other items around/including the test items\n","# ##### start with a graph containing the 5100 items in the test set as starter nodes, and add in the 10 highest-match wordvector items if dot product > threshold\n","# TAKING A LEAP... gonna try with 21700 full items dataset / top 10 matches\n","\n","edge_threshold = 100  # dot product (edge weight) must be greater than this for two item_ids to be connected in the graph\n","\n","graph_items = similar_items[['item_id','close_item_idx']].copy(deep=True).explode('close_item_idx').reset_index(drop=True)\n","graph_weights = similar_items[['item_id','close_item_dot']].copy(deep=True).explode('close_item_dot').reset_index(drop=True)\n","graph_items['weight'] = graph_weights.loc[:]['close_item_dot']\n","graph_items.columns = ['item1_id','item2_id','weight']\n","\n","print(len(graph_items))\n","graph_items = graph_items[graph_items.weight > edge_threshold]\n","print(len(graph_items))\n","graph_items.head()\n","# depending on threshold, we may end up dropping some of the test items (for example, we lose item 22154 if threshold = 150, but not if threshold = 100)\n","# K=15\n","# 332550\n","# x\n","# 284132 (thresh 100)\n","# 265912 (200)\n","# 143099 (500)\n","\n","# K=10\n","# 221700\n","# x\n","# 192094 (100)\n","\n","# K = 20\n","# 443400 -> 374990 (100)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["443400\n","374990\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item1_id</th>\n","      <th>item2_id</th>\n","      <th>weight</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>9920</td>\n","      <td>288</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>9922</td>\n","      <td>288</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>16973</td>\n","      <td>288</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>21346</td>\n","      <td>288</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>21661</td>\n","      <td>288</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item1_id item2_id weight\n","0         0     9920    288\n","1         0     9922    288\n","2         0    16973    288\n","3         0    21346    288\n","4         0    21661    288"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"cNTMKGOREZGL","colab_type":"code","outputId":"836c9351-b4ea-4542-bc07-63c4c865b19c","executionInfo":{"status":"ok","timestamp":1590575482570,"user_tz":240,"elapsed":326855,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","# import pandas df into weighted-edge graph:\n","G = nx.from_pandas_edgelist(graph_items, 'item1_id', 'item2_id', ['weight'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 4.39 s, sys: 74.1 ms, total: 4.46 s\n","Wall time: 4.46 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iR2U0g8iY0AN","colab_type":"code","outputId":"4732f721-5392-4a0a-971f-43049d257588","executionInfo":{"status":"ok","timestamp":1590575568021,"user_tz":240,"elapsed":412295,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","# employ a clustering method that utilizes the edge weights\n","communities2 = community.asyn_lpa_communities(G, weight='weight', seed=42)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 1min 25s, sys: 18.5 ms, total: 1min 25s\n","Wall time: 1min 25s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dtbKGa2CcjJa","colab_type":"code","outputId":"226f73c8-fe30-4773-f692-6d8939972f6c","executionInfo":{"status":"ok","timestamp":1590575571395,"user_tz":240,"elapsed":415662,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["num_communities = 0\n","community_items = set()\n","cluster_nodes = []\n","n_nodes = []\n","weight_avgs = []\n","weight_sums = []\n","weight_maxs = []\n","weight_mins = []\n","weight_stds = []\n","for i,c in enumerate(communities2):\n","    num_communities += 1\n","    community_items = community_items | set(c)\n","    nodelist = list(c)\n","    n_nodes.append(len(nodelist))\n","    edgeweights = []\n","    for m in range(n_nodes[-1]-1):\n","        for n in range(m+1,n_nodes[-1]):\n","            try:\n","                edgeweights.append(G.edges[nodelist[m], nodelist[n]]['weight'])\n","            except:\n","                pass\n","    cluster_nodes.append(nodelist)\n","    weight_avgs.append(np.mean(edgeweights))\n","    weight_sums.append(np.sum(edgeweights))\n","    weight_maxs.append(np.max(edgeweights))\n","    weight_mins.append(np.min(edgeweights))\n","    weight_stds.append(np.std(edgeweights))\n","\n","print(num_communities)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1386\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vw2SkxaFd1qm","colab_type":"code","outputId":"44a1fdae-9502-4630-8145-ca23574b22b1","executionInfo":{"status":"ok","timestamp":1590575571396,"user_tz":240,"elapsed":415654,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["weight_avgs = [round(x) for x in weight_avgs]\n","community_df = pd.DataFrame({'n_nodes':n_nodes,'w_avg':weight_avgs,'w_sum':weight_sums,'w_max':weight_maxs,'w_min':weight_mins,'w_std':weight_stds,'cluster_members':cluster_nodes})\n","print(community_df.head())\n","print(\"\\n\")\n","print(community_df.describe())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   n_nodes  w_avg    w_sum  w_max  w_min     w_std  \\\n","0       60    333   216948  21220    288   830.564   \n","1       47    572   181232  10708    132   753.633   \n","2       45   2114  1002168   5456    256 1,674.615   \n","3       36    382   140196   6688    256   528.828   \n","4       10    717    31528   3424    288 1,075.847   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                    cluster_members  \n","0  [0, 16513, 2, 10113, 9604, 14597, 16518, 16519, 17156, 16521, 9489, 9617, 15634, 20, 9492, 18843, 19611, 21661, 14366, 11039, 12449, 14243, 11940, 14885, 17831, 9643, 21420, 14381, 9519, 16691, 16692, 17212, 19520, 12354, 8136, 10442, 20043, 15819, 16973, 11723, 14545, 20308, 13793, 21346, 17251, 18530, 17125, 18531, 18532, 10083, 11492, 14055, 9065, 17518, 17265, 9843, 19060, 17910, 14329, 13950]  \n","1                                                                                           [13058, 20102, 14096, 9490, 15635, 14484, 7448, 11033, 17819, 9500, 9502, 933, 5416, 9515, 10286, 10287, 10288, 9520, 10290, 10162, 22070, 8631, 1079, 9920, 9922, 9027, 5061, 18942, 5063, 9932, 10833, 12375, 4571, 17757, 15838, 15839, 15840, 13792, 22118, 16616, 14062, 19572, 18805, 19062, 18940, 18941, 16894]  \n","2                                                                                                 [10114, 15751, 11667, 12436, 21525, 21664, 21665, 21666, 21667, 21668, 21669, 21670, 21671, 21672, 21674, 12203, 21675, 21677, 21678, 21676, 6064, 21679, 21680, 21681, 21684, 14901, 14902, 21682, 21683, 17852, 8895, 8896, 8897, 8898, 8899, 12352, 8901, 8902, 11988, 11991, 13290, 13291, 19568, 6908, 7678]  \n","3                                                                                                                                                             [11412, 11417, 16410, 16411, 19232, 21539, 9512, 10793, 10794, 9513, 10797, 10798, 10799, 10800, 10801, 10802, 10803, 10804, 10805, 10806, 10807, 12344, 12343, 10810, 10811, 10814, 10815, 18635, 11239, 2923, 2925, 12782, 21876, 8314, 8315, 8316]  \n","4                                                                                                                                                                                                                                                                                                                                                  [9600, 20228, 9732, 14700, 19538, 19540, 8123, 8124, 8125, 8126]  \n","\n","\n","       n_nodes     w_avg       w_sum     w_max     w_min     w_std\n","count     1386      1386        1386      1386      1386      1386\n","mean    14.723 2,176.986 108,437.896 3,838.543 1,882.759   364.566\n","std     46.782 3,499.277 392,505.580 4,926.234 3,519.212   811.717\n","min          2       112         132       112       104         0\n","25%          2   446.500        2336       998       256         0\n","50%          4      1040        9472      2192       548    11.552\n","75%         13      2336       62632      4741      2006   398.724\n","max       1300     37816     7564692     41760     37816 9,397.001\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k1Ch9mz6BNZa","colab_type":"code","outputId":"7445b15a-a6a5-4e7f-8f8b-87cac9197249","executionInfo":{"status":"ok","timestamp":1590577948886,"user_tz":240,"elapsed":333,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":680}},"source":["cluster_items = community_df[['n_nodes','cluster_members']].copy(deep=True).explode('cluster_members').reset_index(drop=True)\n","print(f'community_df length: {len(community_df)}')\n","print(f'cluster_items df length: {len(cluster_items)}')\n","print(f'number of unique item ids contained in clusters: {cluster_items.cluster_members.nunique()}')\n","for nn in [9,24,49]:\n","    nn_community = community_df.query(\"n_nodes > @nn\").copy(deep=True)\n","    print(f'number of clusters with at least {nn+1} items as members: {len(nn_community)}')\n","    print(nn_community.describe())\n","    print('\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["community_df length: 1386\n","cluster_items df length: 20406\n","number of unique item ids contained in clusters: 20406\n","number of clusters with at least 10 items as members: 421\n","       n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","count      421       421         421       421     421       421\n","mean    40.542   990.131 333,508.561 5,282.546 375.696   690.150\n","std     79.047 1,079.365 658,943.227 5,751.013 334.278   930.635\n","min         10       139        7288       168     104         0\n","25%         15       453       64200      1456     196   174.118\n","50%         23       682      132068      3504     256   425.795\n","75%         40      1098      369912      6904     404   887.429\n","max       1300     11251     7564692     41760    2940 9,397.001\n","\n","\n","number of clusters with at least 25 items as members: 193\n","       n_nodes   w_avg       w_sum     w_max   w_min     w_std\n","count      193     193         193       193     193       193\n","mean    69.917 831.031 592,362.363 6,135.855 288.725   554.506\n","std    109.756 839.341 892,851.345 5,933.162 180.042   780.924\n","min         25     139       33016       244     104         0\n","25%         34     451      181620      2044     184   181.767\n","50%         44     606      344432      4364     256   387.038\n","75%         70     984      629580      7716     344   719.354\n","max       1300    9517     7564692     41760    1296 9,397.001\n","\n","\n","number of clusters with at least 50 items as members: 74\n","       n_nodes   w_avg         w_sum     w_max   w_min     w_std\n","count       74      74            74        74      74        74\n","mean   124.824 738.162 1,014,264.595 7,232.919 252.216   504.133\n","std    163.223 491.147 1,164,855.818 6,298.664 113.481   456.089\n","min         50     157         87276       528     104     9.530\n","25%         62 440.500        443242      2659     148   179.006\n","50%         85     604        677730      4974     244   391.680\n","75%        114     842       1111407      9676     313   717.626\n","max       1300    2933       7564692     27240     592 2,267.530\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hPsvR9H_1q-W","colab_type":"code","colab":{}},"source":["%quickref"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_4eyQwLl2ObO","colab_type":"code","outputId":"10f02bfb-002c-425b-9f27-adb8d8046eef","executionInfo":{"status":"ok","timestamp":1590578229511,"user_tz":240,"elapsed":334,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def tstfun(a,b):\n","    print(a+b)\n","\n",",tstfun one two"],"execution_count":0,"outputs":[{"output_type":"stream","text":["onetwo\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BvopMozVAK2W","colab_type":"code","outputId":"eb4b6077-5d2c-46d6-dc91-323b733ca1da","executionInfo":{"status":"error","timestamp":1590578000959,"user_tz":240,"elapsed":340,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# with K=15 and threshold = 100, we get 1624 clusters, quantiles of n_nodes = 2 min, 2, 4 med, 11, 1161 max; 479 clusters with at least 10 items 479/10... 182/25...73/50;  20405 items actually clustered (out of 21700)\n","# with K=15 and threshold = 200, we get 1664 clusters, quantiles of n_nodes = 2 min, 2, 4 med, 10, 1164 max; 431 clusters with at least 10 items, 19787 items actually clustered (out of 21700)\n","# with K=15 and threshold = 500, we get 1843 clusters, quantiles of n_nodes = 2 min, 2, 3 med,  6,  236 max; 319 clusters with at least 10 items 319/10... 103/25...31/50;  13422 items actually clustered (out of 21700)\n","# with K=10 and threshold = 100, we get 1962 clusters, quantiles of n_nodes = 2 min, 2, 4 med, 10, 1096 max; 529 clusters with at least 10 items 529/10... 152/25...56/50;  20404 items actually clustered (out of 21700)\n","# community_df length: 1962\n","# cluster_items df length: 20404\n","# number of unique item ids contained in clusters: 20404\n","# number of clusters with at least 10 items as members: 529\n","#        n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","# count      529       529         529       529     529       529\n","# mean    28.371 1,118.951 151,510.904 4,402.389 423.274   719.163\n","# std     56.233 1,174.116 292,205.958 5,415.598 359.267   967.964\n","# min         10       132        2904       132     104         0\n","# 25%         12       509       36548      1108     228   149.643\n","# 50%         17       791       75604      2604     320   385.788\n","# 75%         27      1293      165472      5668     504   987.902\n","# max       1096     11827     3816308     41760    2940 9,873.388\n","\n","\n","# number of clusters with at least 25 items as members: 152\n","#        n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","# count      152       152         152       152     152       152\n","# mean    61.704   917.283 309,274.921 4,552.974 339.474   537.762\n","# std     97.197 1,102.951 480,567.401 5,888.855 251.088   947.367\n","# min         25       167       21352       244     104         0\n","# 25%         30   436.250      108438      1088     196   106.725\n","# 50%         39       639      191836      2586     272   255.362\n","# 75%         58 1,001.750      317200      5629     400   671.011\n","# max       1096     11495     3816308     41760    2448 9,873.388\n","\n","\n","# number of clusters with at least 50 items as members: 56\n","#        n_nodes   w_avg       w_sum     w_max   w_min     w_std\n","# count       56      56          56        56      56        56\n","# mean   110.143 670.875 458,976.500 4,891.786 268.071   423.299\n","# std    148.554 403.079 562,950.003 5,501.423 108.325   516.756\n","# min         50     167       84276       372     104     5.652\n","# 25%     56.500 403.500      198484      1288     196    95.334\n","# 50%         72 558.500      314638      2720     256   255.362\n","# 75%    106.250 823.500      534489      6053     320   575.131\n","# max       1096    1989     3392540     22140     608 2,978.267\n","#\n","\n","\n","#####################################################\n","## use clusters n>49, k=20, thresh = 100\n","###################################\n","\n","# # with K=20 and threshold = 100, we get 1387 clusters, quantiles of n_nodes = 2 min, 2, 4 med, 13, 1319 max; 422 clusters with at least 10 items 422/10... 179/25...78/50;  20406 items actually clustered (out of 21700)\n","# community_df length: 1387\n","# cluster_items df length: 20406\n","# number of unique item ids contained in clusters: 20406\n","# number of clusters with at least 10 items as members: 422\n","#        n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","# count      422       422         422       422     422       422\n","# mean    40.405   987.488 334,640.844 5,371.365 382.237   706.833\n","# std     79.321 1,002.201 663,848.830 6,042.819 366.626   930.412\n","# min         10       142        9836       168     104         0\n","# 25%         15   445.500       61230      1387     196   168.636\n","# 50%         22   688.500      130628      3394     256   424.621\n","# 75%         39 1,149.750      363573      6901     416   910.528\n","# max       1319     11251     7558860     41760    3192 9,339.161\n","\n","\n","# number of clusters with at least 25 items as members: 179\n","#        n_nodes     w_avg       w_sum     w_max   w_min     w_std\n","# count      179       179         179       179     179       179\n","# mean    73.682   878.994 633,793.497 6,350.324 295.330   606.438\n","# std    113.674   860.866 924,124.723 6,362.418 230.057   891.521\n","# min         25       165       36576       484     104     9.963\n","# 25%         33   435.500      198778      2168     164   180.056\n","# 50%         44       623      388704      4276     256   377.799\n","# 75%         75 1,072.500      667122      7824     342   767.243\n","# max       1319      8846     7558860     41760    2448 9,339.161\n","\n","\n","# number of clusters with at least 50 items as members: 78\n","#        n_nodes     w_avg         w_sum     w_max   w_min     w_std\n","# count       78        78            78        78      78        78\n","# mean   124.269   822.628 1,054,526.103 7,313.077 248.205   559.201\n","# std    158.805 1,056.614 1,253,915.801 7,405.189 106.790 1,093.614\n","# min         50       179         68600       528     104     9.963\n","# 25%     63.250   405.250        453204      2467     167   154.897\n","# 50%         83   562.500        666060      4136     244   300.057\n","# 75%    138.750   891.250       1162252      9802     272   628.492\n","# max       1319      8846       7558860     41760     592 9,339.161"],"execution_count":0,"outputs":[{"output_type":"stream","text":["UsageError: Cell magic `%%` not found.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"bmZVij-EgUww","colab_type":"code","outputId":"28c77659-f589-468a-8988-61ab8ff19148","executionInfo":{"status":"ok","timestamp":1589075797609,"user_tz":240,"elapsed":834,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["community_df.w_avg.nunique()\n","# can't use this as a category code because not unique among clusters,\n","# but I want to use the average cluster weights property to encode the cluster category\n","# (higher numbers for category code --> stronger clustering; may be useful to have this correlation instead of random generation of category codes)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1234"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"OhgonOqnhH5s","colab_type":"code","outputId":"134a702c-535a-4b74-c909-97bd2916fa6d","executionInfo":{"status":"ok","timestamp":1589075804790,"user_tz":240,"elapsed":880,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# so, I will sort on w_avg, then on number of nodes as perhaps the next most important defining characteristic of a given cluster\n","#  and, to make the categorization unique, I will take the w_avg value and sum it with the index (row number)...\n","#     (with the sorting, this favors even more the clusters with high average item-to-item similarity)\n","community_df = community_df.sort_values(['w_avg','n_nodes']).reset_index(drop=True)\n","community_df['item_cluster_id'] = community_df.index + community_df['w_avg']\n","community_df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>n_nodes</th>\n","      <th>w_avg</th>\n","      <th>w_sum</th>\n","      <th>w_max</th>\n","      <th>w_min</th>\n","      <th>w_std</th>\n","      <th>item_id</th>\n","      <th>item_cluster_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>[8491, 21478]</td>\n","      <td>104</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>[16040, 16178]</td>\n","      <td>105</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>104</td>\n","      <td>312</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>[14938, 17619, 8188]</td>\n","      <td>106</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>104</td>\n","      <td>1040</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>[5841, 5842, 5843, 5844, 5845]</td>\n","      <td>107</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>22</td>\n","      <td>104</td>\n","      <td>9776</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>[388, 401, 405, 409, 281, 282, 414, 418, 293, 294, 295, 298, 427, 303, 310, 311, 442, 451, 328, 335, 375, 253]</td>\n","      <td>108</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   n_nodes  w_avg  w_sum  w_max  w_min  w_std                                                                                                         item_id  item_cluster_id\n","0        2    104    104    104    104      0                                                                                                   [8491, 21478]              104\n","1        2    104    104    104    104      0                                                                                                  [16040, 16178]              105\n","2        3    104    312    104    104      0                                                                                            [14938, 17619, 8188]              106\n","3        5    104   1040    104    104      0                                                                                  [5841, 5842, 5843, 5844, 5845]              107\n","4       22    104   9776    104    104      0  [388, 401, 405, 409, 281, 282, 414, 418, 293, 294, 295, 298, 427, 303, 310, 311, 442, 451, 328, 335, 375, 253]              108"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"fuEH3MFyjuNH","colab_type":"code","outputId":"603762dc-37e3-4141-cd1b-4adfa01f4b09","executionInfo":{"status":"ok","timestamp":1589075820086,"user_tz":240,"elapsed":753,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# unravel / explode the cluster node lists... we know this will not duplicate item ids, from the counting we did above\n","item_clusters = community_df.copy(deep=True).explode('item_id').reset_index().rename(columns = {'index':'cluster_number'})\n","item_clusters.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cluster_number</th>\n","      <th>n_nodes</th>\n","      <th>w_avg</th>\n","      <th>w_sum</th>\n","      <th>w_max</th>\n","      <th>w_min</th>\n","      <th>w_std</th>\n","      <th>item_id</th>\n","      <th>item_cluster_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>8491</td>\n","      <td>104</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>21478</td>\n","      <td>104</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>16040</td>\n","      <td>105</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>16178</td>\n","      <td>105</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>104</td>\n","      <td>312</td>\n","      <td>104</td>\n","      <td>104</td>\n","      <td>0</td>\n","      <td>14938</td>\n","      <td>106</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   cluster_number  n_nodes  w_avg  w_sum  w_max  w_min  w_std item_id  item_cluster_id\n","0               0        2    104    104    104    104      0    8491              104\n","1               0        2    104    104    104    104      0   21478              104\n","2               1        2    104    104    104    104      0   16040              105\n","3               1        2    104    104    104    104      0   16178              105\n","4               2        3    104    312    104    104      0   14938              106"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"OSnYLCamXr2S","colab_type":"code","outputId":"925c984b-5a6e-48bb-e188-c24030a9b258","executionInfo":{"status":"ok","timestamp":1589075831317,"user_tz":240,"elapsed":1025,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["items_clustered = items_clean_delimited[['item_id','i_cat_id','i_tested','item_name']].merge(item_clusters,on='item_id',how='left')\n","items_clustered = items_clustered[['item_id','i_cat_id','item_cluster_id','i_tested','cluster_number','n_nodes','w_avg','w_sum','w_max','w_min','w_std','item_name']]\n","items_clustered.columns = ['item_id','item_category_id','item_cluster_id','item_tested','cluster_number','n_items_in_cluster','w_avg','w_sum','w_max','w_min','w_std','item_name']\n","print(items_clustered.head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  item_id  item_category_id  item_cluster_id  item_tested  cluster_number  n_items_in_cluster  w_avg  w_sum  w_max  w_min     w_std                                                                                         item_name\n","0       0                40              920        False             556                  10    364   8724   2572    264   460.506                                                               movie dvd power in glamor plast dvd\n","1       1                76             2600        False            1322                  12   1278  61348  10520    404 1,497.166  program home and office digital abbyy finereader 12 professional edition full pc digital version\n","2       2                40              802        False             489                  56    313  83032   4984    144   396.113                                                                        movie dvd in glory unv dvd\n","3       3                40              330        False             129                   6    201   2212   1132    108   294.379                                                                      movie dvd blue wave univ dvd\n","4       4                40             1686        False             886                  56    800 264120   2664    104   240.272                                                                           movie dvd box glass dvd\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KtvuccfmrWJy","colab_type":"code","outputId":"542751e3-2da0-4827-d5db-8c852c3b017a","executionInfo":{"status":"ok","timestamp":1589075844045,"user_tz":240,"elapsed":839,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["# how many test items are represented by clusters?\n","tested_clustered = items_clustered[items_clustered.item_tested==True][['item_id','item_category_id','item_cluster_id','item_name']]\n","tested_clustered['unclustered'] = tested_clustered.apply(lambda x: np.NaN if x.item_cluster_id > 0  else x.item_id, axis = 1)\n","print(tested_clustered.head(10))\n","print('\\n')\n","print(tested_clustered.item_id.nunique())\n","unclustered = tested_clustered.unclustered.unique()\n","unclustered = [x for x in unclustered if x > 0]\n","print(len(unclustered))\n","train_items = sales_train.item_id.unique()\n","print(len(train_items))\n","print(len(items))\n","untrained = [x for x in unclustered if x not in train_items]\n","print(len(untrained))\n","print(len(items) - len(train_items) - len(untrained))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   item_id  item_category_id  item_cluster_id                                                            item_name  unclustered\n","30      30                40             1109                      movie dvd 007 coordinate 007 james bond skyfall          nan\n","31      31                37             1109    movie bluray dvd 007 coordinate 007 james bond skyfall bluray dvd          nan\n","32      32                40             4055                                                    movie dvd 1 and 1          nan\n","33      33                37             4055                                  movie bluray dvd 1 and 1 bluray dvd          nan\n","38      38                41              nan  cinema collector 10 most popular comedy twentieth century 10dvd rem           38\n","42      42                57              231                   music mp3 100 best romantic melody mp3 cd digipack          nan\n","45      45                57              231                   music mp3 100 of best folk song mp3 cd cd digipack          nan\n","51      51                57              231                    music mp3 100 best classical work mp3 cd digipack          nan\n","53      53                57              231                   music mp3 100 best russian song mp3 cd cd digipack          nan\n","57      57                57             1146                music mp3 100 pound of multhitov given y mp3 cd jewel          nan\n","\n","\n","5100\n","640\n","21807\n","22170\n","53\n","310\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nuWZ7bPJ4bR2","colab_type":"code","outputId":"fe99c547-79d0-4672-f1eb-1edd458c763a","executionInfo":{"status":"ok","timestamp":1589075909379,"user_tz":240,"elapsed":1078,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# revert to original item_category_id if item is not in clustered items\n","items_clustered['cluster_code'] = items_clustered.apply(lambda x: x.item_cluster_id if x.item_cluster_id > 0 else x.item_category_id, axis = 1)\n","items_clustered.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>item_category_id</th>\n","      <th>item_cluster_id</th>\n","      <th>item_tested</th>\n","      <th>cluster_number</th>\n","      <th>n_items_in_cluster</th>\n","      <th>w_avg</th>\n","      <th>w_sum</th>\n","      <th>w_max</th>\n","      <th>w_min</th>\n","      <th>w_std</th>\n","      <th>item_name</th>\n","      <th>cluster_code</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>920</td>\n","      <td>False</td>\n","      <td>556</td>\n","      <td>10</td>\n","      <td>364</td>\n","      <td>8724</td>\n","      <td>2572</td>\n","      <td>264</td>\n","      <td>460.506</td>\n","      <td>movie dvd power in glamor plast dvd</td>\n","      <td>920</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>76</td>\n","      <td>2600</td>\n","      <td>False</td>\n","      <td>1322</td>\n","      <td>12</td>\n","      <td>1278</td>\n","      <td>61348</td>\n","      <td>10520</td>\n","      <td>404</td>\n","      <td>1,497.166</td>\n","      <td>program home and office digital abbyy finereader 12 professional edition full pc digital version</td>\n","      <td>2600</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>40</td>\n","      <td>802</td>\n","      <td>False</td>\n","      <td>489</td>\n","      <td>56</td>\n","      <td>313</td>\n","      <td>83032</td>\n","      <td>4984</td>\n","      <td>144</td>\n","      <td>396.113</td>\n","      <td>movie dvd in glory unv dvd</td>\n","      <td>802</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>40</td>\n","      <td>330</td>\n","      <td>False</td>\n","      <td>129</td>\n","      <td>6</td>\n","      <td>201</td>\n","      <td>2212</td>\n","      <td>1132</td>\n","      <td>108</td>\n","      <td>294.379</td>\n","      <td>movie dvd blue wave univ dvd</td>\n","      <td>330</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>40</td>\n","      <td>1686</td>\n","      <td>False</td>\n","      <td>886</td>\n","      <td>56</td>\n","      <td>800</td>\n","      <td>264120</td>\n","      <td>2664</td>\n","      <td>104</td>\n","      <td>240.272</td>\n","      <td>movie dvd box glass dvd</td>\n","      <td>1686</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  item_id  item_category_id  item_cluster_id  item_tested  cluster_number  n_items_in_cluster  w_avg  w_sum  w_max  w_min     w_std                                                                                         item_name  cluster_code\n","0       0                40              920        False             556                  10    364   8724   2572    264   460.506                                                               movie dvd power in glamor plast dvd           920\n","1       1                76             2600        False            1322                  12   1278  61348  10520    404 1,497.166  program home and office digital abbyy finereader 12 professional edition full pc digital version          2600\n","2       2                40              802        False             489                  56    313  83032   4984    144   396.113                                                                        movie dvd in glory unv dvd           802\n","3       3                40              330        False             129                   6    201   2212   1132    108   294.379                                                                      movie dvd blue wave univ dvd           330\n","4       4                40             1686        False             886                  56    800 264120   2664    104   240.272                                                                           movie dvd box glass dvd          1686"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"aFs7GZWs6bkF","colab_type":"code","colab":{}},"source":["# # save what we have; maybe refine later\n","\n","# compression_opts = dict(method='gzip',\n","#                         archive_name='items_clustered_21700.csv')  \n","# items_clustered.to_csv('data_output/items_clustered_21700.csv.gz', index=False, compression=compression_opts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"om26-nvyY28r","colab_type":"code","colab":{}},"source":["# def join_friends(gramlist,friend1,friend2,reverse=False):\n","#     \"\"\"\n","#     Combine things that were inadvertantly separated, like \"x\" and \"box\"\n","#     gramlist = list of text strings, each one is a 'delimited' string found by the above code\n","#     friend1 = first string to search for\n","#     friend2 = second string to search for\n","#     reverse = bool, if we want to check and standardize both orders of friends (like 'x box' as well as 'box x')\n","#     \"\"\"\n","#     f1 = (friend1 in gramlist) \n","#     f2 = (friend2 in gramlist)\n","#     f3 = (friend1 + \" \" + friend2) in gramlist\n","#     if reverse:\n","#         f4 = (friend2 + \" \" + friend1) in gramlist\n","#     else:\n","#         f4 = False\n","\n","#     if (f1 and f2) or f3 or f4:\n","#         if f1:\n","#             gramlist.remove(friend1)\n","#         if f2: \n","#             gramlist.remove(friend2)\n","#         if f4:\n","#             gramlist.remove(friend2 + \" \" + friend1)\n","#         if not f3:\n","#             gramlist.append(friend1 + \" \" + friend2)\n","#     return gramlist\n","\n","# def friends(gramlist):\n","#     friends = []\n","#     friends.append(\"x\",\"box\",False) \n","#     friends.append(\"p\",\"s\")\n","#     friends.append(\"bluray\",\"dvd\",True)\n","#     friends.append(\"4k\",\"bluray dvd\",True)\n","#     friends.append(\"4k bluray\",\"dvd\",True)\n","#     friends.append(\"4k\",\"bluray\",True)\n","#     friends.append(\"4k\",\"dvd\",True)\n","#     friends.append(\"3d\",\"bluray dvd\",True)\n","#     friends.append(\"3d bluray\",\"dvd\",True)\n","#     friends.append(\"3d dvd\",\"bluray\",True)\n","#     friends.append(\"3d\",\"dvd\",True)\n","#     friends.append(\"3d\",\"bluray\",True)\n","# tbd... maybe join delimited text and use a regex?"],"execution_count":0,"outputs":[]}]}