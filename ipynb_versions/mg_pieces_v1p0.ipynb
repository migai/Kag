{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mg_pieces_v1p0.ipynb","provenance":[{"file_id":"1Ngk4p79YNnxL-u6sE3Hhzjp8czz-HJDE","timestamp":1605663112351},{"file_id":"1zoSvdwl9M_wq03QHaMQuQdIcdEN0JRSY","timestamp":1594910950502},{"file_id":"12KA-rL8-rk29NOHJN8-DbZ8fGUESsfgV","timestamp":1589287670669},{"file_id":"1nzPRIdf4UB-3biwx8fCJlTnx8bL126aO","timestamp":1588242465890},{"file_id":"https://github.com/migai/Kag/blob/master/template_Kaggle_Coursera_Final_Assignment.ipynb","timestamp":1587141076973}],"collapsed_sections":["MvuCz4_s1ST1","YPp_Nesy2yxn","-YA__znazThG","i_NGdlH8zbz8","r_Oe76PW3aoN","ufy-J0xC2efV","WLdAjg45wEne","UlH3NopEv1Ha","DP8AZkYQvtaj","MY4tx3FwtVq7","1GXED3-jyQC7","0e0MOvMExQoT","LF91nOlf3Km7","8CCw_Tu1xITH","zPM4RLKcn3RE","lKGrJUG7f50F","KDSgvfrtukP1","qG665uxzQ5J5","HuOKMYlern3c","FPzNWMRAyx6M"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"twk8LgLIzEAI"},"source":["# **Intro and Setup**\n","## See \"kaggle_main_...\" code block for GBDT parameter inputs below\n","## See code block beneath that for some example inputs, useful choices"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2XfFrG-yPCR5","executionInfo":{"status":"ok","timestamp":1605966286937,"user_tz":300,"elapsed":2152,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"7431ce15-7e1d-43b4-d463-2e9ae1ffec4e"},"source":["# imports\n","from google.colab import drive\n","\n","from collections import namedtuple, OrderedDict\n","import datetime\n","import io\n","from itertools import chain, product\n","import math\n","import multiprocessing as mp\n","import os\n","from pathlib import Path\n","import platform\n","from pprint import pprint\n","import psutil\n","import re\n","import sys\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","import pkg_resources\n","from tensorflow import test, distribute\n","# %tensorflow_version 2.x\n","# import tensorflow as tf\n","# import subprocess as sp  # query Windows for amount of physically-present RAM\n","\n","# timing\n","from   time import strftime, tzset\n","os.environ['TZ'] = 'EST+05EDT,M4.1.0,M10.5.0'   # allows user to simply print a formatted version of the local date and time; helps keep track of what cells were run, and when\n","tzset()                                         # set the time zone\n","print(f'Done: {strftime(\"%a %X %x\")}')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Done: Sat 08:44:45 11/21/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tlWiGsn8NXAp"},"source":["###**Mount Google Drive for access to Google Drive local repo and data**"]},{"cell_type":"code","metadata":{"cellView":"both","colab":{"base_uri":"https://localhost:8080/"},"id":"8Rr2YrK_M_s5","executionInfo":{"status":"ok","timestamp":1605966310142,"user_tz":300,"elapsed":25328,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"0a3ec4ae-1bf5-49b2-eef8-b5802ec56ab0"},"source":["# click on the URL link presented to you by this command, get your authorization code from Google, \n","#     then paste it into the input box and hit 'enter' to complete mounting of the drive\n","\n","GDRIVE_REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final/Kag\"\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e51YSJh2HZxb"},"source":["## **kag_utils.py**"]},{"cell_type":"code","metadata":{"id":"idQPDVJqHFXw","executionInfo":{"status":"ok","timestamp":1605966310481,"user_tz":300,"elapsed":25654,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["\"\"\"\n","Provide utility functions to assist main Kaggle program.\n","\n","Created on Tue Oct 20 05:55:03 2020\n","@author: mgaidis\n","\n","    functions:\n","        tdstr():        Return a formatted version of present time and date\n","        timer(t_ref):   Return a formatted version of elapsed time relative to t_ref\n","        dict_to_writelines_list(source_dict, k_v_link_text=': '): Format a dict as a list of\n","            text strings ending in newline, suitable for writelines\n","        col_info(target_df, n_cols=3, inter_col_space=5, col_pad=2, target_df_name=None):\n","            Print a formatted version of a DataFrame's column's data types and memory usage.\n","\n","    classes:\n","        AttrDict: create dictionary-like object where keys can be written like instance attrs.\n","            dict['key'] is the same as dict.key\n","\"\"\"\n","\n","# import datetime\n","# import io\n","# import math\n","# import sys\n","# import time\n","# import numpy as np\n","# import pandas as pd\n","\n","\n","def tdstr():\n","    \"\"\"Return formatted version of the time and date NOW.\"\"\"\n","    return f'{time.strftime(\"%a %X %x\")}'\n","\n","\n","def timer(t_ref):\n","    \"\"\"Return formatted version of elapsed time between performance_counters.\"\"\"\n","    return f'{datetime.timedelta(seconds=round(time.perf_counter() - t_ref))}'\n","\n","\n","def df_dict_summary_to_writelines_list(dfdict, colinfo=False, console=True):\n","    \"\"\"Summary of each DataFrame in the argument {dfname: df}, formatted for writelines output.\"\"\"\n","    writelist = []\n","    for dfname, dframe in dfdict.items():\n","        writelist.extend([f'---------- {dfname} ----------\\n',\n","                          (f'DataFrame shape: {dfdict[dfname].shape}     '\n","                           f'DataFrame total memory usage: '\n","                           f'{(dfdict[dfname].memory_usage(deep=True) / 1e6).sum():.0f} MB\\n'),\n","                          f'DataFrame Column Names: {dfdict[dfname].columns.to_list()}\\n'])\n","        if colinfo:\n","            writelist.extend(col_info(dframe, console=False))\n","        writelist.append(f'^^^^^\\n{dfdict[dfname].head(2)}\\n\\n')\n","    if console:\n","        sys.stdout.writelines(writelist)\n","    return writelist\n","\n","\n","def dict_to_writelines_list(source_dict, k_v_link_text=': '):\n","    \"\"\"Format a dict as a list of text strings ending in newline, suitable for writelines.\"\"\"\n","    writelist = []\n","    for source_key, source_val in source_dict.items():\n","        writelist.append(f'{source_key}{k_v_link_text}{source_val}\\n')\n","    return writelist\n","\n","\n","def col_info(target_df, n_cols=3, target_df_name=None, console=True):\n","    \"\"\"\n","    Format and print information regarding pandas DataFrame column datatypes and memory usage.\n","\n","    More readable than standard printing, this fn prints out multiple columns of length \"n_rows\",\n","    where each column is like:\n","        \"column_dtype   column_memory_use(MB)   column_name\"\n","    and finishes with a printout of total DataFrame memory usage.\n","\n","    Inputs:\n","        target_df: pandas DataFrame to analyze and report on\n","        n_cols: int --> Determines the \"width\" of the printed table by specifying (roughly)\n","            how many truples (type, mem, name) (type, mem, name)... to print in each row\n","        target_df_name: string\n","        console: bool --> print to console (vs. just returning a writelines-formatted list)\n","    \"\"\"\n","    writelist = []\n","    col_mem = target_df.memory_usage(deep=True)\n","    if isinstance(col_mem, int):  # Series rather than DataFrame\n","        dfname_str = f'Series {target_df_name}' if target_df_name else 'Series'\n","        writelist.extend([f\"{dfname_str} shape: {target_df.shape}\\n\",\n","                          f\"{dfname_str} column name: ['{target_df.name}']\\n\",\n","                          f\"{dfname_str} column memory: {col_mem}\\n\",\n","                          f\"{dfname_str} column dtype: {target_df.dtypes}\\n\"])\n","    else:\n","        dfname_str = f'DataFrame {target_df_name}' if target_df_name else 'DataFrame'\n","        # change to kB or MB or GB\n","        exponent = (len(bin(col_mem.max())) - 3) // 10  # 1024 = 2**10; bin() 2 char prefix '0b'\n","        col_mem = col_mem / (1024**exponent)\n","        mem_unit = ['Bytes', 'kBytes', 'MBytes', 'GBytes', 'TBytes', 'PBytes'][exponent]\n","        writelist.extend([f'{dfname_str} shape: {target_df.shape}\\n',\n","                          f'{dfname_str} total memory: {col_mem.sum():.0f} {mem_unit}\\n',\n","                          f'{dfname_str} column names: {target_df.columns.to_list()}\\n'])\n","\n","        # Index is in df.memory_usage, but not df.dtypes, so compute and concat to dtypes df\n","        info_df = pd.concat([pd.Series([target_df.index.dtype], index=['Index']),\n","                             target_df.dtypes], axis=0)\n","        # Combine the memory and the dtypes dataframes\n","        info_df = pd.concat([info_df, col_mem], axis=1).reset_index()\n","        info_df.columns = ['Column Name', 'DType', f'{mem_unit}']\n","        # Format printout of the memory values\n","        info_df[mem_unit] = info_df[mem_unit].apply(lambda x: f'{x:.1f}')\n","\n","        n_cols = min(target_df.shape[0] // 3, n_cols)  # don't make columns too short\n","        if n_cols < 2:  # print all truples in just one column... no special formatting\n","            writelist.extend(io.StringIO(info_df.to_string()).readlines())\n","            # print(info_df)\n","        else:\n","            n_rows = math.ceil(info_df.shape[0] / n_cols)\n","            # n_rows, stragglers = divmod(info_df.shape[0], n_cols)\n","            # n_rows += (stragglers > 0)  # add an extra row if not strictly divisible by n_cols\n","\n","            # Create wide DataFrame by copying the original 3 columns and concatenating to the\n","            # original to form n_cols of truples.  Each new addition is shifted up by n_rows.\n","            info_df = pd.concat([info_df.shift(n_rows * x) for x in range(0, -n_cols, -1)], axis=1)\n","\n","            # Truncate rows to eliminate duplicates made by the above copy-shift-concat operation\n","            # Also, make first row = column names (easier for printing)\n","            info_df = info_df.iloc[:n_rows][:].fillna('').T.reset_index().T.reset_index(drop=True)\n","\n","            # compute max string length in each col, add padding=2 to create a clean column look\n","            str_lengths = np.vectorize(len)\n","            col_widths = np.add(str_lengths(info_df.values.astype(str)).max(axis=0), 2)\n","\n","            for row in range(n_rows + 1):  # create, print strings of all columns in each row\n","                print_row = ''\n","                for col in range(len(info_df.columns)):\n","                    # format string for right alignment to fit 'col_widths'\n","                    print_row = print_row + f'{str(info_df.iloc[row][col]):>{col_widths[col]}} '\n","                    # when done with one column of truples; add 4 extra spaces before next column\n","                    print_row = print_row + ' ' * 4 * ((col + 1) % 3 == 0)\n","                # print(print_row)\n","                writelist.append(f'{print_row}\\n')\n","\n","        writelist.extend([f'\\n{dfname_str} shape: {target_df.shape}\\n',\n","                          f'{dfname_str} total memory usage: {col_mem.sum():.0f} {mem_unit}\\n'])\n","    if console:\n","        sys.stdout.writelines(writelist)\n","    return writelist\n","\n","# =============================================================================\n","# https://www.toptal.com/python/python-class-attributes-an-overly-thorough-guide-item 3\n","#  or pympler for tracking instances\n","#\n","# psutil.swap_memory()\n","# Out[19]: sswap(total=58783318016, used=123, free=456, percent=18.2, sin=0, sout=0)\n","# psutil.virtual_memory()\n","# Out[20]: svmem(total=51267125248, available=123, percent=17.3, used=456, free=789)\n","# =============================================================================\n","\n","\n","class AttrDict(dict):\n","    \"\"\"\n","    Give attribute-format access to string keys of dictionaries.\n","\n","    somedict = {'key': 123, 'stuff': 456}\n","    data = AttrDict(somedict)\n","    print(data.key)\n","    print(data.stuff)\n","    >> 123\n","    >> 456\n","    data.key = 'abc'\n","    print(data.key)\n","    print(data['key'])\n","    >> abc\n","    >> abc\n","    def fn(**i): print(i)\n","    fn(**somedict)\n","    >> {'key': 123, 'stuff': 456}\n","    fn(**data)\n","    >> {'key': 'abc', 'stuff': 456}\n","    data.alpha = 'oh'\n","    fn(**data)\n","    {'key': 5, 'stuff': 456, 'alpha': 'oh'}\n","    data\n","    >> {'key': 5, 'stuff': 456, 'alpha': 'oh'}\n","    \"\"\"\n","\n","    def __getattr__(self, k):\n","        \"\"\"Get the attribute with dot notation.\"\"\"\n","        return self[k]\n","\n","    def __setattr__(self, k, v):\n","        \"\"\"Set the attribute with dot notation.\"\"\"\n","        self[k] = v"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9FmPNiUBHkDJ"},"source":["## **kag_config.py**"]},{"cell_type":"code","metadata":{"id":"k8CgZgIhHnrz","executionInfo":{"status":"ok","timestamp":1605966313140,"user_tz":300,"elapsed":28307,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["\"\"\"\n","Containers for somewhat-constant parameters used by the Kaggle routines.\n","\n","Utilize named tuples for convenience in future referencing.\n","CONFIG = namedtuple('packages pd_opts paths data ftr_paths join_str short_re stat_abbr')\n","    CONFIG.packages = list of strings of relevant package names for version control purposes\n","    CONFIG.pd_opts = namedtuple('max_rows max_cols disp_width max_colwidth float_decimals')\n","    CONFIG.paths = namedtuple('home repo feather data logs outputs')\n","    CONFIG.data = OrderedDict(name: InputInfo), where:\n","        name(key) = string name of DataFrame formed from input file data (one key for each file)\n","        InputInfo(value) = namedtuple('filename filepath all_columns') --> filepath includes name\n","    CONFIG.ftr_paths = OrderedDict(module_name: FeatherPaths), where:\n","        module_name(key) = module name of overall program ('eda', 'data', 'tvt')\n","        FeatherPaths = namedtuple('filename filepath') --> filepath includes name\n","            where each tuple contains a list of DataFrames/Filenames/Paths from the respective\n","            tuple's parent module that are suitable for feather file storage to conserve RAM\n","    CONFIG.join_str = '__'  == characters between feature description string elements\n","    CONFIG.short_re = regex pattern for shortening column names for readability (_id, cat, grp)\n","    CONFIG.stat_abbr = dict for shortening stats names for readabilty (count -> cnt, median -> med)\n","\n","Created on Wed Nov  4 08:43:16 2020\n","@author: mgaidis\n","\"\"\"\n","\n","# import re\n","# import sys\n","# from collections import namedtuple, OrderedDict\n","# from pathlib import Path\n","# import pandas as pd\n","\n","# ===============================================================================\n","# # =============================================================================\n","# # User-Configurable Settings to Guide the Program Execution and Display\n","# # =============================================================================\n","# ===============================================================================\n","GROUP_STAT_JOIN_STR = '__'  # characters between feature description string elements\n","SHORTEN_NAME_RE = re.compile('(_id)|(egory)|(gr)(ou)(p)')  # del _id; category -> cat; group -> grp\n","STATS_ABBR = {'count': 'cnt', 'median': 'med', 'nunique()': 'nunq'}\n","\n","# =============================================================================\n","# Package Version Control: relevant packages for logging the version numbers\n","# =============================================================================\n","PACKAGES = ['pandas', 'matplotlib', 'numpy', 'scikit-learn', 'lightgbm']\n","# , 'keras', 'catboost', 'seaborn', 'nltk', 'newtworkx', 'graphx', 'tensorflow'\n","\n","# =============================================================================\n","# Useful file paths and file names\n","# =============================================================================\n","WINDOWS_HOME = Path(\"C:/Users/mgaid/Documents/GitHub\")\n","COLAB_HOME = Path(\"/content/drive/My Drive/Colab Notebooks/NRUHSE_2_Kaggle_Coursera/final\")\n","REPO_DIR = 'Kag'\n","DATA_DIR = 'readonly/final_project_data'\n","DATA_INPUT_FILENAMES = ['items_enc.csv',\n","                        'shops_enc.csv',\n","                        'date_scaling.csv',\n","                        'stt.csv.gz',\n","                        'test.csv.gz']\n","FTR_DIR = 'ftr_files'\n","LOG_DIR = 'logs'\n","OUTPUTS_DIR = 'models_and_predictions'\n","\n","# =============================================================================\n","# Process modules and associated DataFrames possibly stored as ftr files to conserve RAM\n","# =============================================================================\n","FTR_MODULES_DFS = OrderedDict([\n","    ('eda', ['items_enc', 'shops_enc', 'date_scaling', 'stt']),  # 'test' df intentionally omitted\n","    ('data', ['monthly_stt']),\n","    ('tvt', ['train_X', 'train_y', 'val_X', 'val_y', 'test_X'])])\n","\n","# =============================================================================\n","# Preferred options for pandas DataFrame display configuration (adjust based on monitor size)\n","# =============================================================================\n","MAX_ROWS = 60\n","MAX_COLS = 40\n","DISP_WIDTH = 180\n","MAX_COLWIDTH = None\n","FLOAT_DECIMALS = 3\n","\n","# ===============================================================================\n","# # =============================================================================\n","# # END OF User-Configurable Settings to Guide the Program Execution and Display\n","# # =============================================================================\n","# ===============================================================================\n","\n","PandasOpts = namedtuple('PandasOpts', 'max_rows max_cols disp_width max_colwidth float_decimals')\n","pandas_opts = PandasOpts(MAX_ROWS, MAX_COLS, DISP_WIDTH, MAX_COLWIDTH, FLOAT_DECIMALS)\n","\n","home_path = WINDOWS_HOME if sys.platform == 'win32' else COLAB_HOME\n","repo_path = home_path / REPO_DIR  # sync with GitHub (only files < 50 or 100 MB allowed)\n","ftr_path = home_path / FTR_DIR  # local store of fast-loading ftr files, e.g., > 50-100 MB\n","data_path = repo_path / DATA_DIR\n","log_path = repo_path / LOG_DIR\n","output_path = repo_path / OUTPUTS_DIR\n","\n","DrivePaths = namedtuple('DrivePaths', 'home repo feather data logs outputs')\n","kaggle_paths = DrivePaths(home_path, repo_path, ftr_path, data_path, log_path, output_path)\n","for kag_path in kaggle_paths:\n","    kag_path.mkdir(parents=True, exist_ok=True)  # make directories if they do not already exist\n","\n","data_input_dataframe_names = [x.split('.')[0] for x in DATA_INPUT_FILENAMES]\n","data_input_filepaths = [data_path / x for x in DATA_INPUT_FILENAMES]\n","data_input_all_columns = [pd.read_csv(x, nrows=0).columns for x in data_input_filepaths]\n","InputInfo = namedtuple('InputInfo', 'filename filepath all_columns')\n","data_input = OrderedDict([(name, InputInfo(fname, fpath, allcols)) for\n","                          name, fname, fpath, allcols in zip(data_input_dataframe_names,\n","                                                             DATA_INPUT_FILENAMES,\n","                                                             data_input_filepaths,\n","                                                             data_input_all_columns)])\n","\n","ftr_filenames = [[x for x in y] for y in FTR_MODULES_DFS.values()]\n","ftr_filepaths = [[ftr_path / f'{x}.ftr' for x in y] for y in ftr_filenames]\n","FeatherPaths = namedtuple('FeatherPaths', 'filename filepath')\n","feather_paths = OrderedDict([(module_name, FeatherPaths(fnames, fpaths)) for\n","                             module_name, fnames, fpaths in\n","                             zip(FTR_MODULES_DFS.keys(), ftr_filenames, ftr_filepaths)])\n","\n","Cfg = namedtuple('Cfg', 'packages pd_opts paths data ftr_paths join_str short_re stat_abbr')\n","CONFIG = Cfg(packages=PACKAGES,\n","             pd_opts=pandas_opts,\n","             paths=kaggle_paths,\n","             data=data_input,\n","             ftr_paths=feather_paths,\n","             join_str=GROUP_STAT_JOIN_STR,\n","             short_re=SHORTEN_NAME_RE,\n","             stat_abbr=STATS_ABBR)\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_gBDLwSoHw_a"},"source":["## **kag_features.py**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"9aRWky20MtHL","executionInfo":{"status":"ok","timestamp":1605966313510,"user_tz":300,"elapsed":28672,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"695b3bcc-84a3-44bf-c6d6-b573fc3eb0f1"},"source":["\"\"\"\n","Define the various choices for features and lags.\n","\n","Created on Wed Oct 21 08:14:17 2020\n","@author: mgaidis\n","\n","    Enable looping (~~ grid search) over variations in choices of categories to keep,\n","    statistics to use, and lag months (and stats to lag for each month)\n","\n","    Note that instead of 'cartesian product' type looping over all possible combinations of\n","        [lags, category-groups, stats],\n","    we link these specific lag and groups together as one unit for each stats assignment, like:\n","        [[[lags1, groups1, stats_1-1], [lags1, groups2, stats_1-2], [L2, G1, S_2-1], ...]]\n","    instead of\n","        [[[lags1, groups1, stats1], [lags1, groups1, stats2], [L2, G1, S1], [L2, G1, S2], ...]]\n","\n","\"\"\"\n","\n","# import re\n","# from collections import OrderedDict\n","# from kag_config import CONFIG\n","\n","\n","def snake_list_to_camel_string(snake_list):\n","    \"\"\"Convert names from snake to camel, and abbreviate to reduce length for readability.\"\"\"\n","    camel_list = []\n","    for feature_name in snake_list:\n","        feature_name = re.sub(CONFIG.short_re, r'\\3\\5', feature_name)\n","        feature_parts = feature_name.split('_')\n","        camel_list.append(feature_parts[0] + ''.join([f.capitalize() for f in feature_parts[1:]]))\n","    return '_'.join(camel_list)\n","\n","\n","def make_agg_names(group_list, stats_dict, base_features=None):  # vs. base_ = self.cols['no_lag']\n","    \"\"\"\n","    Given group name like 'shop_id__item_id', parse with stats dict to get new column names.\n","\n","    Delete \"_id\", \"category\" -> \"cat\", \"group\" -> \"gp\", and snake -> camelcase to shorten names\n","    Shorten names of statistics to 3 letters\n","    \"\"\"\n","    # =============================================================================\n","    # Handling base features (without statistics during monthly grouping / aggregation)\n","    # =============================================================================\n","    # The first grouping/agg will form the base monthly dataset, and needs to include original\n","    #   features as well as additional statistics based on a subset of those features.\n","    #   Further statistical feature generation and merging does not need to repeat the above.\n","    # =============================================================================\n","    if base_features:\n","        stats_agg_dict = OrderedDict.fromkeys(base_features, ['first'])\n","        stats_agg_dict.update(stats_dict)\n","    else:\n","        stats_agg_dict = stats_dict\n","    group_list = [x for x in group_list if x != 'month']  # 'month' will be assumed; don't need it\n","    group_by = ['month'] + group_list  # monthly aggregates only\n","    agg_dict = OrderedDict([\n","        ('group_name', snake_list_to_camel_string(group_list)),\n","        ('group', group_by),\n","        ('stats', stats_agg_dict),\n","        ('col_names', []),  # include base features that do not get agg stats\n","        ('agg_names', [])])\n","    for data_col, stat_list in stats_agg_dict.items():\n","        for stat in stat_list:\n","            if stat == 'first':  # used to aggregate without applying statistics, to base cols\n","                agg_dict['col_names'].append(data_col)\n","            else:\n","                if stat in CONFIG.stat_abbr.keys():\n","                    stat = CONFIG.stat_abbr[stat]\n","                camel_data_col = snake_list_to_camel_string([data_col])\n","                agg_name = (f'{agg_dict[\"group_name\"]}{CONFIG.join_str}'\n","                            f'{camel_data_col}{stat.capitalize()}')\n","                agg_dict['col_names'].append(agg_name)\n","                agg_dict['agg_names'].append(agg_name)\n","    return agg_dict\n","\n","\n","class LagFeatures:\n","    \"\"\"\n","    Create a set of base + statistical + lagged features for an iteration the main ML loop.\n","\n","    Creating new statistical features:\n","    New features are generated by aggregating statistics while grouping over monthly periods, with\n","      additional grouping elements as desired (e.g., aggregating with group = ['shop_cat'] and\n","      statistics = {'sales': ['sum']} will take the monthly sum of sales for each shop_cat\n","      and make a new feature from it, which will be in a column named shopCat__sales_sum.\n","    If more than one initial grouping element is desired (such as the sum of all sales at shop #x\n","      for item #y, which would be done by grouping ['shop_cat', 'item_id'] while doing the monthly\n","      aggregation of 'sales sum') the column will be named using CONFIG.join_str = '__' and\n","      deleting 'id' with camelCase like so:  shopCat_item__salesSum\n","    Computations are performed using self.lags.min_agg_set with monthly grouping/aggregation.\n","    Then we use the self.lags.month_dict to copy and shift select aggregate columns by n months.\n","\n","    Note on inputs:  'group_list' should include the feature column name(s) to be grouped.\n","        Do not include 'month' in the 'group_list', as that is assumed.\n","        Include full column name for the feature (with '_id' if in original data file col name).\n","        The '_id' will be removed later in creating the stats column names, for readability.\n","\n","    Inputs\n","    ------\n","        lag_feature_list: list of FeatureGroup(FG) namedtuples, where each tuple contains:\n","            1) lag_month : int = a specific number of months to lag by, and\n","            2) group_list : list of strings = the columns on which to group by month, and\n","            3) stats_dict : OrderedDict = specification for aggregation stas on this month/group,\n","                       e.g., OD([('sales', ['count', 'sum']), ('rev', ['sum'])])\n","            eg: [FG(month1, groupname1, statsdict_1-1), FG(month1, groupname2, statsdict_1-2),...]\n","\n","    Attributes\n","    ----------\n","        lag_feature_list: list of lists, copy of the input for posterity\n","        _lag_feature_dict: OrderedDict\n","            key: int = number of months to lag the feature\n","            value: list[agg_name strings] = col names for features to get lagged by KEY months\n","        min_feat_groups_stats_set: strip away monthly lag info/repeats to get just the smallest\n","            set of aggregate grouping stats we need to compute (then we apply lags to this set).\n","            The format is easily interpreted by pandas group/agg funcs.\n","        printable_lag_features: OrderedDict = 'appealing' version of the lag features for print\n","            key: int = number of months being lagged\n","            value: OrderedDict = collection of the various features/stats for KEY lag months\n","                {key: value} =\n","                'group_name': group_name like 'shop_shopGrp'\n","                'group': list like ['month', 'shop_id', 'shop_group']\n","                'stats': stats_dict like {'sales': ['count', 'sum']}\n","                'agg_names': list like ['shop_shopGroup__salesCnt', 'shop_shopGroup__salesSum']\n","\n","        cols: dict = useful groupings of columns for later dataframe manipulations\n","            {key: value} =\n","            all_keep: list of cols (from initial dataframes) needed to create all desired features\n","            keep: dict = names of columns to keep for each individual initial dataframe\n","                key = dfname string; value = list of column string names\n","            cat_feats: list of column string names for the categorical type columns\n","            int_feats: list of column string names for the integer-representable columns\n","            no_lag: list of column string names for those columns holding features without lags\n","            final_stt: list of ordered column string names to specify stt df at end of eda module\n","            min_agg_cols: list of all statistically-aggregated columns in the minimal agg set\n","\n","        lag_month_dict: OrderedDict{\n","                key: int = number of months to lag,\n","                value = OrderedDict{\n","                    key: str = base column name, like 'shop_shopGrp__salesCnt',\n","                    value: str = lag-month-specific col name, like 'shop_shopGroup__salesCnt_L4'}}\n","        all_lag_features: list of all the month-specific column names from the above month_dict\n","\n","    Methods\n","    -------\n","        get_keep_cols(lag_feature_list): from the input info, get a list of ALL datafile-input\n","            DataFrame columns we need to keep, vs. which we can discard to conserve RAM\n","        _assign_column_types(): from the input information, assign the various\n","            data-input dataframe columns to useful groups for future data manipulation\n","        _add_group_features(lag_month, group_name, stats_dict): process an element of input list\n","        get_all_lag_features(): combine useful data structures into something easy to pass on\n","    \"\"\"\n","\n","    def __init__(self, lag_feature_list):\n","        self.lag_feature_list = lag_feature_list\n","        self._lag_feature_dict = OrderedDict()\n","        self.printable_lag_features = OrderedDict()  # pprintable version of lag feature info\n","        self.min_feat_groups_stats_set = OrderedDict()\n","        self.lag_month_dict = OrderedDict()\n","        self.cols = self._assign_column_types()  # useful groupings of columns\n","\n","        for feat_group in self.lag_feature_list:\n","            self._add_group_features(**feat_group._asdict())\n","\n","        self.all_lag_features = self.get_all_lag_features()\n","\n","    def get_keep_cols(self):\n","        \"\"\"Return a list of col names to enable discarding unneeded cols (for speed, RAM).\"\"\"\n","        # Save memory, speedier merges, etc.\n","        base = ['month'] + [*CONFIG.data['test'].all_columns]\n","        cols1 = [col for feat_group in self.lag_feature_list for col in [*feat_group.group_list]]\n","        cols2 = [col for feat_group in self.lag_feature_list for col in [*feat_group.stats_dict]]\n","        return set(cols1 + cols2 + base)\n","\n","    def _assign_column_types(self):\n","        \"\"\"Provide useful groupings of columns for simpler data manipulation in other modules.\"\"\"\n","        columns = {}\n","        columns['all_keep'] = self.get_keep_cols()  # does not distinguish based on DataFrame name\n","        columns['keep'] = {}                        # separate keep cols by DataFrame name\n","        for dfname, dfinfo in CONFIG.data.items():\n","            columns['keep'][dfname] = [x for x in dfinfo.all_columns if x in columns['all_keep']]\n","        columns['cat_feats'] = columns['keep']['shops_enc'] + columns['keep']['items_enc']\n","        columns['no_lag'] = [x for x in columns['cat_feats'] if x not in ['shop_id', 'item_id']]\n","        columns['int_feats'] = ['month', 'shop_id', 'item_id'] + columns['no_lag']\n","        columns['final_stt'] = columns['keep']['stt'] + columns['no_lag']\n","        if 'rev' in columns['all_keep']:\n","            columns['final_stt'].insert(columns['final_stt'].index('sales') + 1, 'rev')\n","        if 'price' not in columns['keep']['stt']:\n","            columns['keep']['stt'].insert(columns['keep']['stt'].index('sales') + 1, 'price')\n","        return columns\n","\n","    def _add_group_features(self, lag_month, group_list, stats_dict):\n","        \"\"\"\n","        Create new group feature(s) based on time-lag and aggregate statistics.\n","\n","        Also create guidelines for smallest set of stats to compute:\n","        min_feat_groups_stats_set = OrderedDict(keys = group name, values = minimal stats needed)\n","        Example formatting of the various attribute containers are shown far below...\n","        \"\"\"\n","        if lag_month not in self._lag_feature_dict:\n","            self._lag_feature_dict[lag_month] = []\n","            self.printable_lag_features[lag_month] = []\n","        if (lag_month == 1) and (group_list == ['shop_id', 'item_id']):\n","            base_features = self.cols['no_lag']\n","        else:\n","            base_features = None\n","        agg_dict = make_agg_names(group_list, stats_dict, base_features)\n","        self._lag_feature_dict[lag_month] += agg_dict.pop('agg_names')\n","        self.printable_lag_features[lag_month].append(agg_dict.copy())\n","        # track only the groups + stats we need to compute, to not repeat for every lag month#.\"\"\"\n","        grp_name = agg_dict.pop('group_name')\n","        del agg_dict['col_names']  # will redo this below with complete compilation of min agg set\n","        if grp_name not in self.min_feat_groups_stats_set:\n","            self.min_feat_groups_stats_set[grp_name] = agg_dict\n","        else:\n","            for data_col, stat_list in stats_dict.items():\n","                if data_col not in self.min_feat_groups_stats_set[grp_name]['stats']:\n","                    self.min_feat_groups_stats_set[grp_name]['stats'][data_col] = stat_list\n","                    continue\n","                for stat in stat_list:\n","                    if stat not in self.min_feat_groups_stats_set[grp_name]['stats'][data_col]:\n","                        self.min_feat_groups_stats_set[grp_name]['stats'][data_col].append(stat)\n","\n","    def get_all_lag_features(self):\n","        \"\"\"Provide lag feature names with associated identification of n months lagged.\"\"\"\n","        self.cols['min_agg_cols'] = []\n","        for gp_name, agg_set_dict in self.min_feat_groups_stats_set.items():\n","            agg_dict = make_agg_names(agg_set_dict['group'], agg_set_dict['stats'])\n","            self.min_feat_groups_stats_set[gp_name]['col_names'] = agg_dict['col_names']\n","            self.cols['min_agg_cols'].extend(agg_dict['agg_names'])  # don't include base cat feats\n","\n","        all_lag_feats = []\n","        for month in sorted([*self._lag_feature_dict]):\n","            self.lag_month_dict[month] = OrderedDict()\n","            for feat in self._lag_feature_dict[month]:\n","                column_name = f'{feat}_L{month}'\n","                self.lag_month_dict[month][feat] = column_name\n","                all_lag_feats.append(column_name)\n","        return all_lag_feats\n","\n","\"\"\"\n","EXAMPLE IS BELOW\n","\"\"\"\n"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nEXAMPLE IS BELOW\\n'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"rjKc54LYHxnp"},"source":["## **kag_program_manager.py**"]},{"cell_type":"code","metadata":{"id":"Xa3emtlyMzu0","executionInfo":{"status":"ok","timestamp":1605966314567,"user_tz":300,"elapsed":29715,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["\"\"\"\n","Provide objects to coordinate operation and input/output of main Kaggle program.\n","\n","Created on Tue Oct 20 05:55:03 2020\n","@author: mgaidis\n","\n","    Classes:\n","        ProgramManager:\n","            Provide oversight of the program's execution:\n","            - Compile computer platform specs and package version control information\n","            - Provide and/or generate file paths for input and output\n","            - Organize distribution of run config params for an efficient loop/split grid search\n","            - Temporary data store to pass information between modules and help reduce RAM demands\n","            - Assist with run documentation (output log files, etc.)\n","        MemStats: obtain and organize information on memory-related issues throughout the program.\n","\n","    Functions:\n","        set_pandas_options(opts=CONFIG.pd_opts):\n","            Adjust pandas setup options for speed enhancement and pleasing display output formats.\n","        get_package_versions(pkg_list=CONFIG.packages):\n","            For version control, describe key package imports with dict like: {pkg: version, ...}.\n","        get_runtime_type():\n","            Check if connected to a CPU vs. GPU-enabled runtime, or possibly a TPU in Colab.\n","        get_phys_dram():\n","            On Windows system, query for actual physical installed DRAM in all available slots.\n","        list_of_lists_splits(param_value):\n","            Given a 'param_value', determine if/how it will result in splits after pd.df.explode.\n","\"\"\"\n","\n","# import os\n","# import sys\n","# import time\n","# from collections import namedtuple, OrderedDict\n","# from itertools import chain\n","# from pprint import pprint\n","\n","# import multiprocessing as mp\n","# import platform\n","# import subprocess as sp  # query Windows for amount of physically-present RAM\n","# import pkg_resources\n","# import psutil\n","\n","# import numpy as np\n","# import pandas as pd\n","\n","# from kag_config import CONFIG\n","# from kag_utils import tdstr, timer, dict_to_writelines_list\n","# from kag_features import LagFeatures\n","\n","FNameContainer = namedtuple('FNameContainer',\n","                            'root base model ftr_paths submit_path output_path log_path')\n","# ScalersContainer = namedtuple('ScalersContainer', 'robust minmax')\n","\n","\n","def set_pandas_options(opts=CONFIG.pd_opts):\n","    \"\"\"Adjust pandas setup options for speed enhancements and for desired UI output formatting.\"\"\"\n","    # speed up operation when using NaNs\n","    pd.set_option('compute.use_bottleneck', False)\n","    # speed up bool ops, large dfs; df.query() and df.eval() will use numexpr\n","    pd.set_option('compute.use_numexpr', False)\n","    # pd.set_option(\"display.max_rows\", opts.max_rows)\n","    pd.set_option(\"display.max_columns\", opts.max_cols)\n","    pd.set_option(\"display.width\", opts.disp_width)\n","    pd.set_option(\"max_colwidth\", opts.max_colwidth)\n","    # decimal places: opts.float_decimals(default=3) for float, 0 for int\n","    pd.options.display.float_format = (\n","        lambda x: f'{x:.0f}' if round(x, 0) == x else f'{x:,.{opts.float_decimals}f}')\n","\n","\n","def get_package_versions(pkg_list=CONFIG.packages):\n","    \"\"\"For version control, describe key package imports with dict like: {pkg: version, ...}.\"\"\"\n","    package_versions = OrderedDict([(\"Python\", platform.python_version())])\n","    for pkg in sorted(pkg_list):\n","        package_versions[pkg] = pkg_resources.get_distribution(pkg).version  # ~ pkg.__version__\n","    return package_versions\n","\n","\n","def get_runtime_type():\n","    \"\"\"Check if connected to a CPU vs. GPU-enabled runtime, or possibly a TPU in Colab.\"\"\"\n","    try:\n","        # from tensorflow import test, distribute\n","        gpu_device_name = test.gpu_device_name()\n","        # if 'GPU' in (gpu_device_name := test.gpu_device_name()):\n","        if 'GPU' in gpu_device_name:\n","            return f'Colab using GPU at: {gpu_device_name}'\n","        try:\n","            tpu = distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","            return f'Colab using TPU at: {tpu.cluster_spec().as_dict()[\"worker\"]}'\n","        except ValueError:\n","            return 'Colab using CPU'\n","    except ImportError:\n","        print('Cannot find runtime type (missing TensorFlow?).')\n","        return 'Unknown runtime type'\n","\n","\n","def get_phys_dram():\n","    \"\"\"On Windows system, query for actual physical installed DRAM in all available slots.\"\"\"\n","    slots = sp.check_output('wmic MemoryChip get Capacity', shell=True).decode().split()[1:]\n","    slots_gb = [int(x) / 2.**30 for x in slots]  # 2**30 converts to GiB\n","    return sum(slots_gb)\n","\n","\n","def list_of_lists_splits(param_value):\n","    \"\"\"Given a 'param_value', determine if/how it will result in splits after pd.df.explode.\"\"\"\n","    split = False\n","    if isinstance(param_value, list):\n","        for param_element in param_value:\n","            if isinstance(param_element, list):\n","                if len(param_element) > 1:\n","                    split = param_element\n","    return split\n","\n","\n","class ProgramManager:\n","    \"\"\"\n","    Provide oversight of the program's execution.\n","\n","    Compile computer platform specs and package version control information\n","    Provide and/or generate file paths for input and output\n","    Organize distribution of run configuration parameters for an efficient loop/split grid search\n","    Serve as a temporary data store to pass information between modules and help reduce RAM use\n","    Assist with run documentation (output log files, etc.)\n","    ...\n","\n","    Attributes\n","    ----------\n","    run_platform: dict     = container for system platform information\n","    n_models: int          = total n splits to run in the program (product of nested loop lengths)\n","    init_datetime: str     = f'{strftime(\"%y%m%d-%H%M\")}' == initialization date-time of prog mgr\n","\n","    t_timer_start: float   = (reused) ref time for fine tracking of module execution elapsed time\n","    model_n: int           = current split number being executed within the looping 'grid search'\n","    filenames: namedtuple  = (root base model ftr_paths submit_path output_path log_path)\n","    feats: LagFeatures()   = LagFeatures(self.split['features'])... formatted input info\n","    splits: OrderedDict    = description of ALL splits to use in the program's 'grid search'\n","                             --> length = total n of modules = len(['module', 'eda', 'data', ...])\n","        key1: str          = module name, such as one of ('module','eda','data', ...)\n","        value1: dict       = description of all user-specified splits within key1's module\n","                             --> length = n splits (rows) 'exploding' key1's module's parameter df\n","            key2: int      = iter n (0 to n_rows-1) of key1's module's 'exploded' parameter df\n","            value2: ODict  = description of values chosen for key1's module's params\n","                             --> length = number of parameters in the key1 module\n","                key3: str  = parameter name\n","                value3:    = user-specified value for key3 param for key2 split in module key1\n","    current_model: ODict   = description of parameters of ALL MODULES for a SINGLE split (model_n)\n","                             --> length = total number of parameter choices for all modules\n","                             (Overwritten as required when looping to a new iteration param set)\n","        key: str           = parameter name\n","        value: (various)   = user-specified parameter value choice for the present iteration/split\n","    # disable scaling at this time...\n","    # robust_scalers: dict   = sklearn robust scaler for each relevant col of monthly_stt df*\n","    # minmax_scalers: dict   = sklearn minmax scaler for each relevant col of monthly_stt df*\n","    #     *shop_item_salesSum column's scalers are needed to inverse-transform the predictions\n","    memory_stats_log: list = instances of mem stats captures throughout the entire program\n","\n","    Methods\n","    -------\n","    mem_capture(program_loc=\"0\", printout=False, single_row=True):\n","        Store immediate memory data in memory_stats_log in the form of a MemStats class\n","        instance.  Optional print of latest or all snapshots to console.\n","    format_memory_stats():\n","        Format all memory stats gathered into the log, for printing or file saving.\n","    print_memory_stats(single_row=True):\n","        Print all program mem stats to console, or just the most recent program location (default).\n","    format_log_mgr_info():\n","        Format relevant information for utilizing 'writelines' to write to a file or the console.\n","    write_log_mgr_info(param_dict=None, console=False)\n","        Write memory information either to the log file, and (optionally) to the console.\n","    update_filenames()\n","        Create named tuple for storing files with filenames adjusted by model_n.\n","    set_features():\n","        Using LagFeatures class, transform user input agg stats/lag features format for pandas.\n","    arrange_run_info(params, output_results):\n","        Explode splits, print details of run to console to help user verify that all is ok.\n","        Return a pd.DataFrame container for storage of output results - one row for each run split.\n","    module_start(module_name):\n","        Print short status and collect memstats; start timer to track module execution time.\n","    module_end(module_name, printout=True, single_row=False):\n","        Collect memory stats; print short status or (optionally) all mem stats since program start.\n","        Save elapsed time for module.\n","    store_dfs(module, dfdict):\n","        Store pd.DataFrames as a class instance attribute. Goal is to avoid pandas gc / RAM issue.\n","    delete_dfs(module):\n","        Delete instance attr, reverting to empty default attr. Goal is to avoid pandas gc issues.\n","    write_ftr_files(module, df_dict):\n","        Write a dictionary of dataframes ({name: df, name2: df2, ...}) to ftr files on disk.\n","        Fast-loading ftr file use is one way to help reclaim unnecessarily-reserved RAM for pandas.\n","    read_ftr_files(module):\n","        Read and return a dict of DataFrames from ftr files on disk. (Matches the above write_ftr.)\n","    \"\"\"\n","\n","    run_platform = OrderedDict([\n","        ('os', sys.platform),  # laptop == 'win32' or COLAB == 'linux'\n","        ('os_full', platform.platform()),\n","        ('runtime', 'Windows' if sys.platform == 'win32' else get_runtime_type()),\n","        ('gb_physical_dram', get_phys_dram() if sys.platform == 'win32' else 0),\n","        ('n_logical_cpu', psutil.cpu_count(logical=True)),\n","        ('n_physical_cpu', psutil.cpu_count(logical=False)),\n","        ('n_multiprocessing_cpu', mp.cpu_count()),\n","        ('chipset', platform.processor())])\n","    n_models = 1\n","    init_datetime = f'{time.strftime(\"%y%m%d-%H%M\")}'\n","    ftr_modules = []\n","    file_rootname = ''\n","\n","    def __init__(self, file_rootname, ftr_modules=None):\n","        set_pandas_options(opts=CONFIG.pd_opts)\n","        ProgramManager.file_rootname = file_rootname\n","        ProgramManager.ftr_modules = ftr_modules if ftr_modules else []\n","        ProgramManager.init_datetime = f'{time.strftime(\"%y%m%d-%H%M\")}'\n","        self.t_timer_start = time.perf_counter()\n","        self.memory_stats_log = []\n","        self.mem_capture(\"Program Start\")\n","\n","        self.model_n = 0\n","        self.filenames = self.update_filenames()\n","        self.all_splits = OrderedDict()\n","        self.split = OrderedDict()\n","        self.feats = None\n","        # self.scalers = self.update_scalers(None, None)\n","        # self.robust_scalers = {}\n","        # self.minmax_scalers = {}\n","        # self.feature_names = None  # tbd, maybe not needed\n","\n","    def mem_capture(self, program_loc=\"0\", printout=False, single_row=True):\n","        \"\"\"Gather memory consumption info.  Print if desired - latest row or all rows.\"\"\"\n","        mem_stat = MemStats(program_loc)\n","        self.memory_stats_log.append(mem_stat)\n","        if printout:\n","            self.print_memory_stats(single_row)\n","\n","    def format_memory_stats(self):\n","        \"\"\"Format all memory stats gathered into the log, for printing or file saving.\"\"\"\n","        mem_str_list = MemStats.print_memstats_header(console=False)  # ok for writelines()\n","        for mem_stat in self.memory_stats_log:\n","            mem_str_list.extend(mem_stat.print_memstats(console=False))\n","        return mem_str_list\n","\n","    def print_memory_stats(self, single_row=True):\n","        \"\"\"Print all program stats, or just the most recent program location (default).\"\"\"\n","        print_list = self.format_memory_stats()\n","        if single_row:\n","            print_list = print_list[:3] + print_list[-1:]\n","        sys.stdout.writelines(print_list)\n","\n","    def format_log_mgr_info(self):\n","        \"\"\"Format relevant information for writing to a file or console, using 'writelines'.\"\"\"\n","        log_data_list = dict_to_writelines_list(get_package_versions(pkg_list=CONFIG.packages),\n","                                                ': version ')\n","        log_data_list += ['\\n'] + dict_to_writelines_list(self.run_platform)\n","        log_data_list += ['\\n'] + self.format_memory_stats()\n","        return log_data_list\n","\n","    def write_log_mgr_info(self, param_dict=None, console=False):\n","        \"\"\"Write relevant information to file.\"\"\"\n","        with open(self.filenames.log_path, 'w+') as output_location:\n","            if param_dict:\n","                pprint(param_dict, output_location)\n","            output_location.writelines(['\\n'] + self.format_log_mgr_info())\n","        if console:  # write to the console\n","            if param_dict:\n","                pprint(param_dict)\n","            sys.stdout.writelines(['\\n'] + self.format_log_mgr_info())\n","\n","    def update_filenames(self):\n","        \"\"\"Modify filenames to reflect iteration number, etc., as we loop through main program.\"\"\"\n","        # Get a basic filename for saving files with various suffixes\n","        if not ProgramManager.file_rootname:\n","            ProgramManager.file_rootname = input(\n","                \"Enter the root filename to use for this run's outputs (e.g., 'v4shops' ): \")\n","        basename = f'{ProgramManager.init_datetime}_{ProgramManager.file_rootname}'\n","        modelname = f'{basename}_{self.model_n:02d}'\n","        submit_path = CONFIG.paths.outputs / f'{modelname}_submission.csv'\n","        output_path = CONFIG.paths.outputs / f'{modelname}_output.csv'\n","        log_path = CONFIG.paths.logs / f'{modelname}_log.txt'\n","        active_ftr_paths = OrderedDict()\n","        for ftr_mod in ProgramManager.ftr_modules:\n","            active_ftr_paths[ftr_mod] = CONFIG.ftr_paths[ftr_mod]\n","        return FNameContainer(ProgramManager.file_rootname, basename, modelname,\n","                              active_ftr_paths, submit_path, output_path, log_path)\n","\n","    def set_features(self):\n","        \"\"\"Expand the input features list into pandas-friendly agg stats dicts and col lists.\"\"\"\n","        self.feats = LagFeatures(self.split['features'])\n","\n","    def arrange_run_info(self, params, output_results):\n","        \"\"\"Explode splits, print details of run to console to help user verify that all is ok.\"\"\"\n","        # =============================================================================\n","        # Find the parameters that have splits in them, and print out to highlight them\n","        # =============================================================================\n","        split_summary = {}\n","        for module_dict in params.values():\n","            for parameter_name, parameter_value in module_dict.items():\n","                if parameter_name != 'features':  # features can be huge; print it below\n","                    splits = list_of_lists_splits(parameter_value)\n","                    if splits:\n","                        split_summary[parameter_name] = splits\n","\n","        # =============================================================================\n","        # Explode the parameter dictionaries so each row is one iteration of modeling parameters\n","        # Keep modules isolated so we don't need to do extra calculations while looping\n","        # =============================================================================\n","        ProgramManager.n_models = 1\n","        self.all_splits = OrderedDict()\n","        for module_name, module_dict in params.items():\n","            module_df = pd.DataFrame(module_dict)\n","            for col in module_df.columns:\n","                module_df = module_df.explode(col)\n","            module_df.reset_index(drop=True, inplace=True)\n","            # to_dict('index'...) gives {int: {row}}, while to_dict('records'...) gives [{row}]\n","            self.all_splits[module_name] = module_df.to_dict('index', into=OrderedDict)\n","            ProgramManager.n_models *= module_df.shape[0]\n","\n","        # Make placeholder rows in outputs_df so n_rows equals the number of models to be trained\n","        # Concat input params with outputs_df to require writing only one log file for entire run\n","        # Could also get [keys] as [*dict] or as d after 'list(map((d:=[]).extend, all_par_list))'\n","        input_params = dict.fromkeys(chain(*params.values()), None)  # {'p': None, ...} for ALL p\n","        output_results.update(input_params)\n","        outputs_df = pd.DataFrame(\n","            list(np.repeat(output_results.values(), ProgramManager.n_models, axis=0)),\n","            columns=list(output_results.keys())).reset_index(drop=True)\n","\n","        # =============================================================================\n","        # Print run 'size' and key splits\n","        # =============================================================================\n","        print(f'N train models: {ProgramManager.n_models}')\n","        print('Splits in this run (excluding features/lags):')\n","        pprint(split_summary)\n","        print('\\n')\n","        self.mem_capture(\"Iteration Parameters Defined\", printout=True, single_row=False)\n","\n","        return outputs_df\n","\n","    def module_start(self, module_name):\n","        \"\"\"Print short status and collect memstats; start timer to track module execution time.\"\"\"\n","        print(f'\\nModel #{self.model_n + 1} of {ProgramManager.n_models}'\n","              f' -- Start {module_name} Module @ {tdstr()}')\n","        self.mem_capture(f'Start {module_name} Module')\n","        self.t_timer_start = time.perf_counter()\n","\n","    def module_end(self, module_name, printout=True, single_row=False):\n","        \"\"\"Print short status and collect memstats; save elapsed time for module.\"\"\"\n","        self.mem_capture(f'End {module_name} Module', printout, single_row)\n","        elapsed = timer(self.t_timer_start)\n","        print(f'{module_name} Module complete; elapsed time = {elapsed}')\n","        self.split[f't_{module_name.lower()}'] = elapsed\n","\n","    def store_dfs(self, module, dfdict):\n","        \"\"\"Store pd.DataFrames as a class instance attr. Goal is to avoid pandas gc / RAM issue.\"\"\"\n","        setattr(ProgramManager, f'{module.lower()}_dfs', {})\n","        setattr(self, f'{module.lower()}_dfs', dfdict)\n","\n","    def delete_dfs(self, module):\n","        \"\"\"Delete instance attr, reverting to empty default attr. Goal is to avoid pd gc issues.\"\"\"\n","        if getattr(self, f'{module.lower()}_dfs') != {}:\n","            try:\n","                delattr(self, f'{module.lower()}_dfs')\n","            except AttributeError as exc:\n","                print(f'Unable to delete instance attr {module.lower()}_dfs: {exc}')\n","\n","    def write_ftr_files(self, module, df_dict):\n","        \"\"\"Write a dictionary of dataframes ({name: df, name2: df2, ...}) to ftr files on disk.\"\"\"\n","        t_write_start = time.perf_counter()\n","        ftr_df_names, ftr_df_paths = self.filenames.ftr_paths[module]\n","        for ftr_df_name, ftr_df_path in zip(ftr_df_names, ftr_df_paths):\n","            df_dict[ftr_df_name].to_feather(ftr_df_path)\n","        return f'{module} ftr file write time: {timer(t_write_start)}'\n","\n","    def read_ftr_files(self, module):\n","        \"\"\"Read and return a dict of dataframes from ftr files on disk.\"\"\"\n","        df_dict = {}\n","        ftr_df_names, ftr_df_paths = self.filenames.ftr_paths[module]\n","        for ftr_df_name, ftr_df_path in zip(ftr_df_names, ftr_df_paths):\n","            df_dict[ftr_df_name] = pd.read_feather(ftr_df_path, columns=None, use_threads=True)\n","        return df_dict\n","\n","    # def update_scalers(self, robust, minmax):\n","    #     \"\"\"Container for sklearn scalers to enable inverse transforming the final predictions.\"\"\"\n","    #     return ScalersContainer(robust, minmax)\n","\n","\n","class MemStats:\n","    \"\"\"\n","    A class to obtain and organize information on memory-related issues throughout the program.\n","\n","    Inspired by pandas garbage collection issue where deleted dataframes can block memory access.\n","    ...\n","\n","    Attributes\n","    ----------\n","    class attributes:\n","    max_meas_str_len, cell_widths, border:\n","        Direct the formatting of tabular output info for ALL collected instances of memory stats\n","\n","    instance attributes:\n","    program_loc : str = description of which module / subroutine created this class instance\n","    pid_mem_use : float = gigabytes of RAM in use per the OS\n","    vm_used : float = gigabytes of RAM in use per the VM\n","    vm_total : float = gigabytes of RAM in total in the VM\n","    vm_available : float = gigabytes of RAM in still available (unused) in the VM\n","    active_proc : list of process objects\n","        info on which live children are still running with multiprocessing\n","        = empty list if processes are properly closed/joined\n","    n_active_proc : int = the number of live child processes still running with multiproc\n","    date_time : string = date and time when the class instance was created\n","\n","    Methods\n","    -------\n","    get_ram(): Queries the OS and VM for memory use.\n","    get_mp_kids(): Query multiproc module to see if child processes are still running ('active')\n","    print_memstats_header(console=True): 3 line heading for tabular printout of memory stats\n","    print_memstats(single_row or all rows): Print the memory stats info for this class instance\n","    \"\"\"\n","\n","    max_meas_str_len = len('Measurement Point')  # = 17\n","    cell_widths = [21, 0, 6, 7, 8, 8, 11]\n","    border = '|'\n","\n","    def __init__(self, program_loc='Start'):\n","        self.program_loc = program_loc\n","        MemStats.max_meas_str_len = max(MemStats.max_meas_str_len, len(self.program_loc))\n","        MemStats.cell_widths[1] = MemStats.max_meas_str_len\n","        self.date_time = f'{tdstr()}'\n","        self.get_ram()\n","        self.get_mp_kids()\n","\n","    def get_ram(self):\n","        \"\"\"Get RAM usage in GB as specified by psutil.Process(pid) and psutil.virtual_memory().\"\"\"\n","        pid = os.getpid()\n","        pyproc = psutil.Process(pid)\n","        self.pid_mem_use = pyproc.memory_info()[0] / 2. ** 30\n","        self.vm_used = psutil.virtual_memory().used / 1e9\n","        self.vm_total = psutil.virtual_memory().total / 1e9\n","        self.vm_available = self.vm_total - self.vm_used\n","\n","    def get_mp_kids(self):\n","        \"\"\"Get a list of active child objects per the multiprocessing configuration being used.\"\"\"\n","        # (Empty list if all multiprocesses are joined / closed properly.)\n","        self.active_proc = mp.active_children()\n","        # self.n_active_proc = len(self.active_proc)\n","\n","    @classmethod\n","    def print_memstats_header(cls, console=True):\n","        \"\"\"Print a header for memory stats info table printout.\"\"\"\n","        # cell_widths = [21, self.max_meas_str_len, 6, 7, 8, 8, 11]\n","        str_list1 = [' ', ' ', 'pid', ' ', 'vm', ' ', ' ']\n","        borders1 = [cls.border, ' ', cls.border, cls.border, ' ', ' ', cls.border, cls.border]\n","        str_list2 = ['Time and Date', 'Measurement Point', 'pid-GB',\n","                     'used-GB', 'avail-GB', 'total-GB', 'Active Proc']\n","        borders2 = [cls.border] * 8\n","        hdr_str1 = borders1[0]\n","        hdr_str2 = borders2[0]\n","        for idx in range(len(cls.cell_widths)):\n","            hdr_str1 = ''.join([hdr_str1, f' {str_list1[idx]:^{cls.cell_widths[idx]}} '])\n","            hdr_str1 = ''.join([hdr_str1, borders1[idx + 1]])\n","            hdr_str2 = ''.join([hdr_str2, f' {str_list2[idx]:^{cls.cell_widths[idx]}} '])\n","            hdr_str2 = ''.join([hdr_str2, borders2[idx + 1]])\n","        hdr_str3 = f'{\"-\" * len(hdr_str1)}'\n","        if console:\n","            print(f'{hdr_str1}\\n{hdr_str2}\\n{hdr_str3}')\n","        return [f'{hdr_str1}\\n', f'{hdr_str2}\\n', f'{hdr_str3}\\n']  # format good for writelines()\n","\n","    def print_memstats(self, console=True):\n","        \"\"\"Print the memory statistics summary to match the header.\"\"\"\n","        mem_str = (f'{self.border} {self.date_time:<{self.cell_widths[0]}} '\n","                   f'{self.border} {self.program_loc:<{self.cell_widths[1]}} '\n","                   f'{self.border} {self.pid_mem_use:>{self.cell_widths[2]}.2f} '\n","                   f'{self.border} {self.vm_used:>{self.cell_widths[3]}.2f} '\n","                   f'{self.border} {self.vm_available:>{self.cell_widths[4]}.2f} '\n","                   f'{self.border} {self.vm_total:>{self.cell_widths[5]}.2f} '\n","                   f'{self.border} {str(self.active_proc):^{self.cell_widths[6]}} '\n","                   f'{self.border}')\n","        if console:\n","            print(mem_str)\n","        return [f'{mem_str}\\n']  # formatted for adoption by writelines()\n","\n","# =============================================================================\n","# Some alternatives for memory tracking:\n","# =============================================================================\n","# https://www.toptal.com/python/python-class-attributes-an-overly-thorough-guide-item 3\n","#  or pympler for tracking instances\n","#\n","# =============================================================================\n","# For reference, here's what you get from psutil queries:\n","# =============================================================================\n","# psutil.swap_memory()\n","# Out[19]: sswap(total=58783318016, used=123, free=456, percent=18.2, sin=0, sout=0)\n","# psutil.virtual_memory()\n","# Out[20]: svmem(total=51267125248, available=123, percent=17.3, used=456, free=789)\n","# =============================================================================\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HToOaW_bHyKh"},"source":["## **kag_eda.py**"]},{"cell_type":"code","metadata":{"id":"bheCqH1AM5WN","executionInfo":{"status":"ok","timestamp":1605966314568,"user_tz":300,"elapsed":29711,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["\"\"\"\n","Exploratory data analysis tweaks and related preprocessing before time-lag feature generation.\n","\n","Issue: freeing up memory used by pandas dataframes that are no longer required by the program\n","    (del + gc.collect() does not reliably accomplish desired result, nor does re-defining the\n","     df as empty pd.DataFrame())\n","To keep from overloading Colab memory limits, we utilize multiprocessing function calls,\n","which use OS-level operations to discard unwanted dataframes, and reliably release unused RAM.\n","\n","Created on Thu Oct 22 21:58:48 2020\n","@author: mgaidis\n","\"\"\"\n","\n","# from collections import namedtuple, OrderedDict\n","# import multiprocessing as mp\n","# import pandas as pd\n","# import numpy as np\n","# from kag_config import CONFIG\n","# from kag_utils import df_dict_summary_to_writelines_list\n","\n","\n","DataFileArgs = namedtuple('DataFileArgs', 'filepath keep_cols')\n","\n","\n","def files_to_dfs_func(load_dfs_args):\n","    \"\"\"\n","    Multiprocess-enabled loading of data to assist with pandas gc issues and RAM use.\n","\n","    Load disk data into pd df using filename defined in kag_config.\n","    Drop unneeded columns for this particular model iteration, as determined in kag_features.\n","    \"\"\"\n","    dataframe = pd.read_csv(load_dfs_args.filepath)\n","    dataframe = dataframe[load_dfs_args.keep_cols]\n","    return dataframe\n","\n","\n","def eda_cleanup(mgr):\n","    \"\"\"\n","    Control the loading of data and application of residual exploratory data analysis (EDA) tweaks.\n","\n","    1) Load pre-processed datafiles from paths defined in the \"kag_config.py\" module.\n","       At present, we utilize the following datafiles after EDA and feature augmentation\n","       described in the following 'utility' notebooks:\n","           'data_cleaning_and_eda_feature_merging_v2_mg.ipynb' --> stt.csv.gz\n","           'nlp_clustering_item_names_v1_june2020_mg.ipynb'    --> items_enc.csv\n","           'time_item_category_shop_correlations_v10_mg.ipynb' --> shops_enc.csv\n","           'calculate_days_per_month.ipynb',\n","           'EDA_sales_by_day_of_week_mg.ipynb'                 --> date_scaling.csv\n","           [no modifications]                                  --> test.csv.gz\n","\n","       stt.csv.gz -> stt = sales_train_test, with additional datetime feature columns\n","       items_enc.csv -> items_enc = items with encoded categorical feature additions,\n","       shops_enc.csv -> shops_enc = shops with encoded categorical feature additions,\n","       date_scaling.csv -> date_scaling = datetime cols and associated retail scaling features\n","       test.csv.gz -> test = template needed to format competion predictions for submission\n","\n","    2) Modify DataFrames using splits defined for the present model iteration\n","       (drop unneded features, merge into stt, scale according to date, adjust datatypes)\n","        a) Use 'multiprocessing' python configuration to help ensure pandas dataframes release RAM\n","           back to the VM when they are no longer needed (simple 'del' command is unreliable;\n","           only workaround I have found to work is to multiprocess to call a new process, which\n","           releases all memory *at the OS level* when process is complete)\n","        b) Also provide option to save quick-loading \"feather\" files and eliminate need to\n","            keep all dataframes in RAM at all times.  This is particularly important when one is\n","            RAM-limited, as is a not-infrequent occurence with big data sets in Google Colab.\n","    \"\"\"\n","    print(\"Loading Files from repo into VM...\\n\")\n","    mgr.feats.cols['keep']['date_scaling'].append(mgr.split['scale_sales'])\n","    load_dfs_arglist = [DataFileArgs(CONFIG.data[name].filepath, mgr.feats.cols['keep'][name])\n","                        for name in [*CONFIG.data]]\n","    # [x.filepath for x in CONFIG.data.values()])))\n","\n","    # ============================================================================================\n","    #   Load data from storage into pandas DataFrames in RAM / VM\n","    # ============================================================================================\n","    # ~~~~  Multiprocessing Block  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    with mp.Pool() as cruncher:\n","        eda_dfs = OrderedDict(zip([*CONFIG.data],\n","                                  cruncher.map(files_to_dfs_func, load_dfs_arglist)))\n","    cruncher.close()\n","    cruncher.join()\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    mgr.mem_capture('Joined load_dfs mp')\n","\n","    mgr.test_df = eda_dfs.pop('test')  # save for future use in formatting Coursera submission\n","    df_dict_summary_to_writelines_list(eda_dfs, colinfo=False, console=True)  # quick df check\n","\n","    # ============================================================================================\n","    #   Merge shops and items into stt\n","    # ============================================================================================\n","    eda_dfs['stt'] = eda_dfs['stt'].merge(eda_dfs['shops_enc'], on='shop_id', how='left')\n","    eda_dfs['stt'] = eda_dfs['stt'].merge(eda_dfs['items_enc'], on='item_id', how='left')\n","    mgr.mem_capture(program_loc=\"Merged stt with shops and items\")\n","    df_dict_summary_to_writelines_list({'merged stt': eda_dfs['stt']}, colinfo=True, console=True)\n","\n","    # ============================================================================================\n","    #   Delete unwanted shops, item cats; scale sales for days in month, etc.\n","    # ============================================================================================\n","    print('----------\\n')\n","    if mgr.split['del_shops']:  # drop undesirable shops\n","        eda_dfs['stt'] = eda_dfs['stt'].query(f'shop_id != {mgr.split[\"del_shops\"]}')\n","        print(f'stt shape after deleting shops #{mgr.split[\"del_shops\"]}: {eda_dfs[\"stt\"].shape}')\n","    if mgr.split['del_item_cats']:  # drop undesirable item categories\n","        eda_dfs['stt'] = eda_dfs['stt'].query(f'item_category_id != {mgr.split[\"del_item_cats\"]}')\n","        print(f'stt shape after deleting item categories {mgr.split[\"del_item_cats\"]}: '\n","              f'{eda_dfs[\"stt\"].shape}\\n')\n","    if mgr.split['scale_sales']:  # scale by date_scaling as desired\n","        eda_dfs['stt'] = eda_dfs['stt'].merge(\n","            eda_dfs['date_scaling'][['month', mgr.split['scale_sales']]], on='month', how='left')\n","        eda_dfs['stt'].sales = eda_dfs['stt'].sales * eda_dfs['stt'][mgr.split['scale_sales']]\n","        eda_dfs['stt'].drop(mgr.split['scale_sales'], axis=1, inplace=True)\n","\n","    # ============================================================================================\n","    #   Insert revenue feature; adjust data types; drop unnecessary cols; set desired col order\n","    # ============================================================================================\n","    eda_dfs['stt']['rev'] = eda_dfs['stt'].sales * eda_dfs['stt'].price / 1000\n","    # float so date_scaling weight is accurate; can use price; divide by 1000 for reasonable range\n","    eda_dfs['stt'][['sales', 'price', 'rev']].astype(np.float32)\n","    # reduce RAM footprint at this point by downcasting integer features\n","    eda_dfs['stt'][mgr.feats.cols['int_feats']] = (eda_dfs['stt'][mgr.feats.cols['int_feats']]\n","                                                   .astype(np.int16))\n","    # drop unnecessary columns and order the others as desired\n","    eda_dfs['stt'] = (eda_dfs['stt'][mgr.feats.cols['final_stt']]\n","                      .reset_index(drop=True))  # reset index saves 25MB\n","    df_dict_summary_to_writelines_list({'final stt': eda_dfs['stt']}, colinfo=True, console=True)\n","\n","    mgr.mem_capture(program_loc=\"Completed stt transformations\")\n","\n","    # ============================================================================================\n","    #   Save ftr files or return dataframes\n","    # ============================================================================================\n","    if 'eda' in mgr.ftr_modules:\n","        mgr.write_ftr_files('eda', eda_dfs)\n","        mgr.mem_capture(program_loc=\"EDA .ftr Saved\")\n","    else:\n","        mgr.store_dfs('eda', eda_dfs)\n","\n","    return mgr\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KjTT6d32Hyiq"},"source":["## **kag_data.py**"]},{"cell_type":"code","metadata":{"id":"7tBv6PqJM95o","executionInfo":{"status":"ok","timestamp":1605966315079,"user_tz":300,"elapsed":30217,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["\"\"\"\n","Group by month, generate new statistical and time-lagged features, and merge with original data.\n","\n","    1) Group training/validation data by months, computing statistics while aggregating\n","        : min_feat_groups_stats_set: dicts formatted to be easily fed into pandas group/agg funcs.\n","        : After performing group/agg on this list, then we apply lags as desired to\n","          each of the various elements, without re-computing the stats for each lag month.\n","    2) Add Cartesian Product fill using unique 'shop_id' and 'item_id' values from each month\n","        : Create dummy rows of relevant month-shop-item combinations to provide model with explicit\n","          'knowledge' of nonexistent transactions.\n","          This can be a RAM abuser, so limit the months of insertion to only the most important\n","          (plus additional earlier rows to account for maximum number of months to lag features).\n","        : If RAM is abundant, one can indicate that cp rows should include all shop-item pairs\n","          from the test set in addition to the default inclusion of only the shop-item pairs\n","          made from cp of unique shop_ids and unique item_ids within a given month.\n","        : Convention during addition of lag statistics is presently to drop any lagged rows wher\n","          the source month shop-item pair is not also present in the destination month.\n","    3) Merge time-lagged statistics, discarding those that don't have a match with an\n","        existing shop-item pair in destination month\n","    4) Clean, Sort, Clip, Scale, and Datatype the monthly_stt df\n","        : (tbd?? Clip per Kaggle competition recommendations to range [0,20])\n","        : Note that int dtypes cannot represent N/A values; use fillna(0) or use np.float32 dtype.\n","          (Price as a feature is not ideal if filling with 0; better to use revenue instead.)\n","        : Scale the feature columns for better use of full range of available datatype values\n","          (np.int16, np.int8, np.uint16,...) unless float\n","        : Use np.iinfo(np.int32) or np.finfo(np.float32) e.g., to get numerical range of np dtypes\n","\n","    : Make frequent use of multiprocessing pools such that pandas DataFrame garbage collection is\n","      not an issue (RAM is reclaimed as soon as pool is closed/joined, whereas pd is unreliable).\n","      By creating iterable sequences to pass to multiprocessing.map(), we can also speed things up.\n","    : Note that the simple action of .reset_index(drop=True) on a pd df can save 25 MB RAM\n","\n","Created on Fri Oct 23 05:22:45 2020\n","@author: mgaidis\n","\"\"\"\n","\n","# from collections import namedtuple\n","# from itertools import product\n","# import multiprocessing as mp\n","# import numpy as np\n","# import pandas as pd\n","# from kag_utils import col_info\n","\n","LagArgs = namedtuple('LagArgs', 'df n_months col_dict shift_eval_str shift_cols')\n","CPArgs = namedtuple('CPArgs', 'month monthly_stt query_str')\n","\n","\n","def group_and_agg(stt, group_agg_dict):\n","    \"\"\"Compute statistics-based features while grouping by month.\"\"\"\n","    grouped_df = stt.groupby(group_agg_dict['group']).agg(group_agg_dict['stats'])\n","    grouped_df.columns = group_agg_dict['col_names']\n","    grouped_df.reset_index(inplace=True)\n","    return grouped_df\n","\n","\n","def create_and_merge_groups(stt, agg_dict):\n","    \"\"\"Create and Merge Groups.\"\"\"\n","    # Initialize monthly_stt to capture standard non-statistical features, then continue\n","    init_group_agg = agg_dict.pop('shop_item')\n","    mo_stt = group_and_agg(stt, init_group_agg)\n","    for group_dict in agg_dict.values():\n","        mo_stt = mo_stt.merge(group_and_agg(stt, group_dict),\n","                              on=group_dict['group'], how='left')\n","    return mo_stt\n","\n","\n","def np_cartesian_product(cp_args):\n","    \"\"\"Create one month's Cartesian Product rows to merge with monthly_stt.\"\"\"\n","    # CPArgs = namedtuple('month monthly_stt query_str')\n","    cartprod_rows = cp_args.monthly_stt[['month', 'shop_id', 'item_id']].query(cp_args.query_str)\n","    return np.array(list(product([cp_args.month],\n","                                 cartprod_rows.shop_id.unique(),\n","                                 cartprod_rows.item_id.unique())),\n","                    dtype=np.int16)\n","\n","\n","def copy_shift_type_lag_cols(lag_args):\n","    \"\"\"Create df for a given number of lag months, to merge with monthly_stt.\"\"\"\n","    #   LagArgs = namedtuple('df n_months col_dict shift_eval_str shift_cols')\n","    lag_df = (lag_args.df[lag_args.shift_cols]\n","              .copy(deep=True)\n","              .rename(columns=lag_args.col_dict))\n","    lag_df['month'] = lag_df['month'] + lag_args.n_months\n","    return lag_df\n","\n","\n","# =============================================================================\n","#     Data Conditioning and Statistical Feature Generation - Main Module\n","# =============================================================================\n","def data_conditioning(mgr):\n","    \"\"\"\n","    Generate features and condition data according to parameter settings.\n","\n","    inputs:     stt (sales train test) dataframe,\n","                shops_enc, items_enc, and the various parameters to guide the actions of\n","                    grouping, aggregating stats, clipping, scaling, adding cartesian product rows,\n","                    creating/merging time-lagged features, and adjusting datatypes for efficiency\n","\n","    outputs:    monthly_stt DataFrame ready for model training\n","                    (grouped by month, cp rows added, lagged stats added, dtypes set)\n","                (disabled for now...  robust_scalers and minmax_scalers\n","                    necessary to inverse transform model predictions before submission)\n","    \"\"\"\n","    # =============================================================================\n","    #     Load ftr files if necessary\n","    # =============================================================================\n","    if 'eda' in mgr.ftr_modules:\n","        eda_dfs = mgr.read_ftr_files('eda')\n","        mgr.mem_capture(program_loc=\"EDA .ftr Loaded\")\n","    else:\n","        eda_dfs = mgr.eda_dfs\n","\n","    # Prefer to clip to Kaggle recommendation [0,20] sales per month only at final prediction time\n","    #   For now, clip sales per day just to eliminate crazy outliers, particularly to avoid issues\n","    #   with outliers in inital data propagating weirdness through agg/group stats. Keep negatives.\n","    eda_dfs['stt'].sales = eda_dfs['stt'].sales.clip(*mgr.split['clip_train'])\n","\n","    # =============================================================================\n","    # 1. Aggregate the Monthly Stats (multiproc for RAM/gc issues, not necessarily for speed)\n","    # =============================================================================\n","    # ~~~~  Multiprocessing Block  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    with mp.Pool() as cruncher:\n","        monthly_stt = cruncher.apply(create_and_merge_groups,\n","                                     [eda_dfs['stt'], mgr.feats.min_feat_groups_stats_set])\n","    cruncher.close()\n","    cruncher.join()\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    print(f'\\nmonthly_stt minimal agg stats grouped and merged: shape = {monthly_stt.shape}')\n","    mgr.mem_capture('Joined Agg mp')\n","    # Make sure pandas hasn't sucked up too much RAM\n","    monthly_stt = monthly_stt.astype(np.float32)\n","    monthly_stt[mgr.feats.cols['int_feats']] = \\\n","        monthly_stt[mgr.feats.cols['int_feats']].astype(np.int16)\n","\n","    # =============================================================================\n","    # 2. Cartesian Product (cp) Rows Insertion\n","    # =============================================================================\n","    cp_arg_list = []\n","    for cp_month in range(max((mgr.split['cp_first_mo'] - max(mgr.feats.lag_month_dict)), 0), 34):\n","        query_str = f'(month == {cp_month})'\n","        if mgr.split['cp_test_pairs']:\n","            query_str = query_str + '|(month == 34)'\n","        cp_arg_list.append(CPArgs(cp_month, monthly_stt, query_str))\n","\n","    # ~~~~  Multiprocessing Block  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    with mp.Pool() as cruncher:\n","        cp_array_list = cruncher.map(np_cartesian_product, cp_arg_list)\n","    cruncher.close()\n","    cruncher.join()\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    mgr.mem_capture('Joined CP mp')\n","\n","    # Merge cartesian product rows into monthly_stt -- will replace 'no_lag' cols afterwards\n","    monthly_stt = (monthly_stt.drop(mgr.feats.cols['no_lag'], axis=1)\n","                              .merge(pd.DataFrame(np.vstack(cp_array_list),\n","                                                  columns=['month', 'shop_id', 'item_id']),\n","                                     how='outer', on=['month', 'shop_id', 'item_id']))\n","    # Insert categorical features info into (empty) CP rows, re-inserting 'no_lag' columns\n","    monthly_stt = monthly_stt.merge(eda_dfs['shops_enc'], how='left', on='shop_id')\n","    monthly_stt = monthly_stt.merge(eda_dfs['items_enc'], how='left', on='item_id')\n","    monthly_stt = ((monthly_stt[mgr.feats.cols['int_feats'] + mgr.feats.cols['min_agg_cols']])\n","                   .sort_values(['month', 'shop_id', 'item_id']).reset_index(drop=True))\n","    mgr.mem_capture('Cartesian Product Rows Added')\n","    col_info(monthly_stt, 2, target_df_name='monthly_stt (with CP rows)')\n","    print(f'\\nNumber of months: {monthly_stt.month.nunique():,d}\\n'\n","          f'Number of shops: {monthly_stt.shop_id.nunique():,d}\\n'\n","          f'Number of items: {monthly_stt.item_id.nunique():,d}\\n'\n","          f'Number of df rows: {len(monthly_stt):,d}\\n'\n","          f'{monthly_stt.describe()}')\n","\n","    # =============================================================================\n","    # 3. Merge Time-Lag Features\n","    # =============================================================================\n","    print(f'Pre-lag monthly_stt DataFrame length: {len(monthly_stt):,d}\\n')\n","    lag_arg_list = []\n","    for lag_mo, lag_dict in mgr.feats.lag_month_dict.items():\n","        lag_arg_list.append(LagArgs(monthly_stt, lag_mo, lag_dict,\n","                                    shift_eval_str=f'month = month + {lag_mo}',\n","                                    shift_cols=['month', 'shop_id', 'item_id'] + [*lag_dict]))\n","\n","    # Get all lag col features in a list of dfs (each list element <--> a given lag month number)\n","    # ~~~~  Multiprocessing Block  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    with mp.Pool() as cruncher:\n","        lag_dfs_list = cruncher.map(copy_shift_type_lag_cols, lag_arg_list)\n","    cruncher.close()\n","    cruncher.join()\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    mgr.mem_capture('Joined Lag mp')\n","\n","    monthly_stt['y_target'] = monthly_stt.shop_item__salesSum.astype(np.float32)\n","    monthly_stt = monthly_stt.drop(mgr.feats.cols['min_agg_cols'], axis=1)\n","    for lagidx, lag_df in enumerate(lag_dfs_list):\n","        print(f'\\nNow merging lag month = {lag_arg_list[lagidx].n_months}...')\n","        monthly_stt = monthly_stt.merge(lag_df, on=['month', 'shop_id', 'item_id'], how='left')\n","    if mgr.split['cp_fillna0'] or np.issubdtype(mgr.split['feat_dtype'], np.integer):\n","        monthly_stt = monthly_stt.fillna(0)\n","    mgr.mem_capture('Time Lag Features Added')\n","\n","    # =============================================================================\n","    #     Save ftr files or return dataframes\n","    # =============================================================================\n","    if 'data' in mgr.ftr_modules:\n","        mgr.write_ftr_files('data', {'monthly_stt': monthly_stt})\n","        mgr.mem_capture(program_loc=\"Data .ftr Saved\")\n","    else:\n","        mgr.store_dfs('data', {'monthly_stt': monthly_stt})\n","\n","    return mgr\n","\n","\n","# =============================================================================\n","# sklearn_preprocessing scaling...\n","# import math\n","# from sklearn.preprocessing import MinMaxScaler, RobustScaler\n","# =============================================================================\n","# Disabled for now: then do Robust Scaling (if desired) to squeeze outliers into central\n","#   distribution, then do MinMax Scaling (if desired) to make best use of full range of\n","#   datatype being used.  Keep in mind that smaller ranges will converge faster during GBDT\n","#   fitting, so do not extend range too much.\n","#   Reverse the order of operations when unscaling/clipping after model predictions.)\n","# # =============================================================================\n","# # Scaling / Transforming is temporarily disabled until a good use case is found\n","# # =============================================================================\n","#     if robust_qtiles:\n","#         for aggcol in mgr.feats.cols['min_agg_cols']:\n","#             mgr.robust_scalers[aggcol] = RobustScaler(with_centering=False,\n","#                                                       quantile_range=robust_qtiles)\n","#             monthly_stt[aggcol] = (mgr.robust_scalers[aggcol].\n","#                                    fit_transform(\n","#                                    monthly_stt[aggcol].to_numpy().reshape(-1, 1)))\n","#     if minmax_range:\n","#         for aggcol in mgr.feats.cols['min_agg_cols']:\n","#             mgr.minmax_scalers[aggcol] = MinMaxScaler(feature_range=minmax_range)\n","#             monthly_stt[aggcol] = (mgr.minmax_scalers[aggcol].\n","#                                    fit_transform(\n","#                                    monthly_stt[aggcol].to_numpy().reshape(-1, 1)))\n","# # =============================================================================\n","# # new\n","# # =============================================================================\n","#             monthly_stt[aggcol] = monthly_stt[aggcol].apply(lambda x: math.ceil(x))\n","#     if mgr.split['feat_dtype'] in [np.int16, np.uint16]:\n","#         monthly_stt = monthly_stt.fillna(0).round()\n","# =============================================================================\n","\n","# =============================================================================\n","#         # np.int16 #.apply(pd.to_numeric, downcast= np.float32)\n","#         # store as integers to save memory, but INTs cannot handle NA (so, do not use price)\n","# =============================================================================\n","\n","# =============================================================================\n","# from functools import partial  # partial 'binds' arguments to a function; avoid need to pass them\n","# from functools import reduce\n","# # def copy_shift_type_lag_cols(lag_args):  # {'mo_stt': monthly_stt, 'lag', 'cols', 'dtype'}\n","# #     \"\"\"Create df for a given number of lag months, to merge with monthly_stt.\"\"\"\n","# #     cols_to_shift = ['month', 'shop_id', 'item_id'] + list(lag_args['cols'].keys())\n","# #     lag_df = lag_args['mo_stt'][cols_to_shift].copy(deep=True).rename(lag_args['cols'])\n","# #     lag = lag_args['lag']\n","# #     lag_df.eval('month = month + @lag', inplace=True).astype(lag_args['dtype'])\n","# #     return lag_df\n","# # def worker_wrapper(arg):\n","# #     args, kwargs = arg\n","# #     return worker(*args, **kwargs)\n","# # def multiproc_arg_wrap(func_arg):\n","# #     func\n","#         # lag_dfs_list = lag_pool.map(partial(copy_shift_type_lag_cols,\n","#         #                                     **{'mo_stt': monthly_stt,\n","#         #                                        'data_type': mgr.split['feat_dtype']}),\n","#         #                             arg_list)\n","# =============================================================================\n","\n","# print(f'\\nmonthly_stt.head:\\n{monthly_stt.head(2)}')\n","# print(f'\\nmonthly_stt.tail:\\n{monthly_stt.tail(2)}')\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2k3Ky5lHy5J"},"source":["## **kag_tvt.py**"]},{"cell_type":"code","metadata":{"id":"0vZQEwUaNEqI","executionInfo":{"status":"ok","timestamp":1605966315081,"user_tz":300,"elapsed":30215,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["\"\"\"\n","Create Train-Validation-Test split to feed into model fitting and prediction.\n","\n","Created on Fri Oct 23 05:23:40 2020\n","@author: mgaidis\n","\"\"\"\n","\n","# import numpy as np\n","# import pandas as pd\n","# from kag_utils import col_info\n","\n","\n","def create_tvt_split(mgr):\n","    \"\"\"Create Train-Validation-Test split to feed into model fitting and prediction.\"\"\"\n","    # =============================================================================\n","    #     Load ftr files if necessary\n","    # =============================================================================\n","    tvt_dfs = {}\n","    if 'data' in mgr.ftr_modules:\n","        tvt_dfs['train_X'] = mgr.read_ftr_files('data').pop('monthly_stt')\n","        mgr.mem_capture(program_loc=\"monthly_stt .ftr Loaded\")\n","    else:\n","        tvt_dfs['train_X'] = mgr.data_dfs.pop('monthly_stt')\n","    # =============================================================================\n","    # tvt_dfs['train_X'] contains everything at this point... now we slice it into train/val/test\n","    # Trying to minimize n variable names to save memory given pd garbage collection issue\n","    # =============================================================================\n","\n","    # if mgr.split['model_type'] == 'LGBM':  # split is sensitive to time-ordered data\n","    # =============================================================================\n","    # 1. Remove early months that don't participate in model training (for both X and y)\n","    # =============================================================================\n","    tvt_dfs['train_X'] = tvt_dfs['train_X'].query(f'month >= {mgr.split[\"tr_start_mo\"]}')\n","    tvt_dfs['train_X'].reset_index(drop=True, inplace=True)\n","\n","    # =============================================================================\n","    # 2. Discretize (integer from float) all the non-categorical columns except y-target\n","    # =============================================================================\n","    if np.issubdtype(mgr.split['feat_dtype'], np.integer):\n","        full_pos_range = np.iinfo(mgr.split['feat_dtype']).max - 1\n","        for ncol in mgr.feats.all_lag_features:\n","            tvt_dfs['train_X'][ncol] = pd.cut(tvt_dfs['train_X'][ncol].clip(lower=0),\n","                                              full_pos_range, labels=False)\n","            tvt_dfs['train_X'][ncol] = tvt_dfs['train_X'][ncol].astype(mgr.split['feat_dtype'])\n","\n","    print(f'\\nTVT df after discretizing (float -> int) and removing unused (early) months:'\n","          f'shape = {tvt_dfs[\"train_X\"].shape}\\n')\n","    col_info(tvt_dfs['train_X'], 2, target_df_name='Full Train-Val-Test DataFrame')\n","    print(f'\\nfinal tvt.head():\\n{tvt_dfs[\"train_X\"].head()}\\n')\n","    mgr.mem_capture('TVT about to split', True, False)\n","\n","    # =============================================================================\n","    # 3. Create the test input data set by selecting just the test month from all data\n","    #     Also, delete the y (prediction) column, as it is unknown for the test month\n","    # =============================================================================\n","    tvt_dfs['test_X'] = tvt_dfs['train_X'].query('month == 34')\n","    tvt_dfs['test_X'] = tvt_dfs['test_X'].drop('y_target', axis=1)\n","\n","    # =============================================================================\n","    # 4. Scrape off the validation data set from the remaining training data\n","    #    val_months == 999 means 'all months from end of training + month 33 at minimum'\n","    #    (otherwise, use only 'val_months' months after training) (val_months = 1, 2, 3, ...)\n","    # =============================================================================\n","    if mgr.split['val_months'] == 999:\n","        tvt_dfs['val_X'] = tvt_dfs['train_X'].query(\n","            f'((month > {mgr.split[\"tr_final_mo\"]}) & (month < 34)) | (month == 33)')\n","    else:\n","        tvt_dfs['val_X'] = tvt_dfs['train_X'].query(\n","            f'(month > {mgr.split[\"tr_final_mo\"]}) &'\n","            f'(month <= {mgr.split[\"tr_final_mo\"] + mgr.split[\"val_months\"]}) &'\n","            f'(month < 34)')\n","\n","    # =============================================================================\n","    # 5. Separate the y target prediction column from the input data columns X\n","    # =============================================================================\n","    tvt_dfs['val_y'] = tvt_dfs['val_X'].pop('y_target')\n","\n","    # =============================================================================\n","    # 6. Drop the undesired rows from the train data, and separate the y column\n","    # =============================================================================\n","    tvt_dfs['train_X'] = tvt_dfs['train_X'].query(f'month <= {mgr.split[\"tr_final_mo\"]}')\n","    tvt_dfs['train_y'] = tvt_dfs['train_X'].pop('y_target')\n","\n","    col_info(tvt_dfs['train_X'], 2, target_df_name='tvt train_X')\n","    # print(tvt_dfs['train_X'].head(2))\n","    col_info(tvt_dfs['train_y'], 2, target_df_name='tvt train_y')\n","    # print(tvt_dfs['train_y'].head(2))\n","\n","    # =============================================================================\n","    # 7. Make sure all data sets are properly categorized and typed\n","    #    'target y' can be high accuracy; 'int_cats' and properly-scaled feats can be int\n","    # =============================================================================\n","    for data_set in tvt_dfs:\n","        if data_set[-1] == 'y':\n","            tvt_dfs[data_set] = tvt_dfs[data_set].astype(np.float32).reset_index(drop=True)\n","        else:\n","            tvt_dfs[data_set] = (tvt_dfs[data_set]\n","                                 .astype(mgr.split['feat_dtype'])\n","                                 .reset_index(drop=True))\n","            tvt_dfs[data_set][mgr.feats.cols['int_feats']] = \\\n","                tvt_dfs[data_set][mgr.feats.cols['int_feats']].astype(np.int16)\n","            if mgr.split['use_categorical']:\n","                tvt_dfs[data_set][mgr.feats.cols['cat_feats']] = \\\n","                    tvt_dfs[data_set][mgr.feats.cols['cat_feats']].astype('category')\n","\n","    # =============================================================================\n","    #     Save ftr files or return dataframes\n","    # =============================================================================\n","    if 'tvt' in mgr.ftr_modules:\n","        mgr.write_ftr_files('tvt', tvt_dfs)\n","        mgr.mem_capture(program_loc=\"TVT .ftr Saved\")\n","    else:\n","        mgr.store_dfs('tvt', tvt_dfs)\n","\n","    mgr.feature_names = tvt_dfs['train_X'].columns  # not needed?... newer lgbm versions can use gbm.feature_name_, but not yet in colab\n","    return mgr\n","\n","# =============================================================================\n","# 2. Discretize (integer from float) all the non-categorical columns except y-target\n","# =============================================================================\n","# Particularly with decision trees, we don't need to use floats for the regression\n","#   accuracy required by this competition.  np.int16 (range = -32768 to 32767) gives\n","#   plenty of feature precision for all of our non-categorical features.  Using int16\n","#   will presumably help us with RAM limitations and with speed of model fitting, so\n","#   here the non-categorical features are discretized to fit mgr.split['feat_dtype'],\n","#   and at the same time (for giggles?) drop negative values for sum/med/cnt, etc.\n","#   The more-conventional int16 is used rather than uint16 to protect against potential\n","#   compatibility issues with model fitting.\n","# As the predictions and comparisions with labeled values are separate from the\n","#   decision-tree methods that handle input features, we can keep the \"y_target\"\n","#   values as float32, and eliminate the need to do any inverse scaling at the end.\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_9V5wY73HzPI"},"source":["## **kag_model.py**"]},{"cell_type":"code","metadata":{"id":"1wQE1tPRNKLm","executionInfo":{"status":"ok","timestamp":1605975367130,"user_tz":300,"elapsed":664,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}}},"source":["\"\"\"\n","Perform Setup, Fit/Train, and Prediction with a gradient-boosted decision tree (GBDT) model.\n","\n","Created on Fri Oct 23 05:24:27 2020\n","@author: mgaidis\n","\"\"\"\n","\n","# import time\n","# import numpy as np\n","# import pandas as pd\n","# =============================================================================\n","#       ML Packages\n","# =============================================================================\n","import lightgbm as lgb  # LGBMRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","# explicitly require expt feature before importing HistGradientBoostingRegressor\n","# from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n","# from sklearn.ensemble import HistGradientBoostingRegressor\n","\n","# from kag_utils import tdstr, timer, AttrDict\n","\n","\n","# =============================================================================\n","# Disabled for Now\n","# def unscale(scaler, target_dict):\n","#     \"\"\"Reverse the scaling done prior to model fitting.\"\"\"\n","#     for name, data in target_dict.items():\n","#         target_dict[name] = scaler.inverse_transform(data.reshape(-1, 1)).squeeze()\n","#     return target_dict\n","# =============================================================================\n","\n","\n","def gbdt_model(mgr, gbdt_setup_params, gbdt_fit_params):\n","    \"\"\"\n","    Perform Setup, Fit/Train, and Prediction with a gradient-boosted decision tree model.\n","\n","    AttrDict 'tvt_dfs' includes train_X, train_y, val_X, val_y, test_X.\n","        (keys = string version of values), or filenames (if stored on disk)\n","    lgbm model = LightGBM is a particular case of a gradient-boosted decision tree model,\n","        so it is a subroutine in this GBDT function.\n","        Other GBMs can also be coded in this module.\n","    \"\"\"\n","    output_dict = AttrDict()  # temporary storage of metrics and results\n","    y_predictions = AttrDict()\n","    y_labels = AttrDict()\n","\n","    # =============================================================================\n","    #     Load ftr files if necessary\n","    # =============================================================================\n","    if 'tvt' in mgr.ftr_modules:\n","        tvt_dfs = mgr.read_ftr_files('tvt')\n","        mgr.mem_capture(program_loc=\"TVT .ftr Loaded\")\n","    else:\n","        tvt_dfs = mgr.tvt_dfs\n","\n","    if True:  # mgr.current_model['model_type'] == 'LGBM':\n","        t0_model_fit = time.perf_counter()\n","        # =============================================================================\n","        #     LGBM Model Setup\n","        # =============================================================================\n","        lgbm = lgb.LGBMRegressor(**gbdt_setup_params)\n","\n","        # =============================================================================\n","        #     LGBM Model Fitting\n","        # =============================================================================\n","        lgbm.fit(\n","            tvt_dfs['train_X'],  # Input feature matrix or df 'train_X' [n_samples, n_features]\n","            tvt_dfs['train_y'],  # Target values 'train_y' (shape = [n_samples])\n","            eval_set=[(tvt_dfs['val_X'], tvt_dfs['val_y'])],  # list [(val_X, val_y)]\n","            eval_names=None,  # Names of eval_set (list of strs or None, opt. (default=None))\n","            **gbdt_fit_params)\n","\n","        # =============================================================================\n","        #     LGBM Metrics Collection\n","        # =============================================================================\n","        output_dict.t_model_fit = timer(t0_model_fit)\n","        output_dict.best_iteration_ = lgbm.best_iteration_\n","        output_dict.best_score_ = lgbm.best_score_['valid_0']['rmse']\n","        # output_dict.feature_name_ = lgbm.feature_name_  # shape [n_features] names list --> not available yet in Colab's lgbm version\n","        output_dict.feature_name_ = mgr.feature_names\n","        output_dict.feature_importances_ = lgbm.feature_importances_\n","        # output_dict.model_params = lgbm.get_params()\n","        print(f'Done fitting; Model LGBM fit time: {output_dict.t_model_fit}')\n","\n","    # =============================================================================\n","    #     GBM Predictions -- possible multiprocessing use here?\n","    # =============================================================================\n","    print(f'{tdstr()} -- Starting predictions...')\n","    t0_model_predict = time.perf_counter()\n","    y_labels.train = tvt_dfs['train_y'].to_numpy()\n","    y_labels.val = tvt_dfs['val_y'].to_numpy()\n","    y_predictions.train = lgbm.predict(tvt_dfs['train_X'], num_iteration=lgbm.best_iteration_)\n","    y_predictions.val = lgbm.predict(tvt_dfs['val_X'], num_iteration=lgbm.best_iteration_)\n","    y_predictions.test = lgbm.predict(tvt_dfs['test_X'], num_iteration=lgbm.best_iteration_)\n","\n","    # =============================================================================\n","    #     Disable Scaling for Now\n","    #     # =============================================================================\n","    #     #     Invert Scaling\n","    #     # =============================================================================\n","    #     # do minmax scaling after robust; and ~inverse~ scaling with minmax first, then robust\n","    #     scaling_col = 'shop_item__salesSum'\n","    #     if mgr.minmax_scalers:\n","    #         y_predictions = unscale(mgr.minmax_scalers[scaling_col], y_predictions)\n","    #         y_labels = unscale(mgr.minmax_scalers[scaling_col], y_labels)\n","    #     if mgr.robust_scalers:\n","    #         y_predictions = unscale(mgr.robust_scalers[scaling_col], y_predictions)\n","    #         y_labels = unscale(mgr.minmax_scalers[scaling_col], y_labels)\n","    # =============================================================================\n","\n","    # =============================================================================\n","    #     Clip as per Kaggle Recommendations\n","    # =============================================================================\n","    for name, data in y_predictions.items():\n","        y_predictions[name] = data.clip(*mgr.split['clip_predict'])\n","\n","    output_dict.t_model_predict = timer(t0_model_predict)\n","    output_dict.train_R2 = r2_score(y_labels.train, y_predictions.train)\n","    output_dict.val_R2 = r2_score(y_labels.val, y_predictions.val)\n","    output_dict.train_rmse = mean_squared_error(y_labels.train, y_predictions.train, squared=False)\n","    output_dict.val_rmse = mean_squared_error(y_labels.val, y_predictions.val, squared=False)\n","\n","    print(f'Model LGBM fit time: {output_dict.t_model_fit}\\n'\n","          f'Transform and Predict train/val/test time: {output_dict.t_model_predict}')\n","    print(f'R^2 train  = {output_dict.train_R2:.4f}    '\n","          f'R^2 val  = {output_dict.val_R2:.4f}\\n'\n","          f'RMSE train = {output_dict.train_rmse:.4f}    '\n","          f'RMSE val = {output_dict.val_rmse:.4f}\\n')\n","\n","    # re-format feature importances? dict with key=featurename val=importance?\n","    # save model?\n","\n","    # =============================================================================\n","    # Merge predictions with IDs from original test dataset; keep only \"ID\" and \"item_cnt_month\"\n","    # =============================================================================\n","    y_submission = pd.DataFrame.from_dict({'item_cnt_month': y_predictions.test,\n","                                           'shop_id': tvt_dfs['test_X'].shop_id,\n","                                           'item_id': tvt_dfs['test_X'].item_id})\n","    y_submission = mgr.test_df.merge(y_submission, on=['shop_id', 'item_id'], how='left')\n","    y_submission = y_submission.reset_index(drop=True).drop(['shop_id', 'item_id'], axis=1)\n","\n","    # =============================================================================\n","    # Save prediction for every one of the run iterations; can ensemble them later if desired\n","    # =============================================================================\n","    y_submission.to_csv(mgr.filenames.submit_path, index=False)\n","\n","    return mgr, output_dict\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5yLzKVuNIK2h"},"source":["## **kaggle_main_01p2.py**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xxx8Nn7VNOcU","executionInfo":{"status":"ok","timestamp":1605975699533,"user_tz":300,"elapsed":327378,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"60838eff-e9e5-4608-92ed-79d4c7587078"},"source":["\"\"\"\n","Feature/Split Interface for Manual Entry + Main controller for Kaggle computations.\n","\n","Created on Wed Oct 21 08:06:32 2020\n","@author: mgaidis\n","\"\"\"\n","\n","# from collections import namedtuple, OrderedDict\n","# import time\n","# import numpy as np\n","\n","# # %tensorflow_version 2.x\n","# # import tensorflow as tf\n","# from kag_utils import timer\n","# from kag_program_manager import ProgramManager\n","# from kag_eda import eda_cleanup\n","# from kag_data import data_conditioning\n","# from kag_tvt import create_tvt_split\n","# from kag_model import gbdt_model\n","\n","\n","# ================================================================================================\n","# Define which (if any) DataFrames should be saved to disk between modules to help with RAM issues\n","#     FEATHERABLE_DF_NAMES = {'eda': ['items_enc', 'shops_enc', 'stt', 'test'],\n","#                             'data': ['monthly_stt'],\n","#                             'tvt': ['train_X', 'train_y', 'val_X', 'val_y', 'test_X']}\n","# ================================================================================================\n","FTR_MODULES = ['eda', 'data']  # write feather data for these modules to disk between modules\n","FILE_ROOTNAME = 'v1p0_test1'  # use None or '' if you wish to input (via console) filename below\n","\n","# ================================================================================================\n","# Define the various choices for features, preprocessing, and ML fitting.\n","# ================================================================================================\n","#     1. Define elements that tweak the feature attributes.  (e.g., shops to delete)\n","#     2. Define preprocessing methods.  (e.g., scaling, clipping)\n","#     3. Define data augmentation strategy.  (e.g., use of cartesian product)\n","#     4. Define train-validation-test split configuration.\n","#     5. Define ML model to use and the related hyperparameters to be used in model fitting.\n","#\n","#     Configuration of data structure is such that we can \"explode\" it to generate a spec row\n","#     for each iteration we wish to perform.  This provides a cartesian product over the desired\n","#     hyperparameters and preprocessing methods below, although we do not perform a full cartesian\n","#     product over the elements defined in 'kag_features.py' (categories, lags, statistics).\n","# ================================================================================================\n","# Define features based on time-lags and statistical aggregations during monthly grouping\n","# ================================================================================================\n","FG = namedtuple('FeatureGroup', 'lag_month group_list stats_dict')\n","iter1_feature_list = [\n","    # lag months = 1\n","    FG(lag_month=1,  # months to lag by\n","       group_list=['shop_id', 'item_id'],  # grouping for aggregate statistics in the ODict below\n","       stats_dict=OrderedDict([('sales', ['sum', 'median', 'count']), ('rev', ['sum'])])),\n","    FG(1, ['shop_id', 'item_category_id'], OrderedDict([('sales', ['sum', 'median', 'count'])])),\n","    FG(1, ['shop_id', 'item_cluster'], OrderedDict([('sales', ['sum', 'median'])])),\n","    FG(1, ['shop_id'], OrderedDict([('sales', ['sum', 'count'])])),\n","    FG(1, ['item_id'], OrderedDict([('sales', ['sum', 'median', 'count']), ('rev', ['sum'])])),\n","    FG(1, ['shop_group'], OrderedDict([('rev', ['sum'])])),\n","    FG(1, ['item_category_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    FG(1, ['item_group'], OrderedDict([('sales', ['sum']), ('rev', ['sum'])])),\n","    FG(1, ['item_cluster'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    # lag months = 2\n","    FG(2, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    FG(2, ['shop_id', 'item_category_id'], OrderedDict([('sales', ['count']), ('rev', ['sum'])])),\n","    FG(2, ['shop_id'], OrderedDict([('sales', ['sum'])])),\n","    FG(2, ['item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    FG(2, ['item_category_id'], OrderedDict([('sales', ['sum', 'count'])])),\n","    FG(2, ['item_cluster'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    # lag months = 3\n","    FG(3, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])),\n","    FG(3, ['shop_id'], OrderedDict([('sales', ['sum'])])),\n","    FG(3, ['item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    FG(3, ['item_category_id'], OrderedDict([('sales', ['sum', 'count'])])),\n","    FG(3, ['item_cluster'], OrderedDict([('sales', ['count'])])),\n","    # lag months = 4\n","    FG(4, ['item_id'], OrderedDict([('sales', ['sum'])])),\n","    # lag months = 5\n","    FG(5, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])),\n","    # lag months = 6\n","    FG(6, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])),\n","    FG(6, ['item_id'], OrderedDict([('sales', ['sum'])])),\n","    # lag months = 8\n","    FG(8, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])]))]\n","\n","# ================================================================================================\n","\n","\n","def define_model_parameters(feature_iterations):\n","    \"\"\"Define splits to iterate over in the execution of the program.\"\"\"\n","    # See far below for details on parameter meanings, and some useful choices to use\n","    #   The huge nesting of lists is to accommodate the pd.explode op to get one split per row\n","    all_splits_params = OrderedDict()\n","    # ============================================================================================\n","    #     model specifications (model type, feature categories, stats, lags)\n","    # ============================================================================================\n","    all_splits_params['model'] = OrderedDict([\n","        ('features',          [feature_iterations]),\n","        ('model_type',        [['LGBM']])])\n","    # ============================================================================================\n","    #     eda exploratory data analysis parameters - coarse adjustments before monthly grouping\n","    # ============================================================================================\n","    all_splits_params['eda'] = OrderedDict([\n","        ('del_shops',       [[[9, 20]]]),\n","        ('del_item_cats',   [[[8, 10, 32, 59, 80, 81, 82]]]),\n","        ('scale_sales',     [['week_retail_weight']])])\n","    # ============================================================================================\n","    #     data conditioning parameters (datatypes, scaling, cartesian product filling)\n","    # ============================================================================================\n","    all_splits_params['data'] = OrderedDict([  # note cp === Cartesian Product\n","        ('cp_fillna0',       [[True]]),\n","        ('cp_first_mo',      [[13]]),\n","        ('cp_test_pairs',    [[False]]),\n","        ('clip_train',       [[(-20 * 30, 20 * 30)]]),  # limit to 20 sales per day avg. in a month\n","        ('feat_dtype',       [[np.int16]])])  # for RAM savings, and possible model fitting speedup\n","    # np.iinfo(np.int32) or np.finfo(np.float32) e.g., to find range of np dtypes\n","    # Disable scaling for now\n","    # ('minmax_range',     [[(0, 16000)]]),  # [[False]]),  # or (1,16000) for sales_count=1?\n","    # ('robust_qtiles',    [[(20, 80)]])  # [[False]]),\n","    # ============================================================================================\n","    #     train/val/test splitting parameters\n","    # ============================================================================================\n","    all_splits_params['tvt'] = OrderedDict([\n","        ('tr_start_mo',     [[13]]),\n","        ('tr_final_mo',     [[29]]),\n","        ('val_months',      [[999]]),\n","        ('use_categorical', [[True]])])\n","    # ============================================================================================\n","    #     gbm regresssor SETUP parameters\n","    # ============================================================================================\n","    all_splits_params['lgbm_setup'] = OrderedDict([\n","        ('boosting_type',     'gbdt'),\n","        ('metric',            [['rmse']]),\n","        ('learning_rate',     [[0.05]]),  # default = 0.1\n","        ('n_estimators',      [[200]]),\n","        ('colsample_bytree',  [[0.4]]),  # = feature_fraction; default 1\n","        ('random_state',      [[42]]),\n","        ('subsample_for_bin', [[200000, 800000]]),\n","        ('num_leaves',        [[31]]),\n","        ('max_depth',         [[-1]]),\n","        ('min_split_gain',    [[0.0]]),\n","        ('min_child_weight',  [[0.001]]),\n","        ('min_child_samples', [[20]]),\n","        ('silent',            [[False]]),\n","        ('importance_type',   [['split']]),\n","        ('reg_alpha',         [[0.0]]),\n","        ('reg_lambda',        [[0.0]]),\n","        ('n_jobs',            (-1)),\n","        ('subsample',         1.0),  # bagging fraction\n","        ('subsample_freq',    0),    # bagging frequency\n","        ('objective',         'regression')])\n","    # ============================================================================================\n","    #     gbm regressor FITTING parameters\n","    # ============================================================================================\n","    all_splits_params['lgbm_fit'] = OrderedDict([\n","        ('early_stopping_rounds',  [[20]]),\n","        ('eval_metric',            [['rmse']]),\n","        ('init_score',             None),\n","        ('eval_init_score',        None),\n","        ('verbose',                [[True]]),\n","        ('feature_name',           [['auto']]),\n","        ('categorical_feature',    [['auto']]),\n","        ('callbacks',              None)])\n","    # ============================================================================================\n","    #     output predict parameters below # (also need inverse scaling transformers)\n","    # ============================================================================================\n","    all_splits_params['predict'] = OrderedDict([\n","        ('clip_predict', [[(0, 20)]])])  # clip final prediction before submission\n","\n","    # ============================================================================================\n","    #     output results container\n","    # ============================================================================================\n","    output_results_dict = OrderedDict([\n","        ('train_rmse',           0.0),\n","        ('train_R2',             0.0),\n","        ('val_rmse',             0.0),\n","        ('val_R2',               0.0),\n","        ('best_iteration_',      0),\n","        ('best_score_',          0.0),\n","        ('feature_name_',        [\"\"]),\n","        ('feature_importances_', [0.0]),\n","        ('t_eda',                \"\"),\n","        ('t_data',               \"\"),\n","        ('t_tvt',                \"\"),\n","        ('t_model_fit',          \"\"),\n","        ('t_model_predict',      \"\"),\n","        ('t_ml',                 \"\"),\n","        ('t_model_n',            \"\"),\n","        ('t_cumulative',         \"\")])\n","\n","    return all_splits_params, output_results_dict\n","\n","# =============================================================================\n","# END OF MANUAL USER INPUT PARAMETERS and FEATURES\n","# =============================================================================\n","# =============================================================================\n","# END OF MANUAL USER INPUT PARAMETERS and FEATURES\n","# =============================================================================\n","# =============================================================================\n","# END OF MANUAL USER INPUT PARAMETERS and FEATURES\n","# =============================================================================\n","\n","\n","# ================================================================================================\n","# Main Control Loop  # possibly need == '__main__' for multiprocessing to work smoothly??\n","# ================================================================================================\n","if __name__ == '__main__':\n","    mgr = ProgramManager(FILE_ROOTNAME, FTR_MODULES)\n","    mgr.mem_capture(program_loc=\"Before defining features\")\n","    # multiple splits in feature combinations can be handled by iterating through this list\n","    feature_iter_list = []  # details on cats to use, agg stats and lag features\n","    feature_iter_list.append(iter1_feature_list)  # ...append(iter2_feature_list) ...\n","\n","    params, output_results = define_model_parameters(feature_iter_list)\n","    mgr.write_log_mgr_info(params, True)  # write to log.txt file + console\n","    outputs_df = mgr.arrange_run_info(params, output_results)\n","\n","    t0_model_n = time.perf_counter()\n","    t0_cumulative = time.perf_counter()\n","    # ============================================================================================\n","    #   Model Module\n","    # ============================================================================================\n","    for model_iter_n, model_iter_dict in mgr.all_splits['model'].items():\n","        mgr.split.update(model_iter_dict)  # intermediate storage of params for logging\n","        mgr.set_features()  # expand inputs into pd-friendly dicts of stats and lists of cols\n","        model_done = (model_iter_n == max(mgr.all_splits['model']))  # done model splits?\n","\n","        # ========================================================================================\n","        #   EDA Module - load data, merge basic features into stt df, delete unwanteds, scaling\n","        # ========================================================================================\n","        for eda_iter_n, eda_iter_dict in mgr.all_splits['eda'].items():\n","            mgr.split.update(eda_iter_dict)\n","            mgr.module_start('EDA')\n","            # ====================================================================================\n","            # mgr = eda_cleanup(mgr, **eda_iter_dict)\n","            mgr = eda_cleanup(mgr)\n","            mgr.module_end(\"EDA\")\n","            # True if done all splits in params.model and eda_expl... will help reclaim RAM below\n","            eda_done = model_done and (eda_iter_n == max(mgr.all_splits['eda']))\n","\n","            # ====================================================================================\n","            #   Data - Monthly Feature Generation (agg, stats, lags) & Cartesian Product Module\n","            # ====================================================================================\n","            for data_iter_n, data_iter_dict in mgr.all_splits['data'].items():\n","                mgr.split.update(data_iter_dict)\n","                mgr.module_start('Data')\n","                # ================================================================================\n","                mgr = data_conditioning(mgr)\n","                mgr.module_end(\"Data\")\n","                data_done = eda_done and (data_iter_n == max(mgr.all_splits['data']))\n","                if data_done and hasattr(mgr, 'eda_dfs'):\n","                    mgr.delete_dfs('eda')  # no future splits need these dfs; reclaim RAM\n","\n","                # ================================================================================\n","                #   TVT Split Module - split data into train/val/test and assign desired dtypes\n","                # ================================================================================\n","                for tvt_iter_n, tvt_iter_dict in mgr.all_splits['tvt'].items():\n","                    mgr.split.update(tvt_iter_dict)\n","                    mgr.module_start('TVT')\n","                    # ============================================================================\n","                    mgr = create_tvt_split(mgr)\n","                    mgr.module_end(\"TVT\")\n","                    tvt_done = data_done and (tvt_iter_n == max(mgr.all_splits['tvt']))\n","                    if tvt_done and hasattr(mgr, 'data_dfs'):\n","                        mgr.delete_dfs('data')  # no future splits need these dfs; reclaim RAM\n","\n","                    # ============================================================================\n","                    #   Model Setup / Fitting / Prediction Module\n","                    # ============================================================================\n","                    for lgbm_setup_iter_dict in mgr.all_splits['lgbm_setup'].values():\n","                        mgr.split.update(lgbm_setup_iter_dict)\n","                        for lgbm_fit_iter_dict in mgr.all_splits['lgbm_fit'].values():\n","                            mgr.split.update(lgbm_fit_iter_dict)\n","                            for predict_iter_dict in mgr.all_splits['predict'].values():\n","                                mgr.split.update(predict_iter_dict)\n","                                mgr.module_start('ML')\n","                                # ================================================================\n","                                mgr, output_dict = gbdt_model(mgr,\n","                                                              lgbm_setup_iter_dict,\n","                                                              lgbm_fit_iter_dict)\n","                                mgr.module_end(\"ML\")\n","                                mgr.split['t_model_n'] = timer(t0_model_n)\n","                                mgr.split['t_cumulative'] = timer(t0_cumulative)\n","                                mgr.split.update(output_dict)\n","                                for param_name, param_value in mgr.split.items():\n","                                    if param_name in outputs_df.columns:\n","                                        outputs_df.at[mgr.model_n, param_name] = param_value\n","                                    else:\n","                                        print(f'param {param_name} not in outputs_df.')\n","                                # save params and results - overwrite this \"log\" each run iter\n","                                outputs_df.to_csv(mgr.filenames.output_path, index=False)\n","\n","                                # ================================================================\n","                                #   Post-Processing Module - write data; check feature importance\n","                                # ================================================================\n","                                mgr.module_start('Postprocessing')\n","                                # ================================================================\n","                                #   Incorporate postprocess module / feature importances\n","                                # ================================================================\n","\n","                                mgr.model_n += 1\n","                                mgr.update_filenames()\n","                                t0_model_n = time.perf_counter()\n","\n","    mgr.write_log_mgr_info(param_dict=params, console=False)  # save (overwrite) log.txt file\n","    # filename_pickle = f'save entire output df, along with {mgr}'  # include postproc stuff too #\n","\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["OrderedDict([('model',\n","              OrderedDict([('features',\n","                            [[[FeatureGroup(lag_month=1, group_list=['shop_id', 'item_id'], stats_dict=OrderedDict([('sales', ['sum', 'median', 'count']), ('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=1, group_list=['shop_id', 'item_category_id'], stats_dict=OrderedDict([('sales', ['sum', 'median', 'count'])])),\n","                               FeatureGroup(lag_month=1, group_list=['shop_id', 'item_cluster'], stats_dict=OrderedDict([('sales', ['sum', 'median'])])),\n","                               FeatureGroup(lag_month=1, group_list=['shop_id'], stats_dict=OrderedDict([('sales', ['sum', 'count'])])),\n","                               FeatureGroup(lag_month=1, group_list=['item_id'], stats_dict=OrderedDict([('sales', ['sum', 'median', 'count']), ('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=1, group_list=['shop_group'], stats_dict=OrderedDict([('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=1, group_list=['item_category_id'], stats_dict=OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=1, group_list=['item_group'], stats_dict=OrderedDict([('sales', ['sum']), ('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=1, group_list=['item_cluster'], stats_dict=OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=2, group_list=['shop_id', 'item_id'], stats_dict=OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=2, group_list=['shop_id', 'item_category_id'], stats_dict=OrderedDict([('sales', ['count']), ('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=2, group_list=['shop_id'], stats_dict=OrderedDict([('sales', ['sum'])])),\n","                               FeatureGroup(lag_month=2, group_list=['item_id'], stats_dict=OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=2, group_list=['item_category_id'], stats_dict=OrderedDict([('sales', ['sum', 'count'])])),\n","                               FeatureGroup(lag_month=2, group_list=['item_cluster'], stats_dict=OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=3, group_list=['shop_id', 'item_id'], stats_dict=OrderedDict([('sales', ['sum'])])),\n","                               FeatureGroup(lag_month=3, group_list=['shop_id'], stats_dict=OrderedDict([('sales', ['sum'])])),\n","                               FeatureGroup(lag_month=3, group_list=['item_id'], stats_dict=OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","                               FeatureGroup(lag_month=3, group_list=['item_category_id'], stats_dict=OrderedDict([('sales', ['sum', 'count'])])),\n","                               FeatureGroup(lag_month=3, group_list=['item_cluster'], stats_dict=OrderedDict([('sales', ['count'])])),\n","                               FeatureGroup(lag_month=4, group_list=['item_id'], stats_dict=OrderedDict([('sales', ['sum'])])),\n","                               FeatureGroup(lag_month=5, group_list=['shop_id', 'item_id'], stats_dict=OrderedDict([('sales', ['sum'])])),\n","                               FeatureGroup(lag_month=6, group_list=['shop_id', 'item_id'], stats_dict=OrderedDict([('sales', ['sum'])])),\n","                               FeatureGroup(lag_month=6, group_list=['item_id'], stats_dict=OrderedDict([('sales', ['sum'])])),\n","                               FeatureGroup(lag_month=8, group_list=['shop_id', 'item_id'], stats_dict=OrderedDict([('sales', ['sum'])]))]]]),\n","                           ('model_type', [['LGBM']])])),\n","             ('eda',\n","              OrderedDict([('del_shops', [[[9, 20]]]),\n","                           ('del_item_cats', [[[8, 10, 32, 59, 80, 81, 82]]]),\n","                           ('scale_sales', [['week_retail_weight']])])),\n","             ('data',\n","              OrderedDict([('cp_fillna0', [[True]]),\n","                           ('cp_first_mo', [[13]]),\n","                           ('cp_test_pairs', [[False]]),\n","                           ('clip_train', [[(-600, 600)]]),\n","                           ('feat_dtype', [[<class 'numpy.int16'>]])])),\n","             ('tvt',\n","              OrderedDict([('tr_start_mo', [[13]]),\n","                           ('tr_final_mo', [[29]]),\n","                           ('val_months', [[999]]),\n","                           ('use_categorical', [[True]])])),\n","             ('lgbm_setup',\n","              OrderedDict([('boosting_type', 'gbdt'),\n","                           ('metric', [['rmse']]),\n","                           ('learning_rate', [[0.05]]),\n","                           ('n_estimators', [[200]]),\n","                           ('colsample_bytree', [[0.4]]),\n","                           ('random_state', [[42]]),\n","                           ('subsample_for_bin', [[200000, 800000]]),\n","                           ('num_leaves', [[31]]),\n","                           ('max_depth', [[-1]]),\n","                           ('min_split_gain', [[0.0]]),\n","                           ('min_child_weight', [[0.001]]),\n","                           ('min_child_samples', [[20]]),\n","                           ('silent', [[False]]),\n","                           ('importance_type', [['split']]),\n","                           ('reg_alpha', [[0.0]]),\n","                           ('reg_lambda', [[0.0]]),\n","                           ('n_jobs', -1),\n","                           ('subsample', 1.0),\n","                           ('subsample_freq', 0),\n","                           ('objective', 'regression')])),\n","             ('lgbm_fit',\n","              OrderedDict([('early_stopping_rounds', [[20]]),\n","                           ('eval_metric', [['rmse']]),\n","                           ('init_score', None),\n","                           ('eval_init_score', None),\n","                           ('verbose', [[True]]),\n","                           ('feature_name', [['auto']]),\n","                           ('categorical_feature', [['auto']]),\n","                           ('callbacks', None)])),\n","             ('predict', OrderedDict([('clip_predict', [[(0, 20)]])]))])\n","\n","Python: version 3.6.9\n","lightgbm: version 2.2.3\n","matplotlib: version 3.2.2\n","numpy: version 1.18.5\n","pandas: version 1.1.4\n","scikit-learn: version 0.22.2.post1\n","\n","os: linux\n","os_full: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\n","runtime: Colab using CPU\n","gb_physical_dram: 0\n","n_logical_cpu: 4\n","n_physical_cpu: 2\n","n_multiprocessing_cpu: 4\n","chipset: x86_64\n","\n","|                                                         |  pid   |              vm               |             |\n","|     Time and Date     |        Measurement Point        | pid-GB | used-GB | avail-GB | total-GB | Active Proc |\n","------------------------------------------------------------------------------------------------------------------\n","| Sat 11:16:12 11/21/20 | Program Start                   |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Before defining features        |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","N train models: 2\n","Splits in this run (excluding features/lags):\n","{'subsample_for_bin': [200000, 800000]}\n","\n","\n","|                                                         |  pid   |              vm               |             |\n","|     Time and Date     |        Measurement Point        | pid-GB | used-GB | avail-GB | total-GB | Active Proc |\n","------------------------------------------------------------------------------------------------------------------\n","| Sat 11:16:12 11/21/20 | Program Start                   |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Before defining features        |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Iteration Parameters Defined    |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","\n","Model #1 of 2 -- Start EDA Module @ Sat 11:16:12 11/21/20\n","Loading Files from repo into VM...\n","\n","---------- items_enc ----------\n","DataFrame shape: (22170, 4)     DataFrame total memory usage: 1 MB\n","DataFrame Column Names: ['item_id', 'item_cluster', 'item_category_id', 'item_group']\n","^^^^^\n","   item_id  item_cluster  item_category_id  item_group\n","0        0           100                40           6\n","1        1           105                76           6\n","\n","---------- shops_enc ----------\n","DataFrame shape: (60, 2)     DataFrame total memory usage: 0 MB\n","DataFrame Column Names: ['shop_id', 'shop_group']\n","^^^^^\n","   shop_id  shop_group\n","0        0           7\n","1        1           7\n","\n","---------- date_scaling ----------\n","DataFrame shape: (35, 2)     DataFrame total memory usage: 0 MB\n","DataFrame Column Names: ['month', 'week_retail_weight']\n","^^^^^\n","   month  week_retail_weight\n","0      0               1.030\n","1      1               1.146\n","\n","---------- stt ----------\n","DataFrame shape: (3150043, 5)     DataFrame total memory usage: 126 MB\n","DataFrame Column Names: ['month', 'sales', 'price', 'shop_id', 'item_id']\n","^^^^^\n","   month  sales  price  shop_id  item_id\n","0      0      1     99        2      991\n","1      0      1   2599        2     1472\n","\n","---------- merged stt ----------\n","DataFrame shape: (3150043, 9)     DataFrame total memory usage: 252 MB\n","DataFrame Column Names: ['month', 'sales', 'price', 'shop_id', 'item_id', 'shop_group', 'item_cluster', 'item_category_id', 'item_group']\n","DataFrame shape: (3150043, 9)\n","DataFrame total memory: 240 MBytes\n","DataFrame column names: ['month', 'sales', 'price', 'shop_id', 'item_id', 'shop_group', 'item_cluster', 'item_category_id', 'item_group']\n","  Column Name     DType   MBytes        Column Name     DType   MBytes            Column Name   DType   MBytes     \n","        Index     int64     24.0            shop_id     int64     24.0       item_category_id   int64     24.0     \n","        month     int64     24.0            item_id     int64     24.0             item_group   int64     24.0     \n","        sales   float64     24.0         shop_group     int64     24.0                                             \n","        price   float64     24.0       item_cluster   float64     24.0                                             \n","\n","DataFrame shape: (3150043, 9)\n","DataFrame total memory usage: 240 MBytes\n","^^^^^\n","   month  sales  price  shop_id  item_id  shop_group  item_cluster  item_category_id  item_group\n","0      0      1     99        2      991           9           463                67           5\n","1      0      1   2599        2     1472           9           585                23           7\n","\n","----------\n","\n","stt shape after deleting shops #[9, 20]: (3144500, 9)\n","stt shape after deleting item categories [8, 10, 32, 59, 80, 81, 82]: (3124617, 9)\n","\n","---------- final stt ----------\n","DataFrame shape: (3124617, 9)     DataFrame total memory usage: 94 MB\n","DataFrame Column Names: ['month', 'sales', 'rev', 'shop_id', 'item_id', 'shop_group', 'item_cluster', 'item_category_id', 'item_group']\n","DataFrame shape: (3124617, 9)\n","DataFrame total memory: 89 MBytes\n","DataFrame column names: ['month', 'sales', 'rev', 'shop_id', 'item_id', 'shop_group', 'item_cluster', 'item_category_id', 'item_group']\n","  Column Name     DType   MBytes        Column Name   DType   MBytes            Column Name   DType   MBytes     \n","        Index     int64      0.0            shop_id   int16      6.0       item_category_id   int16      6.0     \n","        month     int16      6.0            item_id   int16      6.0             item_group   int16      6.0     \n","        sales   float64     23.8         shop_group   int16      6.0                                             \n","          rev   float64     23.8       item_cluster   int16      6.0                                             \n","\n","DataFrame shape: (3124617, 9)\n","DataFrame total memory usage: 89 MBytes\n","^^^^^\n","   month  sales   rev  shop_id  item_id  shop_group  item_cluster  item_category_id  item_group\n","0      0  1.030 0.102        2      991           9           463                67           5\n","1      0  1.030 2.677        2     1472           9           585                23           7\n","\n","|                                                         |  pid   |              vm               |             |\n","|     Time and Date     |        Measurement Point        | pid-GB | used-GB | avail-GB | total-GB | Active Proc |\n","------------------------------------------------------------------------------------------------------------------\n","| Sat 11:16:12 11/21/20 | Program Start                   |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Before defining features        |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Iteration Parameters Defined    |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Start EDA Module                |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:18 11/21/20 | Joined load_dfs mp              |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:19 11/21/20 | Merged stt with shops and items |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Completed stt transformations   |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Saved                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | End EDA Module                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","EDA Module complete; elapsed time = 0:00:10\n","\n","Model #1 of 2 -- Start Data Module @ Sat 11:16:21 11/21/20\n","\n","monthly_stt minimal agg stats grouped and merged: shape = (1811472, 32)\n","DataFrame monthly_stt (with CP rows) shape: (9415958, 32)\n","DataFrame monthly_stt (with CP rows) total memory: 1401 MBytes\n","DataFrame monthly_stt (with CP rows) column names: ['month', 'shop_id', 'item_id', 'shop_group', 'item_cluster', 'item_category_id', 'item_group', 'shop_item__salesSum', 'shop_item__salesMed', 'shop_item__salesCnt', 'shop_item__revSum', 'shop_itemCat__salesSum', 'shop_itemCat__salesMed', 'shop_itemCat__salesCnt', 'shop_itemCat__revSum', 'shop_itemCluster__salesSum', 'shop_itemCluster__salesMed', 'shop__salesSum', 'shop__salesCnt', 'item__salesSum', 'item__salesMed', 'item__salesCnt', 'item__revSum', 'shopGrp__revSum', 'itemCat__salesSum', 'itemCat__salesCnt', 'itemCat__revSum', 'itemGrp__salesSum', 'itemGrp__revSum', 'itemCluster__salesSum', 'itemCluster__salesCnt', 'itemCluster__revSum']\n","                 Column Name     DType   MBytes                      Column Name     DType   MBytes     \n","                       Index     int64      0.0       shop_itemCluster__salesMed   float32     35.9     \n","                       month     int64     71.8                   shop__salesSum   float32     35.9     \n","                     shop_id     int64     71.8                   shop__salesCnt   float32     35.9     \n","                     item_id     int64     71.8                   item__salesSum   float32     35.9     \n","                  shop_group     int64     71.8                   item__salesMed   float32     35.9     \n","                item_cluster   float64     71.8                   item__salesCnt   float32     35.9     \n","            item_category_id     int64     71.8                     item__revSum   float32     35.9     \n","                  item_group     int64     71.8                  shopGrp__revSum   float32     35.9     \n","         shop_item__salesSum   float32     35.9                itemCat__salesSum   float32     35.9     \n","         shop_item__salesMed   float32     35.9                itemCat__salesCnt   float32     35.9     \n","         shop_item__salesCnt   float32     35.9                  itemCat__revSum   float32     35.9     \n","           shop_item__revSum   float32     35.9                itemGrp__salesSum   float32     35.9     \n","      shop_itemCat__salesSum   float32     35.9                  itemGrp__revSum   float32     35.9     \n","      shop_itemCat__salesMed   float32     35.9            itemCluster__salesSum   float32     35.9     \n","      shop_itemCat__salesCnt   float32     35.9            itemCluster__salesCnt   float32     35.9     \n","        shop_itemCat__revSum   float32     35.9              itemCluster__revSum   float32     35.9     \n","  shop_itemCluster__salesSum   float32     35.9                                                         \n","\n","DataFrame monthly_stt (with CP rows) shape: (9415958, 32)\n","DataFrame monthly_stt (with CP rows) total memory usage: 1401 MBytes\n","\n","Number of months: 35\n","Number of shops: 55\n","Number of items: 21,655\n","Number of df rows: 9,415,958\n","        month  shop_id    item_id  shop_group  item_cluster  item_category_id  item_group  shop_item__salesSum  shop_item__salesMed  shop_item__salesCnt  shop_item__revSum  \\\n","count 9415958  9415958    9415958     9415958       9415958           9415958     9415958              1811472              1811472              1811472            1811472   \n","mean   17.560   31.699 11,277.868       6.299       397.814            44.709      10.283                1.844                0.864                1.725              1.707   \n","std     8.733   17.308  6,230.628       3.286       323.926            15.362       7.039                6.796                0.555                1.876             12.234   \n","min         0        2          0           0            37                 0           0              -19.355              -19.355                    1            -34.798   \n","25%        10       17       5849           5           152                37           6                0.880                0.861                    1              0.171   \n","50%        17       31      11377           6           290                40           6                0.979                0.930                    1              0.386   \n","75%        25       47      16557           9           522                55          18                1.811                0.999                    2              1.163   \n","max        34       59      22169          13          1502                83          26            1,333.312              235.689                   31          4,560.535   \n","\n","       shop_itemCat__salesSum  shop_itemCat__salesMed  shop_itemCat__salesCnt  shop_itemCat__revSum  shop_itemCluster__salesSum  shop_itemCluster__salesMed  shop__salesSum  \\\n","count                 1811472                 1811472                 1811472               1811472                     1811472                     1811472         1811472   \n","mean                  266.989                   0.834                 280.939               120.993                      28.291                       0.845       2,715.947   \n","std                   447.595                   0.419                 375.820               185.407                      62.130                       0.470       2,435.847   \n","min                   -19.355                 -19.355                       1               -18.921                     -17.596                      -1.998          -1.074   \n","25%                    26.543                   0.861                      57                19.968                       1.727                       0.861       1,183.740   \n","50%                   118.181                   0.916                     147                61.172                       6.158                       0.916       1,910.865   \n","75%                   307.289                   0.996                     336               150.559                      24.325                       0.999       3,545.664   \n","max                 4,924.797                 235.689                    3110             4,560.535                   1,976.393                     235.689      12,690.449   \n","\n","       shop__salesCnt  item__salesSum  item__salesMed  item__salesCnt  item__revSum  shopGrp__revSum  itemCat__salesSum  itemCat__salesCnt  itemCat__revSum  itemGrp__salesSum  \\\n","count         1811472         1811472         1811472         1811472       1811472          1811472            1811472            1811472          1811472            1811472   \n","mean        2,946.088          47.979           0.839          42.714        51.513       18,964.029          7,733.473          8,797.706        3,755.347         15,962.980   \n","std         1,973.715         198.680           0.373          66.172       387.348       18,117.014          8,306.026          8,221.423        4,233.797         17,483.900   \n","min                 1         -19.800         -19.355               1       -56.074                0                  0                  1                0                  0   \n","25%              1389           4.540           0.861               9         1.246        4,368.411            948.236               1677          812.410          4,358.838   \n","50%              2181          14.683           0.916              24         5.093       10,271.346          5,612.986               6342        2,852.804          8,902.405   \n","75%              4857          41.026           0.999              42        25.905       33,362.684         11,575.057              13078        5,426.150         24,951.816   \n","max             11386       9,141.848          64.316            1267    33,823.984       67,703.492         36,268.742              32340       34,148.738         68,087.688   \n","\n","       itemGrp__revSum  itemCluster__salesSum  itemCluster__salesCnt  itemCluster__revSum  \n","count          1811472                1811472                1811472              1811472  \n","mean         9,437.653                839.305              1,054.178              560.137  \n","std          9,136.590              1,387.880              1,778.019            1,300.963  \n","min                  0                 -1.864                      1              -10.217  \n","25%          2,642.907                 47.300                    104               15.264  \n","50%          5,034.743                217.925                    313              122.095  \n","75%         15,787.282                844.594                   1040              641.807  \n","max         49,351.391             11,311.684                  12516           33,848.836  \n","Pre-lag monthly_stt DataFrame length: 9,415,958\n","\n","\n","Now merging lag month = 1...\n","\n","Now merging lag month = 2...\n","\n","Now merging lag month = 3...\n","\n","Now merging lag month = 4...\n","\n","Now merging lag month = 5...\n","\n","Now merging lag month = 6...\n","\n","Now merging lag month = 8...\n","|                                                         |  pid   |              vm               |             |\n","|     Time and Date     |        Measurement Point        | pid-GB | used-GB | avail-GB | total-GB | Active Proc |\n","------------------------------------------------------------------------------------------------------------------\n","| Sat 11:16:12 11/21/20 | Program Start                   |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Before defining features        |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Iteration Parameters Defined    |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Start EDA Module                |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:18 11/21/20 | Joined load_dfs mp              |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:19 11/21/20 | Merged stt with shops and items |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Completed stt transformations   |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Saved                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | End EDA Module                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Start Data Module               |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Loaded                 |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:38 11/21/20 | Joined Agg mp                   |  12.14 |   13.58 |    13.82 |    27.39 |     []      |\n","| Sat 11:16:46 11/21/20 | Joined CP mp                    |  12.11 |   13.55 |    13.85 |    27.39 |     []      |\n","| Sat 11:17:01 11/21/20 | Cartesian Product Rows Added    |  13.44 |   14.97 |    12.42 |    27.39 |     []      |\n","| Sat 11:18:05 11/21/20 | Joined Lag mp                   |  13.16 |   14.71 |    12.68 |    27.39 |     []      |\n","| Sat 11:18:58 11/21/20 | Time Lag Features Added         |  15.64 |   17.37 |    10.03 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | Data .ftr Saved                 |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | End Data Module                 |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","Data Module complete; elapsed time = 0:02:59\n","\n","Model #1 of 2 -- Start TVT Module @ Sat 11:19:20 11/21/20\n","\n","TVT df after discretizing (float -> int) and removing unused (early) months:shape = (6226880, 59)\n","\n","DataFrame Full Train-Val-Test DataFrame shape: (6226880, 59)\n","DataFrame Full Train-Val-Test DataFrame total memory: 962 MBytes\n","DataFrame Full Train-Val-Test DataFrame column names: ['month', 'shop_id', 'item_id', 'shop_group', 'item_cluster', 'item_category_id', 'item_group', 'y_target', 'shop_item__salesSum_L1', 'shop_item__salesMed_L1', 'shop_item__salesCnt_L1', 'shop_item__revSum_L1', 'shop_itemCat__salesSum_L1', 'shop_itemCat__salesMed_L1', 'shop_itemCat__salesCnt_L1', 'shop_itemCluster__salesSum_L1', 'shop_itemCluster__salesMed_L1', 'shop__salesSum_L1', 'shop__salesCnt_L1', 'item__salesSum_L1', 'item__salesMed_L1', 'item__salesCnt_L1', 'item__revSum_L1', 'shopGrp__revSum_L1', 'itemCat__salesSum_L1', 'itemCat__salesCnt_L1', 'itemCat__revSum_L1', 'itemGrp__salesSum_L1', 'itemGrp__revSum_L1', 'itemCluster__salesSum_L1', 'itemCluster__salesCnt_L1', 'itemCluster__revSum_L1', 'shop_item__salesSum_L2', 'shop_item__salesCnt_L2', 'shop_item__revSum_L2', 'shop_itemCat__salesCnt_L2', 'shop_itemCat__revSum_L2', 'shop__salesSum_L2', 'item__salesSum_L2', 'item__salesCnt_L2', 'item__revSum_L2', 'itemCat__salesSum_L2', 'itemCat__salesCnt_L2', 'itemCluster__salesSum_L2', 'itemCluster__salesCnt_L2', 'itemCluster__revSum_L2', 'shop_item__salesSum_L3', 'shop__salesSum_L3', 'item__salesSum_L3', 'item__salesCnt_L3', 'item__revSum_L3', 'itemCat__salesSum_L3', 'itemCat__salesCnt_L3', 'itemCluster__salesCnt_L3', 'item__salesSum_L4', 'shop_item__salesSum_L5', 'shop_item__salesSum_L6', 'item__salesSum_L6', 'shop_item__salesSum_L8']\n","                    Column Name     DType   MBytes                     Column Name   DType   MBytes     \n","                          Index     int64      0.0        itemCluster__salesSum_L1   int16     11.9     \n","                          month     int64     47.5        itemCluster__salesCnt_L1   int16     11.9     \n","                        shop_id     int64     47.5          itemCluster__revSum_L1   int16     11.9     \n","                        item_id     int64     47.5          shop_item__salesSum_L2   int16     11.9     \n","                     shop_group     int64     47.5          shop_item__salesCnt_L2   int16     11.9     \n","                   item_cluster   float64     47.5            shop_item__revSum_L2   int16     11.9     \n","               item_category_id     int64     47.5       shop_itemCat__salesCnt_L2   int16     11.9     \n","                     item_group     int64     47.5         shop_itemCat__revSum_L2   int16     11.9     \n","                       y_target   float32     23.8               shop__salesSum_L2   int16     11.9     \n","         shop_item__salesSum_L1     int16     11.9               item__salesSum_L2   int16     11.9     \n","         shop_item__salesMed_L1     int16     11.9               item__salesCnt_L2   int16     11.9     \n","         shop_item__salesCnt_L1     int16     11.9                 item__revSum_L2   int16     11.9     \n","           shop_item__revSum_L1     int16     11.9            itemCat__salesSum_L2   int16     11.9     \n","      shop_itemCat__salesSum_L1     int16     11.9            itemCat__salesCnt_L2   int16     11.9     \n","      shop_itemCat__salesMed_L1     int16     11.9        itemCluster__salesSum_L2   int16     11.9     \n","      shop_itemCat__salesCnt_L1     int16     11.9        itemCluster__salesCnt_L2   int16     11.9     \n","  shop_itemCluster__salesSum_L1     int16     11.9          itemCluster__revSum_L2   int16     11.9     \n","  shop_itemCluster__salesMed_L1     int16     11.9          shop_item__salesSum_L3   int16     11.9     \n","              shop__salesSum_L1     int16     11.9               shop__salesSum_L3   int16     11.9     \n","              shop__salesCnt_L1     int16     11.9               item__salesSum_L3   int16     11.9     \n","              item__salesSum_L1     int16     11.9               item__salesCnt_L3   int16     11.9     \n","              item__salesMed_L1     int16     11.9                 item__revSum_L3   int16     11.9     \n","              item__salesCnt_L1     int16     11.9            itemCat__salesSum_L3   int16     11.9     \n","                item__revSum_L1     int16     11.9            itemCat__salesCnt_L3   int16     11.9     \n","             shopGrp__revSum_L1     int16     11.9        itemCluster__salesCnt_L3   int16     11.9     \n","           itemCat__salesSum_L1     int16     11.9               item__salesSum_L4   int16     11.9     \n","           itemCat__salesCnt_L1     int16     11.9          shop_item__salesSum_L5   int16     11.9     \n","             itemCat__revSum_L1     int16     11.9          shop_item__salesSum_L6   int16     11.9     \n","           itemGrp__salesSum_L1     int16     11.9               item__salesSum_L6   int16     11.9     \n","             itemGrp__revSum_L1     int16     11.9          shop_item__salesSum_L8   int16     11.9     \n","\n","DataFrame Full Train-Val-Test DataFrame shape: (6226880, 59)\n","DataFrame Full Train-Val-Test DataFrame total memory usage: 962 MBytes\n","\n","final tvt.head():\n","   month  shop_id  item_id  shop_group  item_cluster  item_category_id  item_group  y_target  shop_item__salesSum_L1  shop_item__salesMed_L1  shop_item__salesCnt_L1  \\\n","0     13        2       30           9           158                40           6         0                       0                       0                       0   \n","1     13        2       31           9           158                37          18         0                       0                       0                       0   \n","2     13        2       32           9           167                40           6         0                      24                     138                    1056   \n","3     13        2       33           9           167                37          18         0                      24                     138                    1056   \n","4     13        2       34           9           168                40           6         0                       0                       0                       0   \n","\n","   shop_item__revSum_L1  shop_itemCat__salesSum_L1  shop_itemCat__salesMed_L1  shop_itemCat__salesCnt_L1  shop_itemCluster__salesSum_L1  shop_itemCluster__salesMed_L1  \\\n","0                     0                          0                          0                          0                              0                              0   \n","1                     0                          0                          0                          0                              0                              0   \n","2                     0                        503                        138                       1197                             33                            138   \n","3                     1                        291                        138                        693                             33                            138   \n","4                     0                          0                          0                          0                              0                              0   \n","\n","   shop__salesSum_L1  shop__salesCnt_L1  item__salesSum_L1  ...  item__salesCnt_L2  item__revSum_L2  itemCat__salesSum_L2  itemCat__salesCnt_L2  itemCluster__salesSum_L2  \\\n","0                  0                  0                  0  ...                  0                0                     0                     0                         0   \n","1                  0                  0                  0  ...                  0                0                     0                     0                         0   \n","2               2719               2585                338  ...                  0                0                     0                     0                         0   \n","3               2719               2585                169  ...               1034                5                 10806                 12648                       315   \n","4                  0                  0                  0  ...                  0                0                     0                     0                         0   \n","\n","   itemCluster__salesCnt_L2  itemCluster__revSum_L2  shop_item__salesSum_L3  shop__salesSum_L3  item__salesSum_L3  item__salesCnt_L3  item__revSum_L3  itemCat__salesSum_L3  \\\n","0                         0                       0                       0                  0                  0                  0                0                     0   \n","1                         0                       0                       0                  0                  0                  0                0                     0   \n","2                         0                       0                       0                  0                  0                  0                0                     0   \n","3                       577                      15                      50               2101                 47                387                2                  9559   \n","4                         0                       0                       0                  0                  0                  0                0                     0   \n","\n","   itemCat__salesCnt_L3  itemCluster__salesCnt_L3  item__salesSum_L4  shop_item__salesSum_L5  shop_item__salesSum_L6  item__salesSum_L6  shop_item__salesSum_L8  \n","0                     0                         0                  0                       0                       0                  0                      29  \n","1                     0                         0                  0                       0                       0                  0                       0  \n","2                     0                         0                  0                       0                       0                  0                       0  \n","3                  9280                       448                  0                       0                       0                  0                       0  \n","4                     0                         0                  0                       0                       0                  0                       0  \n","\n","[5 rows x 59 columns]\n","\n","|                                                         |  pid   |              vm               |             |\n","|     Time and Date     |        Measurement Point        | pid-GB | used-GB | avail-GB | total-GB | Active Proc |\n","------------------------------------------------------------------------------------------------------------------\n","| Sat 11:16:12 11/21/20 | Program Start                   |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Before defining features        |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Iteration Parameters Defined    |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Start EDA Module                |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:18 11/21/20 | Joined load_dfs mp              |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:19 11/21/20 | Merged stt with shops and items |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Completed stt transformations   |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Saved                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | End EDA Module                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Start Data Module               |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Loaded                 |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:38 11/21/20 | Joined Agg mp                   |  12.14 |   13.58 |    13.82 |    27.39 |     []      |\n","| Sat 11:16:46 11/21/20 | Joined CP mp                    |  12.11 |   13.55 |    13.85 |    27.39 |     []      |\n","| Sat 11:17:01 11/21/20 | Cartesian Product Rows Added    |  13.44 |   14.97 |    12.42 |    27.39 |     []      |\n","| Sat 11:18:05 11/21/20 | Joined Lag mp                   |  13.16 |   14.71 |    12.68 |    27.39 |     []      |\n","| Sat 11:18:58 11/21/20 | Time Lag Features Added         |  15.64 |   17.37 |    10.03 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | Data .ftr Saved                 |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | End Data Module                 |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | Start TVT Module                |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:22 11/21/20 | monthly_stt .ftr Loaded         |  15.67 |   17.41 |     9.98 |    27.39 |     []      |\n","| Sat 11:19:49 11/21/20 | TVT about to split              |  15.69 |   17.43 |     9.97 |    27.39 |     []      |\n","DataFrame tvt train_X shape: (5135449, 58)\n","DataFrame tvt train_X total memory: 813 MBytes\n","DataFrame tvt train_X column names: ['month', 'shop_id', 'item_id', 'shop_group', 'item_cluster', 'item_category_id', 'item_group', 'shop_item__salesSum_L1', 'shop_item__salesMed_L1', 'shop_item__salesCnt_L1', 'shop_item__revSum_L1', 'shop_itemCat__salesSum_L1', 'shop_itemCat__salesMed_L1', 'shop_itemCat__salesCnt_L1', 'shop_itemCluster__salesSum_L1', 'shop_itemCluster__salesMed_L1', 'shop__salesSum_L1', 'shop__salesCnt_L1', 'item__salesSum_L1', 'item__salesMed_L1', 'item__salesCnt_L1', 'item__revSum_L1', 'shopGrp__revSum_L1', 'itemCat__salesSum_L1', 'itemCat__salesCnt_L1', 'itemCat__revSum_L1', 'itemGrp__salesSum_L1', 'itemGrp__revSum_L1', 'itemCluster__salesSum_L1', 'itemCluster__salesCnt_L1', 'itemCluster__revSum_L1', 'shop_item__salesSum_L2', 'shop_item__salesCnt_L2', 'shop_item__revSum_L2', 'shop_itemCat__salesCnt_L2', 'shop_itemCat__revSum_L2', 'shop__salesSum_L2', 'item__salesSum_L2', 'item__salesCnt_L2', 'item__revSum_L2', 'itemCat__salesSum_L2', 'itemCat__salesCnt_L2', 'itemCluster__salesSum_L2', 'itemCluster__salesCnt_L2', 'itemCluster__revSum_L2', 'shop_item__salesSum_L3', 'shop__salesSum_L3', 'item__salesSum_L3', 'item__salesCnt_L3', 'item__revSum_L3', 'itemCat__salesSum_L3', 'itemCat__salesCnt_L3', 'itemCluster__salesCnt_L3', 'item__salesSum_L4', 'shop_item__salesSum_L5', 'shop_item__salesSum_L6', 'item__salesSum_L6', 'shop_item__salesSum_L8']\n","                    Column Name     DType   MBytes                     Column Name   DType   MBytes     \n","                          Index     int64     39.2        itemCluster__salesCnt_L1   int16      9.8     \n","                          month     int64     39.2          itemCluster__revSum_L1   int16      9.8     \n","                        shop_id     int64     39.2          shop_item__salesSum_L2   int16      9.8     \n","                        item_id     int64     39.2          shop_item__salesCnt_L2   int16      9.8     \n","                     shop_group     int64     39.2            shop_item__revSum_L2   int16      9.8     \n","                   item_cluster   float64     39.2       shop_itemCat__salesCnt_L2   int16      9.8     \n","               item_category_id     int64     39.2         shop_itemCat__revSum_L2   int16      9.8     \n","                     item_group     int64     39.2               shop__salesSum_L2   int16      9.8     \n","         shop_item__salesSum_L1     int16      9.8               item__salesSum_L2   int16      9.8     \n","         shop_item__salesMed_L1     int16      9.8               item__salesCnt_L2   int16      9.8     \n","         shop_item__salesCnt_L1     int16      9.8                 item__revSum_L2   int16      9.8     \n","           shop_item__revSum_L1     int16      9.8            itemCat__salesSum_L2   int16      9.8     \n","      shop_itemCat__salesSum_L1     int16      9.8            itemCat__salesCnt_L2   int16      9.8     \n","      shop_itemCat__salesMed_L1     int16      9.8        itemCluster__salesSum_L2   int16      9.8     \n","      shop_itemCat__salesCnt_L1     int16      9.8        itemCluster__salesCnt_L2   int16      9.8     \n","  shop_itemCluster__salesSum_L1     int16      9.8          itemCluster__revSum_L2   int16      9.8     \n","  shop_itemCluster__salesMed_L1     int16      9.8          shop_item__salesSum_L3   int16      9.8     \n","              shop__salesSum_L1     int16      9.8               shop__salesSum_L3   int16      9.8     \n","              shop__salesCnt_L1     int16      9.8               item__salesSum_L3   int16      9.8     \n","              item__salesSum_L1     int16      9.8               item__salesCnt_L3   int16      9.8     \n","              item__salesMed_L1     int16      9.8                 item__revSum_L3   int16      9.8     \n","              item__salesCnt_L1     int16      9.8            itemCat__salesSum_L3   int16      9.8     \n","                item__revSum_L1     int16      9.8            itemCat__salesCnt_L3   int16      9.8     \n","             shopGrp__revSum_L1     int16      9.8        itemCluster__salesCnt_L3   int16      9.8     \n","           itemCat__salesSum_L1     int16      9.8               item__salesSum_L4   int16      9.8     \n","           itemCat__salesCnt_L1     int16      9.8          shop_item__salesSum_L5   int16      9.8     \n","             itemCat__revSum_L1     int16      9.8          shop_item__salesSum_L6   int16      9.8     \n","           itemGrp__salesSum_L1     int16      9.8               item__salesSum_L6   int16      9.8     \n","             itemGrp__revSum_L1     int16      9.8          shop_item__salesSum_L8   int16      9.8     \n","       itemCluster__salesSum_L1     int16      9.8                                                      \n","\n","DataFrame tvt train_X shape: (5135449, 58)\n","DataFrame tvt train_X total memory usage: 813 MBytes\n","Series tvt train_y shape: (5135449,)\n","Series tvt train_y column name: ['y_target']\n","Series tvt train_y column memory: 61625388\n","Series tvt train_y column dtype: float32\n","|                                                         |  pid   |              vm               |             |\n","|     Time and Date     |        Measurement Point        | pid-GB | used-GB | avail-GB | total-GB | Active Proc |\n","------------------------------------------------------------------------------------------------------------------\n","| Sat 11:16:12 11/21/20 | Program Start                   |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Before defining features        |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Iteration Parameters Defined    |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Start EDA Module                |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:18 11/21/20 | Joined load_dfs mp              |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:19 11/21/20 | Merged stt with shops and items |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Completed stt transformations   |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Saved                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | End EDA Module                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Start Data Module               |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Loaded                 |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:38 11/21/20 | Joined Agg mp                   |  12.14 |   13.58 |    13.82 |    27.39 |     []      |\n","| Sat 11:16:46 11/21/20 | Joined CP mp                    |  12.11 |   13.55 |    13.85 |    27.39 |     []      |\n","| Sat 11:17:01 11/21/20 | Cartesian Product Rows Added    |  13.44 |   14.97 |    12.42 |    27.39 |     []      |\n","| Sat 11:18:05 11/21/20 | Joined Lag mp                   |  13.16 |   14.71 |    12.68 |    27.39 |     []      |\n","| Sat 11:18:58 11/21/20 | Time Lag Features Added         |  15.64 |   17.37 |    10.03 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | Data .ftr Saved                 |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | End Data Module                 |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | Start TVT Module                |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:22 11/21/20 | monthly_stt .ftr Loaded         |  15.67 |   17.41 |     9.98 |    27.39 |     []      |\n","| Sat 11:19:49 11/21/20 | TVT about to split              |  15.69 |   17.43 |     9.97 |    27.39 |     []      |\n","| Sat 11:19:54 11/21/20 | End TVT Module                  |  15.70 |   17.43 |     9.96 |    27.39 |     []      |\n","TVT Module complete; elapsed time = 0:00:34\n","\n","Model #1 of 2 -- Start ML Module @ Sat 11:19:54 11/21/20\n","[1]\tvalid_0's rmse: 2.15854\n","Training until validation scores don't improve for 20 rounds.\n","[2]\tvalid_0's rmse: 2.07392\n","[3]\tvalid_0's rmse: 1.99941\n","[4]\tvalid_0's rmse: 1.94185\n","[5]\tvalid_0's rmse: 1.88744\n","[6]\tvalid_0's rmse: 1.83958\n","[7]\tvalid_0's rmse: 1.7845\n","[8]\tvalid_0's rmse: 1.75074\n","[9]\tvalid_0's rmse: 1.70527\n","[10]\tvalid_0's rmse: 1.66559\n","[11]\tvalid_0's rmse: 1.63295\n","[12]\tvalid_0's rmse: 1.60203\n","[13]\tvalid_0's rmse: 1.56869\n","[14]\tvalid_0's rmse: 1.53826\n","[15]\tvalid_0's rmse: 1.51428\n","[16]\tvalid_0's rmse: 1.49439\n","[17]\tvalid_0's rmse: 1.46908\n","[18]\tvalid_0's rmse: 1.44581\n","[19]\tvalid_0's rmse: 1.43033\n","[20]\tvalid_0's rmse: 1.41632\n","[21]\tvalid_0's rmse: 1.39698\n","[22]\tvalid_0's rmse: 1.38745\n","[23]\tvalid_0's rmse: 1.37342\n","[24]\tvalid_0's rmse: 1.36307\n","[25]\tvalid_0's rmse: 1.35543\n","[26]\tvalid_0's rmse: 1.34701\n","[27]\tvalid_0's rmse: 1.34125\n","[28]\tvalid_0's rmse: 1.33174\n","[29]\tvalid_0's rmse: 1.32598\n","[30]\tvalid_0's rmse: 1.32056\n","[31]\tvalid_0's rmse: 1.31908\n","[32]\tvalid_0's rmse: 1.3121\n","[33]\tvalid_0's rmse: 1.30707\n","[34]\tvalid_0's rmse: 1.30599\n","[35]\tvalid_0's rmse: 1.30402\n","[36]\tvalid_0's rmse: 1.30276\n","[37]\tvalid_0's rmse: 1.3005\n","[38]\tvalid_0's rmse: 1.29711\n","[39]\tvalid_0's rmse: 1.29862\n","[40]\tvalid_0's rmse: 1.29815\n","[41]\tvalid_0's rmse: 1.29579\n","[42]\tvalid_0's rmse: 1.29627\n","[43]\tvalid_0's rmse: 1.29441\n","[44]\tvalid_0's rmse: 1.29046\n","[45]\tvalid_0's rmse: 1.29579\n","[46]\tvalid_0's rmse: 1.29686\n","[47]\tvalid_0's rmse: 1.29632\n","[48]\tvalid_0's rmse: 1.292\n","[49]\tvalid_0's rmse: 1.29332\n","[50]\tvalid_0's rmse: 1.29548\n","[51]\tvalid_0's rmse: 1.29448\n","[52]\tvalid_0's rmse: 1.29677\n","[53]\tvalid_0's rmse: 1.29865\n","[54]\tvalid_0's rmse: 1.30045\n","[55]\tvalid_0's rmse: 1.30102\n","[56]\tvalid_0's rmse: 1.30069\n","[57]\tvalid_0's rmse: 1.30095\n","[58]\tvalid_0's rmse: 1.29588\n","[59]\tvalid_0's rmse: 1.2963\n","[60]\tvalid_0's rmse: 1.29584\n","[61]\tvalid_0's rmse: 1.29707\n","[62]\tvalid_0's rmse: 1.2961\n","[63]\tvalid_0's rmse: 1.29603\n","[64]\tvalid_0's rmse: 1.29548\n","Early stopping, best iteration is:\n","[44]\tvalid_0's rmse: 1.29046\n","Done fitting; Model LGBM fit time: 0:00:41\n","Sat 11:20:36 11/21/20 -- Starting predictions...\n","Model LGBM fit time: 0:00:41\n","Transform and Predict train/val/test time: 0:00:10\n","R^2 train  = 0.1787    R^2 val  = 0.1951\n","RMSE train = 2.7672    RMSE val = 2.0073\n","\n","a R^2 train  = 0.1787    b R^2 val  = 0.1951\n","c RMSE train = 2.7672    d RMSE val = 2.0073\n","\n","|                                                         |  pid   |              vm               |             |\n","|     Time and Date     |        Measurement Point        | pid-GB | used-GB | avail-GB | total-GB | Active Proc |\n","------------------------------------------------------------------------------------------------------------------\n","| Sat 11:16:12 11/21/20 | Program Start                   |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Before defining features        |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Iteration Parameters Defined    |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Start EDA Module                |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:18 11/21/20 | Joined load_dfs mp              |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:19 11/21/20 | Merged stt with shops and items |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Completed stt transformations   |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Saved                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | End EDA Module                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Start Data Module               |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Loaded                 |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:38 11/21/20 | Joined Agg mp                   |  12.14 |   13.58 |    13.82 |    27.39 |     []      |\n","| Sat 11:16:46 11/21/20 | Joined CP mp                    |  12.11 |   13.55 |    13.85 |    27.39 |     []      |\n","| Sat 11:17:01 11/21/20 | Cartesian Product Rows Added    |  13.44 |   14.97 |    12.42 |    27.39 |     []      |\n","| Sat 11:18:05 11/21/20 | Joined Lag mp                   |  13.16 |   14.71 |    12.68 |    27.39 |     []      |\n","| Sat 11:18:58 11/21/20 | Time Lag Features Added         |  15.64 |   17.37 |    10.03 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | Data .ftr Saved                 |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | End Data Module                 |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | Start TVT Module                |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:22 11/21/20 | monthly_stt .ftr Loaded         |  15.67 |   17.41 |     9.98 |    27.39 |     []      |\n","| Sat 11:19:49 11/21/20 | TVT about to split              |  15.69 |   17.43 |     9.97 |    27.39 |     []      |\n","| Sat 11:19:54 11/21/20 | End TVT Module                  |  15.70 |   17.43 |     9.96 |    27.39 |     []      |\n","| Sat 11:19:54 11/21/20 | Start ML Module                 |  15.70 |   17.43 |     9.96 |    27.39 |     []      |\n","| Sat 11:20:46 11/21/20 | End ML Module                   |  15.72 |   17.46 |     9.93 |    27.39 |     []      |\n","ML Module complete; elapsed time = 0:00:52\n","\n","Model #1 of 2 -- Start Postprocessing Module @ Sat 11:20:46 11/21/20\n","\n","Model #2 of 2 -- Start ML Module @ Sat 11:20:46 11/21/20\n","[1]\tvalid_0's rmse: 2.15561\n","Training until validation scores don't improve for 20 rounds.\n","[2]\tvalid_0's rmse: 2.07544\n","[3]\tvalid_0's rmse: 2.00302\n","[4]\tvalid_0's rmse: 1.93531\n","[5]\tvalid_0's rmse: 1.8748\n","[6]\tvalid_0's rmse: 1.82742\n","[7]\tvalid_0's rmse: 1.77332\n","[8]\tvalid_0's rmse: 1.73766\n","[9]\tvalid_0's rmse: 1.69065\n","[10]\tvalid_0's rmse: 1.65605\n","[11]\tvalid_0's rmse: 1.62788\n","[12]\tvalid_0's rmse: 1.59508\n","[13]\tvalid_0's rmse: 1.56229\n","[14]\tvalid_0's rmse: 1.53661\n","[15]\tvalid_0's rmse: 1.51244\n","[16]\tvalid_0's rmse: 1.48491\n","[17]\tvalid_0's rmse: 1.4602\n","[18]\tvalid_0's rmse: 1.43903\n","[19]\tvalid_0's rmse: 1.42019\n","[20]\tvalid_0's rmse: 1.41406\n","[21]\tvalid_0's rmse: 1.40067\n","[22]\tvalid_0's rmse: 1.38894\n","[23]\tvalid_0's rmse: 1.38682\n","[24]\tvalid_0's rmse: 1.37575\n","[25]\tvalid_0's rmse: 1.36767\n","[26]\tvalid_0's rmse: 1.36347\n","[27]\tvalid_0's rmse: 1.35861\n","[28]\tvalid_0's rmse: 1.34559\n","[29]\tvalid_0's rmse: 1.34246\n","[30]\tvalid_0's rmse: 1.3374\n","[31]\tvalid_0's rmse: 1.336\n","[32]\tvalid_0's rmse: 1.33099\n","[33]\tvalid_0's rmse: 1.32711\n","[34]\tvalid_0's rmse: 1.32799\n","[35]\tvalid_0's rmse: 1.32907\n","[36]\tvalid_0's rmse: 1.32823\n","[37]\tvalid_0's rmse: 1.32954\n","[38]\tvalid_0's rmse: 1.32488\n","[39]\tvalid_0's rmse: 1.32282\n","[40]\tvalid_0's rmse: 1.32358\n","[41]\tvalid_0's rmse: 1.32256\n","[42]\tvalid_0's rmse: 1.32311\n","[43]\tvalid_0's rmse: 1.32012\n","[44]\tvalid_0's rmse: 1.31776\n","[45]\tvalid_0's rmse: 1.32272\n","[46]\tvalid_0's rmse: 1.32606\n","[47]\tvalid_0's rmse: 1.33019\n","[48]\tvalid_0's rmse: 1.3332\n","[49]\tvalid_0's rmse: 1.33344\n","[50]\tvalid_0's rmse: 1.33363\n","[51]\tvalid_0's rmse: 1.33505\n","[52]\tvalid_0's rmse: 1.33714\n","[53]\tvalid_0's rmse: 1.34177\n","[54]\tvalid_0's rmse: 1.34459\n","[55]\tvalid_0's rmse: 1.34576\n","[56]\tvalid_0's rmse: 1.3451\n","[57]\tvalid_0's rmse: 1.34626\n","[58]\tvalid_0's rmse: 1.34499\n","[59]\tvalid_0's rmse: 1.34413\n","[60]\tvalid_0's rmse: 1.3492\n","[61]\tvalid_0's rmse: 1.34906\n","[62]\tvalid_0's rmse: 1.34861\n","[63]\tvalid_0's rmse: 1.34953\n","[64]\tvalid_0's rmse: 1.34927\n","Early stopping, best iteration is:\n","[44]\tvalid_0's rmse: 1.31776\n","Done fitting; Model LGBM fit time: 0:00:42\n","Sat 11:21:28 11/21/20 -- Starting predictions...\n","Model LGBM fit time: 0:00:42\n","Transform and Predict train/val/test time: 0:00:09\n","R^2 train  = 0.1841    R^2 val  = 0.1841\n","RMSE train = 2.7581    RMSE val = 2.0209\n","\n","a R^2 train  = 0.1841    b R^2 val  = 0.1841\n","c RMSE train = 2.7581    d RMSE val = 2.0209\n","\n","|                                                         |  pid   |              vm               |             |\n","|     Time and Date     |        Measurement Point        | pid-GB | used-GB | avail-GB | total-GB | Active Proc |\n","------------------------------------------------------------------------------------------------------------------\n","| Sat 11:16:12 11/21/20 | Program Start                   |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Before defining features        |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Iteration Parameters Defined    |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:12 11/21/20 | Start EDA Module                |  14.13 |   15.72 |    11.68 |    27.39 |     []      |\n","| Sat 11:16:18 11/21/20 | Joined load_dfs mp              |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:19 11/21/20 | Merged stt with shops and items |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Completed stt transformations   |  13.51 |   15.05 |    12.34 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Saved                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | End EDA Module                  |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | Start Data Module               |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:21 11/21/20 | EDA .ftr Loaded                 |  13.48 |   15.02 |    12.38 |    27.39 |     []      |\n","| Sat 11:16:38 11/21/20 | Joined Agg mp                   |  12.14 |   13.58 |    13.82 |    27.39 |     []      |\n","| Sat 11:16:46 11/21/20 | Joined CP mp                    |  12.11 |   13.55 |    13.85 |    27.39 |     []      |\n","| Sat 11:17:01 11/21/20 | Cartesian Product Rows Added    |  13.44 |   14.97 |    12.42 |    27.39 |     []      |\n","| Sat 11:18:05 11/21/20 | Joined Lag mp                   |  13.16 |   14.71 |    12.68 |    27.39 |     []      |\n","| Sat 11:18:58 11/21/20 | Time Lag Features Added         |  15.64 |   17.37 |    10.03 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | Data .ftr Saved                 |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | End Data Module                 |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:20 11/21/20 | Start TVT Module                |  15.67 |   17.40 |     9.99 |    27.39 |     []      |\n","| Sat 11:19:22 11/21/20 | monthly_stt .ftr Loaded         |  15.67 |   17.41 |     9.98 |    27.39 |     []      |\n","| Sat 11:19:49 11/21/20 | TVT about to split              |  15.69 |   17.43 |     9.97 |    27.39 |     []      |\n","| Sat 11:19:54 11/21/20 | End TVT Module                  |  15.70 |   17.43 |     9.96 |    27.39 |     []      |\n","| Sat 11:19:54 11/21/20 | Start ML Module                 |  15.70 |   17.43 |     9.96 |    27.39 |     []      |\n","| Sat 11:20:46 11/21/20 | End ML Module                   |  15.72 |   17.46 |     9.93 |    27.39 |     []      |\n","| Sat 11:20:46 11/21/20 | Start Postprocessing Module     |  15.72 |   17.46 |     9.93 |    27.39 |     []      |\n","| Sat 11:20:46 11/21/20 | Start ML Module                 |  15.72 |   17.46 |     9.93 |    27.39 |     []      |\n","| Sat 11:21:38 11/21/20 | End ML Module                   |  15.72 |   17.45 |     9.94 |    27.39 |     []      |\n","ML Module complete; elapsed time = 0:00:52\n","\n","Model #2 of 2 -- Start Postprocessing Module @ Sat 11:21:38 11/21/20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GF6qWsBIgBUX"},"source":["##**Stop Execution of code below by invoking error**"]},{"cell_type":"code","metadata":{"id":"Hma9PfJGRCHk","cellView":"both"},"source":["# Dummy cell to stop the execution so we don't run any of the random code below (if we select \"Run All\", e.g.)\n","stop_running_code_at_this_cell = yes\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MvuCz4_s1ST1"},"source":["##**The Following Code Cell Contains Example Inputs / Ideas on Choosing Parameters**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":132},"id":"lzg38YwLNkm5","executionInfo":{"status":"ok","timestamp":1605794905365,"user_tz":300,"elapsed":2233,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"d17d51a7-4083-40a7-cff4-9d6e81d225d4"},"source":["# kag_features example and some other comments and choices for LGBM parameters can\n","#     be found in this code block....\n","\n","\n","\"\"\"\n","Example:\n","=============================================================================\n","input:\n","=============================================================================\n","iter1_feature_list = [\n","    # lag months = 1\n","    [1,  # months to lag by\n","     ['shop_id', 'item_id'],  # grouping for aggregate statistics\n","     OrderedDict([('sales', ['sum', 'median', 'count']), ('rev', ['sum'])])],  # aggregate stats\n","    [1, ['shop_id', 'item_category_id'], OrderedDict([('sales', ['sum', 'median', 'count'])])],\n","    [1, ['shop_id', 'item_cluster'], OrderedDict([('sales', ['sum', 'median'])])],\n","    [1, ['shop_id'], OrderedDict([('sales', ['sum', 'count'])])],\n","    [1, ['item_id'], OrderedDict([('sales', ['sum', 'median', 'count']), ('rev', ['sum'])])],\n","    [1, ['shop_group'], OrderedDict([('rev', ['sum'])])],\n","    [1, ['item_category_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])],\n","    [1, ['item_group'], OrderedDict([('sales', ['sum']), ('rev', ['sum'])])],\n","    [1, ['item_cluster'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])],\n","    # lag months = 2\n","    [2, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])],\n","    [2, ['shop_id', 'item_category_id'], OrderedDict([('sales', ['count']), ('rev', ['sum'])])],\n","    [2, ['shop_id'], OrderedDict([('sales', ['sum'])])],\n","    [2, ['item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])],\n","    [2, ['item_category_id'], OrderedDict([('sales', ['sum', 'count'])])],\n","    [2, ['item_cluster'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])],\n","    # lag months = 3\n","    [3, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])],\n","    [3, ['shop_id'], OrderedDict([('sales', ['sum'])])],\n","    [3, ['item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])],\n","    [3, ['item_category_id'], OrderedDict([('sales', ['sum', 'count'])])],\n","    [3, ['item_cluster'], OrderedDict([('sales', ['count'])])],\n","    # lag months = 4\n","    [4, ['item_id'], OrderedDict([('sales', ['sum'])])],\n","    # lag months = 5\n","    [5, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])],\n","    # lag months = 6\n","    [6, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])],\n","    [6, ['item_id'], OrderedDict([('sales', ['sum'])])],\n","    # lag months = 8\n","    [8, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])]]\n","\n","\n","=============================================================================\n","output:\n","=============================================================================\n","printable_lag_features\n","=============================================================================\n","OrderedDict([(1,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('shop_group', ['first']),\n","                                                   ('item_cluster', ['first']),\n","                                                   ('item_category_id', ['first']),\n","                                                   ('item_group', ['first']),\n","                                                   ('sales', ['sum', 'median', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['shop_group',\n","                                           'item_cluster',\n","                                           'item_category_id',\n","                                           'item_group',\n","                                           'shop_item__salesSum',\n","                                           'shop_item__salesMed',\n","                                           'shop_item__salesCnt',\n","                                           'shop_item__revSum'])]),\n","               OrderedDict([('group_name', 'shop_itemCat'),\n","                            ('group', ['month', 'shop_id', 'item_category_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['shop_itemCat__salesSum',\n","                                           'shop_itemCat__salesMed',\n","                                           'shop_itemCat__salesCnt'])]),\n","               OrderedDict([('group_name', 'shop_itemCluster'),\n","                            ('group', ['month', 'shop_id', 'item_cluster']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'median'])])),\n","                            ('col_names', ['shop_itemCluster__salesSum',\n","                                           'shop_itemCluster__salesMed'])]),\n","               OrderedDict([('group_name', 'shop'),\n","                            ('group', ['month', 'shop_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count'])])),\n","                            ('col_names', ['shop__salesSum',\n","                                           'shop__salesCnt'])]),\n","               OrderedDict([('group_name', 'item'),\n","                            ('group', ['month', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['item__salesSum',\n","                                           'item__salesMed',\n","                                           'item__salesCnt',\n","                                           'item__revSum'])]),\n","               OrderedDict([('group_name', 'shopGrp'),\n","                            ('group', ['month', 'shop_group']),\n","                            ('stats', OrderedDict([('rev', ['sum'])])),\n","                            ('col_names', ['shopGrp__revSum'])]),\n","               OrderedDict([('group_name', 'itemCat'),\n","                            ('group', ['month', 'item_category_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['itemCat__salesSum',\n","                                           'itemCat__salesCnt',\n","                                           'itemCat__revSum'])]),\n","               OrderedDict([('group_name', 'itemGrp'),\n","                            ('group', ['month', 'item_group']),\n","                            ('stats', OrderedDict([('sales', ['sum']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['itemGrp__salesSum',\n","                                           'itemGrp__revSum'])]),\n","               OrderedDict([('group_name', 'itemCluster'),\n","                            ('group', ['month', 'item_cluster']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['itemCluster__salesSum',\n","                                           'itemCluster__salesCnt',\n","                                           'itemCluster__revSum'])])\n","               ]),\n","             (2,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['shop_item__salesSum',\n","                                           'shop_item__salesCnt',\n","                                           'shop_item__revSum'])]),\n","               OrderedDict([('group_name', 'shop_itemCat'),\n","                            ('group', ['month', 'shop_id', 'item_category_id']),\n","                            ('stats', OrderedDict([('sales', ['count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['shop_itemCat__salesCnt',\n","                                           'shop_itemCat__revSum'])]),\n","               OrderedDict([('group_name', 'shop'),\n","                            ('group', ['month', 'shop_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop__salesSum'])]),\n","               OrderedDict([('group_name', 'item'),\n","                            ('group', ['month', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['item__salesSum',\n","                                           'item__salesCnt',\n","                                           'item__revSum'])]),\n","               OrderedDict([('group_name', 'itemCat'),\n","                            ('group', ['month', 'item_category_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count'])])),\n","                            ('col_names', ['itemCat__salesSum',\n","                                           'itemCat__salesCnt'])]),\n","               OrderedDict([('group_name', 'itemCluster'),\n","                            ('group', ['month', 'item_cluster']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['itemCluster__salesSum',\n","                                           'itemCluster__salesCnt',\n","                                           'itemCluster__revSum'])])\n","               ]),\n","             (3,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop_item__salesSum'])]),\n","               OrderedDict([('group_name', 'shop'),\n","                            ('group', ['month', 'shop_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop__salesSum'])]),\n","               OrderedDict([('group_name', 'item'),\n","                            ('group', ['month', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['item__salesSum',\n","                                           'item__salesCnt',\n","                                           'item__revSum'])]),\n","               OrderedDict([('group_name', 'itemCat'),\n","                            ('group', ['month', 'item_category_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count'])])),\n","                            ('col_names', ['itemCat__salesSum',\n","                                           'itemCat__salesCnt'])]),\n","               OrderedDict([('group_name', 'itemCluster'),\n","                            ('group', ['month', 'item_cluster']),\n","                            ('stats', OrderedDict([('sales', ['count'])])),\n","                            ('col_names', ['itemCluster__salesCnt'])])\n","               ]),\n","             (4,\n","              [OrderedDict([('group_name', 'item'),\n","                            ('group', ['month', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['item__salesSum'])])\n","               ]),\n","             (5,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop_item__salesSum'])])\n","               ]),\n","             (6,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop_item__salesSum'])]),\n","               OrderedDict([('group_name', 'item'),\n","                            ('group', ['month', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['item__salesSum'])])\n","               ]),\n","             (8,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop_item__salesSum'])])\n","               ])\n","             ])\n","\n","=============================================================================\n","cols\n","=============================================================================\n","{'all_keep': set({'ID',\n","                  'item_category_id',\n","                  'item_cluster',\n","                  'item_group',\n","                  'item_id',\n","                  'month',\n","                  'rev',\n","                  'sales',\n","                  'shop_group',\n","                  'shop_id'}),\n"," 'cat_feats': ['shop_id',\n","               'shop_group',\n","               'item_id',\n","               'item_cluster',\n","               'item_category_id',\n","               'item_group'],\n"," 'final_stt': ['month',\n","               'sales',\n","               'rev',\n","               'shop_id',\n","               'item_id',\n","               'shop_group',\n","               'item_cluster',\n","               'item_category_id',\n","               'item_group'],\n"," 'int_feats': ['month',\n","               'shop_id',\n","               'item_id',\n","               'shop_group',\n","               'item_cluster',\n","               'item_category_id',\n","               'item_group'],\n"," 'keep': {'date_scaling': ['month'],\n","          'items_enc': ['item_id',\n","                        'item_cluster',\n","                        'item_category_id',\n","                        'item_group'],\n","          'shops_enc': ['shop_id', 'shop_group'],\n","          'stt': ['month', 'sales', 'price', 'shop_id', 'item_id'],\n","          'test': ['ID', 'shop_id', 'item_id']},\n"," 'min_agg_cols': ['shop_item__salesSum',\n","                  'shop_item__salesMed',\n","                  'shop_item__salesCnt',\n","                  'shop_item__revSum',\n","                  'shop_itemCat__salesSum',\n","                  'shop_itemCat__salesMed',\n","                  'shop_itemCat__salesCnt',\n","                  'shop_itemCat__revSum',\n","                  'shop_itemCluster__salesSum',\n","                  'shop_itemCluster__salesMed',\n","                  'shop__salesSum',\n","                  'shop__salesCnt',\n","                  'item__salesSum',\n","                  'item__salesMed',\n","                  'item__salesCnt',\n","                  'item__revSum',\n","                  'shopGrp__revSum',\n","                  'itemCat__salesSum',\n","                  'itemCat__salesCnt',\n","                  'itemCat__revSum',\n","                  'itemGrp__salesSum',\n","                  'itemGrp__revSum',\n","                  'itemCluster__salesSum',\n","                  'itemCluster__salesCnt',\n","                  'itemCluster__revSum'],\n"," 'no_lag': ['shop_group', 'item_cluster', 'item_category_id', 'item_group']}\n","\n","=============================================================================\n","all_lag_features\n","=============================================================================\n","['shop_item__salesSum_L1',\n"," 'shop_item__salesMed_L1',\n"," 'shop_item__salesCnt_L1',\n"," 'shop_item__revSum_L1',\n"," 'shop_itemCat__salesSum_L1',\n"," 'shop_itemCat__salesMed_L1',\n"," 'shop_itemCat__salesCnt_L1',\n"," 'shop_itemCluster__salesSum_L1',\n"," 'shop_itemCluster__salesMed_L1',\n"," 'shop__salesSum_L1',\n"," 'shop__salesCnt_L1',\n"," 'item__salesSum_L1',\n"," 'item__salesMed_L1',\n"," 'item__salesCnt_L1',\n"," 'item__revSum_L1',\n"," 'shopGrp__revSum_L1',\n"," 'itemCat__salesSum_L1',\n"," 'itemCat__salesCnt_L1',\n"," 'itemCat__revSum_L1',\n"," 'itemGrp__salesSum_L1',\n"," 'itemGrp__revSum_L1',\n"," 'itemCluster__salesSum_L1',\n"," 'itemCluster__salesCnt_L1',\n"," 'itemCluster__revSum_L1',\n"," 'shop_item__salesSum_L2',\n"," 'shop_item__salesCnt_L2',\n"," 'shop_item__revSum_L2',\n"," 'shop_itemCat__salesCnt_L2',\n"," 'shop_itemCat__revSum_L2',\n"," 'shop__salesSum_L2',\n"," 'item__salesSum_L2',\n"," 'item__salesCnt_L2',\n"," 'item__revSum_L2',\n"," 'itemCat__salesSum_L2',\n"," 'itemCat__salesCnt_L2',\n"," 'itemCluster__salesSum_L2',\n"," 'itemCluster__salesCnt_L2',\n"," 'itemCluster__revSum_L2',\n"," 'shop_item__salesSum_L3',\n"," 'shop__salesSum_L3',\n"," 'item__salesSum_L3',\n"," 'item__salesCnt_L3',\n"," 'item__revSum_L3',\n"," 'itemCat__salesSum_L3',\n"," 'itemCat__salesCnt_L3',\n"," 'itemCluster__salesCnt_L3',\n"," 'item__salesSum_L4',\n"," 'shop_item__salesSum_L5',\n"," 'shop_item__salesSum_L6',\n"," 'item__salesSum_L6',\n"," 'shop_item__salesSum_L8']\n","\n","=============================================================================\n","min_feat_groups_stats_set\n","=============================================================================\n","OrderedDict([\n","    ('shop_item', OrderedDict([('group', ['month', 'shop_id', 'item_id']),\n","                               ('stats', OrderedDict([('shop_group', ['first']),\n","                                                      ('item_cluster', ['first']),\n","                                                      ('item_category_id', ['first']),\n","                                                      ('item_group', ['first']),\n","                                                      ('sales', ['sum', 'median', 'count']),\n","                                                      ('rev', ['sum'])])),\n","                               ('col_names', ['shop_group',\n","                                              'item_cluster',\n","                                              'item_category_id',\n","                                              'item_group',\n","                                              'shop_item__salesSum',\n","                                              'shop_item__salesMed',\n","                                              'shop_item__salesCnt',\n","                                              'shop_item__revSum'])])),\n","    ('shop_itemCat', OrderedDict([('group', ['month', 'shop_id', 'item_category_id']),\n","                                  ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\n","                                                         ('rev', ['sum'])])),\n","                                  ('col_names', ['shop_itemCat__salesSum',\n","                                                 'shop_itemCat__salesMed',\n","                                                 'shop_itemCat__salesCnt',\n","                                                 'shop_itemCat__revSum'])])),\n","    ('shop_itemCluster', OrderedDict([('group', ['month', 'shop_id', 'item_cluster']),\n","                                      ('stats', OrderedDict([('sales', ['sum', 'median'])])),\n","                                      ('col_names', ['shop_itemCluster__salesSum',\n","                                                     'shop_itemCluster__salesMed'])])),\n","    ('shop', OrderedDict([('group', ['month', 'shop_id']),\n","                          ('stats', OrderedDict([('sales', ['sum', 'count'])])),\n","                          ('col_names', ['shop__salesSum',\n","                                         'shop__salesCnt'])])),\n","    ('item', OrderedDict([('group', ['month', 'item_id']),\n","                          ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\n","                                                 ('rev', ['sum'])])),\n","                          ('col_names', ['item__salesSum',\n","                                         'item__salesMed',\n","                                         'item__salesCnt',\n","                                         'item__revSum'])])),\n","    ('shopGrp', OrderedDict([('group', ['month', 'shop_group']),\n","                             ('stats', OrderedDict([('rev', ['sum'])])),\n","                             ('col_names', ['shopGrp__revSum'])])),\n","    ('itemCat', OrderedDict([('group', ['month', 'item_category_id']),\n","                             ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                    ('rev', ['sum'])])),\n","                             ('col_names', ['itemCat__salesSum',\n","                                            'itemCat__salesCnt',\n","                                            'itemCat__revSum'])])),\n","    ('itemGrp', OrderedDict([('group', ['month', 'item_group']),\n","                             ('stats', OrderedDict([('sales', ['sum']),\n","                                                    ('rev', ['sum'])])),\n","                             ('col_names', ['itemGrp__salesSum',\n","                                            'itemGrp__revSum'])])),\n","    ('itemCluster', OrderedDict([('group', ['month', 'item_cluster']),\n","                                 ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                        ('rev', ['sum'])])),\n","                                 ('col_names', ['itemCluster__salesSum',\n","                                                'itemCluster__salesCnt',\n","                                                'itemCluster__revSum'])]))\n","    ])\n","\n","=============================================================================\n","lag_month_dict\n","=============================================================================\n","OrderedDict([(1, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L1'),\n","                              ('shop_item__salesMed', 'shop_item__salesMed_L1'),\n","                              ('shop_item__salesCnt', 'shop_item__salesCnt_L1'),\n","                              ('shop_item__revSum', 'shop_item__revSum_L1'),\n","                              ('shop_itemCat__salesSum', 'shop_itemCat__salesSum_L1'),\n","                              ('shop_itemCat__salesMed', 'shop_itemCat__salesMed_L1'),\n","                              ('shop_itemCat__salesCnt', 'shop_itemCat__salesCnt_L1'),\n","                              ('shop_itemCluster__salesSum', 'shop_itemCluster__salesSum_L1'),\n","                              ('shop_itemCluster__salesMed', 'shop_itemCluster__salesMed_L1'),\n","                              ('shop__salesSum', 'shop__salesSum_L1'),\n","                              ('shop__salesCnt', 'shop__salesCnt_L1'),\n","                              ('item__salesSum', 'item__salesSum_L1'),\n","                              ('item__salesMed', 'item__salesMed_L1'),\n","                              ('item__salesCnt', 'item__salesCnt_L1'),\n","                              ('item__revSum', 'item__revSum_L1'),\n","                              ('shopGrp__revSum', 'shopGrp__revSum_L1'),\n","                              ('itemCat__salesSum', 'itemCat__salesSum_L1'),\n","                              ('itemCat__salesCnt', 'itemCat__salesCnt_L1'),\n","                              ('itemCat__revSum', 'itemCat__revSum_L1'),\n","                              ('itemGrp__salesSum', 'itemGrp__salesSum_L1'),\n","                              ('itemGrp__revSum', 'itemGrp__revSum_L1'),\n","                              ('itemCluster__salesSum', 'itemCluster__salesSum_L1'),\n","                              ('itemCluster__salesCnt', 'itemCluster__salesCnt_L1'),\n","                              ('itemCluster__revSum', 'itemCluster__revSum_L1')])),\n","             (2, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L2'),\n","                              ('shop_item__salesCnt', 'shop_item__salesCnt_L2'),\n","                              ('shop_item__revSum', 'shop_item__revSum_L2'),\n","                              ('shop_itemCat__salesCnt', 'shop_itemCat__salesCnt_L2'),\n","                              ('shop_itemCat__revSum', 'shop_itemCat__revSum_L2'),\n","                              ('shop__salesSum', 'shop__salesSum_L2'),\n","                              ('item__salesSum', 'item__salesSum_L2'),\n","                              ('item__salesCnt', 'item__salesCnt_L2'),\n","                              ('item__revSum', 'item__revSum_L2'),\n","                              ('itemCat__salesSum', 'itemCat__salesSum_L2'),\n","                              ('itemCat__salesCnt', 'itemCat__salesCnt_L2'),\n","                              ('itemCluster__salesSum', 'itemCluster__salesSum_L2'),\n","                              ('itemCluster__salesCnt', 'itemCluster__salesCnt_L2'),\n","                              ('itemCluster__revSum', 'itemCluster__revSum_L2')])),\n","             (3, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L3'),\n","                              ('shop__salesSum', 'shop__salesSum_L3'),\n","                              ('item__salesSum', 'item__salesSum_L3'),\n","                              ('item__salesCnt', 'item__salesCnt_L3'),\n","                              ('item__revSum', 'item__revSum_L3'),\n","                              ('itemCat__salesSum', 'itemCat__salesSum_L3'),\n","                              ('itemCat__salesCnt', 'itemCat__salesCnt_L3'),\n","                              ('itemCluster__salesCnt', 'itemCluster__salesCnt_L3')])),\n","             (4, OrderedDict([('item__salesSum', 'item__salesSum_L4')])),\n","             (5, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L5')])),\n","             (6, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L6'),\n","                              ('item__salesSum', 'item__salesSum_L6')])),\n","             (8, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L8')]))\n","            ])\n","\"\"\"\n","\n","# =============================================================================\n","# Discarding Unused DataFrame Columns\n","# =============================================================================\n","# For reference, all column names of the loaded dataframes to choose from:\n","# items_enc_cols = ['item_id', 'item_tested', 'item_cluster', 'item_category_id',\n","#                   'item_cat_tested', 'item_group', 'item_category1', 'item_category2',\n","#                   'item_category3', 'item_category4']\n","# shops_enc_cols = ['shop_id','shop_tested','shop_group','shop_type','s_type_broad',\n","#                   'shop_federal_district','fd_popdens','fd_gdp','shop_city']\n","# date_scaling_cols = ['month', 'year', 'season', 'MoY', 'days_in_M',\n","#                      'weekday_weight', 'retail_sales', 'week_retail_weight']\n","# stt_cols = ['day', 'week', 'qtr', 'season', 'month', 'price', 'sales', 'shop_id', 'item_id']\n","# test_cols = ['ID', 'shop_id', 'item_id']\n","# =============================================================================\n","# To save memory, we can discard unnecessary features here by specifying only those that we use\n","# =============================================================================\n","# keep_cols = {'items_enc': ['item_id', 'item_category_id', 'item_group', 'item_cluster'],\n","#              'shops_enc': ['shop_id', 'shop_group'],\n","#              'date_scaling': ['month', 'week_retail_weight'],\n","#              'stt': ['month', 'sales', 'price', 'shop_id', 'item_id'],\n","#              'test': ['ID', 'shop_id', 'item_id']}\n","\n","\n","\n","\"\"\"\n","# ================================================================================================\n","# Some Useful/Common Choices for Parameter Splits:\n","# ================================================================================================\n","('model_type',      [['HGBR']])])  # for SKLearn version of GBDT (to be implemented)\n","('del_shops',       [[[9, 20]]]),\n","('del_shops',       [[[0, 1, 8, 9, 11, 13, 17, 20, 23, 27, 29, 30, 32, 33, 40, 43, 51, 54]]]),\n","('del_shops',       [[[8, 9, 13, 20, 23, 32, 33, 40]]]),\n","('del_shops',       [[False]]),\n","('del_item_cats',   [[[8, 10, 32, 59, 80, 81, 82]]]),\n","('del_item_cats',   [[[8, 80, 81, 82]]]),\n","('del_item_cats',   [[[1,4,8,10,13,14,17,18,32,39,46,48,50,51,52,53,59,66,68,80,81,82]]]),\n","('del_item_cats',   [[False]]),\n","('scale_sales',     [['week_retail_weight']])])\n","('scale_sales',     [['days_in_M']])])\n","('scale_sales',     [['weekday_weight']])])  # relative numbers of each weekday\n","('scale_sales',     [['retail_sales']])])  # Russian recession retail sales idx\n","('scale_sales',     [[False]])])\n","('cp_first_mo',     [[False]]),  # if no Cartesian Product fill is desired (to be implemented)\n","('feat_dtype',       [[np.float32]]),\n","('feat_dtype',       [[np.uint16]]),\n","('minmax_range',     [[(0, 32700)]]),  # matches with int16\n","('minmax_range',     [[(0, 65500)]]),  # matches with uint16\n","('tr_start_mo',     [[24]]),  # 24 gives less than 1yr data, but avoids Dec. 'outlier' of 2014\n","('tr_final_mo',     [[29, 32]]),\n","('tr_final_mo',     [[29, 30, 32]]),\n","('val_months',      [[1]])])\n","('val_months',      [[2]])])\n","\n","# ================================================================================================\n","# Clarification on meaning of certain parameters:\n","# ================================================================================================\n","\n","('features', [feature_iterations]  # list of class instances, 1 per iteration\n","# cp === Cartesian Product\n","('cp_fillna0', True  # fill n/a cp rows with 0 (bad for price-based stats, ok for revenue)\n","('cp_first_mo', 13  # mo + maxlag to start adding cp (eg, maxlag=6 and cp_first_mo=10 fills 4-33)\n","('cp_test_pairs', False  # force include all of test set 'shop-item pairs'\n","    # along with each month's conventional Cartesian Product fill\n","('clip_train', (0, 20)  # this clips sales after doing monthly groupings (monthly_stt df)\n","('feat_dtype', np.int16  # if df has np.NaNs, int type cannot represent - must use float32\n","('minmax_range', (0, 16000)  # use >0 for best LGBM results; smaller=faster fit; False=no scaling\n","('robust_qtiles', (20, 80)  # replace tuple with False if no scaling desired\n","('use_categorical', True  # relevant df cols -> categorical dtype just before modeling\n","('val_months', 999  # 1 # 2 # 999= all months after tr; else n mo after tr_final_mo\n","('colsample_bytree', 0.4  # feature_fraction; default 1 for LGBM, 0 for HGBR = (reverse of LGBM)\n","('importance_type', 'split'= n times feat used in model; 'gain'= total gains of splits using feat\n","('eval_metric', if metrics splits, use eg [['rmse',['rmse','l2']]] to get 'rmse' + ['rmse','l2']\n","('verbose', # int=4 prints every 4th iter; True=every iter; False=no print except best and last\n","('feature_name', # list of strings or if 'auto' and data is from pd df, data col names are used\n","('categorical_feature', If 'auto' and data is pd df, pd unordered categorical columns are used\n","\n","\n","\n","# FEATURES Example:\n","=============================================================================\n","input:\n","=============================================================================\n","iter1_feature_list = [\n","    # lag months = 1\n","    FG(1,  # months to lag by\n","     ['shop_id', 'item_id'],  # grouping for aggregate statistics\n","     OrderedDict([('sales', ['sum', 'median', 'count']), ('rev', ['sum'])])),  # aggregate stats\n","    FG(1, ['shop_id', 'item_category_id'], OrderedDict([('sales', ['sum', 'median', 'count'])])),\n","    FG(1, ['shop_id', 'item_cluster'], OrderedDict([('sales', ['sum', 'median'])])),\n","    FG(1, ['shop_id'], OrderedDict([('sales', ['sum', 'count'])])),\n","    FG(1, ['item_id'], OrderedDict([('sales', ['sum', 'median', 'count']), ('rev', ['sum'])])),\n","    FG(1, ['shop_group'], OrderedDict([('rev', ['sum'])])),\n","    FG(1, ['item_category_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    FG(1, ['item_group'], OrderedDict([('sales', ['sum']), ('rev', ['sum'])])),\n","    FG(1, ['item_cluster'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    # lag months = 2\n","    FG(2, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    FG(2, ['shop_id', 'item_category_id'], OrderedDict([('sales', ['count']), ('rev', ['sum'])])),\n","    FG(2, ['shop_id'], OrderedDict([('sales', ['sum'])])),\n","    FG(2, ['item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    FG(2, ['item_category_id'], OrderedDict([('sales', ['sum', 'count'])])),\n","    FG(2, ['item_cluster'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    # lag months = 3\n","    FG(3, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])),\n","    FG(3, ['shop_id'], OrderedDict([('sales', ['sum'])])),\n","    FG(3, ['item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\n","    FG(3, ['item_category_id'], OrderedDict([('sales', ['sum', 'count'])])),\n","    FG(3, ['item_cluster'], OrderedDict([('sales', ['count'])])),\n","    # lag months = 4\n","    FG(4, ['item_id'], OrderedDict([('sales', ['sum'])])),\n","    # lag months = 5\n","    FG(5, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])),\n","    # lag months = 6\n","    FG(6, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])),\n","    FG(6, ['item_id'], OrderedDict([('sales', ['sum'])])),\n","    # lag months = 8\n","    FG(8, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])]))]\n","\n","\n","=============================================================================\n","output:\n","=============================================================================\n","printable_lag_features\n","=============================================================================\n","OrderedDict([(1,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('shop_group', ['first']),\n","                                                   ('item_cluster', ['first']),\n","                                                   ('item_category_id', ['first']),\n","                                                   ('item_group', ['first']),\n","                                                   ('sales', ['sum', 'median', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['shop_group',\n","                                           'item_cluster',\n","                                           'item_category_id',\n","                                           'item_group',\n","                                           'shop_item__salesSum',\n","                                           'shop_item__salesMed',\n","                                           'shop_item__salesCnt',\n","                                           'shop_item__revSum'])]),\n","               OrderedDict([('group_name', 'shop_itemCat'),\n","                            ('group', ['month', 'shop_id', 'item_category_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['shop_itemCat__salesSum',\n","                                           'shop_itemCat__salesMed',\n","                                           'shop_itemCat__salesCnt'])]),\n","               OrderedDict([('group_name', 'shop_itemCluster'),\n","                            ('group', ['month', 'shop_id', 'item_cluster']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'median'])])),\n","                            ('col_names', ['shop_itemCluster__salesSum',\n","                                           'shop_itemCluster__salesMed'])]),\n","               OrderedDict([('group_name', 'shop'),\n","                            ('group', ['month', 'shop_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count'])])),\n","                            ('col_names', ['shop__salesSum',\n","                                           'shop__salesCnt'])]),\n","               OrderedDict([('group_name', 'item'),\n","                            ('group', ['month', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['item__salesSum',\n","                                           'item__salesMed',\n","                                           'item__salesCnt',\n","                                           'item__revSum'])]),\n","               OrderedDict([('group_name', 'shopGrp'),\n","                            ('group', ['month', 'shop_group']),\n","                            ('stats', OrderedDict([('rev', ['sum'])])),\n","                            ('col_names', ['shopGrp__revSum'])]),\n","               OrderedDict([('group_name', 'itemCat'),\n","                            ('group', ['month', 'item_category_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['itemCat__salesSum',\n","                                           'itemCat__salesCnt',\n","                                           'itemCat__revSum'])]),\n","               OrderedDict([('group_name', 'itemGrp'),\n","                            ('group', ['month', 'item_group']),\n","                            ('stats', OrderedDict([('sales', ['sum']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['itemGrp__salesSum',\n","                                           'itemGrp__revSum'])]),\n","               OrderedDict([('group_name', 'itemCluster'),\n","                            ('group', ['month', 'item_cluster']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['itemCluster__salesSum',\n","                                           'itemCluster__salesCnt',\n","                                           'itemCluster__revSum'])])\n","               ]),\n","             (2,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['shop_item__salesSum',\n","                                           'shop_item__salesCnt',\n","                                           'shop_item__revSum'])]),\n","               OrderedDict([('group_name', 'shop_itemCat'),\n","                            ('group', ['month', 'shop_id', 'item_category_id']),\n","                            ('stats', OrderedDict([('sales', ['count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['shop_itemCat__salesCnt',\n","                                           'shop_itemCat__revSum'])]),\n","               OrderedDict([('group_name', 'shop'),\n","                            ('group', ['month', 'shop_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop__salesSum'])]),\n","               OrderedDict([('group_name', 'item'),\n","                            ('group', ['month', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['item__salesSum',\n","                                           'item__salesCnt',\n","                                           'item__revSum'])]),\n","               OrderedDict([('group_name', 'itemCat'),\n","                            ('group', ['month', 'item_category_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count'])])),\n","                            ('col_names', ['itemCat__salesSum',\n","                                           'itemCat__salesCnt'])]),\n","               OrderedDict([('group_name', 'itemCluster'),\n","                            ('group', ['month', 'item_cluster']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['itemCluster__salesSum',\n","                                           'itemCluster__salesCnt',\n","                                           'itemCluster__revSum'])])\n","               ]),\n","             (3,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop_item__salesSum'])]),\n","               OrderedDict([('group_name', 'shop'),\n","                            ('group', ['month', 'shop_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop__salesSum'])]),\n","               OrderedDict([('group_name', 'item'),\n","                            ('group', ['month', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                   ('rev', ['sum'])])),\n","                            ('col_names', ['item__salesSum',\n","                                           'item__salesCnt',\n","                                           'item__revSum'])]),\n","               OrderedDict([('group_name', 'itemCat'),\n","                            ('group', ['month', 'item_category_id']),\n","                            ('stats', OrderedDict([('sales', ['sum', 'count'])])),\n","                            ('col_names', ['itemCat__salesSum',\n","                                           'itemCat__salesCnt'])]),\n","               OrderedDict([('group_name', 'itemCluster'),\n","                            ('group', ['month', 'item_cluster']),\n","                            ('stats', OrderedDict([('sales', ['count'])])),\n","                            ('col_names', ['itemCluster__salesCnt'])])\n","               ]),\n","             (4,\n","              [OrderedDict([('group_name', 'item'),\n","                            ('group', ['month', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['item__salesSum'])])\n","               ]),\n","             (5,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop_item__salesSum'])])\n","               ]),\n","             (6,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop_item__salesSum'])]),\n","               OrderedDict([('group_name', 'item'),\n","                            ('group', ['month', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['item__salesSum'])])\n","               ]),\n","             (8,\n","              [OrderedDict([('group_name', 'shop_item'),\n","                            ('group', ['month', 'shop_id', 'item_id']),\n","                            ('stats', OrderedDict([('sales', ['sum'])])),\n","                            ('col_names', ['shop_item__salesSum'])])\n","               ])\n","             ])\n","\n","=============================================================================\n","cols\n","=============================================================================\n","{'all_keep': set({'ID',\n","                  'item_category_id',\n","                  'item_cluster',\n","                  'item_group',\n","                  'item_id',\n","                  'month',\n","                  'rev',\n","                  'sales',\n","                  'shop_group',\n","                  'shop_id'}),\n"," 'cat_feats': ['shop_id',\n","               'shop_group',\n","               'item_id',\n","               'item_cluster',\n","               'item_category_id',\n","               'item_group'],\n"," 'final_stt': ['month',\n","               'sales',\n","               'rev',\n","               'shop_id',\n","               'item_id',\n","               'shop_group',\n","               'item_cluster',\n","               'item_category_id',\n","               'item_group'],\n"," 'int_feats': ['month',\n","               'shop_id',\n","               'item_id',\n","               'shop_group',\n","               'item_cluster',\n","               'item_category_id',\n","               'item_group'],\n"," 'keep': {'date_scaling': ['month'],\n","          'items_enc': ['item_id',\n","                        'item_cluster',\n","                        'item_category_id',\n","                        'item_group'],\n","          'shops_enc': ['shop_id', 'shop_group'],\n","          'stt': ['month', 'sales', 'price', 'shop_id', 'item_id'],\n","          'test': ['ID', 'shop_id', 'item_id']},\n"," 'min_agg_cols': ['shop_item__salesSum',\n","                  'shop_item__salesMed',\n","                  'shop_item__salesCnt',\n","                  'shop_item__revSum',\n","                  'shop_itemCat__salesSum',\n","                  'shop_itemCat__salesMed',\n","                  'shop_itemCat__salesCnt',\n","                  'shop_itemCat__revSum',\n","                  'shop_itemCluster__salesSum',\n","                  'shop_itemCluster__salesMed',\n","                  'shop__salesSum',\n","                  'shop__salesCnt',\n","                  'item__salesSum',\n","                  'item__salesMed',\n","                  'item__salesCnt',\n","                  'item__revSum',\n","                  'shopGrp__revSum',\n","                  'itemCat__salesSum',\n","                  'itemCat__salesCnt',\n","                  'itemCat__revSum',\n","                  'itemGrp__salesSum',\n","                  'itemGrp__revSum',\n","                  'itemCluster__salesSum',\n","                  'itemCluster__salesCnt',\n","                  'itemCluster__revSum'],\n"," 'no_lag': ['shop_group', 'item_cluster', 'item_category_id', 'item_group']}\n","\n","=============================================================================\n","all_lag_features\n","=============================================================================\n","['shop_item__salesSum_L1',\n"," 'shop_item__salesMed_L1',\n"," 'shop_item__salesCnt_L1',\n"," 'shop_item__revSum_L1',\n"," 'shop_itemCat__salesSum_L1',\n"," 'shop_itemCat__salesMed_L1',\n"," 'shop_itemCat__salesCnt_L1',\n"," 'shop_itemCluster__salesSum_L1',\n"," 'shop_itemCluster__salesMed_L1',\n"," 'shop__salesSum_L1',\n"," 'shop__salesCnt_L1',\n"," 'item__salesSum_L1',\n"," 'item__salesMed_L1',\n"," 'item__salesCnt_L1',\n"," 'item__revSum_L1',\n"," 'shopGrp__revSum_L1',\n"," 'itemCat__salesSum_L1',\n"," 'itemCat__salesCnt_L1',\n"," 'itemCat__revSum_L1',\n"," 'itemGrp__salesSum_L1',\n"," 'itemGrp__revSum_L1',\n"," 'itemCluster__salesSum_L1',\n"," 'itemCluster__salesCnt_L1',\n"," 'itemCluster__revSum_L1',\n"," 'shop_item__salesSum_L2',\n"," 'shop_item__salesCnt_L2',\n"," 'shop_item__revSum_L2',\n"," 'shop_itemCat__salesCnt_L2',\n"," 'shop_itemCat__revSum_L2',\n"," 'shop__salesSum_L2',\n"," 'item__salesSum_L2',\n"," 'item__salesCnt_L2',\n"," 'item__revSum_L2',\n"," 'itemCat__salesSum_L2',\n"," 'itemCat__salesCnt_L2',\n"," 'itemCluster__salesSum_L2',\n"," 'itemCluster__salesCnt_L2',\n"," 'itemCluster__revSum_L2',\n"," 'shop_item__salesSum_L3',\n"," 'shop__salesSum_L3',\n"," 'item__salesSum_L3',\n"," 'item__salesCnt_L3',\n"," 'item__revSum_L3',\n"," 'itemCat__salesSum_L3',\n"," 'itemCat__salesCnt_L3',\n"," 'itemCluster__salesCnt_L3',\n"," 'item__salesSum_L4',\n"," 'shop_item__salesSum_L5',\n"," 'shop_item__salesSum_L6',\n"," 'item__salesSum_L6',\n"," 'shop_item__salesSum_L8']\n","\n","=============================================================================\n","min_feat_groups_stats_set\n","=============================================================================\n","OrderedDict([\n","    ('shop_item', OrderedDict([('group', ['month', 'shop_id', 'item_id']),\n","                               ('stats', OrderedDict([('shop_group', ['first']),\n","                                                      ('item_cluster', ['first']),\n","                                                      ('item_category_id', ['first']),\n","                                                      ('item_group', ['first']),\n","                                                      ('sales', ['sum', 'median', 'count']),\n","                                                      ('rev', ['sum'])])),\n","                               ('col_names', ['shop_group',\n","                                              'item_cluster',\n","                                              'item_category_id',\n","                                              'item_group',\n","                                              'shop_item__salesSum',\n","                                              'shop_item__salesMed',\n","                                              'shop_item__salesCnt',\n","                                              'shop_item__revSum'])])),\n","    ('shop_itemCat', OrderedDict([('group', ['month', 'shop_id', 'item_category_id']),\n","                                  ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\n","                                                         ('rev', ['sum'])])),\n","                                  ('col_names', ['shop_itemCat__salesSum',\n","                                                 'shop_itemCat__salesMed',\n","                                                 'shop_itemCat__salesCnt',\n","                                                 'shop_itemCat__revSum'])])),\n","    ('shop_itemCluster', OrderedDict([('group', ['month', 'shop_id', 'item_cluster']),\n","                                      ('stats', OrderedDict([('sales', ['sum', 'median'])])),\n","                                      ('col_names', ['shop_itemCluster__salesSum',\n","                                                     'shop_itemCluster__salesMed'])])),\n","    ('shop', OrderedDict([('group', ['month', 'shop_id']),\n","                          ('stats', OrderedDict([('sales', ['sum', 'count'])])),\n","                          ('col_names', ['shop__salesSum',\n","                                         'shop__salesCnt'])])),\n","    ('item', OrderedDict([('group', ['month', 'item_id']),\n","                          ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\n","                                                 ('rev', ['sum'])])),\n","                          ('col_names', ['item__salesSum',\n","                                         'item__salesMed',\n","                                         'item__salesCnt',\n","                                         'item__revSum'])])),\n","    ('shopGrp', OrderedDict([('group', ['month', 'shop_group']),\n","                             ('stats', OrderedDict([('rev', ['sum'])])),\n","                             ('col_names', ['shopGrp__revSum'])])),\n","    ('itemCat', OrderedDict([('group', ['month', 'item_category_id']),\n","                             ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                    ('rev', ['sum'])])),\n","                             ('col_names', ['itemCat__salesSum',\n","                                            'itemCat__salesCnt',\n","                                            'itemCat__revSum'])])),\n","    ('itemGrp', OrderedDict([('group', ['month', 'item_group']),\n","                             ('stats', OrderedDict([('sales', ['sum']),\n","                                                    ('rev', ['sum'])])),\n","                             ('col_names', ['itemGrp__salesSum',\n","                                            'itemGrp__revSum'])])),\n","    ('itemCluster', OrderedDict([('group', ['month', 'item_cluster']),\n","                                 ('stats', OrderedDict([('sales', ['sum', 'count']),\n","                                                        ('rev', ['sum'])])),\n","                                 ('col_names', ['itemCluster__salesSum',\n","                                                'itemCluster__salesCnt',\n","                                                'itemCluster__revSum'])]))\n","    ])\n","\n","=============================================================================\n","lag_month_dict\n","=============================================================================\n","OrderedDict([(1, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L1'),\n","                              ('shop_item__salesMed', 'shop_item__salesMed_L1'),\n","                              ('shop_item__salesCnt', 'shop_item__salesCnt_L1'),\n","                              ('shop_item__revSum', 'shop_item__revSum_L1'),\n","                              ('shop_itemCat__salesSum', 'shop_itemCat__salesSum_L1'),\n","                              ('shop_itemCat__salesMed', 'shop_itemCat__salesMed_L1'),\n","                              ('shop_itemCat__salesCnt', 'shop_itemCat__salesCnt_L1'),\n","                              ('shop_itemCluster__salesSum', 'shop_itemCluster__salesSum_L1'),\n","                              ('shop_itemCluster__salesMed', 'shop_itemCluster__salesMed_L1'),\n","                              ('shop__salesSum', 'shop__salesSum_L1'),\n","                              ('shop__salesCnt', 'shop__salesCnt_L1'),\n","                              ('item__salesSum', 'item__salesSum_L1'),\n","                              ('item__salesMed', 'item__salesMed_L1'),\n","                              ('item__salesCnt', 'item__salesCnt_L1'),\n","                              ('item__revSum', 'item__revSum_L1'),\n","                              ('shopGrp__revSum', 'shopGrp__revSum_L1'),\n","                              ('itemCat__salesSum', 'itemCat__salesSum_L1'),\n","                              ('itemCat__salesCnt', 'itemCat__salesCnt_L1'),\n","                              ('itemCat__revSum', 'itemCat__revSum_L1'),\n","                              ('itemGrp__salesSum', 'itemGrp__salesSum_L1'),\n","                              ('itemGrp__revSum', 'itemGrp__revSum_L1'),\n","                              ('itemCluster__salesSum', 'itemCluster__salesSum_L1'),\n","                              ('itemCluster__salesCnt', 'itemCluster__salesCnt_L1'),\n","                              ('itemCluster__revSum', 'itemCluster__revSum_L1')])),\n","             (2, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L2'),\n","                              ('shop_item__salesCnt', 'shop_item__salesCnt_L2'),\n","                              ('shop_item__revSum', 'shop_item__revSum_L2'),\n","                              ('shop_itemCat__salesCnt', 'shop_itemCat__salesCnt_L2'),\n","                              ('shop_itemCat__revSum', 'shop_itemCat__revSum_L2'),\n","                              ('shop__salesSum', 'shop__salesSum_L2'),\n","                              ('item__salesSum', 'item__salesSum_L2'),\n","                              ('item__salesCnt', 'item__salesCnt_L2'),\n","                              ('item__revSum', 'item__revSum_L2'),\n","                              ('itemCat__salesSum', 'itemCat__salesSum_L2'),\n","                              ('itemCat__salesCnt', 'itemCat__salesCnt_L2'),\n","                              ('itemCluster__salesSum', 'itemCluster__salesSum_L2'),\n","                              ('itemCluster__salesCnt', 'itemCluster__salesCnt_L2'),\n","                              ('itemCluster__revSum', 'itemCluster__revSum_L2')])),\n","             (3, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L3'),\n","                              ('shop__salesSum', 'shop__salesSum_L3'),\n","                              ('item__salesSum', 'item__salesSum_L3'),\n","                              ('item__salesCnt', 'item__salesCnt_L3'),\n","                              ('item__revSum', 'item__revSum_L3'),\n","                              ('itemCat__salesSum', 'itemCat__salesSum_L3'),\n","                              ('itemCat__salesCnt', 'itemCat__salesCnt_L3'),\n","                              ('itemCluster__salesCnt', 'itemCluster__salesCnt_L3')])),\n","             (4, OrderedDict([('item__salesSum', 'item__salesSum_L4')])),\n","             (5, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L5')])),\n","             (6, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L6'),\n","                              ('item__salesSum', 'item__salesSum_L6')])),\n","             (8, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L8')]))\n","            ])\n","\n","# =============================================================================\n","# Example print_package_versions:\n","# =============================================================================\n","Python version: 3.8.3\n","lightgbm version: 3.0.0\n","matplotlib version: 3.3.2\n","numpy version: 1.19.2\n","pandas version: 1.1.3\n","scikit-learn version: 0.23.2\n","\n","os: win32\n","os_full: Windows-10-10.0.19041-SP0\n","runtime: Windows\n","gb_physical_dram: 48.0\n","n_logical_cpu: 12\n","n_physical_cpu: 6\n","n_multiprocessing_cpu: 12\n","chipset: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\n","\n","# =============================================================================\n","# Discarding Unused DataFrame Columns\n","# =============================================================================\n","For reference, all column names of the loaded dataframes to choose from:\n","items_enc_cols = ['item_id', 'item_tested', 'item_cluster', 'item_category_id',\n","                  'item_cat_tested', 'item_group', 'item_category1', 'item_category2',\n","                  'item_category3', 'item_category4']\n","shops_enc_cols = ['shop_id','shop_tested','shop_group','shop_type','s_type_broad',\n","                  'shop_federal_district','fd_popdens','fd_gdp','shop_city']\n","date_scaling_cols = ['month', 'year', 'season', 'MoY', 'days_in_M',\n","                      'weekday_weight', 'retail_sales', 'week_retail_weight']\n","stt_cols = ['day', 'week', 'qtr', 'season', 'month', 'price', 'sales', 'shop_id', 'item_id']\n","test_cols = ['ID', 'shop_id', 'item_id']\n","=============================================================================\n","To save memory, we can discard unnecessary features here by specifying only those that we use\n","=============================================================================\n","keep_cols = {'items_enc': ['item_id', 'item_category_id', 'item_group', 'item_cluster'],\n","              'shops_enc': ['shop_id', 'shop_group'],\n","              'date_scaling': ['month', 'week_retail_weight'],\n","              'stt': ['month', 'sales', 'price', 'shop_id', 'item_id'],\n","              'test': ['ID', 'shop_id', 'item_id']}\n","\n","# =============================================================================\n","# AttrDict example/explanation:\n","# =============================================================================\n","    somedict = {'key': 123, 'stuff': 456}\n","    data = AttrDict(somedict)\n","    print(data.key)\n","    print(data.stuff)\n","    >> 123\n","    >> 456\n","    data.key = 'abc'\n","    print(data.key)\n","    print(data['key'])\n","    >> abc\n","    >> abc\n","    def fn(**i): print(i)\n","    fn(**somedict)\n","    >> {'key': 123, 'stuff': 456}\n","    fn(**data)\n","    >> {'key': 'abc', 'stuff': 456}\n","    data.alpha = 'oh'\n","    fn(**data)\n","    {'key': 5, 'stuff': 456, 'alpha': 'oh'}\n","    data\n","    >> {'key': 5, 'stuff': 456, 'alpha': 'oh'}\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n# ================================================================================================\\n# Some Useful/Common Choices for Parameter Splits:\\n# ================================================================================================\\n('model_type',      [['HGBR']])])  # for SKLearn version of GBDT (to be implemented)\\n('del_shops',       [[[9, 20]]]),\\n('del_shops',       [[[0, 1, 8, 9, 11, 13, 17, 20, 23, 27, 29, 30, 32, 33, 40, 43, 51, 54]]]),\\n('del_shops',       [[[8, 9, 13, 20, 23, 32, 33, 40]]]),\\n('del_shops',       [[False]]),\\n('del_item_cats',   [[[8, 10, 32, 59, 80, 81, 82]]]),\\n('del_item_cats',   [[[8, 80, 81, 82]]]),\\n('del_item_cats',   [[[1,4,8,10,13,14,17,18,32,39,46,48,50,51,52,53,59,66,68,80,81,82]]]),\\n('del_item_cats',   [[False]]),\\n('scale_sales',     [['week_retail_weight']])])\\n('scale_sales',     [['days_in_M']])])\\n('scale_sales',     [['weekday_weight']])])  # relative numbers of each weekday\\n('scale_sales',     [['retail_sales']])])  # Russian recession retail sales idx\\n('scale_sales',     [[False]])])\\n('cp_first_mo',     [[False]]),  # if no Cartesian Product fill is desired (to be implemented)\\n('feat_dtype',       [[np.float32]]),\\n('feat_dtype',       [[np.uint16]]),\\n('minmax_range',     [[(0, 32700)]]),  # matches with int16\\n('minmax_range',     [[(0, 65500)]]),  # matches with uint16\\n('tr_start_mo',     [[24]]),  # 24 gives less than 1yr data, but avoids Dec. 'outlier' of 2014\\n('tr_final_mo',     [[29, 32]]),\\n('tr_final_mo',     [[29, 30, 32]]),\\n('val_months',      [[1]])])\\n('val_months',      [[2]])])\\n\\n# ================================================================================================\\n# Clarification on meaning of certain parameters:\\n# ================================================================================================\\n\\n('features', [feature_iterations]  # list of class instances, 1 per iteration\\n# cp === Cartesian Product\\n('cp_fillna0', True  # fill n/a cp rows with 0 (bad for price-based stats, ok for revenue)\\n('cp_first_mo', 13  # mo + maxlag to start adding cp (eg, maxlag=6 and cp_first_mo=10 fills 4-33)\\n('cp_test_pairs', False  # force include all of test set 'shop-item pairs'\\n    # along with each month's conventional Cartesian Product fill\\n('clip_train', (0, 20)  # this clips sales after doing monthly groupings (monthly_stt df)\\n('feat_dtype', np.int16  # if df has np.NaNs, int type cannot represent - must use float32\\n('minmax_range', (0, 16000)  # use >0 for best LGBM results; smaller=faster fit; False=no scaling\\n('robust_qtiles', (20, 80)  # replace tuple with False if no scaling desired\\n('use_categorical', True  # relevant df cols -> categorical dtype just before modeling\\n('val_months', 999  # 1 # 2 # 999= all months after tr; else n mo after tr_final_mo\\n('colsample_bytree', 0.4  # feature_fraction; default 1 for LGBM, 0 for HGBR = (reverse of LGBM)\\n('importance_type', 'split'= n times feat used in model; 'gain'= total gains of splits using feat\\n('eval_metric', if metrics splits, use eg [['rmse',['rmse','l2']]] to get 'rmse' + ['rmse','l2']\\n('verbose', # int=4 prints every 4th iter; True=every iter; False=no print except best and last\\n('feature_name', # list of strings or if 'auto' and data is from pd df, data col names are used\\n('categorical_feature', If 'auto' and data is pd df, pd unordered categorical columns are used\\n\\n\\n\\n# FEATURES Example:\\n=============================================================================\\ninput:\\n=============================================================================\\niter1_feature_list = [\\n    # lag months = 1\\n    FG(1,  # months to lag by\\n     ['shop_id', 'item_id'],  # grouping for aggregate statistics\\n     OrderedDict([('sales', ['sum', 'median', 'count']), ('rev', ['sum'])])),  # aggregate stats\\n    FG(1, ['shop_id', 'item_category_id'], OrderedDict([('sales', ['sum', 'median', 'count'])])),\\n    FG(1, ['shop_id', 'item_cluster'], OrderedDict([('sales', ['sum', 'median'])])),\\n    FG(1, ['shop_id'], OrderedDict([('sales', ['sum', 'count'])])),\\n    FG(1, ['item_id'], OrderedDict([('sales', ['sum', 'median', 'count']), ('rev', ['sum'])])),\\n    FG(1, ['shop_group'], OrderedDict([('rev', ['sum'])])),\\n    FG(1, ['item_category_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\\n    FG(1, ['item_group'], OrderedDict([('sales', ['sum']), ('rev', ['sum'])])),\\n    FG(1, ['item_cluster'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\\n    # lag months = 2\\n    FG(2, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\\n    FG(2, ['shop_id', 'item_category_id'], OrderedDict([('sales', ['count']), ('rev', ['sum'])])),\\n    FG(2, ['shop_id'], OrderedDict([('sales', ['sum'])])),\\n    FG(2, ['item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\\n    FG(2, ['item_category_id'], OrderedDict([('sales', ['sum', 'count'])])),\\n    FG(2, ['item_cluster'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\\n    # lag months = 3\\n    FG(3, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])),\\n    FG(3, ['shop_id'], OrderedDict([('sales', ['sum'])])),\\n    FG(3, ['item_id'], OrderedDict([('sales', ['sum', 'count']), ('rev', ['sum'])])),\\n    FG(3, ['item_category_id'], OrderedDict([('sales', ['sum', 'count'])])),\\n    FG(3, ['item_cluster'], OrderedDict([('sales', ['count'])])),\\n    # lag months = 4\\n    FG(4, ['item_id'], OrderedDict([('sales', ['sum'])])),\\n    # lag months = 5\\n    FG(5, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])),\\n    # lag months = 6\\n    FG(6, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])])),\\n    FG(6, ['item_id'], OrderedDict([('sales', ['sum'])])),\\n    # lag months = 8\\n    FG(8, ['shop_id', 'item_id'], OrderedDict([('sales', ['sum'])]))]\\n\\n\\n=============================================================================\\noutput:\\n=============================================================================\\nprintable_lag_features\\n=============================================================================\\nOrderedDict([(1,\\n              [OrderedDict([('group_name', 'shop_item'),\\n                            ('group', ['month', 'shop_id', 'item_id']),\\n                            ('stats', OrderedDict([('shop_group', ['first']),\\n                                                   ('item_cluster', ['first']),\\n                                                   ('item_category_id', ['first']),\\n                                                   ('item_group', ['first']),\\n                                                   ('sales', ['sum', 'median', 'count']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['shop_group',\\n                                           'item_cluster',\\n                                           'item_category_id',\\n                                           'item_group',\\n                                           'shop_item__salesSum',\\n                                           'shop_item__salesMed',\\n                                           'shop_item__salesCnt',\\n                                           'shop_item__revSum'])]),\\n               OrderedDict([('group_name', 'shop_itemCat'),\\n                            ('group', ['month', 'shop_id', 'item_category_id']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['shop_itemCat__salesSum',\\n                                           'shop_itemCat__salesMed',\\n                                           'shop_itemCat__salesCnt'])]),\\n               OrderedDict([('group_name', 'shop_itemCluster'),\\n                            ('group', ['month', 'shop_id', 'item_cluster']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'median'])])),\\n                            ('col_names', ['shop_itemCluster__salesSum',\\n                                           'shop_itemCluster__salesMed'])]),\\n               OrderedDict([('group_name', 'shop'),\\n                            ('group', ['month', 'shop_id']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'count'])])),\\n                            ('col_names', ['shop__salesSum',\\n                                           'shop__salesCnt'])]),\\n               OrderedDict([('group_name', 'item'),\\n                            ('group', ['month', 'item_id']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['item__salesSum',\\n                                           'item__salesMed',\\n                                           'item__salesCnt',\\n                                           'item__revSum'])]),\\n               OrderedDict([('group_name', 'shopGrp'),\\n                            ('group', ['month', 'shop_group']),\\n                            ('stats', OrderedDict([('rev', ['sum'])])),\\n                            ('col_names', ['shopGrp__revSum'])]),\\n               OrderedDict([('group_name', 'itemCat'),\\n                            ('group', ['month', 'item_category_id']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'count']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['itemCat__salesSum',\\n                                           'itemCat__salesCnt',\\n                                           'itemCat__revSum'])]),\\n               OrderedDict([('group_name', 'itemGrp'),\\n                            ('group', ['month', 'item_group']),\\n                            ('stats', OrderedDict([('sales', ['sum']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['itemGrp__salesSum',\\n                                           'itemGrp__revSum'])]),\\n               OrderedDict([('group_name', 'itemCluster'),\\n                            ('group', ['month', 'item_cluster']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'count']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['itemCluster__salesSum',\\n                                           'itemCluster__salesCnt',\\n                                           'itemCluster__revSum'])])\\n               ]),\\n             (2,\\n              [OrderedDict([('group_name', 'shop_item'),\\n                            ('group', ['month', 'shop_id', 'item_id']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'count']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['shop_item__salesSum',\\n                                           'shop_item__salesCnt',\\n                                           'shop_item__revSum'])]),\\n               OrderedDict([('group_name', 'shop_itemCat'),\\n                            ('group', ['month', 'shop_id', 'item_category_id']),\\n                            ('stats', OrderedDict([('sales', ['count']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['shop_itemCat__salesCnt',\\n                                           'shop_itemCat__revSum'])]),\\n               OrderedDict([('group_name', 'shop'),\\n                            ('group', ['month', 'shop_id']),\\n                            ('stats', OrderedDict([('sales', ['sum'])])),\\n                            ('col_names', ['shop__salesSum'])]),\\n               OrderedDict([('group_name', 'item'),\\n                            ('group', ['month', 'item_id']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'count']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['item__salesSum',\\n                                           'item__salesCnt',\\n                                           'item__revSum'])]),\\n               OrderedDict([('group_name', 'itemCat'),\\n                            ('group', ['month', 'item_category_id']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'count'])])),\\n                            ('col_names', ['itemCat__salesSum',\\n                                           'itemCat__salesCnt'])]),\\n               OrderedDict([('group_name', 'itemCluster'),\\n                            ('group', ['month', 'item_cluster']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'count']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['itemCluster__salesSum',\\n                                           'itemCluster__salesCnt',\\n                                           'itemCluster__revSum'])])\\n               ]),\\n             (3,\\n              [OrderedDict([('group_name', 'shop_item'),\\n                            ('group', ['month', 'shop_id', 'item_id']),\\n                            ('stats', OrderedDict([('sales', ['sum'])])),\\n                            ('col_names', ['shop_item__salesSum'])]),\\n               OrderedDict([('group_name', 'shop'),\\n                            ('group', ['month', 'shop_id']),\\n                            ('stats', OrderedDict([('sales', ['sum'])])),\\n                            ('col_names', ['shop__salesSum'])]),\\n               OrderedDict([('group_name', 'item'),\\n                            ('group', ['month', 'item_id']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'count']),\\n                                                   ('rev', ['sum'])])),\\n                            ('col_names', ['item__salesSum',\\n                                           'item__salesCnt',\\n                                           'item__revSum'])]),\\n               OrderedDict([('group_name', 'itemCat'),\\n                            ('group', ['month', 'item_category_id']),\\n                            ('stats', OrderedDict([('sales', ['sum', 'count'])])),\\n                            ('col_names', ['itemCat__salesSum',\\n                                           'itemCat__salesCnt'])]),\\n               OrderedDict([('group_name', 'itemCluster'),\\n                            ('group', ['month', 'item_cluster']),\\n                            ('stats', OrderedDict([('sales', ['count'])])),\\n                            ('col_names', ['itemCluster__salesCnt'])])\\n               ]),\\n             (4,\\n              [OrderedDict([('group_name', 'item'),\\n                            ('group', ['month', 'item_id']),\\n                            ('stats', OrderedDict([('sales', ['sum'])])),\\n                            ('col_names', ['item__salesSum'])])\\n               ]),\\n             (5,\\n              [OrderedDict([('group_name', 'shop_item'),\\n                            ('group', ['month', 'shop_id', 'item_id']),\\n                            ('stats', OrderedDict([('sales', ['sum'])])),\\n                            ('col_names', ['shop_item__salesSum'])])\\n               ]),\\n             (6,\\n              [OrderedDict([('group_name', 'shop_item'),\\n                            ('group', ['month', 'shop_id', 'item_id']),\\n                            ('stats', OrderedDict([('sales', ['sum'])])),\\n                            ('col_names', ['shop_item__salesSum'])]),\\n               OrderedDict([('group_name', 'item'),\\n                            ('group', ['month', 'item_id']),\\n                            ('stats', OrderedDict([('sales', ['sum'])])),\\n                            ('col_names', ['item__salesSum'])])\\n               ]),\\n             (8,\\n              [OrderedDict([('group_name', 'shop_item'),\\n                            ('group', ['month', 'shop_id', 'item_id']),\\n                            ('stats', OrderedDict([('sales', ['sum'])])),\\n                            ('col_names', ['shop_item__salesSum'])])\\n               ])\\n             ])\\n\\n=============================================================================\\ncols\\n=============================================================================\\n{'all_keep': set({'ID',\\n                  'item_category_id',\\n                  'item_cluster',\\n                  'item_group',\\n                  'item_id',\\n                  'month',\\n                  'rev',\\n                  'sales',\\n                  'shop_group',\\n                  'shop_id'}),\\n 'cat_feats': ['shop_id',\\n               'shop_group',\\n               'item_id',\\n               'item_cluster',\\n               'item_category_id',\\n               'item_group'],\\n 'final_stt': ['month',\\n               'sales',\\n               'rev',\\n               'shop_id',\\n               'item_id',\\n               'shop_group',\\n               'item_cluster',\\n               'item_category_id',\\n               'item_group'],\\n 'int_feats': ['month',\\n               'shop_id',\\n               'item_id',\\n               'shop_group',\\n               'item_cluster',\\n               'item_category_id',\\n               'item_group'],\\n 'keep': {'date_scaling': ['month'],\\n          'items_enc': ['item_id',\\n                        'item_cluster',\\n                        'item_category_id',\\n                        'item_group'],\\n          'shops_enc': ['shop_id', 'shop_group'],\\n          'stt': ['month', 'sales', 'price', 'shop_id', 'item_id'],\\n          'test': ['ID', 'shop_id', 'item_id']},\\n 'min_agg_cols': ['shop_item__salesSum',\\n                  'shop_item__salesMed',\\n                  'shop_item__salesCnt',\\n                  'shop_item__revSum',\\n                  'shop_itemCat__salesSum',\\n                  'shop_itemCat__salesMed',\\n                  'shop_itemCat__salesCnt',\\n                  'shop_itemCat__revSum',\\n                  'shop_itemCluster__salesSum',\\n                  'shop_itemCluster__salesMed',\\n                  'shop__salesSum',\\n                  'shop__salesCnt',\\n                  'item__salesSum',\\n                  'item__salesMed',\\n                  'item__salesCnt',\\n                  'item__revSum',\\n                  'shopGrp__revSum',\\n                  'itemCat__salesSum',\\n                  'itemCat__salesCnt',\\n                  'itemCat__revSum',\\n                  'itemGrp__salesSum',\\n                  'itemGrp__revSum',\\n                  'itemCluster__salesSum',\\n                  'itemCluster__salesCnt',\\n                  'itemCluster__revSum'],\\n 'no_lag': ['shop_group', 'item_cluster', 'item_category_id', 'item_group']}\\n\\n=============================================================================\\nall_lag_features\\n=============================================================================\\n['shop_item__salesSum_L1',\\n 'shop_item__salesMed_L1',\\n 'shop_item__salesCnt_L1',\\n 'shop_item__revSum_L1',\\n 'shop_itemCat__salesSum_L1',\\n 'shop_itemCat__salesMed_L1',\\n 'shop_itemCat__salesCnt_L1',\\n 'shop_itemCluster__salesSum_L1',\\n 'shop_itemCluster__salesMed_L1',\\n 'shop__salesSum_L1',\\n 'shop__salesCnt_L1',\\n 'item__salesSum_L1',\\n 'item__salesMed_L1',\\n 'item__salesCnt_L1',\\n 'item__revSum_L1',\\n 'shopGrp__revSum_L1',\\n 'itemCat__salesSum_L1',\\n 'itemCat__salesCnt_L1',\\n 'itemCat__revSum_L1',\\n 'itemGrp__salesSum_L1',\\n 'itemGrp__revSum_L1',\\n 'itemCluster__salesSum_L1',\\n 'itemCluster__salesCnt_L1',\\n 'itemCluster__revSum_L1',\\n 'shop_item__salesSum_L2',\\n 'shop_item__salesCnt_L2',\\n 'shop_item__revSum_L2',\\n 'shop_itemCat__salesCnt_L2',\\n 'shop_itemCat__revSum_L2',\\n 'shop__salesSum_L2',\\n 'item__salesSum_L2',\\n 'item__salesCnt_L2',\\n 'item__revSum_L2',\\n 'itemCat__salesSum_L2',\\n 'itemCat__salesCnt_L2',\\n 'itemCluster__salesSum_L2',\\n 'itemCluster__salesCnt_L2',\\n 'itemCluster__revSum_L2',\\n 'shop_item__salesSum_L3',\\n 'shop__salesSum_L3',\\n 'item__salesSum_L3',\\n 'item__salesCnt_L3',\\n 'item__revSum_L3',\\n 'itemCat__salesSum_L3',\\n 'itemCat__salesCnt_L3',\\n 'itemCluster__salesCnt_L3',\\n 'item__salesSum_L4',\\n 'shop_item__salesSum_L5',\\n 'shop_item__salesSum_L6',\\n 'item__salesSum_L6',\\n 'shop_item__salesSum_L8']\\n\\n=============================================================================\\nmin_feat_groups_stats_set\\n=============================================================================\\nOrderedDict([\\n    ('shop_item', OrderedDict([('group', ['month', 'shop_id', 'item_id']),\\n                               ('stats', OrderedDict([('shop_group', ['first']),\\n                                                      ('item_cluster', ['first']),\\n                                                      ('item_category_id', ['first']),\\n                                                      ('item_group', ['first']),\\n                                                      ('sales', ['sum', 'median', 'count']),\\n                                                      ('rev', ['sum'])])),\\n                               ('col_names', ['shop_group',\\n                                              'item_cluster',\\n                                              'item_category_id',\\n                                              'item_group',\\n                                              'shop_item__salesSum',\\n                                              'shop_item__salesMed',\\n                                              'shop_item__salesCnt',\\n                                              'shop_item__revSum'])])),\\n    ('shop_itemCat', OrderedDict([('group', ['month', 'shop_id', 'item_category_id']),\\n                                  ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\\n                                                         ('rev', ['sum'])])),\\n                                  ('col_names', ['shop_itemCat__salesSum',\\n                                                 'shop_itemCat__salesMed',\\n                                                 'shop_itemCat__salesCnt',\\n                                                 'shop_itemCat__revSum'])])),\\n    ('shop_itemCluster', OrderedDict([('group', ['month', 'shop_id', 'item_cluster']),\\n                                      ('stats', OrderedDict([('sales', ['sum', 'median'])])),\\n                                      ('col_names', ['shop_itemCluster__salesSum',\\n                                                     'shop_itemCluster__salesMed'])])),\\n    ('shop', OrderedDict([('group', ['month', 'shop_id']),\\n                          ('stats', OrderedDict([('sales', ['sum', 'count'])])),\\n                          ('col_names', ['shop__salesSum',\\n                                         'shop__salesCnt'])])),\\n    ('item', OrderedDict([('group', ['month', 'item_id']),\\n                          ('stats', OrderedDict([('sales', ['sum', 'median', 'count']),\\n                                                 ('rev', ['sum'])])),\\n                          ('col_names', ['item__salesSum',\\n                                         'item__salesMed',\\n                                         'item__salesCnt',\\n                                         'item__revSum'])])),\\n    ('shopGrp', OrderedDict([('group', ['month', 'shop_group']),\\n                             ('stats', OrderedDict([('rev', ['sum'])])),\\n                             ('col_names', ['shopGrp__revSum'])])),\\n    ('itemCat', OrderedDict([('group', ['month', 'item_category_id']),\\n                             ('stats', OrderedDict([('sales', ['sum', 'count']),\\n                                                    ('rev', ['sum'])])),\\n                             ('col_names', ['itemCat__salesSum',\\n                                            'itemCat__salesCnt',\\n                                            'itemCat__revSum'])])),\\n    ('itemGrp', OrderedDict([('group', ['month', 'item_group']),\\n                             ('stats', OrderedDict([('sales', ['sum']),\\n                                                    ('rev', ['sum'])])),\\n                             ('col_names', ['itemGrp__salesSum',\\n                                            'itemGrp__revSum'])])),\\n    ('itemCluster', OrderedDict([('group', ['month', 'item_cluster']),\\n                                 ('stats', OrderedDict([('sales', ['sum', 'count']),\\n                                                        ('rev', ['sum'])])),\\n                                 ('col_names', ['itemCluster__salesSum',\\n                                                'itemCluster__salesCnt',\\n                                                'itemCluster__revSum'])]))\\n    ])\\n\\n=============================================================================\\nlag_month_dict\\n=============================================================================\\nOrderedDict([(1, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L1'),\\n                              ('shop_item__salesMed', 'shop_item__salesMed_L1'),\\n                              ('shop_item__salesCnt', 'shop_item__salesCnt_L1'),\\n                              ('shop_item__revSum', 'shop_item__revSum_L1'),\\n                              ('shop_itemCat__salesSum', 'shop_itemCat__salesSum_L1'),\\n                              ('shop_itemCat__salesMed', 'shop_itemCat__salesMed_L1'),\\n                              ('shop_itemCat__salesCnt', 'shop_itemCat__salesCnt_L1'),\\n                              ('shop_itemCluster__salesSum', 'shop_itemCluster__salesSum_L1'),\\n                              ('shop_itemCluster__salesMed', 'shop_itemCluster__salesMed_L1'),\\n                              ('shop__salesSum', 'shop__salesSum_L1'),\\n                              ('shop__salesCnt', 'shop__salesCnt_L1'),\\n                              ('item__salesSum', 'item__salesSum_L1'),\\n                              ('item__salesMed', 'item__salesMed_L1'),\\n                              ('item__salesCnt', 'item__salesCnt_L1'),\\n                              ('item__revSum', 'item__revSum_L1'),\\n                              ('shopGrp__revSum', 'shopGrp__revSum_L1'),\\n                              ('itemCat__salesSum', 'itemCat__salesSum_L1'),\\n                              ('itemCat__salesCnt', 'itemCat__salesCnt_L1'),\\n                              ('itemCat__revSum', 'itemCat__revSum_L1'),\\n                              ('itemGrp__salesSum', 'itemGrp__salesSum_L1'),\\n                              ('itemGrp__revSum', 'itemGrp__revSum_L1'),\\n                              ('itemCluster__salesSum', 'itemCluster__salesSum_L1'),\\n                              ('itemCluster__salesCnt', 'itemCluster__salesCnt_L1'),\\n                              ('itemCluster__revSum', 'itemCluster__revSum_L1')])),\\n             (2, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L2'),\\n                              ('shop_item__salesCnt', 'shop_item__salesCnt_L2'),\\n                              ('shop_item__revSum', 'shop_item__revSum_L2'),\\n                              ('shop_itemCat__salesCnt', 'shop_itemCat__salesCnt_L2'),\\n                              ('shop_itemCat__revSum', 'shop_itemCat__revSum_L2'),\\n                              ('shop__salesSum', 'shop__salesSum_L2'),\\n                              ('item__salesSum', 'item__salesSum_L2'),\\n                              ('item__salesCnt', 'item__salesCnt_L2'),\\n                              ('item__revSum', 'item__revSum_L2'),\\n                              ('itemCat__salesSum', 'itemCat__salesSum_L2'),\\n                              ('itemCat__salesCnt', 'itemCat__salesCnt_L2'),\\n                              ('itemCluster__salesSum', 'itemCluster__salesSum_L2'),\\n                              ('itemCluster__salesCnt', 'itemCluster__salesCnt_L2'),\\n                              ('itemCluster__revSum', 'itemCluster__revSum_L2')])),\\n             (3, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L3'),\\n                              ('shop__salesSum', 'shop__salesSum_L3'),\\n                              ('item__salesSum', 'item__salesSum_L3'),\\n                              ('item__salesCnt', 'item__salesCnt_L3'),\\n                              ('item__revSum', 'item__revSum_L3'),\\n                              ('itemCat__salesSum', 'itemCat__salesSum_L3'),\\n                              ('itemCat__salesCnt', 'itemCat__salesCnt_L3'),\\n                              ('itemCluster__salesCnt', 'itemCluster__salesCnt_L3')])),\\n             (4, OrderedDict([('item__salesSum', 'item__salesSum_L4')])),\\n             (5, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L5')])),\\n             (6, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L6'),\\n                              ('item__salesSum', 'item__salesSum_L6')])),\\n             (8, OrderedDict([('shop_item__salesSum', 'shop_item__salesSum_L8')]))\\n            ])\\n\\n# =============================================================================\\n# Example print_package_versions:\\n# =============================================================================\\nPython version: 3.8.3\\nlightgbm version: 3.0.0\\nmatplotlib version: 3.3.2\\nnumpy version: 1.19.2\\npandas version: 1.1.3\\nscikit-learn version: 0.23.2\\n\\nos: win32\\nos_full: Windows-10-10.0.19041-SP0\\nruntime: Windows\\ngb_physical_dram: 48.0\\nn_logical_cpu: 12\\nn_physical_cpu: 6\\nn_multiprocessing_cpu: 12\\nchipset: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\\n\\n# =============================================================================\\n# Discarding Unused DataFrame Columns\\n# =============================================================================\\nFor reference, all column names of the loaded dataframes to choose from:\\nitems_enc_cols = ['item_id', 'item_tested', 'item_cluster', 'item_category_id',\\n                  'item_cat_tested', 'item_group', 'item_category1', 'item_category2',\\n                  'item_category3', 'item_category4']\\nshops_enc_cols = ['shop_id','shop_tested','shop_group','shop_type','s_type_broad',\\n                  'shop_federal_district','fd_popdens','fd_gdp','shop_city']\\ndate_scaling_cols = ['month', 'year', 'season', 'MoY', 'days_in_M',\\n                      'weekday_weight', 'retail_sales', 'week_retail_weight']\\nstt_cols = ['day', 'week', 'qtr', 'season', 'month', 'price', 'sales', 'shop_id', 'item_id']\\ntest_cols = ['ID', 'shop_id', 'item_id']\\n=============================================================================\\nTo save memory, we can discard unnecessary features here by specifying only those that we use\\n=============================================================================\\nkeep_cols = {'items_enc': ['item_id', 'item_category_id', 'item_group', 'item_cluster'],\\n              'shops_enc': ['shop_id', 'shop_group'],\\n              'date_scaling': ['month', 'week_retail_weight'],\\n              'stt': ['month', 'sales', 'price', 'shop_id', 'item_id'],\\n              'test': ['ID', 'shop_id', 'item_id']}\\n\\n# =============================================================================\\n# AttrDict example/explanation:\\n# =============================================================================\\n    somedict = {'key': 123, 'stuff': 456}\\n    data = AttrDict(somedict)\\n    print(data.key)\\n    print(data.stuff)\\n    >> 123\\n    >> 456\\n    data.key = 'abc'\\n    print(data.key)\\n    print(data['key'])\\n    >> abc\\n    >> abc\\n    def fn(**i): print(i)\\n    fn(**somedict)\\n    >> {'key': 123, 'stuff': 456}\\n    fn(**data)\\n    >> {'key': 'abc', 'stuff': 456}\\n    data.alpha = 'oh'\\n    fn(**data)\\n    {'key': 5, 'stuff': 456, 'alpha': 'oh'}\\n    data\\n    >> {'key': 5, 'stuff': 456, 'alpha': 'oh'}\\n\""]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"YPp_Nesy2yxn"},"source":["##**Final Project for Coursera's 'How to Win a Data Science Competition'**\n","April, 2020;  Andreas Theodoulou and Michael Gaidis;  (Competition Info last updated:  3 years ago)"]},{"cell_type":"markdown","metadata":{"id":"-YA__znazThG"},"source":["###**About this Competition**\n","\n","You are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n","\n","Evaluation: root mean squared error (RMSE). True target values are clipped into [0,20] range."]},{"cell_type":"markdown","metadata":{"id":"i_NGdlH8zbz8"},"source":["###**File descriptions**\n","\n","***sales_train.csv*** - the training set. Daily historical data from January 2013 to October 2015.\n","\n","***test.csv*** - the test set. You need to forecast the sales for these shops and products for November 2015.\n","\n","***sample_submission.csv*** - a sample submission file in the correct format.\n","\n","***items.csv*** - supplemental information about the items/products.\n","\n","***item_categories.csv***  - supplemental information about the items categories.\n","\n","***shops.csv***- supplemental information about the shops."]},{"cell_type":"markdown","metadata":{"id":"r_Oe76PW3aoN"},"source":["###**Data fields**\n","\n","***ID*** - an Id that represents a (Shop, Item) tuple within the test set\n","\n","***shop_id*** - unique identifier of a shop\n","\n","***item_id*** - unique identifier of a product\n","\n","***item_category_id*** - unique identifier of item category\n","\n","***item_cnt_day*** - number of products sold. You are predicting a monthly amount of this measure\n","\n","***item_price*** - current price of an item\n","\n","***date*** - date in format dd/mm/yyyy\n","\n","***month*** - a consecutive month number. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n","\n","***item_name*** - name of item\n","\n","***shop_name*** - name of shop\n","\n","***item_category_name*** - name of item category"]},{"cell_type":"markdown","metadata":{"id":"ufy-J0xC2efV"},"source":["## **Colab Prep Tips** for those using Google Colab\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WLdAjg45wEne"},"source":["### **Save Previous Work**\n","* Click **File -> Save a copy in Drive** and click **Open in new tab** in the pop-up window to save your progress in Google Drive. (This places the copy at the top level of Colab directory.)\n","* Or, in Google Drive before opening this notebook, right-click on this ipynb and select ***make a copy***, then with the copy in the same directory, right-click and select ***rename*** to update the version number.  Finally, right-click on the new version and ***open in colab***."]},{"cell_type":"markdown","metadata":{"id":"UlH3NopEv1Ha"},"source":["### **Select Runtime Type** *before* running notebook:\n","* Click **Runtime -> Change runtime type** and select **GPU** or **TPU** in Hardware accelerator box to enable faster training."]},{"cell_type":"markdown","metadata":{"id":"DP8AZkYQvtaj"},"source":["### **Keep Colab Active**\n","* To keep Colab connected by clicking on Colab window once every minute, go to Chrome Dev Tools --> Console Tab --> run the following code (April 2020):\n","</br>Take note that this should prevent disconnecting after each 1.5 hours of inactivity, but each runtime, if you don't have Colab Pro, will be terminated after 12 hours. (Pro = 24 hours) (Interval below is in millisec.)\n","```\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"#ok\").click()\n","}\n","setInterval(ClickConnect,60000)\n","```\n","Note that it will throw an error, its ok, it means that the Disconnection notification is not shown. Once it appear it will be clicked to reconnect.\n","\n","* If that doesn't work, try this in the console:\n","```\n","function ClickConnect(){\n","    console.log(\"Clicked on connect button\"); \n","    document.querySelector(\"colab-connect-button\").click()\n","}\n","setInterval(ClickConnect,60000)\n","```\n","* Lastly, can try this (older):\n","```\n","function KeepClicking(){\n","   console.log(\"Clicking\");\n","   document.querySelector(\"colab-toolbar-button#connect\").click()\n","}setInterval(KeepClicking,600000)\n","```"]},{"cell_type":"markdown","metadata":{"id":"SNR_OjCZcfxf"},"source":["### **Save Previous Work**\n","* Click **File -> Save a copy in Drive** and click **Open in new tab** in the pop-up window to save your progress in Google Drive. (This places the copy at the top level of Colab directory.)\n","* Or, in Google Drive before opening this notebook, right-click on this ipynb and select ***make a copy***, then with the copy in the same directory, right-click and select ***rename*** to update the version number.  Finally, right-click on the new version and ***open in colab***.\n","```\n","from datetime import datetime\n","from pytz import timezone\n","amsterdam = timezone('Europe/Amsterdam')\n","ams_time = amsterdam.localize(datetime(2002, 10, 27, 6, 0, 0))\n","print(ams_time)\n","# 2002-10-27 06:00:00+01:00\n","# It will also know when it's Summer Time\n","# in Amsterdam (similar to Daylight Savings Time):\n","ams_time = amsterdam.localize(datetime(2002, 6, 27, 6, 0, 0))\n","print(ams_time)\n","# 2002-06-27 06:00:00+02:00\n","```"]},{"cell_type":"markdown","metadata":{"id":"Ind-TjBOu6gs"},"source":["### **Import Python Packages; Set Environment Options; Identify Input Data Files**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9uV9AzFbtdJZ"},"source":["### **Analysis and Descriptive (Helper) Functions**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZA5YyPKB7QAt"},"source":["###**Define Feature Columns, Statistics, and Lags**"]},{"cell_type":"markdown","metadata":{"id":"gFbWag7-zoDU"},"source":["###**Define Dictionaries / Dataframes to Enable Looping Grid Search for Optimal Parameters**"]},{"cell_type":"markdown","metadata":{"id":"MY4tx3FwtVq7"},"source":["##**Mount Google Drive for access to Google Drive local repo; Load Data**"]},{"cell_type":"code","metadata":{"id":"iww85stFma4N"},"source":["# import psutil\n","# print(psutil.cpu_count(logical=False))\n","# print(psutil.cpu_freq(percpu=False))\n","# for k,v in os.environ.items():\n","#     print(k, v)\n","# import sys\n","# print(sys.platform)\n","\n","# Output:\n","# 2\n","# None\n","# ENV /root/.bashrc\n","# GCS_READ_CACHE_BLOCK_SIZE_MB 16\n","# CLOUDSDK_CONFIG /content/.config\n","# CUDA_VERSION 10.1.243\n","# PATH /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin\n","# HOME /root\n","# LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","# LANG en_US.UTF-8\n","# SHELL /bin/bash\n","# LIBRARY_PATH /usr/local/cuda/lib64/stubs\n","# CUDA_PKG_VERSION 10-1=10.1.243-1\n","# SHLVL 1\n","# GCE_METADATA_TIMEOUT 0\n","# NCCL_VERSION 2.7.8\n","# NVIDIA_VISIBLE_DEVICES all\n","# TF_FORCE_GPU_ALLOW_GROWTH true\n","# DEBIAN_FRONTEND noninteractive\n","# CUDNN_VERSION 7.6.5.32\n","# LAST_FORCED_REBUILD 20200910\n","# JPY_PARENT_PID 24\n","# PYTHONPATH /env/python\n","# DATALAB_SETTINGS_OVERRIDES {\"kernelManagerProxyPort\":6000,\"kernelManagerProxyHost\":\"172.28.0.3\",\"jupyterArgs\":[\"--ip=\\\"172.28.0.2\\\"\"]}\n","# NO_GCE_CHECK True\n","# GLIBCXX_FORCE_NEW 1\n","# NVIDIA_DRIVER_CAPABILITIES compute,utility\n","# _ /tools/node/bin/node\n","# LD_PRELOAD /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4\n","# NVIDIA_REQUIRE_CUDA cuda>=10.1 brand=tesla,driver>=396,driver<397 brand=tesla,driver>=410,driver<411 brand=tesla,driver>=418,driver<419\n","# OLDPWD /\n","# HOSTNAME 931aab700078\n","# COLAB_GPU 0\n","# PWD /\n","# CLOUDSDK_PYTHON python3\n","# GLIBCPP_FORCE_NEW 1\n","# PYTHONWARNINGS ignore:::pip._internal.cli.base_command\n","# TBE_CREDS_ADDR 172.28.0.1:8008\n","# TERM xterm-color\n","# CLICOLOR 1\n","# PAGER cat\n","# GIT_PAGER cat\n","# MPLBACKEND module://ipykernel.pylab.backend_inline\n","# TZ EST+05EDT,M4.1.0,M10.5.0\n","# KMP_DUPLICATE_LIB_OK True\n","# KMP_INIT_AT_FORK FALSE\n","# linux"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9sjtsvzM-dqV","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1601485562741,"user_tz":240,"elapsed":249,"user":{"displayName":"Michael Gaidis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhePD6jAHDISTPR4NIxczUMyvKb_tUrKiPQtlZeog=s64","userId":"15986747394900387491"}},"outputId":"8ccbc25e-2b03-4ba6-cb78-53e0de33dd5d"},"source":["ilp = Path(\"/usr/lib/python3.6/importlib/__init__.py\")\n","print(ilp.read_text())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"\"\"A pure Python implementation of import.\"\"\"\n","__all__ = ['__import__', 'import_module', 'invalidate_caches', 'reload']\n","\n","# Bootstrap help #####################################################\n","\n","# Until bootstrapping is complete, DO NOT import any modules that attempt\n","# to import importlib._bootstrap (directly or indirectly). Since this\n","# partially initialised package would be present in sys.modules, those\n","# modules would get an uninitialised copy of the source version, instead\n","# of a fully initialised version (either the frozen one or the one\n","# initialised below if the frozen one is not available).\n","import _imp  # Just the builtin component, NOT the full Python module\n","import sys\n","\n","try:\n","    import _frozen_importlib as _bootstrap\n","except ImportError:\n","    from . import _bootstrap\n","    _bootstrap._setup(sys, _imp)\n","else:\n","    # importlib._bootstrap is the built-in import, ensure we don't create\n","    # a second copy of the module.\n","    _bootstrap.__name__ = 'importlib._bootstrap'\n","    _bootstrap.__package__ = 'importlib'\n","    try:\n","        _bootstrap.__file__ = __file__.replace('__init__.py', '_bootstrap.py')\n","    except NameError:\n","        # __file__ is not guaranteed to be defined, e.g. if this code gets\n","        # frozen by a tool like cx_Freeze.\n","        pass\n","    sys.modules['importlib._bootstrap'] = _bootstrap\n","\n","try:\n","    import _frozen_importlib_external as _bootstrap_external\n","except ImportError:\n","    from . import _bootstrap_external\n","    _bootstrap_external._setup(_bootstrap)\n","    _bootstrap._bootstrap_external = _bootstrap_external\n","else:\n","    _bootstrap_external.__name__ = 'importlib._bootstrap_external'\n","    _bootstrap_external.__package__ = 'importlib'\n","    try:\n","        _bootstrap_external.__file__ = __file__.replace('__init__.py', '_bootstrap_external.py')\n","    except NameError:\n","        # __file__ is not guaranteed to be defined, e.g. if this code gets\n","        # frozen by a tool like cx_Freeze.\n","        pass\n","    sys.modules['importlib._bootstrap_external'] = _bootstrap_external\n","\n","# To simplify imports in test code\n","_w_long = _bootstrap_external._w_long\n","_r_long = _bootstrap_external._r_long\n","\n","# Fully bootstrapped at this point, import whatever you like, circular\n","# dependencies and startup overhead minimisation permitting :)\n","\n","import types\n","import warnings\n","\n","\n","# Public API #########################################################\n","\n","from ._bootstrap import __import__\n","\n","\n","def invalidate_caches():\n","    \"\"\"Call the invalidate_caches() method on all meta path finders stored in\n","    sys.meta_path (where implemented).\"\"\"\n","    for finder in sys.meta_path:\n","        if hasattr(finder, 'invalidate_caches'):\n","            finder.invalidate_caches()\n","\n","\n","def find_loader(name, path=None):\n","    \"\"\"Return the loader for the specified module.\n","\n","    This is a backward-compatible wrapper around find_spec().\n","\n","    This function is deprecated in favor of importlib.util.find_spec().\n","\n","    \"\"\"\n","    warnings.warn('Use importlib.util.find_spec() instead.',\n","                  DeprecationWarning, stacklevel=2)\n","    try:\n","        loader = sys.modules[name].__loader__\n","        if loader is None:\n","            raise ValueError('{}.__loader__ is None'.format(name))\n","        else:\n","            return loader\n","    except KeyError:\n","        pass\n","    except AttributeError:\n","        raise ValueError('{}.__loader__ is not set'.format(name)) from None\n","\n","    spec = _bootstrap._find_spec(name, path)\n","    # We won't worry about malformed specs (missing attributes).\n","    if spec is None:\n","        return None\n","    if spec.loader is None:\n","        if spec.submodule_search_locations is None:\n","            raise ImportError('spec for {} missing loader'.format(name),\n","                              name=name)\n","        raise ImportError('namespace packages do not have loaders',\n","                          name=name)\n","    return spec.loader\n","\n","\n","def import_module(name, package=None):\n","    \"\"\"Import a module.\n","\n","    The 'package' argument is required when performing a relative import. It\n","    specifies the package to use as the anchor point from which to resolve the\n","    relative import to an absolute import.\n","\n","    \"\"\"\n","    level = 0\n","    if name.startswith('.'):\n","        if not package:\n","            msg = (\"the 'package' argument is required to perform a relative \"\n","                   \"import for {!r}\")\n","            raise TypeError(msg.format(name))\n","        for character in name:\n","            if character != '.':\n","                break\n","            level += 1\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","\n","\n","_RELOADING = {}\n","\n","\n","def reload(module):\n","    \"\"\"Reload the module and return it.\n","\n","    The module must have been successfully imported before.\n","\n","    \"\"\"\n","    if not module or not isinstance(module, types.ModuleType):\n","        raise TypeError(\"reload() argument must be a module\")\n","    try:\n","        name = module.__spec__.name\n","    except AttributeError:\n","        name = module.__name__\n","\n","    if sys.modules.get(name) is not module:\n","        msg = \"module {} not in sys.modules\"\n","        raise ImportError(msg.format(name), name=name)\n","    if name in _RELOADING:\n","        return _RELOADING[name]\n","    _RELOADING[name] = module\n","    try:\n","        parent_name = name.rpartition('.')[0]\n","        if parent_name:\n","            try:\n","                parent = sys.modules[parent_name]\n","            except KeyError:\n","                msg = \"parent {!r} not in sys.modules\"\n","                raise ImportError(msg.format(parent_name),\n","                                  name=parent_name) from None\n","            else:\n","                pkgpath = parent.__path__\n","        else:\n","            pkgpath = None\n","        target = module\n","        spec = module.__spec__ = _bootstrap._find_spec(name, pkgpath, target)\n","        _bootstrap._exec(spec, module)\n","        # The module may have replaced itself in sys.modules!\n","        return sys.modules[name]\n","    finally:\n","        try:\n","            del _RELOADING[name]\n","        except KeyError:\n","            pass\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1GXED3-jyQC7"},"source":["##**Data Preparation: Feature Merging and Feature Generation**\n","###**1) Compute and Merge Statistics-Based Features on Grouped-by-Month training data**\n","* Note that features based on price are nonsensical if we add cartesian product fill.  However, item sales and item revenues are OK to use.\n","\n","###**2) Add Cartesian Product rows to the training data:**\n","* Idea is to help the model by informing it that we explicitly have no information about certain relevant shop_item pairs in certain months.\n","* Each month in train data will have additional rows such that the Cartesian Product of all shops and items ALREADY PRESENT IN THAT MONTH will be included.* * When we merge lagged features below, we will only forward-shift the shop-item pairs that are present in the later month. *(Might revisit later, if memory requirements are not too big, can forward-shift all shop-item pairs.)*\n","* **If not adding Cartesian Product, or if fillna(0) is used, can round features to integers, saving memory (pandas integers cannot store np.NaN; need float32)**\n","\n","###**3) Add Lagged Statistics columns to the training data:**"]},{"cell_type":"code","metadata":{"id":"Kr5WvevpX2Uw"},"source":["# domino tiles:  https://www.fileformat.info/info/unicode/block/domino_tiles/utf8test.htm\n","print('\\u2227'*5,'\\u2228'*5,'\\u2303'*5,'\\u2304'*5,'^^^','\\u02c5','\\u02c4','\\u02c6'*5,'\\u02ec'*5,'\\u22c0'*5,'\\u22c1'*5,'\\u2306'*5,'\\u2305'*5,'\\u23f7'*5)\n","print('\\u25b2'*5,'\\u25bc'*5,'\\u25c6'*5,'\\u25d2'*5,'\\u25d3'*5,'\\u25b4'*5,'\\u25be'*5,'\\u2635'*5,'\\u269c'*5,'\\u26d6'*5,'\\u2b81'*5,'\\u2b7f'*5,'\\u26dd'*5)\n","print('\\u26ba'*5,'\\u26bb'*5,'\\u2622'*5,'\\u262f'*5,'\\u2934'*5,'\\u2935'*5,'\\u2b71'*5,'\\u2b73'*5,'\\u2b9d'*5,'\\u2b9f'*5,'\\u2bc5'*5,'\\u2bc6'*5)\n","print('\\u2ba4'*5,'\\u2ba5'*5,'\\u2182'*5,'\\u2180'*5,'\\u2b12'*5,'\\u2b13'*5,'\\u2b18'*5,'\\u2b19'*5,'\\u273d'*5,'\\u2720'*5,'\\u1cf2'*5,'\\u1cf6'*5,'\\u205a'*5,'\\u2021'*5)\n","print('\\u224b'*5,'\\u224d'*5,'\\u2259'*5,'\\u225a'*5,'\\u2263'*5,'\\u2251'*5,'\\u2253'*5,'\\u22ce'*5,'\\u22cf'*5,'\\u2339'*5,'\\u2797'*5,'\\u27d7'*5,'\\u267b'*5)\n","print('\\u29d6'*5,'\\u29d7'*5,'\\u2bc1'*5,'\\u2b27'*5,'\\u2a77'*5,'\\u2a8b'*5,'\\u2ad8'*5,'\\u2e44'*5,'\\u2e0e'*5,'\\u2e1e'*5,'\\u2e1f'*5,'\\u3013'*5)\n","print('\\U0001f503'*5,'\\U0001f501'*5,'\\U0001f3ac','\\U0001f5aa','\\U0001f5ab','\\U0001f536'*5,'\\U0001f06d'*5,'\\U0001f6d1'*5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0e0MOvMExQoT"},"source":["##**To Do List:**"]},{"cell_type":"markdown","metadata":{"id":"rxqHPvEuB-En"},"source":["###**Loop over things to compute statistics, column scaling, adding cartesian product rows, and adding lagged features**\n","* multiprocess --> pool(merge,[months list]) do months in parallel? (maybe split monthly_stt by months, then do the merges in parallel, then concatenate all the months back together)\n","* multiprocess --> pool(merge,[lags list]) do lags in parallel?; check for proper column order / reset if necessary  (can I just add the shifted columns, and delete any N/A things where I don't have a shop-item match?, or make a big df with all the lags and then just one single merge (how=\"left\") )\n","\n","###**Loop over model fitting parameter splits**\n","* multiprocess.Pool (if not too overwhelming, can do several (or all) model fitting iterations in parallel)\n","* (?replicate \"test\" with simple code?) so we don't need to load and carry \"test\" dataframe in memory throughout.  Or, load from disk when loading ftr files in prediction module.\n","* del all dataframes containing data, after all loops over model params are done\n","\n","###**Additional routines**\n","* Plot feature importance ... each iteration, and ensemble averages (e.g., if rmse is < xxx).  Make df of feature importances (names=columns) vs. run iteration number (rows) and compute mean, stddev, range, quantiles\n","* Compute ensemble averages: straight average, weighted by rmse, etc.     \n","```\n","Simple ensemble averaging ensemble_y_pred_test = []\n","? ensemble_y_pred_test.append(y_pred_test)\n","? y_test_pred_avg = np.mean(ensemble_y_pred_test, axis=0)\n","compute feature importances averaged over ensemble\n","```\n","* Look at other locations for multiprocessing\n","* Look at other locations for timing blocks (and maybe save in MEMORY_STATS, as in have a MEMORY_STATS append at the end of every timed block)\n","* Look at other locations for MEMORY_STATS\n","</br>\n","\n","* Categorical features with LGBM: double-check it is working?\n","```\n","categorical_feature 🔗︎, default = \"\", type = multi-int or string, aliases: cat_feature, categorical_column, cat_column\n","        used to specify categorical features\n","        use number for index, e.g. categorical_feature=0,1,2 means columns 0, 1 and 2 are categorical features\n","        add a prefix name: for column name, e.g. categorical_feature=name:c1,c2,c3 means c1, c2 and c3\n","        Index starts from 0 and it doesn’t count the label column when passing type is int\n","        All values should be less than Int32.MaxValue (2147483647)\n","        Using large values could be memory consuming. Tree decision rule works best when categorical features are presented by consecutive integers starting from zero\n","        All negative values will be treated as missing values\n","```\n","Scikit-learn API: If ‘auto’ and data is pandas DataFrame, pandas unordered categorical columns are used. *????Note: (WHAT API?) only supports categorical int type (**not applicable** for data represented as **pandas DataFrame** in Python-package)*... double-check that we are using a working API\n","</br>\n","\n","* Special code needed for **GPU** enabled-LGBM modeling??\n","</br>\n","\n","* Possible setup for continued training, especially if we find runtimes are cut off by Colab.  (will probably need to save lgbm.model to disk at each step)...  init_model (string, Booster, LGBMModel or None, optional (default=None)) – \n","Filename of LightGBM model, Booster instance or LGBMModel instance used for **continue training**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LF91nOlf3Km7"},"source":["#**Potentially Useful Code Snippets**"]},{"cell_type":"markdown","metadata":{"id":"8CCw_Tu1xITH"},"source":["###**Ensembling and Trend/Feature Importance**"]},{"cell_type":"code","metadata":{"id":"eRDLNnGzvIIB"},"source":["# ENSEMBLING and FEATURE IMPORTANCE / TRENDS ############################################\n","\n","# ensembling_fn = True # ensembling_fn(output_file_names):\n","#     # can pull the submission files off the disk using OUTPUTS_df[model filename], after optionally setting a threshold for inclusion in the\n","#     #    ensemble, such as OUTPUTS_df[val_rmse] must be in the lowest quantile or something similar\n","#     # average, weighted-average, other method, to combine anything already saved to disk (default = straight avg of all runs in above loop)\n","\n","# compute_trends_fn = True # compute_trends_fn(output_results):\n","#     # create a df of features & feature importances for each run (or \"explode\" sideways the OUTPUTS_df features & importances)... use pd.info to\n","#     #    compute quantiles, mean, stddev for each of the features, and determine if anything looks interesting\n","#     # look at feature importances all together, and see if anything obvious good or bad\n","#     '''\n","#     Might want to look at the predict_contrib parameter for LGBM:  https://lightgbm.readthedocs.io/en/latest/Parameters.html\n","#     '''\n","#     # look at splits and see if any parameters obviously good or bad (correlation matrix of parameters with output results?)\n","#     #feat_imp = pd.DataFrame.from_dict(OUTPUTS_df[\"feature_importances_\"])\n","#     #OUTPUTS_df.at[RUN_n,\"feature_name_\"]\n","#     make empty df with columns from list at = outputs.at[0,featname]\n","#     append rows with elements = list elements in feature_importances_ for each feature name\n","#     df now has as many rows as N_TRAIN_iterations\n","#     compute df.info stats or quantiles/mean/std and store somewhere; make some plots; make some automated recommendations?\n","nocode=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zPM4RLKcn3RE"},"source":["###**Averaging Several Stored Predictions/Submissions from Disk**"]},{"cell_type":"code","metadata":{"id":"dzzImBPS3CsO","cellView":"both"},"source":["# average several submission files to get ensemble average\n","%cd \"{GDRIVE_REPO_PATH}\"\n","# source_dir = Path('models_and_predictions/bagging_LGBM')\n","# prediction_files = source_dir.iterdir()\n","source_dir = 'models_and_predictions/bagging_LGBM'\n","prediction_files = os.listdir(source_dir)\n","print(\"Loading Files from Google Drive repo into Colab...\\n\")\n","\n","# filename to save ensemble average predictions for submission\n","ensemble_name = 'LGBMv6v7_bag06'\n","\n","print(f'filename {ensemble_name}')\n","# Loop to load the data files into appropriately-named pandas DataFrames, and save in np array for easy calc of ensemble average\n","preds = []\n","for f_name in prediction_files:\n","    filename = f_name.rsplit(\"/\")[-1]\n","    data_frame_name = filename.split(\".\")[0][:-11]\n","    path_name = os.path.join('models_and_predictions/bagging_LGBM/'+ filename)\n","    exec(data_frame_name + \" = pd.read_csv(path_name)\")\n","    print(f'Data Frame: {data_frame_name}; n_rows = {len(eval(data_frame_name))}, n_cols = ')\n","    preds.append(eval(data_frame_name).item_cnt_month.to_numpy())\n","\n","# Simple ensemble averaging\n","pred_ens_avg = np.mean(preds, axis=0)\n","ensemble_submission = LGBMv6mg_17_.copy(deep=True)\n","ensemble_submission.item_cnt_month = pred_ens_avg\n","\n","ensemble_submission.to_csv(\"./models_and_predictions/\" + ensemble_name + '_submission.csv', index=False)\n","\n","display(ensemble_submission.head(8))\n","print(f'filename {ensemble_name} saved: {strftime(\"%a %X %x\")}')\n","print('Coursera:  ')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lKGrJUG7f50F"},"source":["###**Feature Importances**"]},{"cell_type":"code","metadata":{"id":"LbszXTq0vhZ-","cellView":"both"},"source":["# Plot feature importance - Results Visualization\n","itercount=0\n","if ITERS.at[itercount,'_model_type'] == 'LGBM':\n","    print_threshold = 25\n","    feature_importances_ = ITERS.at[itercount,'feature_importances_']\n","    feature_name_        = ITERS.at[itercount,'feature_name_']\n","    fi = pd.DataFrame(zip(feature_name_,feature_importances_),columns=['feature','value'])\n","    fi = fi.sort_values('value',ascending=False,ignore_index=True)\n","    fi['norm_value'] = round(100*fi.value / fi.value.max(),2)\n","    fi['lag'] = fi.feature.apply(lambda x: (x.split('L')[-1]) if len(x.split('L'))> 1 else 0)\n","    fi['feature_base'] = fi.feature.apply(lambda x: x.split('_L')[0])\n","    print(fi.iloc[list(range(0,8))+list(range(-7,0)),:]) #[[1,3,5,7,-7,-5]][:])\n","    # model_filename_fi = ITERS.at[itercount,'_model_type']+ITERS.at[itercount,'_model_filename'] + \"_feature_importance.csv\"\n","    # fi.to_csv(\"./models_and_predictions/\" + model_filename_fi, index=False)\n","    # printout to assist with removing low-importance features for following runs\n","    if fi.norm_value.min() < print_threshold:\n","        fi_low = fi[fi.norm_value < print_threshold]\n","        fi_low = fi_low.sort_values(['lag','norm_value'])\n","        fi_low.norm_value = fi_low.norm_value.apply(lambda x: f'{round(x):d}')\n","        fi_low['lag_feature_importance'] = fi_low.apply(lambda x: f\"{f'L{x.lag} fi{x.norm_value}':{len(x.feature_base)}s}\",axis=1)\n","        print(fi_low.lag_feature_importance.to_list())\n","        print(fi_low.feature_base.to_list())\n","    # make importances relative to max importance\n","    feature_importances_ = 100.0 * (feature_importances_ / feature_importances_.max())\n","    sorted_idx = np.arange(feature_importances_.shape[0])\n","    pos = np.arange(sorted_idx.shape[0]) + .5\n","    plt.figure(figsize=(24,12)) \n","    plt.bar(pos, feature_importances_[sorted_idx], align='center')\n","    plt.xticks(pos, feature_name_[sorted_idx])\n","    plt.ylabel('Relative Importance')\n","    plt.title('Variable Importance')\n","    plt.tick_params(axis='x', which='major', labelsize = 13, labelrotation=90)\n","    plt.grid(True,which='major',axis='y')\n","    plt.tick_params(axis='y', which='major', grid_color='black',grid_alpha=0.7)\n","    # plt.savefig('LGBM_feature_importance_v1.4_mg.png')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KDSgvfrtukP1"},"source":["###**Using GPUs with LGBM**"]},{"cell_type":"code","metadata":{"id":"PNBWs59yutWO"},"source":["# GPU use with LGBM modeling:\n","'''\n","May want to see if we can better inform LGBM routine when we are using a GPU\n","https://lightgbm.readthedocs.io/en/latest/GPU-Targets.html#query-opencl-devices-in-your-system\n","Your system might have multiple GPUs from different vendors (“platforms”) installed. Setting up LightGBM GPU device requires two parameters: \n","OpenCL Platform ID (gpu_platform_id) and OpenCL Device ID (gpu_device_id). Generally speaking, each vendor provides an OpenCL platform, \n","and devices from the same vendor have different device IDs under that platform. For example, if your system has an Intel integrated GPU and \n","two discrete GPUs from AMD, you will have two OpenCL platforms (with gpu_platform_id=0 and gpu_platform_id=1). If the platform 0 is Intel, \n","it has one device (gpu_device_id=0) representing the Intel GPU; if the platform 1 is AMD, it has two devices (gpu_device_id=0, gpu_device_id=1) \n","representing the two AMD GPUs. If you have a discrete GPU by AMD/NVIDIA and an integrated GPU by Intel, make sure to select the correct gpu_platform_id \n","to use the discrete GPU as it usually provides better performance.\n","\n","On Windows, OpenCL devices can be queried using GPUCapsViewer, under the OpenCL tab. http://www.ozone3d.net/gpu_caps_viewer/ \n","Note that the platform and device IDs reported by this utility start from 1. So you should minus the reported IDs by 1.\n","\n","On Linux, OpenCL devices can be listed using the clinfo command. On Ubuntu, you can install clinfo by executing sudo apt-get install clinfo.\n","\n","Make sure you list the OpenCL devices in your system and set gpu_platform_id and gpu_device_id correctly. \n","In the following examples, our system has 1 GPU platform (gpu_platform_id = 0) from AMD APP SDK. \n","The first device gpu_device_id = 0 is a GPU device (AMD Oland), and the second device gpu_device_id = 1 is the x86 CPU backend.\n","\n","R Example of using GPU (gpu_platform_id = 0 and gpu_device_id = 0 in our system):\n","> params <- list(objective = \"regression\",\n","+                metric = \"rmse\",\n","+                device = \"gpu\",\n","+                gpu_platform_id = 0,\n","+                gpu_device_id = 0,\n","+                nthread = 1,\n","+                boost_from_average = FALSE,\n","+                num_tree_per_iteration = 10,\n","+                max_bin = 32)\n","> model <- lgb.train(params,\n","+                    dtrain,\n","+                    2,\n","+                    valids,\n","+                    min_data = 1,\n","+                    learning_rate = 1,\n","+                    early_stopping_rounds = 10)\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 232\n","[LightGBM] [Info] Number of data: 6513, number of used features: 116\n","[LightGBM] [Info] Using GPU Device: Oland, Vendor: Advanced Micro Devices, Inc.\n","[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 12\n","[LightGBM] [Info] 40 dense feature groups (0.12 MB) transferred to GPU in 0.004211 secs. 76 sparse feature groups.\n","[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Trained a tree with leaves=16 and max_depth=8\n","[1]:    test's rmse:1.10643e-17\n","[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Trained a tree with leaves=7 and max_depth=5\n","[2]:    test's rmse:0\n","\n","Running on OpenCL CPU backend devices is in generally slow, and we observe crashes on some Windows and macOS systems. \n","Make sure you check the Using GPU Device line in the log and it is not using a CPU. The above log shows that we are using Oland GPU from AMD and not CPU.\n","\n","Example of using CPU (gpu_platform_id = 0, gpu_device_id = 1). The GPU device reported is Intel(R) Core(TM) i7-4600U CPU, \n","so it is using the CPU backend rather than a real GPU.\n","\n","> params <- list(objective = \"regression\",\n","+                metric = \"rmse\",\n","+                device = \"gpu\",\n","+                gpu_platform_id = 0,\n","+                gpu_device_id = 1,\n","+                nthread = 1,\n","+                boost_from_average = FALSE,\n","+                num_tree_per_iteration = 10,\n","+                max_bin = 32)\n","> model <- lgb.train(params,\n","+                    dtrain,\n","+                    2,\n","+                    valids,\n","+                    min_data = 1,\n","+                    learning_rate = 1,\n","+                    early_stopping_rounds = 10)\n","[LightGBM] [Info] This is the GPU trainer!!\n","[LightGBM] [Info] Total Bins 232\n","[LightGBM] [Info] Number of data: 6513, number of used features: 116\n","[LightGBM] [Info] Using requested OpenCL platform 0 device 1\n","[LightGBM] [Info] Using GPU Device: Intel(R) Core(TM) i7-4600U CPU @ 2.10GHz, Vendor: GenuineIntel\n","[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n","[LightGBM] [Info] GPU programs have been built\n","[LightGBM] [Info] Size of histogram bin entry: 12\n","[LightGBM] [Info] 40 dense feature groups (0.12 MB) transferred to GPU in 0.004540 secs. 76 sparse feature groups.\n","[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Trained a tree with leaves=16 and max_depth=8\n","[1]:    test's rmse:1.10643e-17\n","[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Trained a tree with leaves=7 and max_depth=5\n","[2]:    test's rmse:0\n","\n","Known issues:\n","Using a bad combination of gpu_platform_id and gpu_device_id can potentially lead to a crash due to OpenCL driver issues on some machines \n","(you will lose your entire session content). Beware of it.\n","****\n","**** some systems have integrated graphics card (Intel HD Graphics) and a dedicated graphics card (AMD, NVIDIA), the dedicated graphics card may \n","automatically override the integrated graphics card. The workaround is to disable your dedicated graphics card to use your integrated graphics card.\n","'''\n","nocode=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qG665uxzQ5J5"},"source":["###**Old code: LightGBM - Lightweight Gradient-Boosted Decision Tree**\n","###**Old code: SK_HGBR - SKLearn Histogram Gradient Boosting Regressor**"]},{"cell_type":"code","metadata":{"id":"_qUFDLNRQ_TC","cellView":"both"},"source":["# model_gbdt = lgb.LGBMRegressor(\n","#     objective='regression',\n","#     boosting_type='gbdt',           # gbdt= Gradient Boosting Decision Tree; dart= Dropouts meet Multiple Additive Regression Trees; goss= Gradient-based One-Side Sampling; rf= Random Forest\n","#     learning_rate=params[\"lr\"],     # You can use callbacks parameter of fit method to shrink/adapt learning rate in training using reset_parameter callback\n","#     n_estimators=params[\"maxit\"],   # Number of boosted trees to fit = max_iterations\n","#     metric='rmse',\n","#     subsample_for_bin=200000,       # Number of samples for constructing bins\n","#     num_leaves=31,                  # Maximum tree leaves for base learners\n","#     max_depth=-1,                   # Maximum tree depth for base learners, <=0 means no limit\n","#     min_split_gain=0.0,             # Minimum loss reduction required to make a further partition on a leaf node of the tree\n","#     min_child_weight=0.001,         # Minimum sum of instance weight (hessian) needed in a child (leaf)\n","#     min_child_samples=20,           # Minimum number of data needed in a child (leaf)\n","#     colsample_bytree=params[\"reg\"], # dropout fraction of columns during fitting (max=1 = no dropout)\n","#     random_state=params[\"seed\"],    # seed value\n","#     silent=False,                   # whether to print info during fitting\n","#     importance_type='split',        # feature importance type: 'split'= N times feature is used in model; 'gain'= total gains of splits which use the feature\n","#     reg_alpha=0.0,                  # L1 regularization\n","#     reg_lambda=0.0,                 # L2 regularization\n","#     n_jobs=- 1,                     # N parallel threads to use on computer\n","#     subsample=1.0,                  # row fraction used for training: keep at 1 for time series data\n","#     subsample_freq=0                # keep at 0 for time series\n","#     )\n","\n","\n","# model_gbdt.fit( \n","#     data['X_train'],                        # Input feature matrix (array-like or sparse matrix of shape = [n_samples, n_features])\n","#     data['y_train'],                        # The target values (class labels in classification, real numbers in regression) (array-like of shape = [n_samples])\n","#     eval_set=[(data['X_val'], data['y_val'])],              # can have multiple tuples of validation data inside this list\n","#     eval_names=None,                        # Names of eval_set (list of strings or None, optional (default=None))\n","#     eval_metric='rmse',                     # Default: 'l2' (= mean squared error, 'mse') for LGBMRegressor; options include 'l2_root'='root_mean_squared_error'='rmse' and 'l1'='mean_absolute_error'='mae' + more\n","#     early_stopping_rounds=params[\"estop\"],  # Activates early stopping. The model will train until the validation score stops improving. Validation score needs to improve at least every early_stopping_rounds \n","#                                             #     to continue training. Requires at least one validation data and one metric. If there’s more than one, will check all of them. But the training data is ignored anyway. \n","#                                             #     To check only the first metric, set the first_metric_only parameter to True in additional parameters **kwargs of the model constructor.\n","#     init_score=None,                        # Init score of train data\n","#     eval_init_score=None,                   # Init score of eval data (list of arrays or None, optional (default=None))\n","#     verbose=CONSTANTS[\"VERBOSITY\"] ,        # If True, metric on the eval set is printed at each boosting stage. If n=int, the metric on the eval set is printed at every nth boosting stage. Best and final also print.\n","#     feature_name='auto',                    # Feature names. If 'auto' and data is pandas DataFrame, data columns names are used. (list of strings or 'auto', optional (default='auto'))\n","#     categorical_feature='auto',             # Categorical features (list of strings or int, or 'auto', optional (default='auto')) If list of int, interpreted as indices. \n","#                                             # If list of strings, interpreted as feature names (need to specify feature_name as well). \n","#                                             # If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used. All values in categorical features should be less than int32 max value (2147483647). \n","#                                             # Large values could be memory-consuming. Consider using consecutive integers starting from zero. All negative values in categorical features are treated as missing values.\n","#     callbacks=None                          # List of callback functions that are applied at each iteration (list of callback functions or None, optional (default=None)) See Callbacks in Python API for more information.\n","#     )\n","\n","    # if mod_type == 'HGBR':\n","    #     # TTSplit should use TRAIN_FINAL = 33 (train on all data), and it will return also val=month33 for calculation at end (only)\n","    #     model_gbdt = HistGradientBoostingRegressor(\n","    #         learning_rate=LR, \n","    #         max_iter=maxiter, \n","    #         l2_regularization = reg,\n","    #         early_stopping=False, \n","    #         verbosity = verb,\n","    #         random_state=seed_val)\n","    \n","    #     tic = perf_counter()\n","    #     model_gbdt.fit(X_train_np, y_train)\n","    #     toc = perf_counter()\n","    #     model_fit_time = datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","    #     print(f\"model HGBR fit time: {model_fit_time}\")\n","    #     best_iter = maxiter\n","    #     best_val_rmse = 0\n","# model_params = {\n","        #         'objective':param_df.at[iternum,'objective'],\n","        #         'boosting_type':param_df.at[iternum,'boosting_type'],\n","        #         'learning_rate':param_df.at[iternum,'learning_rate'],\n","        #         'n_estimators':param_df.at[iternum,'n_estimators'],\n","        #         'metric':param_df.at[iternum,'metric'],\n","        #         'subsample_for_bin':param_df.at[iternum,'subsample_for_bin'],\n","        #         'num_leaves':param_df.at[iternum,'num_leaves'],\n","        #         'max_depth':param_df.at[iternum,'max_depth'],\n","        #         'min_split_gain':param_df.at[iternum,'min_split_gain'],\n","        #         'min_child_weight':param_df.at[iternum,'min_child_weight'],\n","        #         'min_child_samples':param_df.at[iternum,'min_child_samples'],\n","        #         'colsample_bytree':param_df.at[iternum,'colsample_bytree'],\n","        #         'random_state':param_df.at[iternum,'random_state'],\n","        #         'silent':param_df.at[iternum,'silent'],\n","        #         'importance_type':param_df.at[iternum,'importance_type'],\n","        #         'reg_alpha':param_df.at[iternum,'reg_alpha'],\n","        #         'reg_lambda':param_df.at[iternum,'reg_lambda'],\n","        #         'n_jobs':param_df.at[iternum,'n_jobs'],\n","        #         'subsample':param_df.at[iternum,'subsample'],\n","        #         'subsample_freq':param_df.at[iternum,'subsample_freq']\n","                # }\n","\n","        # fit_params = {\n","        #         'eval_metric':param_df.at[iternum,'eval_metric'],\n","        #         'early_stopping_rounds':param_df.at[iternum,'early_stopping_rounds'],\n","        #         'init_score':param_df.at[iternum,'init_score'],\n","        #         'eval_init_score':param_df.at[iternum,'eval_init_score'],\n","        #         'verbose':param_df.at[iternum,'verbose'],\n","        #         'feature_name':param_df.at[iternum,'feature_name'],\n","        #         'categorical_feature':param_df.at[iternum,'categorical_feature'],\n","        #         'callbacks':param_df.at[iternum,'callbacks']\n","        #         }\n","\n","        # param_df.at[iternum,\"feature_name_\"]            = model_gbdt.feature_name_\n","\n","\n","\n","\n","# # Parameters Dictionary stores everything for dumping to file later\n","# SPEC = OrderedDict()\n","# FEATURES[\"_MODEL_NAME\"] = 'LGBMv13_15ens'   # 'LGBMv10_11ens'  # Name of file model substring to save data submission to (= False if user to input it below)\n","# FEATURES[\"_MODEL_TYPE\"] = 'LGBM'  # 'HGBR'\n","# FEATURES[\"_TEST_MONTH\"] = 34\n","\n","# # Optional operations to delete irrelevant shops or item categories, and to scale sales by month length, etc.;  set to FALSE if no operation desired\n","# FEATURES[\"_EDA_DELETE_SHOPS\"]     = [9,20] #[0,1,8,9,11,13,17,20,23,27,29,30,32,33,40,43,51,54] #[8, 9, 13, 20, 23, 32, 33, 40] # [9,20] #  # False # these are online shops, untested shops, and early-termination + online shops\n","# FEATURES[\"_EDA_DELETE_ITEM_CATS\"] = [8, 10, 32, 59, 80, 81, 82]  #[1,4,8,10,13,14,17,18,32,39,46,48,50,51,52,53,59,66,68,80,81,82] #  #[8, 80, 81, 82]  # False # hokey categories, untested categories, really hokey categories\n","# FEATURES[\"_EDA_SCALE_MONTH\"]         = 'week_retail_weight'  # False # scale sales by days in month, number of each weekday, and Russian recession retail sales index\n","\n","# # columns to keep for this round of modeling (dropping some of the less important features to save memory):\n","# FEATURES[\"COLS_KEEP_ITEMS\"]             = ['item_id', 'item_group', 'item_cluster', 'item_category_id']  #, 'item_category4']\n","# FEATURES[\"COLS_KEEP_SHOPS\"]             = ['shop_id','shop_group']\n","# FEATURES[\"COLS_KEEP_DATE_SCALING\"]      = ['month', 'days_in_M', 'weekday_weight', 'retail_sales', 'week_retail_weight']\n","# FEATURES[\"COLS_KEEP_BASE_TRAIN_TEST\"]   = ['month', 'price', 'sales', 'shop_id', 'item_id']\n","\n","# # re-order columns for organized readability, for the (to be created) combined sales-train-test (stt) dataset\n","# FEATURES[\"COLS_ORDER_STT\"]        = ['month', 'sales', 'revenue', 'shop_id', 'item_id', 'shop_group', 'item_category_id', 'item_group', 'item_cluster'] #,   'revenue','item_category4','shop_group'\n","# FEATURES[\"PROVIDED_INTEGER_FEATURES\"]        = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in {'sales','price','revenue'}]  \n","# FEATURES[\"FEATURES_MONTHLY_STT_START\"]       = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in {'month','sales','price','revenue','shop_id','item_id'}]  # these are categorical features that need to be merged onto test data set\n","# FEATURES[\"PROVIDED_CATEGORICAL_FEATURES\"]    = [e for e in FEATURES[\"COLS_ORDER_STT\"] if e not in {'month','sales','price','revenue'}]\n","# FEATURES[\"_USE_CATEGORICAL\"]         = True  # pd dataframe columns \"PROVIDED_CATEGORICAL_FEATURES\" are changed to categorical dtype just before model fitting/creation\n","\n","# FEATURES[\"AGG_STATS\"] = OrderedDict()\n","# FEATURES[\"AGG_STATS\"][\"sales\"]     = ['sum', 'median', 'count']\n","# FEATURES[\"AGG_STATS\"][\"revenue\"]   = ['sum']  # revenue can handle fillna(0) cartesian product; price doesn't make sense with fillna(0), so don't use that at this time\n","# #FEATURES[\"AGG_STATS\"][\"price\"]     = ['median','std']\n","\n","# # aggregate statistics columns (initial computation shall be 'sales per month' prediction target for shop_id-item_id pair grouping)\n","# FEATURES[\"STATS_FEATURES\"] = [['shop_id', 'item_id'], ['shop_id', 'item_category_id'], ['shop_id', 'item_cluster']] + FEATURES[\"PROVIDED_CATEGORICAL_FEATURES\"]\n","\n","# FEATURES[\"LAGS_MONTHS\"] = [1,2,3,4,5,6,7,8]  # month lags to include in model \n","# FEATURES[\"LAG_FEATURES\"] = {}\n","# for i in FEATURES[\"LAGS_MONTHS\"]:\n","#     FEATURES[\"LAG_FEATURES\"][i] = ['y_sales', 'shop_id_x_item_category_id_sales_sum', 'item_id_sales_sum', 'item_cluster_sales_sum'] \n","# FEATURES[\"LAG_FEATURES\"][1] = ['y_sales', 'shop_id_x_item_id_sales_median', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', \n","#                      'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_median', 'shop_id_x_item_category_id_sales_count', \n","#                      'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_median', \n","#                      'shop_id_sales_sum', 'shop_id_sales_count', \n","#                      'item_id_sales_sum', 'item_id_sales_median', 'item_id_sales_count', 'item_id_revenue_sum', \n","#                      'shop_group_revenue_sum', \n","#                      'item_category_id_sales_sum', 'item_category_id_sales_count', 'item_category_id_revenue_sum', \n","#                      'item_group_sales_sum', 'item_group_revenue_sum', \n","#                      'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","\n","# FEATURES[\"LAG_FEATURES\"][2] = ['y_sales', 'shop_id_x_item_id_sales_count', 'shop_id_x_item_id_revenue_sum', \n","#                      'shop_id_x_item_category_id_sales_sum', 'shop_id_x_item_category_id_sales_count', 'shop_id_x_item_category_id_revenue_sum', \n","#                      'shop_id_x_item_cluster_sales_sum', 'shop_id_x_item_cluster_sales_count', \n","#                      'shop_id_sales_sum', 'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', \n","#                      'item_category_id_sales_sum', 'item_category_id_sales_count', \n","#                      'item_group_sales_sum', \n","#                      'item_cluster_sales_sum', 'item_cluster_sales_count', 'item_cluster_revenue_sum']\n","\n","# FEATURES[\"LAG_FEATURES\"][3] = ['y_sales', 'shop_id_x_item_id_sales_count', \n","#                      'shop_id_x_item_category_id_sales_sum', \n","#                      'shop_id_sales_sum', \n","#                      'item_id_sales_sum', 'item_id_sales_count', 'item_id_revenue_sum', \n","#                      'item_category_id_sales_sum', 'item_category_id_sales_count', \n","#                      'item_cluster_sales_sum', 'item_cluster_sales_count']\n","\n","# # keep at least the highest importance feature for each lag, but remove all others with < 20% importance (month 13-32 training)\n","# FEATURES[\"LAG_FEATURES\"][2] = [e for e in FEATURES[\"LAG_FEATURES\"][2] if e not in {'item_group_sales_sum','shop_id_x_item_category_id_sales_sum','shop_id_x_item_cluster_sales_sum','shop_id_x_item_cluster_sales_count'}]\n","# FEATURES[\"LAG_FEATURES\"][3] = [e for e in FEATURES[\"LAG_FEATURES\"][3] if e not in {'item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum','shop_id_x_item_id_sales_count'}]\n","# FEATURES[\"LAG_FEATURES\"][4] = [e for e in FEATURES[\"LAG_FEATURES\"][4] if e not in {'shop_id_x_item_category_id_sales_sum','y_sales','item_cluster_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][5] = [e for e in FEATURES[\"LAG_FEATURES\"][5] if e not in {'item_cluster_sales_sum','item_id_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][6] = [e for e in FEATURES[\"LAG_FEATURES\"][6] if e not in {'item_id_sales_sum','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][7] = [e for e in FEATURES[\"LAG_FEATURES\"][7] if e not in {'y_sales','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","# FEATURES[\"LAG_FEATURES\"][8] = [e for e in FEATURES[\"LAG_FEATURES\"][8] if e not in {'item_id_sales_sum','item_cluster_sales_sum','shop_id_x_item_category_id_sales_sum'}]\n","\n","# # LAG_STATS_SET is SET of all aggregate statistics columns for all lags (allows us to shed the other stats, keeping memory requirements low)\n","# LAG_STATS_SET = FEATURES[\"LAG_FEATURES\"][1]\n","# for l in FEATURES[\"LAGS_MONTHS\"][1:]:\n","#     LAG_STATS_SET = LAG_STATS_SET + [x for x in FEATURES[\"LAG_FEATURES\"][l] if x not in LAG_STATS_SET]\n","# FEATURES[\"STT_MONTHLY_COLS\"] = FEATURES[\"PROVIDED_INTEGER_FEATURES\"] + LAG_STATS_SET\n","\n","# # Define various constants that drive the attributes of the various features\n","# FEATURES[\"_CLIP_TRAIN_H\"]   = 20          # this clips sales after doing monthly groupings (monthly_stt dataframe) will also clip item_cnt_month predictions to 20 after the model runs\n","# FEATURES[\"_CLIP_TRAIN_L\"]   = 0                   \n","# FEATURES[\"_CLIP_PREDICT_H\"] = 20          # this clips the final result before submission to coursera\n","# FEATURES[\"_CLIP_PREDICT_L\"] = 0    \n","\n","# FEATURES[\"_USE_ROBUST_SCALER\"]         = True        # scale features to reduce influence of outliers\n","# FEATURES[\"_ROBUST_SCALER_QUANTILES\"]   = (20,80)\n","# FEATURES[\"_USE_MINMAX_SCALER\"]         = True        # scale features to use large range of np.int16\n","# FEATURES[\"_MINMAX_SCALER_RANGE\"]       = (0,16000)   # int16 = (0,32700); uint16 = (0,65500)  --> keep this range positive for best results with LGBM; keep range smaller for faster LGBM fitting\n","# FEATURES[\"_FEATURE_DATA_TYPE\"]         = np.int16    # np.float32 #np.int16   np.uint16          # if fill n/a = 0, can adjust feature values to be integer values and save memory (not finding that int can store np.NAN)\n","# FEATURES[\"_USE_CARTPROD_FILL\"]         = True        # use cartesian fill, or not\n","# FEATURES[\"_CARTPROD_TEST_PAIRS\"]  = False       # include all shop-item pairings from test month as well as the in-month pairings\n","# FEATURES[\"_CARTPROD_FILLNA0\"]    = True        # fill n/a cartesian additions with zeros (not good for price-based stats, however)\n","# FEATURES[\"_CARTPROD_FIRST_MONTH\"] = 13          # month number + max lag to start adding Cartesian product rows (i.e., maxlag=6mo and CARTPROD_FILL_MONTH_BEGIN=10 will cartesian fill from 4 to 33)\n","# FEATURES[\"TRAIN_MONTH_START\"]         = [13]        # == 24 ==> less than a year of data, but avoids December 'outlier' of 2014\n","# FEATURES[\"TRAIN_MONTH_END\"]           = [29]        # [29,32] #,30,32]\n","# FEATURES[\"N_VAL_MONTHS\"]              = [False]     #1 # ; if false, val is all months after training, up to and including 33; otherwise val is this many months after train_month_end\n","\n","# # Define hyperparameters for modeling\n","# FEATURES[\"LEARNING_RATE\"]       = [0.05]  # default = 0.1\n","# FEATURES[\"MAX_ITERATIONS\"]      = [200] # default = 100\n","# FEATURES[\"EARLY_STOPPING\"]      = [20]\n","# FEATURES[\"REGULARIZATION\"]      = [0.4] # default = 1 for LGBM, 0 for HGBR (these models use inverse forms of regularization)\n","# FEATURES[\"VERBOSITY\"]           = True #4 four is to print every 4th iteration; True is every iteration; False is no print except best and last\n","# FEATURES[\"SEED_VALUES\"]         = [42]\n","\n","# FEATURES[\"ALL_exploded_shape[0]\"] = (len(FEATURES[\"SEED_VALUES\"])*len(FEATURES[\"N_VAL_MONTHS\"])*len(FEATURES[\"TRAIN_MONTH_END\"])*len(FEATURES[\"TRAIN_MONTH_START\"])*\n","#                          len(FEATURES[\"EARLY_STOPPING\"])*len(FEATURES[\"MAX_ITERATIONS\"])*len(FEATURES[\"REGULARIZATION\"])*len(FEATURES[\"LEARNING_RATE\"]) )\n","\n","\n","# print(f'Done: {strftime(\"%a %X %x\")}')\n","\n","\n","\n","\n","\n","\n","\n","\n","# def unscale(scaler,target):\n","#     return scaler.inverse_transform(target.reshape(-1, 1)).squeeze()\n","\n","# def GBDT_model(data=df, CONSTANTS=SPEC, params=OrderedDict()):\n","#     \"\"\"\n","#     data is entire dataframe with train, validation, and test rows, and all columns including target prediction at \"y_target\"\n","#     constants is dictionary of setup constants\n","#     params is dictionary of this particular model train/val split and model fitting/prediction parameters\n","#     \"\"\"\n","#     results = OrderedDict()\n","#     if CONSTANTS[\"_MODEL_TYPE\"] == 'LGBM':\n","        \n","#         train_start = params[\"train_start_mo\"]\n","#         train_end   = params[\"train_final_mo\"]\n","#         val_months  = params[\"val_mo\"]\n","#         test_month  = CONSTANTS[\"TEST_MONTH\"]\n","\n","#         train   = data.query('(month >= @train_start) & (month <= @train_end)')\n","#         y_train = train['y_target'].astype(np.float32)\n","#         y_train = y_train.reset_index(drop=True)\n","#         X_train = train.drop(['y_target'], axis=1)\n","#         X_train = X_train.reset_index(drop=True)\n","#         feature_names = X_train.columns\n","\n","#         if val_months:\n","#             val = data.query('(month > (@train_end)) & (month <= (@train_end + @val_months)) & (month < @test_month)')\n","#         else:\n","#             val = data.query('((month > (@train_end)) & (month < @test_month)) | (month == (@test_month-1))')\n","#         y_val = val['y_target'].astype(np.float32)\n","#         y_val = y_val.reset_index(drop=True)\n","#         X_val = val.drop(['y_target'], axis=1)\n","#         X_val = X_val.reset_index(drop=True)\n","\n","#         X_test = data.query('month == @test_month').drop(['y_target'], axis=1)\n","#         X_test = X_test.reset_index(drop=True)\n","\n","#         print('X_train:')\n","#         print_col_info(X_train,8)\n","#         print(f'\\n{X_train.head(2)}\\n\\n')\n","#         print('X_val:')\n","#         print_col_info(X_val,8)\n","#         print(f'\\n{X_val.head(2)}\\n\\n')\n","#         print('X_test:')\n","#         print_col_info(X_test,8)\n","#         print(f'\\n{X_test.head(2)}\\n\\n')\n","#         data_types = X_train.dtypes\n","\n","#         del [[data, train, val]]\n","\n","#         print('Starting training...')\n","#         model_gbdt = lgb.LGBMRegressor(\n","#             objective='regression',\n","#             boosting_type='gbdt',           # gbdt= Gradient Boosting Decision Tree; dart= Dropouts meet Multiple Additive Regression Trees; goss= Gradient-based One-Side Sampling; rf= Random Forest\n","#             learning_rate=params[\"lr\"],     # You can use callbacks parameter of fit method to shrink/adapt learning rate in training using reset_parameter callback\n","#             n_estimators=params[\"maxit\"],   # Number of boosted trees to fit = max_iterations\n","#             metric='rmse',\n","#             subsample_for_bin=200000,       # Number of samples for constructing bins\n","#             num_leaves=31,                  # Maximum tree leaves for base learners\n","#             max_depth=-1,                   # Maximum tree depth for base learners, <=0 means no limit\n","#             min_split_gain=0.0,             # Minimum loss reduction required to make a further partition on a leaf node of the tree\n","#             min_child_weight=0.001,         # Minimum sum of instance weight (hessian) needed in a child (leaf)\n","#             min_child_samples=20,           # Minimum number of data needed in a child (leaf)\n","#             colsample_bytree=params[\"reg\"], # dropout fraction of columns during fitting (max=1 = no dropout)\n","#             random_state=params[\"seed\"],    # seed value\n","#             silent=False,                   # whether to print info during fitting\n","#             importance_type='split',        # feature importance type: 'split'= N times feature is used in model; 'gain'= total gains of splits which use the feature\n","#             reg_alpha=0.0,                  # L1 regularization\n","#             reg_lambda=0.0,                 # L2 regularization\n","#             n_jobs=- 1,                     # N parallel threads to use on computer\n","#             subsample=1.0,                  # row fraction used for training: keep at 1 for time series data\n","#             subsample_freq=0,               # keep at 0 for time series\n","#             )\n","\n","#         tic = perf_counter()\n","#         model_gbdt.fit( \n","#             X_train,                                # Input feature matrix (array-like or sparse matrix of shape = [n_samples, n_features])\n","#             y_train,                                # The target values (class labels in classification, real numbers in regression) (array-like of shape = [n_samples])\n","#             eval_set=[(X_val, y_val)],              # can have multiple tuples of validation data inside this list\n","#             eval_names=None,                        # Names of eval_set (list of strings or None, optional (default=None))\n","#             eval_metric='rmse',                     # Default: 'l2' (= mean squared error, 'mse') for LGBMRegressor; options include 'l2_root'='root_mean_squared_error'='rmse' and 'l1'='mean_absolute_error'='mae' + more\n","#             early_stopping_rounds=params[\"estop\"],  # Activates early stopping. The model will train until the validation score stops improving. Validation score needs to improve at least every early_stopping_rounds \n","#                                                     #     to continue training. Requires at least one validation data and one metric. If there’s more than one, will check all of them. But the training data is ignored anyway. \n","#                                                     #     To check only the first metric, set the first_metric_only parameter to True in additional parameters **kwargs of the model constructor.\n","#             init_score=None,                        # Init score of train data\n","#             eval_init_score=None,                   # Init score of eval data (list of arrays or None, optional (default=None))\n","#             verbose=CONSTANTS[\"VERBOSITY\"] ,        # If True, metric on the eval set is printed at each boosting stage. If n=int, the metric on the eval set is printed at every nth boosting stage. Best and final also print.\n","#             feature_name='auto',                    # Feature names. If 'auto' and data is pandas DataFrame, data columns names are used. (list of strings or 'auto', optional (default='auto'))\n","#             categorical_feature='auto',             # Categorical features (list of strings or int, or 'auto', optional (default='auto')) If list of int, interpreted as indices. \n","#                                                     # If list of strings, interpreted as feature names (need to specify feature_name as well). \n","#                                                     # If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used. All values in categorical features should be less than int32 max value (2147483647). \n","#                                                     # Large values could be memory-consuming. Consider using consecutive integers starting from zero. All negative values in categorical features are treated as missing values.\n","#             callbacks=None                          # List of callback functions that are applied at each iteration (list of callback functions or None, optional (default=None)) See Callbacks in Python API for more information.\n","#             )\n","\n","#         toc = perf_counter()\n","#         results[\"model_fit_time\"] = datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","#         print(f'Model LGBM fit time: {results[\"model_fit_time\"]}')\n","#         results[\"best_iter\"] = model_gbdt.best_iteration_\n","#         results[\"best_val_rmse\"] = 0 #best_score\n","\n","\n","#     # if mod_type == 'HGBR':\n","#     #     # TTSplit should use TRAIN_FINAL = 33 (train on all data), and it will return also val=month33 for calculation at end (only)\n","#     #     model_gbdt = HistGradientBoostingRegressor(\n","#     #         learning_rate=LR, \n","#     #         max_iter=maxiter, \n","#     #         l2_regularization = reg,\n","#     #         early_stopping=False, \n","#     #         verbosity = verb,\n","#     #         random_state=seed_val)\n","    \n","#     #     tic = perf_counter()\n","#     #     model_gbdt.fit(X_train_np, y_train)\n","#     #     toc = perf_counter()\n","#     #     model_fit_time = datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","#     #     print(f\"model HGBR fit time: {model_fit_time}\")\n","#     #     best_iter = maxiter\n","#     #     best_val_rmse = 0\n","        \n","#     print(\"Starting predictions...\")\n","#     tic = perf_counter()\n","#     y_pred_train =  model_gbdt.predict( X_train, num_iteration=model_gbdt.best_iteration_ )\n","#     y_pred_val =    model_gbdt.predict( X_val,   num_iteration=model_gbdt.best_iteration_ )\n","#     y_pred_test =   model_gbdt.predict( X_test,  num_iteration=model_gbdt.best_iteration_ )\n","#     y_train =       y_train.to_numpy()\n","#     y_val =         y_val.to_numpy()\n","#     # always do minmax scaling after robust scaling; and do inverse scaling with minmax first, then robust\n","#     if CONSTANTS[\"_USE_MINMAX_SCALER\"]:\n","#         y_pred_train =  unscale(minmax_scalers['y_sales'],  y_pred_train)\n","#         y_pred_val =    unscale(minmax_scalers['y_sales'],  y_pred_val)\n","#         y_pred_test =   unscale(minmax_scalers['y_sales'],  y_pred_test)\n","#         y_train =       unscale(minmax_scalers['y_sales'],  y_train)\n","#         y_val =         unscale(minmax_scalers['y_sales'],  y_val)\n","#     if CONSTANTS[\"_USE_ROBUST_SCALER\"]:\n","#         y_pred_train =  unscale(robust_scalers['y_sales'],  y_pred_train)\n","#         y_pred_val =    unscale(robust_scalers['y_sales'],  y_pred_val)\n","#         y_pred_test =   unscale(robust_scalers['y_sales'],  y_pred_test)\n","#         y_train =       unscale(robust_scalers['y_sales'],  y_train)\n","#         y_val =         unscale(robust_scalers['y_sales'],  y_val)\n","#     y_pred_train =  y_pred_train.clip(CONSTANTS[\"_CLIP_PREDICT_L\"], CONSTANTS[\"_CLIP_PREDICT_H\"])\n","#     y_pred_val =    y_pred_val.clip(  CONSTANTS[\"_CLIP_PREDICT_L\"], CONSTANTS[\"_CLIP_PREDICT_H\"])\n","#     y_pred_test =   y_pred_test.clip( CONSTANTS[\"_CLIP_PREDICT_L\"], CONSTANTS[\"_CLIP_PREDICT_H\"]) \n","#     toc = perf_counter()\n","#     results[\"predict_time\"] = datetime.utcfromtimestamp(toc-tic).strftime('%H:%M:%S')\n","#     print(f'Transform and Predict train/val/test time: {results[\"predict_time\"]}')\n","\n","#     results[\"train_r2\"],   results[\"val_r2\"]    = sk_r2(y_train, y_pred_train),            sk_r2(y_val, y_pred_val)\n","#     results[\"train_rmse\"], results[\"val_rmse\"]  = np.sqrt(sk_mse(y_train, y_pred_train)),  np.sqrt(sk_mse(y_val, y_pred_val))\n","#     print(f'R^2 train  = {results[\"train_r2\"]:.4f}      R^2 val  = {results[\"val_r2\"]:.4f}')\n","#     print(f'RMSE train = {results[\"train_rmse\"]:.4f}    RMSE val = {results[\"val_rmse\"]:.4f}\\n')\n","\n","#     return model_gbdt, model_gbdt.get_params(), X_test, y_pred_test, feature_names, data_types, results\n","\n","# print(f'Done: {strftime(\"%a %X %x\")}')\n","\n","\n","\n","\n","\n","# ensemble_feature_names = []\n","# ensemble_y_pred_test = []\n","# ensemble_df_columns = ['lr', 'reg', 'max_iter', 'estop', 'start', 'end', 'n_val_mo', 'seed', 'trR2', 'valR2', 'tr_rmse', 'val_rmse', 'best_iter', 'best_val_rmse', 'model_time','predict_time','total_time']\n","# ensemble_df_rows = []\n","# model_params = OrderedDict()\n","# itercount = 0\n","# for lr in FEATURES[\"LEARNING_RATE\"]:\n","#     for reg in FEATURES[\"REGULARIZATION\"]:\n","#         for maxit in FEATURES[\"MAX_ITERATIONS\"]:\n","#             for estop in FEATURES[\"EARLY_STOPPING\"]:\n","#                 for train_start_mo in FEATURES[\"TRAIN_MONTH_START\"]:\n","#                     for train_final_mo in FEATURES[\"TRAIN_MONTH_END\"]:\n","#                         for val_mo in FEATURES[\"N_VAL_MONTHS\"]:\n","#                             for seed in FEATURES[\"SEED_VALUES\"]:\n","#                                 itercount += 1\n","#                                 print(f'\\n\\nBelow: Model {itercount} of {FEATURES[\"ALL_exploded_shape[0]\"]}: LR = {lr}; LFF = {reg}, train_start = {train_start_mo}; train_end = {train_final_mo}; seed = {seed}\\n')\n","#                                 time0 = time.time()\n","#                                 model_params[\"lr\"] = lr\n","#                                 model_params['reg'] = reg\n","#                                 model_params['maxit'] = maxit\n","#                                 model_params['estop'] = estop\n","#                                 model_params['train_start_mo'] = train_start_mo\n","#                                 model_params['train_final_mo'] = train_final_mo\n","#                                 model_params['val_mo'] = val_mo\n","#                                 model_params['seed'] = seed\n","#                                 ##model_fit, y_pred_test, train_r2, val_r2, train_rmse, val_rmse, best_iter, best_val_rmse, model_fit_time, predict_time = \n","#                                 model_fit, model_params, X_test, y_pred_test, feature_names, data_types, results = GBDT_model(df, SPEC, model_params)\n","#                                 time2 = time.time(); model_time = datetime.utcfromtimestamp(time2 - time0).strftime('%H:%M:%S')\n","\n","#                                 ensemble_feature_names.append(feature_names)\n","#                                 ensemble_y_pred_test.append(y_pred_test)\n","#                                 ##ensemble_df_rows.append([lr,reg,maxit,estop,train_start_mo,train_final_mo,val_mo,seed,train_r2,val_r2,train_rmse,val_rmse,best_iter,best_val_rmse,model_fit_time,predict_time,model_time])\n","\n","#                                 # intermediate save after each model fit set of parameters, in case of crash or disconnect from Colab\n","#                                 # Simple ensemble averaging\n","#                                 y_test_pred_avg = np.mean(ensemble_y_pred_test, axis=0)\n","#                                 # Merge the test predictions with IDs from the original test dataset, and keep only columns \"ID\" and \"item_cnt_month\"\n","#                                 y_submission = pd.DataFrame.from_dict({'item_cnt_month':y_test_pred_avg,'shop_id':X_test.shop_id,'item_id':X_test.item_id})\n","#                                 y_submission = test.merge(y_submission, on=['shop_id','item_id'], how= 'left').reset_index(drop=True).drop(['shop_id','item_id'],axis=1)\n","#                                 y_submission.to_csv(\"./models_and_predictions/\" + FEATURES[\"_MODEL_NAME\"] + '_submission.csv', index=False)\n","#                                 ##ensemble_scores = pd.DataFrame(ensemble_df_rows, columns = ensemble_df_columns)\n","#                                 ##ensemble_scores.to_csv(\"./models_and_predictions/\" + model_filename_ens, index=False)\n","#                                 time3 = time.time(); iteration_time = datetime.utcfromtimestamp(time3 - time0).strftime('%H:%M:%S')\n","#                                 #print(f'TTSplit Execution Time = {ttsplit_time};  \n","#                                 print(f'Model fit/predict Execution Time = {model_time};  Total Iteration Execution Time = {iteration_time}')\n","#                                 print(f'Below: Model {itercount} of {FEATURES[\"ALL_exploded_shape[0]\"]}: LR = {lr}; LFF = {reg}, train_start = {train_start_mo}; train_end = {train_final_mo}; seed = {seed}\\n')\n","# print(model_params)\n","# print(feature_names)\n","# print(data_types)\n","# print(results)\n","# #display(ensemble_scores)\n","\n","# print(f'\\nDone: {strftime(\"%a %X %x\")}\\n')\n","\n","nocode=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HuOKMYlern3c"},"source":["##**Random Stuff**"]},{"cell_type":"markdown","metadata":{"id":"FPzNWMRAyx6M"},"source":["###**K-Fold Training Splits; Ensemble Average; Save Intermediate Results**"]},{"cell_type":"code","metadata":{"id":"D2tNdYiiNa7x","cellView":"both"},"source":["\n","%cd \"{GDRIVE_REPO_PATH}\"\n","\n","ensemble_y_pred_test = []\n","ensemble_df_columns = ['tr_rmse','val_rmse','trR2','valR2','lr','reg','max_iter','estop','bin_sample','start','end','val_key','seed','best_iter','best_val_rmse','model_t','predict_t','total_t']\n","ensemble_df_rows = []\n","\n","\n","    if not ITERS.at[itercount,\"_model_filename\"]:\n","        ITERS.at[itercount,\"_model_filename\"] = input(\"Enter the Base Model Name Substring for Output File Naming (like: 'v4mg_01' )\")\n","    filename_parameters = ITERS.at[itercount,\"_model_type\"] + ITERS.at[itercount,\"_model_filename\"] + \"_params.csv\"\n","    filename_submission = ITERS.at[itercount,\"_model_type\"] + ITERS.at[itercount,\"_model_filename\"] + '_submission.csv'\n","\n","    print(f'\\n\\nBelow: Model {itercount+1} of {len(ITERS)}: lr= {ITERS.at[itercount,\"learning_rate\"]}; Reg= {ITERS.at[itercount,\"colsample_bytree\"]}, ',end='')\n","    print(f'train_start = {ITERS.at[itercount,\"_train_start_month\"]}; train_end = {ITERS.at[itercount,\"_train_final_month\"]}; seed = {ITERS.at[itercount,\"random_state\"]}\\n')\n","    \n","    time0 = time.time()\n","    # CHANGE --> only redo this inside the loop if the months change\n","    DataSets, ITERS.at[itercount,\"feature_name_\"] = TTSplit(data=df, params=ITERS, iternum=itercount)\n","\n","    y_pred_test, results_dict, parameters_of_model = GBDT_model(DataSets, model_params_dict, fit_params_dict) #ITERS, itercount)\n","    time1 = time.time()\n","    \n","\n","    ITERS.at[itercount,\"time_predict_end\"] = datetime.utcfromtimestamp(time1 - time0).strftime('%H:%M:%S')\n","    print(f'Total Iteration Execution Time = {ITERS.at[itercount,\"time_predict_end\"]}')\n","\n","    # intermediate save after each model fit set of parameters, in case of crash or disconnect from Colab\n","    # Simple ensemble averaging\n","    ensemble_y_pred_test.append(y_pred_test)\n","    y_test_pred_avg = np.mean(ensemble_y_pred_test, axis=0)\n","    # Merge the test predictions with IDs from the original test dataset, and keep only columns \"ID\" and \"item_cnt_month\"\n","    y_submission = pd.DataFrame.from_dict({'item_cnt_month':y_test_pred_avg,'shop_id':DataSets['X_test'].shop_id,'item_id':DataSets['X_test'].item_id})\n","    y_submission = test.merge(y_submission, on=['shop_id','item_id'], how= 'left').reset_index(drop=True).drop(['shop_id','item_id'],axis=1)\n","    y_submission.to_csv(\"./models_and_predictions/\" + filename_submission, index=False)\n","\n","    ITERS.to_csv(\"./models_and_predictions/\" + filename_parameters, index=False)\n","\n","    ensemble_df_rows.append(ITERS[['tr_rmse','val_rmse','tr_R2','val_R2','learning_rate','colsample_bytree','n_estimators','early_stopping_rounds','subsample_for_bin','_train_start_month','_train_final_month',\n","                                          '_validate_months','random_state','best_iteration_','best_score_','time_model_fit','time_model_predict','time_predict_end']].iloc[itercount].to_list())\n","    ensemble_scores = pd.DataFrame(ensemble_df_rows, columns = ensemble_df_columns)\n","\n","    print(f'\\nModel {itercount+1} of {len(ITERS)}: lr= {ITERS.at[itercount,\"learning_rate\"]}; Reg= {ITERS.at[itercount,\"colsample_bytree\"]}, ',end='')\n","    print(f'train_start = {ITERS.at[itercount,\"_train_start_month\"]}; train_end = {ITERS.at[itercount,\"_train_final_month\"]}; seed = {ITERS.at[itercount,\"random_state\"]}\\n')\n","    display(ensemble_scores)\n","    \n","    itercount += 1\n","\n","print(f'\\nDone: {strftime(\"%a %X %x\")}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4U1U6nZy8wd"},"source":["###**Document Results**"]},{"cell_type":"code","metadata":{"id":"RhUA6AyHvjf3","cellView":"both"},"source":["# Printout for copy-paste version control\n","\n","print('\\n------------------------------------------\\n------------------------------------------')\n","print(f'{FEATURES[\"_MODEL_NAME\"]}  Model Type: {FEATURES[\"_MODEL_TYPE\"]}\\nCoursera: \\n------------------------------------------')\n","display_params()\n","print('------')\n","print(ensemble_scores)\n","print('------')\n","print(ensemble_scores.describe(percentiles=[], include=np.number))\n","print(f'------\\nHighest and Lowest Feature Importance for Final Model:\\n{fi.iloc[list(range(0,8))+list(range(-7,0)),:]}\\n------')\n","print(y_submission.head(8))\n","print('------------------------------------------\\n\\n')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c98tKblpRWya"},"source":["###**Record Results**"]}]}