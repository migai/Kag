{"cells":[{"metadata":{"_uuid":"4a19893507bb9423333646de4751bb7d0bbffc71"},"cell_type":"markdown","source":"WORK IN PROGRESS\n\nThis is the continuation of my work on EDA which can be found **[here](https://www.kaggle.com/dennise/coursera-competition-getting-started-eda/edit ).**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# Kaggle internals\nimport os\nprint(os.listdir(\"../input\"))\n\n# Libraries\nfrom math import sqrt\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\n# Data\ntest=pd.read_csv('../input/competitive-data-science-final-project/test.csv.gz',compression='gzip')\nsales_train=pd.read_csv(\"../input/coursera-competition-getting-started-eda/sales_train.csv\")               # From my EDA Kernel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3283e49068e7bd32ab8203914827e336b090b6fe"},"cell_type":"code","source":"# Data cleaning\n# I need to have clean data that I can use to train \n\n# No NaNs in the data\n# Shops are cleaned up\n# Should I do something with negative item counts? Not for now, I think its ok.\n# Probably cleaning the outliers - the B2B deal values could be a good idea. But lets leave it for now.\n# I have to aggregate the figures to monthly values, right?\n# Construct a feature into monthly aggregated figures with \"working days\"\n\n# The value to be found (y) is \"item_cnt_month\"\n# IDs are categorical features. Do I have to encode item_ids eg one_hot? Huge and sparse!\n\"\"\"\ndate: We dont need\ndate_block_num: keep \nshop_id: keep\nitem_id: keep\nitem_price: keep and construct groups\nday: delete\nmonth: keep (for seasonality)\nyear: delete\nweekday: delete\nitem_category_id: keep\nitems_english: delete\nmeta_category: keep\nshops_english: delete\ntown: keep\nregion: keep\nrevenue: delete\nvalue: delete\n\"\"\"\n\n# Aggregate to monthly counts\ndf=pd.DataFrame()\ndf=sales_train.groupby([\"date_block_num\",\"shop_id\",\"item_id\",\"month\",\"item_price\",\"item_category_id\",\"meta_category\",\"town\",\"region\"],as_index=False).sum()[[\"date_block_num\",\"shop_id\",\"item_id\",\"month\",\"item_price\",\"item_category_id\",\"meta_category\",\"town\",\"region\",\"item_cnt_day\"]]\ndf[\"item_cnt_day\"].clip(0,20,inplace=True)\ndf=df.rename(columns = {'item_cnt_day':'item_cnt_month'})\n# unravel\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"672d10515bdb0e5bc0cb3ec8b3c13a8773649e6b","scrolled":true},"cell_type":"code","source":"# Feature selection\n# Features are the input variables to the model\n\"\"\"\ndate_block_num: numerical \nshop_id: categorical\nitem_id: categorical\nitem_price: numerical\nmonth: categorical\nitem_category_id: categorical\nmeta_category: categorical\ntown: categorical\nregion: categorical\n\"\"\"\n\n# Create price categories\ndf[\"price_category\"]=np.nan\ndf[\"price_category\"][(df[\"item_price\"]>=0)&(df[\"item_price\"]<=100)]=0\ndf[\"price_category\"][(df[\"item_price\"]>100)&(df[\"item_price\"]<=200)]=1\ndf[\"price_category\"][(df[\"item_price\"]>200)&(df[\"item_price\"]<=400)]=2\ndf[\"price_category\"][(df[\"item_price\"]>400)&(df[\"item_price\"]<=750)]=3\ndf[\"price_category\"][(df[\"item_price\"]>750)&(df[\"item_price\"]<=1000)]=4\ndf[\"price_category\"][(df[\"item_price\"]>1000)&(df[\"item_price\"]<=2000)]=5\ndf[\"price_category\"][(df[\"item_price\"]>2000)&(df[\"item_price\"]<=3000)]=6\ndf[\"price_category\"][df[\"item_price\"]>3000]=7\n\nsns.countplot(df.price_category)\n\n# Label encode meta_category, town and region\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nle.fit(df.meta_category)\ndf[\"meta_category\"]=le.transform(df.meta_category)\n\nle.fit(df.town)\ndf[\"town\"]=le.transform(df.town)\n\nle.fit(df.region)\ndf[\"region\"]=le.transform(df.region)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7ad15e163581fe28be02af1931395e86b818f4d"},"cell_type":"markdown","source":"Let's get a first model, a linear model, to run to better understand what am I actually modelling\nBased on some [former work ]( https://www.kaggle.com/dennise/sklearn-runthrough-everybody-need-a-titanic-kernel)of mine "},{"metadata":{"trusted":true,"_uuid":"29700a62d5e9f0a1dfe1ca6200574efbdb588f53"},"cell_type":"code","source":"X_train=df.drop(\"item_cnt_month\", axis=1)\n# Reason for dropping item_price explained below\ny_train=df[\"item_cnt_month\"]\n\nX_train.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b13aa68b760249303388afd5f590dd9513a4998"},"cell_type":"code","source":"linmodel=LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cebeeabf2f167aa65fdc4f92b918b1688397ddeb"},"cell_type":"code","source":"linmodel.fit(X_train,y_train)\n\npredictions=linmodel.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e04f31da4dd98221262066a184451ed4b37e7000"},"cell_type":"code","source":"print(sqrt(mean_squared_error(y_train,predictions)))\n# Dropping item_price didnt result in a lower RMSE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36e1a1f1ee1da48889dbb5d8def2ed07660d5fa3"},"cell_type":"code","source":"linmodel.score(X_train, y_train)\n# R^2 of 1,6%?! wow.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3712831605867e1923016d2a3cb0233339135f7"},"cell_type":"code","source":"linmodel.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa6c949c0d5e515d2f217634ed9c87fc8baee46b"},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e819cc0b19e60d729220fcb0c9178c4e01eed43"},"cell_type":"markdown","source":"Observations from our very first linear model:\n* The later in time (date_block_num) is higher, the smaller sold amoung (Makes sense as EDA showed declining volumes over time\n* item_id also negative. As item_ids are increasing same reason applies for lower and lower volumes\n* month is positively correlated. makes sense as Year-End / Christmas business has high volumes\n* price_category is positively correlated. The higher the category, the higher the volume!? That doesnt seem right\n* Same for price. Higher price indicates higher sales!?"},{"metadata":{"trusted":true,"_uuid":"baf7129997216fbb445f3fe3afbd091838ce56fd"},"cell_type":"code","source":"# Apply model to test-data\n\n# At first I have to \"blow-up\" the test data to have some features\n\n# test2=test.merge(X_train[[\"item_id\",\"shop_id\",\"item_category_id\",\"meta_category\",\"town\",\"region\",\"price_category\"]])\n# Here several things went bad:\n# Merge ignored when e.g. there was no item 5320 in shop 5 and ignored these lines\n# If shop,item combinations appeared more than once all lines were added, therefore blowing up the test set\n\n# Following carefully instructions from documentation:\n# https://pandas.pydata.org/pandas-docs/stable/merging.html\n\n# And do it step by step. I found out that nearly 40% of item-shop-id pairs are new, therefore lots of nans\n# after merge for eg price if i merge on these two items.\n# but town, region, category etc depends only on one of them\n\n# Start with adding shop-related values\ntest2 = pd.merge(test,X_train[[\"shop_id\",\"town\",\"region\"]].drop_duplicates(), how=\"left\",on=[\"shop_id\"])\n\n# Continue with item-related values\ntest3 = pd.merge(test2,X_train[[\"item_id\",\"item_category_id\",\"meta_category\"]].drop_duplicates(), how=\"left\",on=[\"item_id\"])\n\n# And now the tricky part: The price\n# For existing shop-item pairs probably smart to use the latest price point\n# As there are different steps to fill the price column I do it in single pieces and later join all methods back together\npart1 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==33].drop_duplicates().groupby([\"item_id\",\"shop_id\"],as_index=False).max(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\npart2 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==32].drop_duplicates(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\npart3 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==31].drop_duplicates(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\npart4 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==30].drop_duplicates(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\npart5 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==29].drop_duplicates(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\npart6 = pd.merge(test3,X_train[[\"item_id\",\"shop_id\",\"item_price\"]][X_train[\"date_block_num\"]==28].drop_duplicates(), how=\"left\",on=[\"shop_id\",\"item_id\"]).dropna()\n# half a year should be enough\n\n# For non-existing pairs lets use the average price of items\npart7 = pd.merge(test3,X_train[[\"item_id\",\"item_price\"]].drop_duplicates().groupby(\"item_id\").mean(), how=\"left\",on=[\"item_id\"]).dropna()\n\n# Now join together. Tricky. Only one value needed. Strict order. Part 1 better than part2, i.e. if \n# part 1 exists dont add part 2 and so forth\nprices=part1\nprint(\"After adding month -1:\")\nprint(prices.shape)\nids=prices.item_id\nprices=pd.concat([prices,part2[~part2.item_id.isin(ids)]])\nprint(\"After adding month -2:\")\nprint(prices.shape)\nids=prices.item_id\nprices=pd.concat([prices,part3[~part3.item_id.isin(ids)]])\nprint(\"After adding month -3:\")\nprint(prices.shape)\nids=prices.item_id\nprices=pd.concat([prices,part4[~part4.item_id.isin(ids)]])\nprint(\"After adding month -4:\")\nprint(prices.shape)\nids=prices.item_id\nprices=pd.concat([prices,part5[~part5.item_id.isin(ids)]])\nprint(\"After adding month -5:\")\nprint(prices.shape)\nids=prices.item_id\nprices=pd.concat([prices,part6[~part6.item_id.isin(ids)]])\nids=prices.item_id\nprint(\"After adding month -6:\")\nprint(prices.shape)\n\n# Until here it is clear. I build up a list of shop-item-pairs and the according prices that exist\n# Step 7 is different. This is supposed to fill \"the rest\" except 15k rows of unknown item_id\n\n# It is not about item_id, it is the shift from shop-item pairs to only according to item price-setting\n# I need shop-item-pair IDs\nprices[\"shop_item_pair_id\"]=prices.shop_id.astype(str)+\"-\"+prices.item_id.astype(str)\npart7[\"shop_item_pair_id\"]=part7.shop_id.astype(str)+\"-\"+part7.item_id.astype(str)\nprices=pd.concat([prices,part7[~part7.shop_item_pair_id.isin(prices.shop_item_pair_id)]])\nprices.drop(\"shop_item_pair_id\", axis=1,inplace=True)\nprint(\"After adding prices only according to item_id:\")\nprint(prices.shape)\n\n# Merge with original DF to include also the 15k lines with unknown items\nfinal_test=pd.merge(test3, prices[[\"shop_id\",\"item_id\",\"item_price\"]], how=\"left\",on=[\"shop_id\",\"item_id\"])\nfinal_test=final_test.groupby([\"ID\"],as_index=False).max()\n\n# Lets add the month and price_category\nfinal_test[\"date_block_num\"]=34\nfinal_test[\"month\"]=11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d022cbed4a2d0f4a43a4d34a40c063ce3dba047d"},"cell_type":"code","source":"# This check helped me to find duplicates\n\ncounts = final_test['ID'].value_counts()\nfinal_test[final_test['ID'].isin(counts.index[counts > 1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc5ee3b2c632b7c217c2452e0636e50189ed3fde"},"cell_type":"code","source":"# Now we have the issue with NaNs for shop-item pairs that did not exists in X_train\nprint(final_test[final_test[\"item_price\"].isnull()].head())\n# 15k lines of new items. Fill with global price median\nfinal_test.item_price.fillna(final_test.item_price.median(),inplace=True)\nfinal_test.item_category_id.fillna(final_test.item_category_id.median(),inplace=True)\nfinal_test.meta_category.fillna(final_test.meta_category.median(),inplace=True)\nprint(final_test[final_test[\"item_price\"].isnull()])\nprint(final_test.shape)\n\nfinal_test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfa2b24e812b63fe86a250074310f432c4353a4c"},"cell_type":"code","source":"print(X_train.columns)\nprint(final_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b53fbffc6b7b4ce27f521ca3c3118e65b3a71cc"},"cell_type":"code","source":"# Create price categories\nfinal_test[\"price_category\"]=np.nan\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>=0)&(final_test[\"item_price\"]<=100)]=0\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>100)&(final_test[\"item_price\"]<=200)]=1\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>200)&(final_test[\"item_price\"]<=400)]=2\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>400)&(final_test[\"item_price\"]<=750)]=3\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>750)&(final_test[\"item_price\"]<=1000)]=4\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>1000)&(final_test[\"item_price\"]<=2000)]=5\nfinal_test[\"price_category\"][(final_test[\"item_price\"]>2000)&(final_test[\"item_price\"]<=3000)]=6\nfinal_test[\"price_category\"][final_test[\"item_price\"]>3000]=7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f346050fed22afe8ba92dcb2623d6f1e11becd0"},"cell_type":"code","source":"# Right order\nfinal_test=final_test[['ID','date_block_num','shop_id','item_id','month','item_price','item_category_id','meta_category','town','region','price_category']]\n\n# Now predict\npredictions=linmodel.predict(final_test.drop(\"ID\",axis=1))\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32e3d7f12739775f3f25efc52721959e336e6246"},"cell_type":"code","source":"output=pd.DataFrame()\noutput[\"ID\"]=final_test[\"ID\"]\noutput[\"item_cnt_month\"]=predictions\noutput.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d58d9aaa4ee8407ab8977e1b4ae0b1222d2bcfe7"},"cell_type":"code","source":"output.to_csv('linmodel1.csv',index=False)\n# Score of 1.99739. Not surprisingly very bad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e48109f2f86511424c717820eab4673a6c085dc0"},"cell_type":"code","source":"# Checks to do:\n# - Why is amount always between 1 and 2,5? Very low, isnt it?\n# - Is is correct that there are so few fitting shop/item-id pairs from the past 6 month? only 30k?","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4c00fd94ba941c1dbdb80ef12a3b7f172364316"},"cell_type":"markdown","source":"A 2nd linear model: Include the past sales history per row to estimate todays sales"},{"metadata":{"trusted":true,"_uuid":"6c3d681bdc5b5f098b88d52b4e4229377248a945"},"cell_type":"code","source":"# Still continue with a more meaningful linear model:\n## Pivot by month to wide format\ntrain = df.pivot_table(index=['shop_id','item_id'], columns='date_block_num', values='item_cnt_month',aggfunc='sum').fillna(0.0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f4b2dd740bc796366a0c26379aeb16ddcd2c82f0"},"cell_type":"code","source":"train=train.reset_index()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b3ae9b2df6ae8c5a1ec987a2ea61bd81313f216","scrolled":true},"cell_type":"code","source":"final_train=train.merge(df[[\"shop_id\",\"item_id\",\"item_price\",\"item_category_id\",\"meta_category\",\"town\",\"region\",\"price_category\"]])\nfinal_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b76d7d9eb5744d899b1d1cb6dac12542d27ed08f"},"cell_type":"code","source":"final_train[final_train.shop_id.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01608289af77e3ca66097164f653ca2453c12f82"},"cell_type":"code","source":"print(final_train.shape)\nfinal_train.dropna(inplace=True)\nprint(final_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4aa6983590c388a40349f0843bdecd1b2d5361fe"},"cell_type":"code","source":"linmodel=LinearRegression()\nlinmodel.fit(final_train.drop(33,axis=1),final_train[33])\n\npredictions=linmodel.predict(final_train.drop(33,axis=1))\n\nprint(sqrt(mean_squared_error(final_train[33],predictions)))\n# vs. 2,35 in the naive linear model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea70d75d08fe91063d138de1e1e331b67ef1ca6a"},"cell_type":"code","source":"linmodel.score(final_train.drop(33,axis=1),final_train[33])\n# improved from 1,6% to 89%?! wow.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1b73345ee1e14610fecd21539f5a1ce6e3ea3d6"},"cell_type":"code","source":"# The columns that I need in my testset\nprint(final_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8b12a43e8faff42d30e3f1dfd79e89678dd39ed"},"cell_type":"code","source":"print(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c5b12f97abbc20f2caf7f384822ed9a2b06db80","scrolled":true},"cell_type":"code","source":"# Now prepare the test-set\n# After many many many,... many (really many!!!) tries to fill the panda dataframe I figured out my approach:\n# It took me hours of painful tries to pandas-merge the complete df...\n\n# Fill complete set with the roughest estimation (dont use shop_id and item_id at all)\n# Increasingly overwrite data with increasing level of knowledge (use item_id only, then overwrite with shop-item_id pairs)\n# In this way there will be no NaNs after the process as in step 1 everything is filled and only overwritten when a better estimate/data is available\n\n# Structure:\n# Step 1: Delete \"ID\"\n# Step 2: Fill geographic columns (town, region)\n# Step 3: Fill 0-33 with median values\n# Step 4: Overwrite 0-33 with values where shop, item-pairs are known\n# Step 5: Overwrite 0-33 with values where item is known\n# Step 6: Fill item_price\n# Step 7: Fill price_category\n# Step 8: Fill 'item_category_id' and 'meta_category' with median value\n# Step 9: Overwrite both, if known item\n\n# Step1:\ntest.drop(\"ID\",axis=1,inplace=True)\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79f6e4706fa371a64c489713ffa44c3f68197848"},"cell_type":"code","source":"# Step 2: Fill geographic columns (town, region)\ntest.merge(final_train[[\"shop_id\",\"town\",\"region\"]])\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9bc9d3563efa006a8d1526f02c7ded67dc52c91"},"cell_type":"code","source":"\n# Very easily filled are the geographic columns:\nunique_shops=final_train.shop_id.unique()\nprint(unique_shops)\n\n#test.merge(final_train[[\"shop_id\",\"town\",\"region\"]].unique())\n\n# Step 1: Fill with median values for every column\n\n\n#test2=pd.merge(test,final_train, how=\"left\", on=[\"shop_id\",\"item_id\"])\n#test2=test2.groupby(\"ID\",as_index=False).max()\n\nfor i in range(33):\n    test[i]=final_train[i+1].mean()\n\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c5c714a03f79f06c46ef0bd01541b6cc9136385"},"cell_type":"code","source":"test2[test2[0].isnull()].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"041b9c8ff2baf24382dcc3dde2033555ed944fc2"},"cell_type":"code","source":"print(final_train.groupby(\"item_id\",as_index=False).median().shape)\nprint(final_train.groupby(\"item_id\",as_index=False).median().head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da28460f67448190a3ae71d3ffd73d540d68e664","scrolled":true},"cell_type":"code","source":"final_train.groupby(\"item_id\",as_index=False).median().set_index('item_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb2b3980b3ddd866dfbe59ee0f1de89fb5d94f7f"},"cell_type":"code","source":"test2.set_index(\"item_id\",inplace=True)\ntest2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1832b38ced52ec2f9c933f4761bbd2636f45a404"},"cell_type":"code","source":"print(test2.set_index(\"item_id\").fillna(final_train.groupby(\"item_id\",as_index=False).median(),df.set_index('item_id')).head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb53fc16cea2d315632b0361fe3f7f349c572be4"},"cell_type":"code","source":"# Fill based only on item_id median in month\n#print(test2.shape)\n#print(test2[test2[0].isnull()])\ntest2.fillna()\n#test2[test2[0].isnull()]=pd.merge(test2[test2[0].isnull()],final_train.groupby(\"item_id\",as_index=False).median(),how=\"left\",on=[\"item_id\"])\n#print(pd.merge(test2[test2[0].isnull()],final_train.groupby(\"item_id\",as_index=False).median(),how=\"left\",on=[\"item_id\"]))\n#print(test2.shape)\ntest2\n\n# Shift by one month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dce98be0916257516639a1e16cc21880f4d646f"},"cell_type":"code","source":"# Already better but of course categories, towns need to be one_hot_encoded","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"From week 3: \nYou can get a rather good score after creating some lag-based features like in advice from previous week and \nfeeding them into gradient boosted trees model.\nApart from item/shop pair lags you can try adding lagged values of total shop or total item sales \n(which are essentially mean-encodings). All of that is going to add some new information."},{"metadata":{"trusted":true,"_uuid":"a24f586bc2edd27c75b8de6ead2f1f3a25674b03"},"cell_type":"code","source":"# Much more logical will be to add the past months as features\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f366365b4e56c2c18c404aeddc9b90036ef6dae3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"074f6fdb36861d0fdc2a1d4ef5464b9222b74505"},"cell_type":"markdown","source":"LSTM-Model\nAnd here a possible template for a LSTM model\nhttps://www.kaggle.com/shubhammank/lstm-for-beginners"},{"metadata":{"_uuid":"272265063a61a0fda5057251eebffc92be3810d2"},"cell_type":"markdown","source":"Tree-Model\n\nLets use this template for XGBoost modelling to learn what it is about:\nhttps://www.kaggle.com/alexeyb/coursera-winning-kaggle-competitions/notebook"},{"metadata":{"trusted":true,"_uuid":"2de790739c27e79f58efa39dd7666fffe3fd7787"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
