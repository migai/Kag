-> do the encoding with log or sqrt compression... set nbins = max_bins being used in LGBM
-> consider embedding to better encode the categories
-> consider clustering shops by what they sell... set up a graph with shops and items (be sure I can differentiate the two, either by adding a string or by increasing item_id by +60), with weights between shops and item nodes = sales in last 3 mo, 6mo, 12mo, all time(?)... then cluster to find the best groups and assign as cluster code for shops.

--> flatten out xmas sales? (clip, or interpolate)... does clipping to 20 help much?
  --> can group by item or shop_item or shop or category, do linear regression with month23, month11 first set equal to avg(month22/24 or month10/12)
--> set = 0 prediction for items that have 0 sales in last 6 months (?)
-> if shop-item sales only in last 1 or 2 months, then just average or trend them and hard-wire to predictions

--> cartesian product interpolate / infer sales by (1) interpolating from existing shop-item pairs in the same cluster -- probably best to average the item sales within the cluster, by shop (i.e., pull all elements from matching cluster that have same shop_id, and average the sales of all items in this cluster-shop group)... (or, average the sales of all items in the cluster, scaled first by total sales per shop in the last 6 months) , then (2) training the model, then going back to fill the sales in, then train again, re-fill, etc. until it seems to converge.
or, (3) linear fit to cluster sales by shop vs. month


ffill to compute item price in test rows

for trends in data, if you order columns by type, then by time lag, then you can use pd.DF.diff or .pct_change method with axis= columns
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html#pandas.DataFrame.diff
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pct_change.html#pandas.DataFrame.pct_change

rolling function in pandas seems interesting for computing time-shifted numbers; likely simpler for now to use 'shift' function, possibly with 'align' axes

sales_train is ordered by date_block_num, then by (roughly) shop_id, then (roughly) by item_id
--> do we need to shuffle this before feeding into model?  definitely if we do k-fold train/val split


https://www.tensorflow.org/tutorials/structured_data/feature_columns#import_tensorflow_and_other_libraries
--> use tensorflow embedding_column to encode categorical variables with many labels/dimensions


https://medium.com/dunder-data/from-pandas-to-scikit-learn-a-new-exciting-workflow-e88e2271ef62
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html
KBinsDiscretizer from scikit-learn
--> reduce the number of variants of continuous variables like price


look at monthly sales of items at small price vs. big price... if there is a difference, it might make sense to do two models, kind of like ensembling, but one for big price rows/items, and one for low price rows/items  (or item counts instead of price)


#All item ids that are being sold in the last month in the train data
sales_train[sales_train.date_block_num==33]["item_id"]
print(sales_train[sales_train.date_block_num==33]["item_id"].nunique())
# All item ids from all times
print(sales_train.item_id.nunique())

# Wow: more than 3/4 of items out of sale. But makes sense. Music titles, old programs, old consoles and PCs

--> if no sales the last 2-3 months, delete from train, and predict zero sales for Nov 2015
--> items with sales in 2015 sept but not oct: perhaps need lag(2month) instead of 1

Is it true that the train data does not show all data? That random rows (30% or so) are missing? Maybe it is an idea to construct these rows and fill these NaN values with something meaningul? A moving average or something? These values for train could be learned through a model to then feed a complete dataset into the final algorithm... (easily written - already scared thinking of implementing it :-) )
--> add shop_item empty rows for test pairs not in sales_train
--> train on month 0 to get value for shop_item at month 0 (or, maybe start at 20)
      then on month 1 to get values to put into shop_item at month 1 (perhaps with 1mo time lag data)
	  etc., until we fill up dummy values for at least the most relevant months (20-33? 32-33?)
	  Then,  train on all data
Or, take mean value of all others in same item category_code with the given test shop_id

Shops data: include continuous variables for city pop, fd popdens, fd gdp/person (some high avg. for online stores with fd = None); add another continuous variable: city pop / fd pop dens  (i.e., giving some idea of importance of the city to the surrounding area)... scale these with std scalar from 0 to 1 or -1 to 1...?
Try better encoding for categ. variables in shops and items... mean encoding? std scalar? to help with running linear regression model.



# Aggregate to monthly level the required metrics

monthly_sales = train.groupby(["month","shop_id","item_id"])[
    "date","item_price","item_cnt_day"].agg({"date":["min",'max'],"item_price":"mean","item_cnt_day":"sum"})
	
	

Add Holiday feature...  also need weekend??maybe
holiday_dict = {
    0: 6,
    1: 3,
    2: 2,
    3: 8,
    4: 3,
    5: 3,
    6: 2,
    7: 8,
    8: 4,
    9: 8,
    10: 5,
    11: 4,
}
new_train['holidays_in_month'] = new_train['month'].map(holiday_dict)


Russian Stock Exchange Trading Volume (in Trillions)
moex = {
    12: 659, 13: 640, 14: 1231,
    15: 881, 16: 764, 17: 663,
    18: 743, 19: 627, 20: 692,
    21: 736, 22: 680, 23: 1092,
    24: 657, 25: 863, 26: 720,
    27: 819, 28: 574, 29: 568,
    30: 633, 31: 658, 32: 611,
    33: 770, 34: 723,
}
new_train['moex_value'] = new_train.date_block_num.map(moex)


*********************************
*********************************
change type of correlation method in shops and shops-categories...
pearson (default) looks for linear correlation, and I need something more like cosine similarity
(also, pearson and other methods give values from -1 to +1, so I shouldn't filter out negative values perhaps)

*********************************
**** TO DO IF TIME PERMITS:  ****


revisit shops, item_categories, correlations:
********************************************

7.1) what is the story with shop 36 having so few training rows?

7.2) sort columns of correlation heatmap so similar shops are adjacent in the plot
-  try sorting by shop category, shop district, sum() of correlation column for a shop
-  make heatmap based on item category correlations rather than item correlations

7.3) shops dataset -- add columns with 
- min month present in train / max month present in train... lower weight to ones that have no sales close to Nov 2015
- top 3 item sales
- unit quantities for those 3 sales
- total number of units sold as per the train dataset
- total rows in the train dataset


1) check item IDs in shop 9, make sure they are covered elsewhere by other shops, and that they don't forecast sales of other items or at other shops... if shop 9 is irrelevant (and, since it isn't in the test set), drop shop 9 from training dataset

2) do the same for shop 13 (supermarket)

3) complete the investigation of "nasty" shop-item pairs (few or no training rows, but are present in test), and see if we can do something special about them

4) look at variance in item_price vs. time (vs. shop, shop_category,item_category,...) and see if anything is going to blow up on us, or if anything is predictive

*********************************
*********************************

***********************************
items dataset preprocessing: 
- min month present in train / max month present in train... lower weight to ones that have no sales close to Nov 2015
- add column for T/F if in test dataset
- add colum for total rows in train dataset
- add column for total units sold in train dataset
- sort or group by category and plot histograms of each(?)

6.2a) do a TF-IDF kind of thing --> less weight for ngrams that appear most often
and, less weight for ngrams that appear with high counts in en_50k

6.6) explore the unused items (not in test dataset) and see what feature groups they fall into, and if they are relevant for training

************************************

****************************************************************************************

Ok in week 2 of the course it was said that score should be 1,16777

"A good exercise is to reproduce previous_value_benchmark. As the name suggest - in this benchmark for the each shop/item pair our predictions are just monthly sales from the previous month, i.e. October 2015.

The most important step at reproducing this score is correctly aggregating daily data and constructing monthly sales data frame. You need to get lagged values, fill NaNs with zeros and clip the values into [0,20] range. If you do it correctly, you'll get precisely 1.16777 on the public leaderboard.

Generating features like this is a necessary basis for more complex models. Also, if you decide to fit some model, don't forget to clip the target into [0,20] range, it makes a big difference."

